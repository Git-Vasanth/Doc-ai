

--- Start of content from file: (AK Peters Visualization) Christian Tominski_ Heidrun Schumann - Interactive Visual Data Analysis-CRC Press (2020).pdf ---
Interactive Visual Data Analysis
AK Peters Visualization Series This series aims to capture new developments and summarize what is known over the whole spectrum of visualization by publishing a broad range of textbooks, reference works, and handbooks. It will contain books from all sub!ields of visualization, including visual analytics, information visualization, and scienti!ic visualization. The scope will largely follow the calls of the major conferences such as VIS: techniques, algorithms, theoretical foundations and models, quantitative and qualitative evaluation, design studies, and applications. Series Editor: Tamara Munzner University of British Columbia, Vancouver, Canada Visualization Analysis and Design Tamara Munzner Information Theory Tools for Visualization Min Chen, Miquel Feixas, Ivan Viola, Anton Bardera, Han-Wei Shen, Mateu Sbert Data-Driven Storytelling Nathalie Henry Riche, Christophe Hurter, Nicholas Diakopoulos, Sheelagh Carpendale Interactive Visual Data Analysis Christian Tominski, HeidrunSchumann For more informa!on about this series please visit: h"ps://www.crcpress.com/AK-Peters-Visualiza!on-Series/book-series/CRCVIS
Interactive Visual Data Analysis Christian Tominski Heidrun Schumann
CRCPress Taylor&FrancisGroup 6000BrokenSoundParkwayNW,Suite300 BocaRaton,FL33487-2742 ©2020byTaylor&FrancisGroup,LLC CRCPressisanimprintofTaylor&FrancisGroup,anInformabusiness NoclaimtooriginalU.S.Governmentworks Printedonacid-freepaper InternationalStandardBookNumber-13:978-1-4987-5398-2(Hardback) 978-0-3678-9875-5(Paperback) Thisbookcontainsinformationobtainedfromauthenticandhighlyregardedsources.Rea- sonable efforts have been made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the conse- quences of their use. The authors and publishers have attempted to trace the copyright holdersofallmaterialreproducedinthispublicationandapologizetocopyrightholdersif permissiontopublishinthisformhasnotbeenobtained.Ifanycopyrightmaterialhasnot beenacknowledgedpleasewriteandletusknowsowemayrectifyinanyfuturereprint. Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means,nowknownorhereafterinvented,includingphotocopying,microfilming,andrecord- ing,orinanyinformationstorageorretrievalsystem,withoutwrittenpermissionfromthe publishers. For permission to photocopy or use material electronically from this work, please access www.copyright.com(http://www.copyright.com/)orcontacttheCopyrightClearanceCen- ter, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not- for-profit organization that provides licenses and registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, a separate system ofpaymenthasbeenarranged. Trademark Notice:Productorcorporatenamesmaybetrademarksorregisteredtrade- marks,andareusedonlyforidentificationandexplanationwithoutintenttoinfringe. Visit the Taylor & Francis Web site at http://www.taylorandfrancis.com and the CRC Press Web site at http://www.crcpress.com
To the Rostock visualization group.
Contents Foreword xiii Preface xv Authors xvii Chapter 1(cid:4) Introduction 1 1.1 BASICCONSIDERATIONS 2 1.1.1 Visualization, Interaction, and Computation 2 1.1.2 Five Ws of Interactive Visual Data Analysis 4 1.2 INTRODUCTORYEXAMPLES 5 1.2.1 Starting Simple 5 1.2.2 Enhancing the Data Analysis 8 1.2.3 Considering Advanced Techniques 10 1.3 BOOKOUTLINE 13 Chapter 2(cid:4) Criteria, Factors, and Models 15 2.1 CRITERIA 16 2.2 INFLUENCINGFACTORS 19 2.2.1 The Subject: Data 19 2.2.2 The Objective: Analysis Tasks 28 2.2.3 The Context: Users and Technologies 35 2.2.4 Demonstrating Example 38 vii
viii (cid:4) Contents 2.3 PROCESSMODELS 41 2.3.1 Design 41 2.3.2 Data Transformation 44 2.3.3 Knowledge Generation 47 2.4 SUMMARY 48 Chapter 3(cid:4) Visualization Methods and Techniques 51 3.1 VISUALENCODINGANDPRESENTATION 54 3.1.1 Encoding Data Values 54 3.1.2 Presentation 62 3.2 MULTIVARIATEDATAVISUALIZATION 67 3.2.1 Table-based Visualization 67 3.2.2 Combined Bivariate Visualization 69 3.2.3 Polyline-based Visualization 71 3.2.4 Glyph-based Visualization 73 3.2.5 Pixel-based Visualization 75 3.2.6 Nested Visualization 77 3.3 VISUALIZATIONOFTEMPORALDATA 82 3.3.1 Time and Temporal Data 82 3.3.2 Visualization Techniques 86 3.4 VISUALIZATIONOFGEO-SPATIALDATA 95 3.4.1 Geographic Space and Geo-spatial Data 96 3.4.2 General Visualization Strategies 99 3.4.3 Visualizing Spatio-temporal Data 106 3.5 GRAPHVISUALIZATION 111 3.5.1 Graph Data 111 3.5.2 Basic Visual Representations 113 3.5.3 Visualizing Multi-faceted Graphs 118 3.6 SUMMARY 124 Chapter 4(cid:4) Interacting with Visualizations 129 4.1 HUMANINTHELOOP 131 4.1.1 Interaction Intents and Action Patterns 132 4.1.2 The Action Cycle 135
Contents (cid:4) ix 4.2 REQUIREMENTSFOREFFICIENTINTERACTION 136 4.2.1 Interaction Costs 136 4.2.2 Directness of Interaction 138 4.2.3 Design Guidelines 143 4.3 BASICOPERATIONSFORINTERACTION 144 4.3.1 Taking Action 144 4.3.2 Generating Feedback 146 4.4 INTERACTIVESELECTIONANDACCENTUATION 148 4.4.1 Specifying Selections 149 4.4.2 Visual Emphasis and Attenuation 153 4.4.3 Enhanced Selection Support 156 4.5 NAVIGATINGZOOMABLEVISUALIZATIONS 159 4.5.1 Basics and Conceptual Considerations 160 4.5.2 Visual Interface and Interaction 162 4.5.3 Interaction Aids and Visual Cues 164 4.5.4 Beyond Zooming in Two Dimensions 168 4.6 INTERACTIVELENSES 173 4.6.1 Conceptual Model 173 4.6.2 Adjustable Properties 176 4.6.3 Lenses in Action 178 4.7 INTERACTIVEVISUALCOMPARISON 184 4.7.1 Basics and Requirements 184 4.7.2 Naturally Inspired Comparison 186 4.7.3 Reducing Comparison Costs 190 4.8 INTERACTIONBEYONDMOUSEANDKEYBOARD 194 4.8.1 Touching Visualizations 194 4.8.2 Interacting with Tangibles 197 4.8.3 Moving the Body to Explore Visualizations 202 4.9 SUMMARY 204 Chapter 5(cid:4) Automatic Analysis Support 207 5.1 DECLUTTERINGVISUALREPRESENTATIONS 209 5.1.1 Computing and Visualizing Density 209 5.1.2 Bundling Geometrical Primitives 212 5.2 FOCUSINGONRELEVANTDATA 214
x (cid:4) Contents 5.2.1 Degree of Interest 214 5.2.2 Feature-based Visual Analysis 220 5.2.3 Analyzing Features of Chaotic Movement 224 5.3 ABSTRACTINGDATA 231 5.3.1 Sampling and Aggregation 231 5.3.2 Exploring Multi-scale Data Abstractions 233 5.4 GROUPINGSIMILARDATAELEMENTS 238 5.4.1 Classification 239 5.4.2 Data Clustering 243 5.4.3 Clustering Multivariate Dynamic Graphs 250 5.5 REDUCINGDIMENSIONALITY 257 5.5.1 Principal Component Analysis 258 5.5.2 Visual Data Analysis with Principal Components 260 5.6 SUMMARY 263 Chapter 6(cid:4) Advanced Concepts 267 6.1 VISUALIZATIONINMULTI-DISPLAYENVIRONMENTS 268 6.1.1 Environment and Requirements 269 6.1.2 Supporting Collaborative Visual Data Analysis 270 6.1.3 Multi-display Analysis of Climate Change Impact 276 6.2 GUIDINGTHEUSER 277 6.2.1 Characterization of Guidance 278 6.2.2 Guiding the Navigation in Hierarchical Graphs 283 6.2.3 GuidingtheVisualAnalysisofHeterogeneousData286 6.3 PROGRESSIVEVISUALDATAANALYSIS 288 6.3.1 Conceptual Considerations 290 6.3.2 Multi-threading Architecture 294 6.3.3 Scenarios 297 6.4 SUMMARY 303 Chapter 7(cid:4) Summary 305 7.1 WHAT’SBEENDISCUSSED 305 7.2 HOWTOCONTINUE 307
Contents (cid:4) xi Bibliography 311 Index 339 Figure Credits 343
Foreword Big Data has attracted much attention. Many examples are given in the news how new data-driven applications and algorithms provide new insights, enable more effective and efficient operation, and support experts in a wide variety of application domains. This suggests that getting value out of data is easy, but practitioners in the field know better. Obtaining novel, actionable insights from large and complex data is hard work. In many cases, the human in the loop is indispensable to find unexpected relations, to judge if there is true value in the data and to check if results are valid, as a human is needed to bring in both deep domain expertise and common sense. How to bring the human in the loop? One route is to use visualization: translate data into images and take advantage of the incredible strength of the human visual system, enabling the viewer to detect patterns quickly and leading to deep insights. However, visualization has its limits and for instance larger datasets quickly lead to cluttered images, like the hairballs that result when networks with more than a hundred nodes are visualized. An important next level is to bring in interaction: enable people to select the data they are interested in. When carefully designed, such interactive visualization systems enable exploration of much larger datasets. But, how to find relevant relations in collections of millionsofitems,withhundredsofattributes?Statistics,machinelearning,and data mining provide many analysis methods geared towards finding patterns and correlations automatically, but such analysis methods inevitably also lead toreductionandpossiblythelossofvitalinformation.ThecredoofInteractive Visual Data Analysis, also known as visual analytics, is therefore to use a combination of visualization, interaction, and automated analysis methods to explore huge amounts of data and to obtain solid insights. Problem solved? Unfortunately not. The design of effective systems for visualdataanalysisisfarfromtrivial,andrequiresawidevarietyofknow-how, skills, and experience. Besides pure technical skills, many other aspects are rel- evant.Foreachelementofsuchasystemawidevarietyofalternativesolutions are available, selecting the most suitable requires a thorough understanding of their strengths and limitations. When standard solutions do not suffice, creativity is needed to find new methods or ingenious combinations of existing ones. And, above all, development of such systems is a design process, having a deep understanding of this process and dedication to the needs and wants of the prospective users is essential. xiii
xiv (cid:4) Foreword I teach students to develop interactive systems for visual data analysis. And, I struggle with this. Should I guide them through the complete zoo of all different visualization, interaction, and analysis techniques, for all kinds of different data? That would lead to a very lengthy and not really exciting course. What I really missed is a compact textbook that provides an overview of interactive visual data analysis: a book that guides through the design process, discusses the supporting disciplines and how they fit together, and gives examples of more advanced topics. When Christian Tominski and Heidrun Schumann wrote me that they had prepared such a book, I was very excited, and even more when I read it. This book really fills a gap. There are of course many texts on visualization, inter- action, and data analysis, all with their own angle, but an integrated overview is rare, whereas I think that the real magic comes from the combination. The authors have an extensive experience in doing research in visualization and developing real-world solutions for real-world problems, and that shows. Many examplescomefromtheirownworkandgiveusanopportunitytolookoverthe shoulders of experts. They have a broad experience with many different topics andareas,andhaveattackedmanycomplexcases,suchasdynamicgeo-spatial data. I therefore expect that many readers can take direct advantage of the ideas, solutions, and examples given. Besidestheirexperienceinresearchanddevelopment,theauthorsalsohave a solid track record in communicating their insight in the field: their book “Visualization of Time-Oriented Data”, together with Wolfgang Aigner and Silvia Miksch, is a classic. In the current book, they again show their mastery in handling a complex topic, and I am very impressed by their thoughtful treatment and analysis, their clear and compact writing style, and all clear and useful illustrations. In short, I highly recommend this book. It provides clear guidance, a thorough overview, and many ideas to students, teachers and everybody who aims to develop effective systems for interactive visual data analysis. I hope this book will inspire many to develop effective solutions for interactive visual data analysis that enable experts to extract great value and deep insights from huge, complex datasets. Jarke J. van Wijk Department of Mathematics and Computer Science Eindhoven University of Technology, The Netherlands October 2019
Preface In the year 2000, Heidrun Schumann co-authored the first German textbook on visualization. Christian Tominski was among the first students to use that textbook during their studies. Now, about twenty years later, we, Heidrun and Christian, are happy to present this new book on interactive visual data analysis. AboutthisBook TheideaforthisbookwasbornatIEEEVIS2014inParis during a discussion of Heidrun and Tamara Munzner, who had just published her own book Visualization Analysis and Design. The goal was to write a book that provides a comprehensive overview of the principles of interactive visual data analysis. It soon became clear to us that this was not an easy endeavor. Before us were five years of systematization, categorization, harmonization, and over and over again in-depth discussions on how the content could be best prepared, organized, and presented to our readers. The result is a book with five core chapters. What differentiates our book most from others is its visual analytics focus, that is, the synthesis of visuals, interactivity, and analytics. The book introduces criteria for designing interactive visual data analysis solutions, discusses factors influencing the design, and examines the involved processes. The reader is made familiar with the basics of visual encoding and gets to know numerous visualization techniques for multivariate data, temporal data, geo-spatial data, and graph data. A dedicated chapter introduces general concepts for interacting with visualizations and illustrates how modern interaction technology can facilitate the visual data analysis in many ways. Addressing today’s large and complex data, the book covers relevant automatic analytical computations to support the visual data analysis. The book also sheds light on advanced concepts for visualization in multi-display environments, user guidance during the data analysis, and progressive visual data analysis. In her review of this book, Tamara Munzner commented: “I was consistently impressed by how broad the set of things you’ve done is.” The entire book is richly illustrated with many examples and use cases. Here, we have chosen to rely primarily on our own visual analytics research. This allowed us to create many new schematic depictions and expressive visualizations to illustrate the discussed topics. A great advantage of creating new illustrations is that we could make them available free of charge under Creative Commons Attribution 4.0 International License (CC BY 4.0). xv
xvi (cid:4) Preface The multitude of topics described in the book can be of interest to a wide range of readers. Students can use the book to learn about interactive visual dataanalysis.Theywillbenefitmostfromthestructuredtop-downviewoffered by the book. Visualization experts can use the book as a reference and for teaching. The advanced concepts discussed at the end of the book may serve as inspiration for new research. Last but not least, domain experts will find the many examples and use cases interesting. Given the broad scope of the book, practitioners from many domains may gain from it. Acknowledgments This book might have been written by two people, but it representstheworkofmanypeople.Becausethisbookwouldnotexistwithout our own previous research, special thanks go to all of our former and current colleagues from the Rostock visualization group. Moreover, we would like to say many thanks to our friends and partners with whom we collaborated and co-authored papers that had a substantial impact on this book. We are also very grateful for support in creating images and transferring rightstouseimages.Manythanksgoto(inalphabeticalorder)MarcoAngelini, NicolasBelmonte,ThomasButkiewicz,SteveDübel,ChristianEichner,Steffen Hadlak, Helwig Hauser, Alexander Lex, Martin Luboschik, Thomas Nocke, Axel Radloff-Delosea, Martin Röhlig, and Sylvia Saalfeld. Feedback from reviewers greatly helped to improve the book. Many thanks go to Wolfgang Aigner for his review of the chapter on interaction. Martin Luboschikprovidedvaluablecommentsonthevisualizationchapter.Hans-Jörg Schulznotonlygaveususefulfeedbackonthechaptersonfundamentalaspects and automatic analysis support as well as on the summary and the preface, he also took part in many discussions and contributed in the early phase of the book project. Tamara Munzner deserves special thanks for her in-depth review of the entire almost finished book. Her comments helped us a lot in finalizing and polishing the book. Sunil Nair, the editorial director at Taylor & Francis, and his editorial assistants Kirsten Barr and Shikha Garg supported us on the publisher’s side. Finally, we would like to thank our families for providing the warm and heartfelt atmosphere that is necessary to successfully complete a book project. Thank you for your love and support! Christian Tominski Heidrun Schumann Institute for Visual & Analytic Computing University of Rostock, Germany October 2019
Authors Christian Tominski is a researcher and lecturer at the Institute for Visual & Analytic Computing at the University of Rostock, Germany. He received doctoral (Dr.-Ing.) and post-doctoral (Dr.-Ing. habil.) degrees in 2006 and 2015, respectively. His main research interests are in visualization of and interaction with data. He is particularly interested in effective and efficient techniques for interactively exploring and editing complex data. Christian has published numerous papers on new visualization and interaction techniques for multivariatedata,temporaldata,geo-spatialdata,andgraphs.Heco-authored twobooksonthevisualizationoftime-orienteddatain2011andoninteraction for visualization in 2015. Christian has developed several visualization systems and tools, including the LandVis system for spatio-temporal health data, the VisAxes tool for time-oriented data, and the CGV system for coordinated graph visualization. Heidrun Schumann is a professor at the University of Rostock, Germany, where she is heading the Chair of Computer Graphics at the Institute for Visual & Analytic Computing. She received doctoral degree (Dr.-Ing.) in 1981 and post-doctoral degree (Dr.-Ing. habil.) in 1989. Her research and teaching activities cover a variety of topics related to computer graphics, including information visualization, visual analytics, and rendering. She is interested in visualizingcomplexdatainspaceandtime,combiningvisualizationandterrain rendering, and facilitating visual data analysis with progressive methods. A key focus of Heidrun’s work is to intertwine visual, analytic, and interactive methods for making sense of data. Heidrun published more than 200 articles in top venues and journals. She co-authored the first German textbook on data visualization in 2000 and a textbook specifically on the visualization of time-oriented data in 2011. In 2014, Heidrun was elected as a Fellow of the Eurographics Association. xvii
1 CHAPTER Introduction CONTENTS 1.1 Basic Considerations ........................................ 2 1.1.1 Visualization, Interaction, and Computation ....... 2 1.1.2 Five Ws of Interactive Visual Data Analysis ....... 4 1.2 Introductory Examples ..................................... 5 1.2.1 Starting Simple ..................................... 5 1.2.2 Enhancing the Data Analysis ....................... 8 1.2.3 Considering Advanced Techniques .................. 10 1.3 Book Outline ................................................ 13 DATA have become a most valuable good. Doctors rely on rich databases about diagnoses and medications to give patients the best possible treat- ments. Enterprises generate profits based on data about needs and preferences of potential customers. Scientists make new discoveries and contribute to a vast body of scholarly data. Data are everywhere in the information age. Data are collected by huge numbers of devices equipped with various sensors. A smartphone with a dozen and more different sensors is not uncommon. Data are also generated computationally. Sophisticated models are constructed and simulated in an attempt to estimate how our climate may look like in a few centuries. And we as humans are sources of data as well. Social networks and messaging services record our interests and daily activities. Now with so much data available, the question is how can we make sense of them? Well, the data have to be explored and analyzed in order to derive valuable information. To this end, a channel has to be established for the data to enter into the human mind where insight can be generated. The classic way of ingesting data is to decipher alphanumerically encoded transcripts, or simplytexts.Yet,readingpilesofdocumentsistootimeconsuming.Therefore, data are often aggregated in reports, which may contain structured tabular information. This already helps in extracting the key messages. But complex relationships within the data may still be too difficult to identify. This is where interactive visual data analysis enters the stage. While text and reports are serial media, where one piece of data has to be processed 1
2 (cid:4) Interactive Visual Data Analysis after the other, visual methods aim for the human visual system at its full bandwidth. Humans are amazingly fast in extracting information from graph- ical depictions. Graphical means are not only beneficial for communicating information,theyalsoserveasscaffoldsforhumansensemaking.Mentalmodels can be established more easily with the help of visual abstractions and visually acquired information can be remembered better than textual descriptions. We all know the idiom: “A picture is worth a thousand words.” But this is not quite right. More correct would be “A picture can be worth hundreds of thousands of words.” This slightly provocative statement hints at two important aspects. First, can be suggests that there are not only good visual representations of data but also exemplars that are not so helpful. Second, the increase in the number of words is to indicate that, in the information age, we are facing big data. Thisbookisaboutconceptsandmethodsfortheinteractivevisualanalysis of large and complex data by jointly exploiting the power of humans and computers. In this book, you will learn that solutions for interactive visual data analysis are not created in passing. Careful design is necessary before expressive visual representation can be shown on a computer display. Useful interaction is essential to enable users to engage in a dialog with the data and the information contained therein. Especially in the light of big data, we need support from analytic computations to help us extract interesting features from the data. 1.1 BASIC CONSIDERATIONS Before we go into any details about interactive visual data analysis, let us briefly look at some fundamental terms, ideas, and concepts. 1.1.1 Visualization,Interaction,andComputation Visualization is a computational process that generates visual representations of data. A first definition has been established by visualization pioneers in 1987. Their definition reads as follows [MDB87]: “Visualizationisamethodofcomputing.Ittransformsthesymbolicintothe geometric,enablingresearcherstoobservetheirsimulationsandcomputa- tions.Visualizationoffersamethodtoseetheunseen.Itenrichestheprocess ofscientificdiscoveryandfostersprofoundandunexpectedinsights.” McCormicketal.,1987 This definition describes a transformation that involves several entities and steps. First, there are the data into which we seek insight. Second, there is the
Introduction (cid:4) 3 computer that transforms the data into visual representations. Finally, there is the human who is making sense of the visual representation. Visual representations are fundamental for visually driven data-intensive work, but they alone can hardly satisfy all the analytic needs we are facing in the information age. We need support from interaction mechanisms and computational analysis methods. Already in 1981, Bertin recognized the need for interactively adjustable visual representations [Ber81]: “Agraphicisnot‘drawn’onceandforall;itis‘constructed’andreconstructed untilitrevealsalltherelationshipsconstitutedbytheinterplayofthedata.The bestgraphicoperationsarethosecarriedoutbythedecision-makerhimself.” Bertin,1981 Interaction adds the necessary flexibility to visualization. It allows us to activelytakepartinthevisualdataanalysis.Wemaywanttofocusondifferent features of the data, look at the data from different perspectives, or adjust visual representations so as to crystallize the desired insights. While interaction incorporates human competences into the sense-making process, automatic computational methods utilize the power of the machine. Large and complex data usually cannot be visualized in their entirety. Auto- matic computational analyses crunch the data in search for characteristic features or meaningful abstractions that are easier to digest than the raw data. The important interplay of visualization, interaction and computational analysis is summarized in the Visual Analytics Mantra by Keim and col- leagues [Kei+06]: “AnalyseFirst– ShowtheImportant– Zoom,FilterandAnalyseFurther– DetailsonDemand” Keimetal.,2006 According to this mantra, visual analysis starts with an automatic analytic phase. The important features extracted in this phase are then visualized. Via interaction, the visual representation is adjusted, the data are filtered, and further analytic computations are triggered. Details are readily available upon request.
4 (cid:4) Interactive Visual Data Analysis This tight interleaving of computational and human efforts is the key benefit of interactive visual data analysis as a knowledge-generation approach. The computer can process large amounts of data quickly and accurately. The human has enormous pattern-detection abilities and is proficient in creative thinking and flexible decision-making. A direct consequence of the interplay of data, humans, and computers is that knowledge from different fields has to be brought together for a successful dataanalysis.Relevanttopicsincludevisualdesign,computergraphics,human- computer interaction, user interfaces, psychology, data science, and algorithms, to name only a few. The need to get diverse methods work in concert makes the development of practical solutions a non-trivial endeavor. 1.1.2 FiveWsofInteractiveVisualDataAnalysis In order to come up with helpful data analysis tools, their context of use needs to be taken into account in the first place. One way to describe the context is to follow a variation of the Five Ws: What, why, who, where, and when. What data are to be analyzed? There are many different types of data, such as player statistics, census data, movement trajectories, and biological networks. Each type of data comes with its own individual characteristics, including data scale, dimensionality, and heterogeneity. Why are the data analyzed? The objective is to help people accomplish theirgoals,forexample,findinggoverningfactorsinageneregulatorynetwork. Goals typically involve a number of analytic tasks, such as identifying data values or setting patterns in relation. Whowillanalyzethedata? Adoctorwhostudiesdatainday-to-dayclinical routine needs different analysis tools than a strategic investor who is exploring streamsofnewsdatainsearchfornewmarketopportunities.Individualabilities and preferences play a role as well. Where will the data be analyzed? The regular workplace is certainly the classic desktop setup with a display, mouse, and keyboard. Yet, there are also large display walls and interactive surfaces that offer new opportunities for interactive visual data analysis. Whenwillthedatabeanalyzed? Aswithanytool,visualization,interaction, and computation are means that must be at hand at the right time. A data analysis may follow domain-specific workflows where each step is associated with its own individual requirements.
Introduction (cid:4) 5 These Five Ws suggest that there are many factors influencing the develop- ment of data analysis tools, including data types, analytic tasks, user groups, display environments, domain conventions, and so forth. Factors related to the What and the Why are crucial for the practical applicability of data analysis tools. Certainly, any visually driven and interactively controlled tool has to consider human factors, the Who, with regard to perceptual, cognitive, and physical abilities, expertise, background, and preferences. The Where and When aspects become increasingly relevant when the data analysis runs on multiple heterogeneous displays, supports collaborative sessions, or follows domain-specific workflows. InthelightoftheFiveWsitisclearthataninteractivevisualdataanalysis solution, in order to be successful, has to be tailored for a specific purpose and setting.Giventhewealthofanalyticquestionswearefacingintheinformation age, a large variety of concepts and techniques is needed. Next, we look at a few introductory examples. 1.2 INTRODUCTORY EXAMPLES So far, we have sketched the basic idea of interactive visual data analysis on a rather abstract level. In the following, a series of examples will demonstrate the communicative power of visual analysis approaches, on the one hand, and the involved design decisions and challenges, on the other hand. The examples will take us from basic visual representations to advanced analysis scenarios. On the way, we will increase the degree of sophistication of the examples by enhancing the visual mapping, integrating interaction mecha- nisms and automatic computations, combining multiple views, incorporating user guidance, and considering multi-display environments. 1.2.1 StartingSimple The data we will analyze are graphs (the What). Graphs are a general model for describing entities and relations among them. They are universally useful in many different domains. Biologists model natural phenomena via gene regulation networks, climate researchers make use of climate networks to simulate weather on earth, and crime investigators sketch connections between suspects to solve complicated cases. Typical examples from our daily lives are computer networks and social networks. A graph generally consists of nodes, edges, and attributes. Nodes represent entities, whereas edges represent relationships between the entities. Nodes as well as edges can have attributes that store additional information. Before we start with visual examples, let us first take a look at the raw data to be visualized. Listing 1.1 shows our graph stored in the JSON format. Lines 2–11 contain three nodes, each associated with an id and a label. Lines 14–22 define two edges. Edges are specified between a source node (src) and a destination node (dst), both referenced by their id. An additional attribute stores the weight (or strength) of the connection between the two nodes.
6 (cid:4) Interactive Visual Data Analysis Listing1.1 A graph with nodes, edges, and attributes 1 { 2 "nodes": [ 3 { "id": 0, 4 "label": "Myriel" 5 }, 6 { "id": 1, 7 "label": "Napoleon" 8 }, 9 { "id": 2, 10 "label": "Mlle Baptistine" 11 }, 12 // More nodes here ... 13 ], 14 "edges": [ 15 { "src": 1, 16 "dst": 0, 17 "weight": 1 18 }, 19 { "src": 2, 20 "dst": 0, 21 "weight": 8 22 }, 23 // More edges here ... 24 ] 25 } Our listing with three nodes and two edges is only an abbreviated view as indicated by the comments in lines 12 and 23. The graph that we are about to visualize actually contains 77 nodes and 254 edges. It captures the co-occurrence of characters in the chapters of Victor Hugo’s Les Misérables. With only the listing of the graph, it is extremely difficult to make sense of the information hidden in the data. Therefore, we (the Who) will now perform a visual analysis to gain insight into the graph. In the first place, we are interested in the structure of the graph (the Why). A basic method for visualizing graphs is a node-link diagram. Figure 1.1a shows such a simple visual representation of the graph. Nodes are visualized as dots, and edges are represented as links between the dots. There are many different options for laying out the dots on the display. In our case, a force- directed layout algorithm has been applied. As we can see, the structure of the graph, that is, who is connected to whom, becomes quite clear, merely by drawing dots and links. Let us refine our interest in the data. We are now interested in who are the major characters with the most connections to other characters. We can already extract this information by looking at the number of edges that are connectedtoanode.Yet,itisabitcumbersometocountedges,andtheclutter of edges makes counting difficult anyway. So how can we make the desired information more readily visible?
Introduction (cid:4) 7 (a)Plain structure. (b)Encoding degree via color. (c)Encoding degree via color and size. (d)Encoding weight via line width. Figure1.1 Node-link diagram visualizing a graph’s structure and attributes. The number of edges per node is also called the node degree. This derived numeric attribute can be visualized alongside the graph structure. To this end, we assign to each dot a color that represents the node degree. Dark green nodes exhibit a large node degree, while light green nodes have a low degree. Figure1.1billustrates thecolor coding.With thisenhancednode-link diagram, we can immediately identify the dark green dot in the center of the figure as the major character in the network. But we are still not fully satisfied. Low-degree nodes are of the same size as theimportanthigh-degreenodes.Wewanttofurtheraccentuatetheimportant charactersinthenetworkandattenuatethelessrelevantsupportingcharacters. Complementing the color coding, we vary the size of the dots depending on the node degree. As can be seen in Figure 1.1c, it is now much easier to assess the importance of characters. Our visual representation is now quite expressive in terms of information about the nodes of the graph. However, the edges have received only little
8 (cid:4) Interactive Visual Data Analysis attention. To render a more complete picture of the data, it would be nice to visualize the edge weight as well. This can be achieved by varying the width of the lines connecting the colored dots. In Figure 1.1d, bold lines indicate strong edges, whereas thin lines stand for weak edges. With this additional visual encoding, we can easily see to whom a character is most prominently connected. Insummary,wehavenowvisualizedthegraphstructureandtwoassociated graph attributes. The structure is nicely visible as dots and links. The two attributes node degree and edge weight are encoded visually via color plus size and line width, respectively. The resulting visual representation enables us to see the key characteristics of the data. By reading the data’s textual representation we could have obtained the same characteristics, but it would have cost us much more time and painstaking brainwork. 1.2.2 EnhancingtheDataAnalysis The previous simple examples illustrated the potential of visualization. Yet, simplevisualrepresentationsaloneareoftennotenoughtosolvemorecomplex problems. The graph that we visualized consisted of 77 nodes and 254 edges only. However, it is not uncommon to work with graphs with thousands of nodesandedges,anddozensofattributes.Climatenetworksareanexampleof suchlargeandcomplexgraphs.Theyaregeneratedfromlarge-scalesimulations of meteorological phenomena with the goal to better understand and predict the development of climatic conditions on earth. With increasing size and complexity of the data, we have to go beyond the simple visual representations introduced earlier. When visualizing large data, we risk ending up with visual representations that are cluttered. Adequate countermeasures have to be taken. Moreover, it is hardly possible to encode all aspects of the data into a single image. It is rather necessary to provide multipleviewsonthedata,whereeachviewemphasizesaparticulardatafacet. The next two examples illustrate these lines of thought. Consider the climate network visualized in Figure 1.2a. It contains 6,816 nodes and 116,470 edges. Its visual representation is actually a mess; there are simply too many dots and links. What can we do about this? A standard approach in such situations is to focus on relevant subsets of the data. Subsets can be created dynamically using interactive filtering mechanisms that enable users to specify the parts of the data they are interested in. For the climate network we may be interested in those nodes that are crucial for the transfer or flow in the network. Such nodes are characterized by a high centrality, a graph-theoretic measure. An automatic algorithm can be used to calculate the centrality for each node of the network. Then it is up to the user to determine interactively a suitable threshold for filtering out low-centrality nodes and their incident edges. Figure 1.2b shows the climate network where nodes with a centrality below 65,000 have been filtered out. As a result the visual representation contains
Introduction (cid:4) 9 (a)Full graph with 6,816 nodes and 116,470 edges. (b)Filtered graph with 938 nodes and 5,324 edges. Figure1.2 Dynamic filtering to focus on relevant parts of a climate network. only those graph elements that are relevant with respect to the interest of the user. Now, there are no more than 938 nodes and 5,324 edges. This obviously reduces clutter and generates a better view on the structures hidden in the data. With dynamic filtering as described so far, it is possible to deal with problems caused by data size. Another challenge is data complexity. It relates to the many semantic aspects that may be linked to the data. We already mentioned the graph structure and the graph attributes as important aspects. Additionally, there can be spatial and temporal aspects. A climate network is usually given in a spatial frame of reference and it may also be subject to
10 (cid:4) Interactive Visual Data Analysis Figure1.3 Multiple-views visualization of a climate network. change over time. In order to understand data comprehensively it is necessary tounderstandtheindividualaspectsandtheirinterplay.Thisrequiresmultiple dedicatedvisualrepresentations,eachaddressingtheparticularitiesofaspecific aspect. Figure 1.3 depicts a system where multiple views work in concert to visualize our climate network. Without going into too much detail, there are a density plot (top left), a node-link view combined with a map (center), a globe view (right), a multivariate attribute plot (below node-link and globe), a filter slider (bottom), and a few auxiliary controls. All these views are linked. That is, picking a data item in one view will highlight that item in all other views. This linking among views is essential for integrating the different data facets and enabling the user to form a comprehensive understanding of the data. 1.2.3 ConsideringAdvancedTechniques In the previous paragraphs, we computed graph-theoretic measures, added interactive filtering, and combined multiple linked views to create a compre- hensive overview of the data. But how far can we get with interactive visual data analysis? Certainly, there are limits. The visualization has to fit into the available display space. Interaction should not overwhelm users with too many things to carry out manually. Analytic computations have to generate results in a timely fashion. As we try to push these limits, we have to consider advanced techniques. For the purpose of illustration, we briefly look at two examples. One aims to guide users during the data analysis and the other to expand the screen space for visualization. It has already been mentioned that interaction is crucial to creative sense- making. However, interaction can also be demanding. The user has to deal
Introduction (cid:4) 11 (a)Where should I go next? (b)Visual cues hint at candidates! Figure1.4 Guidance provides assistance during data navigation. with several questions: What can I do to get closer to my goal, which action sequence do I have to take, how are the individual interactions carried out? An advanced visual analysis system is capable of providing guidance to assist the user in answering such questions. We illustrate such guidance by the example of another common question: Which part of the data should be visited? A typical approach to study data more closely is to zoom in as shown in Figure 1.4a. Note that we visualize a different graph now, a graph about co-occurrence of search engine query keywords with 2,619 nodes and 29,517 edges. While zoomed in, it is possible to see details, but only for a fraction of the graph. So, the data analysis is an iterative process during which one part of the graph is visited after the other. This iterative process requires users to answer the question where to go next. Should the user be hesitant to continue the navigation of the data, this may indicate that guidance should be provided (the When). If this is the case, a computational method scans the immediate neighbor- hoodofthecurrentlyvisiblepartofthegraphinsearchfornodesoredgesthat are potentially interesting according to a user-specified degree-of-interest (DoI) function.Thevisualrepresentationisthenenhancedwithvisualcuesthatpoint at the most promising candidates. Figure 1.4b shows a few nodes emphasized withredcircles.Thesenodesareworthinvestigatingindetail.Moreover,arrows at the view border suggest directions in which further interesting nodes can be found. The user is free to follow the given recommendations or to continue the exploration otherwise. Of course, guidance is a delicate means of user support. Ifguidanceisobtrusive,usersmaynotacceptit.Ifitiswell-balanced,however, guidance can be a valuable tool. Foroursecondexampleofadvancedvisualdataanalysis,weareaddressing the screen space limit. When pushing this limit, a natural step forward is to
12 (cid:4) Interactive Visual Data Analysis Figure1.5 Visual data analysis in a multi-display environment. Reprinted from [RLS11]. go for bigger displays. Instead of working in the classic desktop environment, several displays are combined to form so-called multi-display environments (the Where). As shown in Figure 1.5, there are stationary public displays and dynamicprivatedisplays,whichcanenterandleavetheenvironmentasneeded. Each display may contain several views and the views can be focused and re-arranged to suit the analytic situation at hand. On the one hand, multi-display environments provide more space for dis- playing visual representations at high resolution. There can be even multiple users working collaboratively to analyze the data. On the other hand, new challenges need to be tackled. How to distribute views on the displays, how to deal with users occluding the displays, and how to interact with views at the greater scale? Finding answers to these questions and supporting the user in such potent workspaces is part of ongoing visualization research. In this section, we presented a series of visualization examples. We started with basic visual encodings, incrementally enhanced the visual representations, and finally looked at some advanced techniques. These examples are kind of a teaser of what to expect from this book. The next section will provide some more detailed information on the book structure.
Introduction (cid:4) 13 1.3 BOOK OUTLINE This book is structured into six chapters: the first introductory chapter you are currently reading plus five chapters on interactive visual data analysis to come. Figure 1.6 provides an overview of the chapter structure. Chapter 2 is concerned with fundamental aspects pertaining to interactive visual data analysis. We will look into design criteria, factors influencing the design, and models describing the involved processes. Chapter 3 is about visualization. You will learn about the basic methods of visual encoding and presentation, and about various visualization techniques for different types of data. Chapter 4 is dedicated to interaction. The chapter discusses general inter- action concepts and illustrates how interaction techniques can facilitate the visual data analysis in many ways. Chapter 5 deals with automatic computations to support the visual data analysis. The primary goal will be to reduce the complexity of the data and their visual representations. Chapter 6 sheds some light on advanced concepts for interactive visual data analysis, including multi-display visualization environments, user guidance, and progressive visual data analysis. Chapter 7 will briefly summarize the book and outline ideas for readers to continue with the topic of interactive visual data analysis. CH. 1 CH. 2 CH. 3 CH. 4 CH. 5 CH. 6 CH. 7 Introduction Criteria, Visualization Interacting with Automatic Advanced Summary Influencing Factors, Methods and Visualizations Analysis Support Concepts and Models Techniques Figure1.6 Chapter structure of this book. Icons by icons8.com. FURTHER READING General Literature: [SM00] • [Spe07] • [WGK15]
2 CHAPTER Criteria, Factors, and Models CONTENTS 2.1 Criteria ...................................................... 16 2.2 Influencing Factors .......................................... 19 2.2.1 The Subject: Data .................................. 19 2.2.2 The Objective: Analysis Tasks ...................... 28 2.2.3 The Context: Users and Technologies .............. 35 2.2.4 Demonstrating Example ............................ 38 2.3 Process Models .............................................. 41 2.3.1 Design ............................................... 41 2.3.2 Data Transformation ................................ 44 2.3.3 Knowledge Generation .............................. 47 2.4 Summary .................................................... 48 INTERACTIVE VISUAL data analysis is highly context-dependent. We will need different techniques for analyzing time-series data than for graph data.Wewillwanttousecompletelydifferentvisualrepresentationsforgetting an overview of the overall data distribution than for inspecting individual patterns and trends. And we will most likely interact differently when working with data on an interactive surface as compared to a standard desktop. There is no silver bullet solution that simply scales to all possible analysis scenarios. If there was such a universal approach, there would be no necessity for this book. The first step to designing context-dependent solutions is to know the fundamental requirements posed by interactive visual analysis scenarios. This aspect will be addressed by introducing corresponding criteria in Section 2.1. Second, we need to describe the influencing factors that characterize analysis scenarios. This primarily concerns the data to be analyzed and the tasks to be accomplished, but also the people who carry out the analysis and the environment in which it takes place. These influencing factors will be dealt with in detail in Section 2.2. Finally, we need to understand the fundamental 15
16 (cid:4) Interactive Visual Data Analysis processes behind interactive visual data analysis. These will be discussed in Section2.3,wherewecoverthedesignprocess,thedata-transformationprocess, and the knowledge-generation process. Throughout this chapter, we will use illustrating examples to convey the major points. 2.1 CRITERIA Ifyouwanttobesuccessfulinanalyzingdatawithinteractivevisualtools,you cannot just create an arbitrary visual representation, add some interactivity to it, and spice it with a little computational support. Much of the potential of interactive visual data analysis would be wasted. Even worse, you could end up with findings that are simply not true. Any follow-up decisions based on your analysis would be tainted. Consider for example the visual representation from Figure 2.1a. The data captures the Germans’ life satisfaction in 2017. Satisfaction is measured on a scale from 0 (very dissatisfied) to 10 (very satisfied). The map represents the average satisfaction per region with a red-yellow-green color scale. To the casual observer, Figure 2.1a suggests that people in the eastern parts of Germany are mostly dissatisfied. This is where the visual representation fails, because the visual impression is not backed by the data. A closer look at the legend tells us that the visualized satisfaction is between 6.83 and 7.43, which means people are closer to being satisfied than to being unhappy. 7.43 7.43 6.89 6.89 7.28 7.28 7.20 7.20 6.94 6.94 6.83 6.86 6.83 6.86 7.12 7.12 7.11 7.11 6.92 6.92 7.20 7.27 6.97 7.20 7.27 6.97 7.15 7.15 7.26 7.26 7.21 7.21 6.83 7.43 Very dissatisfied - 0 6.83 7.43 10 - Very satisfied (a)Failing visual representation. (b)Succeeding visual representation. Figure2.1 Visualization of life satisfaction in Germany.
Criteria, Factors, and Models (cid:4) 17 Figure 2.1b shows a successful visualization. The color scale is mapped appropriately to the possible range of values between 0 and 10. The visual representationnowsuggeststhatGermanpeoplearemostlysatisfiedwiththeir lives. There are only slight differences between the different parts of Germany. The take-home message of this example is that we always have to ask what makes a good visual representation that actually helps us analyze our data? In fact, we would like to be able to assess the overall quality of interactive visual analysis solutions. However, as indicated before, there are many influencing factors, including the properties of the data, the nature of the tasks, the characteristics of the human, the modalities of the output and input system, and the resources of the environment. These factors bear complex questions in themselves and are not easy to define formally. This makes it hard to come up with a coherent definition of quality. Instead, we establish three quality criteria that interactive visual approaches to data analysis should obey. Expressiveness In the first place, an interactive visual representation must be expressive. Expressiveness is a mandatory condition. The assessment of whether an interactive visual representation is expressive or not can only be made in relation to the data to be visualized and the task to be sup- ported. Apparently, we have to consider visual expressiveness and interactive expressiveness separately. Avisualrepresentationisexpressiveifitcommunicatesthedesiredinforma- tioncontainedinthedata,andonlythisinformation.Inotherwords,thevisual representation neither fabricates nor withholds information, but objectively reflects the information that we need to accomplish our task. Accordingly, an interactive representation is expressive if it allows us to carryouttheactionsneededtoacquirethedesiredinformation,andonlythese actions. Put differently, the user is enabled to do exactly what is necessary for the task at hand. Effectiveness Secondly, an interactive visual representation should be effec- tive. Effectiveness is a goal-oriented condition. It relates to the degree to which we as humans are able to achieve our analysis task. An interactive visual representation is effective if it is geared to the human sensory and motor systems, that is, our abilities to observe and interact with our environment. As we are dealing with visual representations, the properties ofthehumanvisualsystemareofprimaryrelevance.Inthissense,effectiveness captures how well we can extract the information needed for our task from a visual representation. In a similar vein, we can characterize effectiveness as a measure of how well we can convey an interaction intent to the computer. Most of the time, this concerns our physical ability to move our fingers, hands, or the whole body with different speed and accuracy, but also commands issued with our vocal tract.
18 (cid:4) Interactive Visual Data Analysis TABLE2.1 Quality criteria of interactive visual data analysis. Criterion Concern Expressiveness Faithfully map data and tasks Effectiveness Enable users to accomplish task Efficiency Balance benefits and costs Efficiency Finally, interactive visual analysis should be efficient. Efficiency is a desired property. It adds the economic aspect: The gains from using an interactive visual approach should outweigh the computational resources and human effort needed to carry out the analysis. Resource-wise,weareinterestedinhowmuchtimeandhowmuchmemoryit takestoperformcalculationsonthedataandtotransformthemintoexpressive andeffectiveimages.Thedisplayspaceneededisanefficiency-relevantresource as well. On the users’ side, effort goes into interpreting visual representations and carrying out interactions. Interpreting visual representations is primarily a mental effort, but also related to physical eye movements. On the other hand, carrying out interactions is mostly a physical effort, but requires mental activities as well for planning and coordinating the interaction. With expressiveness, effectiveness, and efficiency we have described three criteriathatcontributetoanotionofqualityofinteractivevisualdataanalysis. Table 2.1 summarizes the main points of our discussion. Note that the order in which we introduced the criteria bears an important message: Expressiveness first! No matter how hard we try to be effective and efficient, an interactive visualization is not useful at all if it misrepresents the data. One can also speak of representational primacy [AS05]. Already in 1983, Tufte defines this as a fundamental principle of good graphical representa- tions [Tuf83]: “Aboveallelseshowthedata.” Tufte,1983 It is important to realize that the introduced quality criteria are difficult to evaluate formally. While some aspects would be easy to quantify, such as, computation time or display space, others are not. For example, the benefit of a visual data analysis is hard to capture, but we would need it to determine efficiency. Moreover, we deal with aspects of the human user, whose cognitive processes are not yet fully understood and whose goals and tasks are hard to define on a formal level.
Criteria, Factors, and Models (cid:4) 19 Nonetheless, the described quality criteria provide us with a basis for critically questioning the visual representations, the provided interactions, and the involved calculations and transformations. 2.2 INFLUENCING FACTORS In order to build interactive visual analysis solutions that are expressive, effective, and efficient, we must take a closer look at the factors that influence the analysis. These factors pertain to the subject of the analysis, that is, the data, to the objective of the analysis, that is, the goals and tasks, and to the context of the analysis, that is, the human and technical resources involved. 2.2.1 TheSubject:Data Data on a computer are but sequences of zeros and ones. They are worth nothing without knowledge about how to decipher them. Consequently, if we want to make sense of data, we need both the data themselves plus a description of how to interpret them. This description of data properties is the first factor to be taken into account for choosing or building appropriate analysis solutions. DataDomain So, what characterizes an individual datum (or data value)? Any data value comes from a data domain. The data domain is the set of values that can potentially appear in the data. An important property of a data domain is its scale. The scale determines whatrelationsandoperationsarepossibleforthedatavaluesinthedomain.At the top level, we can differentiate qualitative (or categorical) and quantitative (or numerical) data. At a second level, we can further categorize qualitative data into nominal and ordinal data, and quantitative data into discrete and continuous data. The different data scales and the relations and operations they permit are summarized in Table 2.2. Next, we look a bit closer at these different data. TABLE2.2 Operations possible in different data domains. Qualitative Quantitative Nominal Ordinal Discrete Continuous Equality • Order • • Distance • • • Interpolation • • • •
20 (cid:4) Interactive Visual Data Analysis Nominal Data Fornominaldata,wecanassumetheexistenceofanequality relation =, which allows us to determine whether two values are equal. This is the most primitive insight that can be gained about data values. Additionally, we can count frequencies of nominal values. An example of nominal data would be identifiers such as names {Anika, Ilka, Maika, Tilo, ...}. Ordinal Data For ordinal data, an order relation < exists in addition to the equality relation. This allows us to determine whether a data value is smaller (or less, before, weaker, of lower rank) than another data value. With the help of the order relation, it is possible to sort or rank data values. An example of ordinal data would be age groups such as {children, youths, adults, elders}. With the order relation, we can say that children < adults. Nominal and ordinal data do not have an inherent notion of distances between data values. This changes with quantitative data, which are based on metric scales as we will see next. Discrete Data Discretedataarenumericdatawhosedomaincanbeequated to the set of whole numbers Z. This implies that we can count discrete values and determine the difference between any two data values by means of a distance function. An example of discrete data would be the number of people visiting a doctor. If one day 34 people seek treatment and another day 23, we can naturally derive a difference of 11 persons. Continuous Data Continuous data are numeric data whose domain can be equated to the set of real numbers R. As such, continuous data are uncountable. We can also say they are dense, that is, between any two data values, a third data value exists. This property is a necessary condition for being able to carry out interpolations on the data. Anexampleofcontinuousdatawouldbetemperaturevaluesasmeasured hourly by a weather station. From two data values 10.6℃ and 13.2℃ measured at 8:00 and 9:00 we may interpolate 11.9℃ for 8:30 (in the case of stable weather conditions). Note that it is not always obvious to which category a data value belongs. For example, when dealing with customer numbers or zip codes, we may think they are discrete data. However, they are actually categorical identifiers, for which neither order nor distance have a meaningful interpretation. Laterinthisbook,wewillseethatthedifferentdatascalesrequiredifferent visual encodings. For example, for ordinal data, the ordering of data values must be clearly communicated, yet without suggesting any notion of distances
Criteria, Factors, and Models (cid:4) 21 between data values. More details on how to appropriately visualize data depending on their scale will be discussed in Chapter 3. With the scale of data domains, we can characterize individual data values. If multiple numerical data values are combined, we can further distinguish scalar data, vector data, and tensor data. If we are dealing with a single numeric value, we speak of a scalar. A scalar datum contains but the value itself, such as the temperature values mentioned before. Multiple numeric values can be combined to form a vector. A vector datum defines a direction plus a magnitude based on the vector components. An example is velocity vectors to describe motion in physics. Still more information is captured by a tensor. A tensor datum consists of multiple directions and magnitudes. The order of a tensor defines how much information it bears. Scalars and vectors are special cases of tensors with order 0 and 1, respectively. A 2nd-order tensor can be represented by a matrix, tensorsofhigherorderbymulti-dimensionalarrays.Anexampleoftensordata are 2nd-order stress tensors whose 3×3 components describe the stress at a point inside deformed material. In this book, we focus on scalar data. The interactive visual analysis of vectordataandtensordataisachallengingproblemonitsown.Theinterested reader is referred to books specialized on these topics [HJ05; Tel14]. DataStructure In the previous paragraphs, we introduced data domains to characterize an individual piece of data. However, an interactive visual analysis of a single piece of data does not make much sense. We are interested in analyzing entire sets of data. In reality, data are often unstructured and contain various chunks of heterogeneous information. The analysis of such messy data is extremely difficult. Therefore, data should ideally be stored in or be transferred to structured formats. One such format that is universally applicable is the data table. A data tableconsistsofrowsandcolumns.Thecolumnsrepresentdatavariables.Each variable is associated with a data domain that specifies the values that can possibly appear in a column. The values that actually do appear in a column define the value range. The rows of a data table represent data tuples. A tuple consists of a set of data values and can be understood as a unit of data that describes the properties of an observed entity. There is one value for each variable, and the value is from the variable’s domain. Depending on the context of use, tuples are also called observations, records, items, or objects. We will often use the more general term data element. Particularly in scenarios where relations exist between data entities, these relations may define another layer of structural organization. Hierarchies are common to structure data in a top-down fashion according to different levels of detail (or abstraction). More generally, data can be modeled as graphs whose nodes and edges represent data entities and the relations among
22 (cid:4) Interactive Visual Data Analysis them, respectively. Examples of graph-structured data are social networks or biochemical reaction networks. Both hierarchies and graphs can be stored by using two data tables, one for the entities and one for the relations. Another option to store data in a structured manner is to use data grids. Data grids are particularly important in the realm of flow visualization and volume visualization. As these topics are not in the scope of this book, we again refer the reader to relevant specialized literature [HJ05; PB07; Tel14]. DataSpace A data table as introduced before serves to structure sets of data values. However, structure alone is not sufficient. We additionally need to consider the characteristics of the data space that is spanned by the variables. An important point is the distinction between independent and dependent variables. Independent variables correspond to the dimensions of the space where the data have been collected, observed, or simulated. In turn, the depen- dent variables describe the attributes of what has been collected, observed, or simulated.Moreformally,wemaymodelthisaspectasafunctionaldependency as follows: f :(D ×D ×···×D )→(A ×A ×···×A ) 1 2 n 1 2 m where D denote the dimensions (independent variables) and A the i i attributes (dependent variables). The definition of f implies that a point in the reference space is associated with exactly one data point in the attribute space. A schematic depiction is given in Figure 2.2. Reference Attribute Space Space Figure 2.2 Functional dependency between the reference space and the attribute space. For a point in the reference space, there is exactly one point in the attribute space. It is typically this functional dependency that has to be made visible and understood during interactive visual data analysis. Assume, for example, your dataconsistoftemperatureandairpressureobservedatdifferentlocationsand times: (latitude×longitude×time)→(temperature×pressure). You might be interested in studying how the measured attributes vary over time or where extrema are located. In order to support these and similar analysis questions, it is necessary to visualize the functional dependency between dimensions and attributes faithfully. Note that there are also analysis objectives for which the functional dependency plays only a minor role, for example, when acquiring a general overview of the data’s value ranges. As a graphical summary, Figure 2.3 collects the key terms that we have discussed so far.
Criteria, Factors, and Models (cid:4) 23 Dimensions selpuT Attributes D D A A A 1 2 1 2 3 t 2 1 t 5 2 t 0 3 t 54.32° 13.05° 5 4 97 Data Element 4 t 1 5 t 9 6 t 5 7 t 1 8 Data Domain -10 0 Value Range 9 20 Figure2.3 Key terms for characterizing data. DataSize Withthehelpofatabularmodelofdataandthedistinctionofdimensionsand attributes, we can now study another important characteristic of data: their size. To this end, let’s take a look at a data table with dimensions, attributes, and tuples as illustrated in Table 2.3. We can see that the size of a dataset is determined by n, the number of dimensions, by m, the number of attributes, and by k, the number of tuples. TABLE2.3 A data table. D D ··· D A A ··· A 1 2 n 1 2 m d d ··· d a a ··· a 1,1 1,2 1,n 1,1 1,2 1,m . . . . . . d d ··· d a a ··· a k,1 k,2 k,n k,1 k,2 k,m In general, we can say we are dealing with k tuples of n-dimensional m-variate data. Data that consist of more than one dimension are denoted as multi-dimensional data. If there are many dimensions, the term high- dimensional data is common. A similar notation is used for the attributes. If there is only one or two attributes, we use the terms univariate and bivariate data, respectively. The presence of several attributes is indicated by the term multivariate data. Obviously, the larger m, n, and k are, the more challenging and complex the interactive visual analysis will be and the more sophisticated tools will be needed to support the analysis. The question of whether a dataset can be regarded as large or not is often discussed controversially and typically cannot be answered unambiguously.
24 (cid:4) Interactive Visual Data Analysis One way to resolve this issue is to think of the different bottlenecks that data have to pass on their way from the computer to the human mind. This can be made clear by asking the following questions: • How much data fit in storage? • How much data fit in memory? • How much data fit on the display? • How much data can we digest? If for any of these questions our data do exceed the corresponding limit, we mayconsiderthemlarge.Theearlieralimitisexceededinthislistofbottlenecks, the larger the data typically are. In any case, we will need additional methods, such as interaction or computational support, to overcome the bottlenecks. These topics will be studied in Chapters 4 and 5. So far, we have discussed data properties that can, to a certain degree, be extracted from the data. We can look at the values and make assumptions abouttheassociateddatadomains.Wecaninspectthevariablesanddetermine which are independent and dependent. And of course, we are able to count the variables and tuples of a dataset to get its size. Next, we will deal with a property that cannot be derived, but must be provided to us: the data scope. DataScope In order to create expressive visual representations of data, we need to know thedata’sscope.Thedatascopecharacterizesthevalidityofdataatoraround the point of observation. We can distinguish between three types of scopes as illustrated in Figure 2.4: • Global Scope: Data are valid across the entire reference space. • Local Scope: Data are valid at a point of reference and its vicinity. • Point Scope: Data are valid only at a point of reference. Reference Space Reference Space Reference Space (a)Global scope. (b)Local scope. (c)Point scope. Figure2.4 The scope defines to which extent an observation is valid.
Criteria, Factors, and Models (cid:4) 25 Whether individual values are valid globally, locally, or point-wise cannot be determined from a dataset per se. Instead a data description is needed, which should include hints as to which scope is applicable. For specific applications, we may assume a certain data scope. One such example is geo-spatial data, which, according to Tobler’s first law, have a local scope, because attributes measured at proximal locations tend to be correlated[Tob70].Unfortunately,thelocalscopeisnotalwaysdefinedprecisely. From a visualization perspective, the question is how the local validity of data can be represented? Let us illustrate this with the three different representations of measure- ments of water quality as shown in Figure 2.5. The data are color-coded with a green-to-red color scale, where green represents high quality and red signals low quality. Figure 2.5a neglects the local scope and shows us just the data points. This kind of representation is rather sparse and does not really support building a good understanding of the data. (a)Data points only. (b)Voronoi partitioning. (c)Shepard interpolation. Figure2.5 Visualizing the local scope of measurements of water quality. In Figures 2.5b and 2.5c, interpolation comes to our help. By interpolation, wecanassignacolortoallpixelsinordertocreateadensevisualrepresentation thatcommunicatesthelocalscopeofthedata.Figure2.5busesnearest-neighbor interpolation, which corresponds to a partitioning of the space into discrete Voronoi regions. Now we get a much better impression of the water quality. However,thediscreteregionssuggestdiscontinuitiesattheirborders.Certainly, no such discontinuities exist in the water. In Figure 2.5c, we used Shepard’s interpolationmethod[She68].Thevisualrepresentationissmooth,whichmore closely corresponds to what we expect from the data. Note, however, that interpolating scattered data is a non-trivial problem. There are various methods and each comes with its own set of parameters and corresponding results. Which methods and parameterizations are appropriate must be decided on a case-by-case basis.
26 (cid:4) Interactive Visual Data Analysis Meta-data All of the previously discussed properties of data (domain, structure, space, size, and scope) should be described and provided along with the actual data. This data description is also called meta-data, or data about data. Ideally, the meta-data are curated with the same scrutiny as the data themselves, because the meta-data are the first to be consulted for an informed decision on how to visually analyze the actual data. Moreover,meta-datacan(andshould)containadditionalinformationabout the evolution of the data. This includes the data’s past, the data’s present, and the data’s future [Sch+17]. Intermsofthedata’spast,wemaythinkofinformationabouthowthedata have been collected, observed, or simulated. This so-called data provenance information can be quite important when it comes to understanding and replicating the results from analytic activities. An example of provenance meta-data is indicators of data quality to specify how much confidence we may have in the recorded data. A closely related notion is that of data uncertainty, which tells us if and to which degree the data may be uncertain. Meta-data about the data’s present state basically specify how the data are stored and how they can be retrieved. This covers all the data descriptors described earlier plus technical details about the data format. For example, the meta-data can tell us if a variable is allowed to include missing data (or null values), and if yes, how they are signaled. Thedata’sfuturemaybecoveredbymeta-dataaswell.Thismainlyincludes information about what operations may be performed on the data. Such data utility information is somewhat rarer and often not given explicitly. But it can be quite valuable. For example, we already mentioned data interpolation as difficult to deal with. Meta-data can give us information about whether interpolation is possible and under which assumptions. In summary, meta-data are as important as the data to be analyzed. Figure2.6providesanoverviewofthevariousaspectsdiscussedintheprevious paragraphs. While our considerations remained rather theoretical so far, the next section will be more practical by describing common classes of data. META-DATA Data Value Data Set Data Evolution Data Domain Data Structure Data Provenance Data Space Data Format Data Size Data Utility Data Scope Figure2.6 Meta-data to characterize the data to be analyzed.
Criteria, Factors, and Models (cid:4) 27 ClassesofData Let us next take a look at classes of data that are common in practice. We introduce the following abstract notation: A stands for (one or more) data attributes,T fortime,andS forspace,andRforstructuralrelationships among the data. Depending on which of these aspects are present, different classes of data can be distinguished. Figure 2.7 provides an overview. More details and example data are described below, where the arrow symbol → is used to indicate a functional dependency between reference space and attribute space. Time Multivariate Data A T Temporal Data Multivariate Graphs Space-time R S Space Graph Data Spatial Data Dynamic Graphs Spatio-temporal Data Multivariate Dynamic Graphs Spatio-temporal Graphs Multivariate Spatial Graphs Multi-faceted Graphs Spatial Graphs Figure2.7 Four-set Venn diagram illustrating different classes of data. Multivariate Data Multivariate data consist of several data attributes A. Typically, the data analysis will concern the distribution of data values and correlations among the attributes. An example is player statistics as maintained in many leagues of various sports. Temporal Data For temporal data, the reference space is defined by the dimension of time, and one or more time-dependent attributes are observed over time T →A. The data analysis typically seeks to under- stand how the attributes evolve over time. This includes the detection of trendsorcyclicbehavior.Stockpricesareacommonexampleoftemporal data. Spatial Data If data are given in a spatial frame of reference, we denote them as spatial data S → A. Two-dimensional and three-dimensional spatial data are common. The analysis of spatial data is centered around finding spatial patterns, clusters, or hot-spots. An example of spatial data could be the distribution and richness of mineral deposits. Spatio-temporal Data Quite often, spatial data do also include a temporal dependency, in which case, we speak of spatio-temporal data S×T →A. An analysis of such data combines spatial and temporal issues, such as the development of spatial clusters over time or cyclic re-occurrence of hot-spots. Spatio-temporal data are ubiquitous in the form of weather data.
28 (cid:4) Interactive Visual Data Analysis Graph Data Graphs consist of a set of data entities, the nodes, and a set of relations between the entities, the edges. In the first place, we are interested in understanding the structural information R inherent in graphs. Additionally, nodes and edges of a graph can also be associated withdataattributestobeanalyzedR→A.Graphscanevenbelinkedto a spatial context or vary over time, in which case different dependencies couldbedefined,suchasR→S×T×AorS×T →R×A.Anexample of so-called multi-faceted graphs is climate networks, which are employed to simulate and predict climatic phenomena on earth. The introduced classes of data are deliberately kept general. This allows us to map practical analysis problems to these classes. For example, the analysis of documents or document collections could be done by modeling documents as attribute vectors among which relations exist R → A. The analysis of medical images naturally maps to S →A, where S corresponds to the pixel grid and A is the pixel color. In a similar way, the analysis of flow data maps to S →A (or S×T →A, if the data are time-varying), where S is a 2D or 3D grid-structured space and A defines the components of vectors. If the visual analysis is about more complex artifacts such as software systems or distributed processes, it is typically necessary to break them down into smaller conceptual pieces before a meaningful mapping to A, T, S, and R is possible. With the introduction of general data classes, we end our discussion of data characteristics as the first influencing factor to be considered. Next, we continue with analysis tasks as the second influencing factor. 2.2.2 TheObjective:AnalysisTasks Intheprevioussection,weinvestigatedthesubjectofinteractivevisualanalysis, the data. In this section, we study the objective, that is, the analysis tasks. Tasks are important particularly for two reasons. First, they determine which portions of the data are relevant for the analysis, and second, they are crucial for meeting the effectiveness criterion as introduced in Section 2.2. Let us illustrate this with an example. Figure 2.8 shows two color-coded maps with identical geo-spatial data. Figure 2.8a uses a yellowish-to-green color scale where color varies in perceptually uniform steps to support the task of identifying the data values. The map in Figure 2.8b uses a color scale that emphasizes extrema, whereas intermediate values are attenuated by the use of gray tones. This encoding facilitates the task of locating minima (in blue) and maxima (in red) on the map. Our example clearly demonstrates that the generation of effective visual representations depends on the task at hand. In fact, we can state that a particular visual representation supports particular tasks: Data + visual representation 7→ tasks. In turn, this implies that specific tasks require dedicated visual representations: Data + task 7→ visual representation. The very same dependency on the task exists for the
Criteria, Factors, and Models (cid:4) 29 (a)Coloring suited to identifying values. (b)Coloring suited to locating extrema. Figure2.8 Different visual encodings to support different tasks. interactiondesignandtheinvolvedanalyticalcomputations.Aswesee,itisthe task that determines which design, technique, or method would be appropriate for an interactive visual analysis of data. However, tasks are notoriously difficult to conceptualize due to their multi- facetedcharacter.Ataskisusuallycarriedouttoachievealargergoal.Moreover, a task is typically focused on an analytic question that operates on a relevant part of the data, the task’s target. Finally, a task can be accomplished by employing different means. In the following, we will describe in more detail the goals, analytic questions, targets, and means that characterize tasks. Goals Goals describe the intent with which analysis tasks are pursued. Goals have an overarching character in that achieving a goal typically involves several steps of analytic activity. General goals are to explore, describe, explain, confirm, or present the data [Sch+13a; LTM18]. Exploration is geared towards making first observations, such as discovering trends in heterogeneous data or detecting outliers in homogeneous data. This also includes noticing the absence of something that was actually expected in the data such as a commonly known pattern or trend that is not there. In a sense, exploration follows an I-know-it-when-I-see-it approach to undirected search, or as pioneers of visual analytics put it [TC05]: “[...]detecttheexpectedanddiscovertheunexpected.” ThomasandCook,2005
30 (cid:4) Interactive Visual Data Analysis Description isallaboutcharacterizinganobservationbytheassociateddata elements, and thereby deriving a specification for an observation. For example, an outlier can be described by its characteristic values and, if available, its spatio-temporal context. A proper description may serve as a basis for configuring further analysis steps. In particular, a description allows for sharing first insights with other people, who can later be involved in verifying the analysis results. Explanation means identifying all contributing data and finding the main causes behind an observation. This involves investigating several ques- tions. Is the observation by itself significant or did we just interpret too much into the noise among the data? Does the observation re-occur throughout the data or are we looking at a singular outlier produced by unlikely circumstances? If the observation does re-occur, does it show up reliably under the same conditions, thus forming a pattern, or are its appearances seemingly random? Such investigations are necessary to understand the expected and recog- nizethereasonsbehindtheunexpected.Bytryingtoexplainobservations, we will obtain more and deeper insights, which will lead to hypotheses about the data. Confirmation aims to verify the hypotheses. Unlike exploration, confirma- tion is a directed search. We look for something concrete, some evidence to either back up or refute a hypothesis in terms of validity, generality, or reliability. For this purpose, we may study alternative visual represen- tations of the data or re-parameterize the analytic computations in order to check whether this leads to the same results. If available, related data may be consulted to test if they exhibit similar results. Presentation is to communicate confirmed analysis results. While explana- tion and confirmation were about convincing ourselves, presentation is about convincing others of what we have found in the data. This is best done by telling a story about the data, the analysis, and the results. Such a story can act at different levels of emphasis. We may inform an audience by letting the results speak for themselves, explicate the results to an audience, or even persuade an audience into agreement with the results. The audience in this context can be the listeners of a talk, the readers of an article, or colleagues participating in a scientific discussion. Exploration, description, explanation, confirmation, and presentation can be understood as subsequent phases in the data analysis process. We start exploring the data until we make an observation. Next we describe the finding and try to explain it. Then we confirm that our hypotheses about the data are valid. Finally, we are ready to present the confirmed analysis results. Note how this workflow proceeds with increasing specificity from not knowing that certain observations can be made in the data to knowing them well enough to present them.
Criteria, Factors, and Models (cid:4) 31 Thepreviousparagraphsoutlinedinbroadstrokesdifferentgoalsofanalytic activities, yet without detailing what is actually done. This is where concrete analytic questions enter the stage. AnalyticQuestions Analytic questions describe what specifically is sought in a particular analysis step. There are many different questions that are relevant in data analysis scenarios [KK17]. In general, we can distinguish between two fundamental categories: elementary and synoptic questions [AA06]. Elementary Questions Elementary questions refer to individual data ele- ments. This includes questions that concern a single data element, but also multiple data elements. What is essential is that each data element is studied individually. Elementary questions may be concerned with: • Identify: What is the value? • Locate: Where is the value? • Compare: Is it less or more? • Rank: Is there any order? • Connect: Are they related? • Distinguish: What makes the difference? Synoptic Questions Synoptic questions refer to groups of data. As such, synoptic questions are concerned with the characteristics of sets of data ele- ments, rather than individual data elements. The previously listed questions can operate on groups as well. Additionally, synoptic questions may ask: • Group: Do they belong together? • Correlate: Are there any dependencies? • Trends: Do they develop systematically? • Cycles: Do they re-occur periodically? • Outliers: Are they special with respect to the rest? • Features: What is characteristic for the data? Note that the two lists of analytic questions are not comprehensive. One can easily imagine many more questions that could be asked about data. The further readings collected at the end of this chapter provide additional information in this regard.
32 (cid:4) Interactive Visual Data Analysis To recap, analytic questions specify what we would like to know about the data. However, as indicated by the unspecific word they in the above lists, a question does not tell us where to look. This aspect is covered by the target of tasks as described next. Targets Typically, not all data are equally important in the context of a specific analytical question. The target of a task is about where in the data a task actually operates. But why is this important? Specifying a task’s target enables us to narrow down what we need to look at to accomplish the task. Are we interested in the data values just by themselves, or how they distribute in a particular spatial or temporal context? Do we want to identify individual data values of the whole data set, or only with respect to a certain subset? Do we search for global trends, or are we interested in more fine-grained variations? Data of Interest More specifically, the target defines the data of interest, that is, the subset of the data that is indeed relevant to the task. Conceptually, such a subset can be created with respect to the data variables and the data elements. A target can cover all variables of the data space or only a particular subset of variables. Restricting the view on particular variables is called data projection. Thedataofinterestcanbenarroweddownfurtherbyfocusingonparticular data elements. To this end, we may specify criteria that data elements must exhibitinordertobeconsideredrelevanttotheanalysis.Thiswayitispossible to restrict the data of interest to specific value ranges. Restricting the view on particular data elements is called data selection. Figure 2.9 illustrates how projection and selection constitute the target. Note that the target is not necessarily a closed chunk of data as illustrated in the figure. The general case is that the target is made up of several pieces of data that are distributed across the data space. Looking at Figure 2.9 we can see that the target is much smaller than the entiredata.Itcanevenbeassmallasasingledataelement,forexample,when working on elementary questions such as identifying a value in the reference space. If the target is a proper subset of the data, elementary or synoptic questions could be addressed depending on whether the data elements are considered individually or as self-contained groups. A target that refers to all data elements typically supports questions of synoptic character. Data Granularity For the case that the data are structured hierarchically according to different levels of detail (or abstraction), a target is further characterizedbyitsgranularity.Thegranularitydefinesthelevelofabstraction needed to accomplish a task.
Criteria, Factors, and Models (cid:4) 33 Projection V V V V V 1 2 3 4 5 d 1 d 2 d 3 d 4 d Target Selection 5 d 6 d 7 d 8 Figure2.9 A target as defined by projection and selection. Carrying out tasks at a low level of detail (high level of abstraction) is suited to get a general overview of the data. At this coarse-grained level, tasks are concerned with investigating fundamental data features such as general correlations or overall trends. On the other hand, tasks can operate on a high level of detail (low level of abstraction). Fine-granular targets typically permit insights into details that cannot be seen in an overview. An example would be small fluctuations in an otherwise monotonically developing trend. Weknownowhowtargetsdefinewhereinthedataataskoperatesinterms of the data of interest that are studied at a particular data granularity. Next, we briefly describe the means that can be employed to carry out tasks. Means There are many options for carrying out the tasks. For example, if we need to locate a particular data element whose specifics are known, we can query the datadirectly.However,ifthedataelementisonlyvaguelyspecified,formulating aqueryishardlypossible.Inthiscase,wehavetoscanthevisualrepresentation in order to find it. As we see, querying and scanning are both suitable means to accomplish the same task. In general, the means describe how a task is performed. From a conceptual point of view, one can differentiate between visual, interactive, and computa- tional means. Visual Means subsume all kinds of visual inspection. The task is carried out with the eyes, in fact, with the entire human visual system. For example, wemightvisuallyidentifyadatavaluebylookingupacolorinthevisual representation’s color legend. Interactive Means relate to interactive information gathering. In this case, tasks are carried out by the hands or other parts of the human motor
34 (cid:4) Interactive Visual Data Analysis system. To continue the previous example, a data value could also be identified by hovering over it with the mouse cursor to bring up a label that shows the exact value. Computational Means stand for calculations in general. In this case, the computer produces the desired results. For example, we could perform a cluster analysis using algorithmic computations on the input data in order to group similar data elements. Each category of means is based on its own concepts and methods, which will be detailed in Chapter 3, Chapter 4, and Chapter 5. Nonetheless, visual, interactive, and computational means are usually applied in concert to fully exploit the cognitive capabilities of the human and the computational power of the machine. Let’s illustrate this with the example of finding outliers. We might start with an initial visual inspection of the data followed by an interactive selection of relevant data. Then, a computation of distances could relate the selected data to other data elements in order to detect outliers. Finally, we may again want to do a visual inspection to examine whether outliers have been flagged correctly. In this section, we have characterized analysis tasks with respect to goals, analytic questions, targets, and means, which indicate why a task is pursued, what ataskseeks,where ataskoperatesinthedata,andhow ataskisactually carried out. Figure 2.10 collects the key terms that we dealt with. They allow us to formulate tasks like describe groups of data elements with low values by marking them interactively. Two more examples are given in Table 2.4. Such taskdescriptionscanthenbeusedtoinformthedesignorsupporttheselection of interactive visual analysis solutions [Sch+13a]. In conclusion, the characteristics of the tasks and the properties of the data are two important influencing factors. Together, they form the basis for some of the most influential early classifications of visual approaches to data analysis [KK93; Shn96]. However, data and task are not the only factors to be observed as we will see in the next section. TASKS Goals Questions Targets Means Elementary Synoptic Explore Data of Interest Visual Describe Granularity Interactive Explain Identify Group Computational Confirm Locate Correlate Present ... ... Figure2.10 Goals, questions, targets, and means characterize analysis tasks.
Criteria, Factors, and Models (cid:4) 35 TABLE2.4 Examples of tasks. Goal Question Target Means Explore locations of maximum values visually Describe groups of low-value elements by marking Confirm cyclic behavior of temperature by statistics 2.2.3 TheContext:UsersandTechnologies To enable an effective and efficient interactive visual data analysis, the context of the analysis has to be considered as the third important influencing factor. Primarily, we have to ask who is analyzing the data and where does the analysis take place. In this sense, the context subsumes aspects of the users who conduct the analysis and aspects of the technologies that constitute the analysis environment. It is beyond the scope of this section to comprehensively characterize human users and different technologies. Instead, we will briefly look into the key concerns that need to be taken into account. HumanUsers Concerning the question of who is carrying out the data analysis, the following aspects are relevant: Human Factors An important factor is the properties of human individuals. What are we able to see with our visual system and what are we able to do with our motor system? Interactive visual analysis solutions should becenteredaroundtheabilitiesofhumansingeneral.Forexample,when interacting at larger displays via touch gestures, our precision is usually limited for the majority of users. Moreover, no user is like the other. Therefore, it is important to consider not only general characteristics of humans, but also the properties of individual users. For example, we need to adapt visual encodings for people with color vision deficiencies so as to allow them to gain the same insight as people with normal vision. User Background and Expertise People who work with interactive visual analysis solutions come from different backgrounds and have different expertises. Visual analysis experts excel in creating interactive visual analysis solu- tions. They apply their tools routinely to examine various kinds of data. However, they are usually not trained in the field to which the analysis is supposed to bring new insights. On the other hand, domain experts are focused on application-specific problems and data, of which they often have a quite good mental model.
36 (cid:4) Interactive Visual Data Analysis Theyexpectanalysistoolstoadapttotheirworkflows,andnottheother wayaround.Domainexpertscouldpreferspecificrepresentationsorways of doing things, simply because they are widely used in their application background, even if other alternatives would be better suited. Casual users from the general public will typically favor basic interactive charts and maps over more sophisticated visual analysis systems. There- fore, casual users need to be provided with visual representations that are easy to interpret and interactions that are easy to carry out. It can even be necessary to include incentives and guidance to motivate people to take a closer look and to assist them in doing so. Application Domain Only if we have a sufficient understanding of the problemsthatneedtobesolvedinanapplicationdomaincanwedevelop suitable analysis solutions. In addition to addressing domain problems, we also need to consider domain conventions. For example, the U.S. Geological Survey suggests using specific colors for geologic maps. As a consequence, these colors cannot be employed for other visualization purposes than geologic characteristics in this particular context. Application domains can also pose hard constraints on the analysis system. For example, medical diagnosis systems certainly must be super reliable and heavily tested before they can be used in clinical practice. Single-user and Collaborative Analysis Dataanalysescanbecarriedout by a single user or by multiple collaborating users. In the former case, the applied means can be tailored to the needs and preferences of a single individual. In collaborative settings, however, we need to consider a larger pool of methods that additionally need to be flexibly parame- terizable in order to be able to attune them to a broader variety of user needs. Moreover, while single-user analysis can focus on the exchange of information between the user and the machine, collaborative analysis further needs to consider the interaction between users, for example, for sharing visual representations and discussing findings. Technology To address the question of where the data analysis is performed, the following technology-oriented aspects are relevant: Computational Resources Analyzing data with the help of computers implies the use of computational resources. In the case of interactive visual data analysis, we need resources to create and render visual rep- resentations, ideally in high quality and at interactive frame rates. For larger data, we need additional computational resources to carry out analytical pre-processing steps. Depending on the computational resources of the environment and in linewiththeactualanalysisobjective,itmustbedecidedhowtobalance
Criteria, Factors, and Models (cid:4) 37 betweenthedifferentdemands.Forexample,whenusersinteract,immedi- ate visual feedback is crucial. So, visualization quality could be sacrificed temporarily in favor of quicker response rates during interaction. Similar trade-offs are necessary in many analysis situations. Display Technologies Nowadays, there exist a variety of technologies for presenting data. It makes a big difference to visualize data on a small portrait-oriented display of a mobile phone, a large landscape-oriented mega-pixel display wall, or a professional designer monitor with high contrast and a wide color gamut. In general, display technologies are characterized by their physical size, their aspect ratio, their resolution, and their ability to reproduce colors faithfully. The physical size largely determines if a display is applicable in particular environments. The available pixel resolution indicates how much data can be represented. The display’s aspect ratio influences the layout of the presented data. The available colors naturally have an impact on the use of colors in a visualization. Input Modalities An analysis environment is also characterized by its input modalities. They decide about how the interaction is carried out and howpreciselyaninputcanbemade.Classicmouseandkeyboarddevices are common in regular desktop scenarios. Modern displays facilitate interaction via touch technology. This has the advantage that the interaction can take place exactly where the data are shown, that is, on the display. A disadvantage, though, is that smaller graphical objects are more difficult to pick accurately. In the case of large display walls, mouse and touch interaction alone are impractical due to their limited reach. Alternatives are to track the user’s position and gaze and utilize the tracking information to steer the visual data analysis. As we see, a variety of aspects characterize the context of interactive visual data analysis. Although we have described them one by one, the different aspects exhibit numerous interdependencies. For example, visualizing larger data on a display wall implies that we also have to think of the increased resources that we need to render the visualization and of the interaction modalities that will allow a human user to operate in a large-display setting ergonomically. For another example, if we intend to develop a color-intensive visual representation, we must make sure that it is used on displays with a wide color gamut, and we should also think of ways to circumvent the problem of defective color vision. In summary, we have now described the data, the tasks, and the context as the major influencing factors. Together, they not only inform the design of interactive visual analysis solutions, but they also determine whether a
38 (cid:4) Interactive Visual Data Analysis solution meets the criteria defined in Section 2.1, that is, how well and how fast users can interprete their data, and how balanced the use of resources is. The condensed take-home message of this section is: If you want an expres- sive, effective, and efficient analysis solution, you first need to know what data are to be studied, what tasks are to be carried out, and what the context is in which the analysis is conducted. The more comprehensive the answers to these questions are, the easier it will be to come up with an appropriate solution. 2.2.4 DemonstratingExample In the following, we will demonstrate how the data, the tasks, and the context may manifest in practice. In order to keep our example manageable, we will brieflycharacterizethedataandthecontext,whereasthetaskswillbedescribed in greater detail. The Data and the Context For our example, we will use a mete- orological time series, which conceptually is an instance of temporal data T → A. The data are multivariate in that they comprise several attributes, including air temperature, air pressure, wind speed, hours of sunshine,cloud cover,precipitation,andprecipitation type.The individual values are scalars with continuous, discrete and nominal scale. For instance,air temperatureiscontinuous,hours of sunshineisdiscrete,and precipitation type is nominal. We will be investigating more than 23,725 daily measurements, which amounts to about 65 years worth of data. Forthesakeofsimplicity,wewillassumethedataanalysisiscarriedoutby a single user and takes place on a standard desktop computer with a regular display and the typical mouse and keyboard input. Although our example has been supported by an expert from climate impact research, we do not include any application-specific conventions or constraints. This is again to keep the example simple. The Task Now, let’s assume our goal is to explore the data in search of something interesting. More specifically, we are interested if there are any patterns in the data corresponding to seasonal variations, which means, we are dealing with a synoptic analysis question for which we need all data elements.Nonetheless,inordertonarrowdownouranalysisproblemsomewhat, we begin by concentrating on the three attributes hours of sunshine, air temperature, and cloud cover as the target. The charts in Figure 2.11 suggest that we will be accomplishing our task with the help of visual means. There is one chart per targeted attribute. Each chart shows time along the horizontal axis and a time-dependent attribute along the vertical axis. For each pair of date and measurement, a dot is placed in the chart.
Criteria, Factors, and Models (cid:4) 39 18h 16h 14h 12h 10h 8h 6h 4h 2h 0h January February March April May June July August September October November December (a)Hours of sunshine. 30°C 25°C 20°C 15°C 10°C 5°C 0°C -5°C -10°C -15°C -20°C January February March April May June July August September October November December (b)Air temperature. 80/80 70/80 60/80 50/80 40/80 30/80 20/80 10/80 0/80 January February March April May June July August September October November December (c)Cloud cover. Figure2.11 Meteorological measurements over the course of the year. NotethatthetimeaxisspansonlythedaysoftheyearfromJanuary1ston the left to December 31st on the right. In fact, only day and month of a date determine where to place a dot horizontally, the year is ignored. Conceptually, this corresponds to a projection with the effect of mapping the data of several years onto a common one-year scale. As a result, we obtain charts that are well suited to get an overview of seasonal trends. For example, we can easily see the expected pattern that the hours of sunshine in Figure 2.11a peak around the day of the summer solstice on June 21st and reach the bottom around the day of the winter solstice on December 21st. In Figure 2.11b, we can see that the air temperature in principle follows the hours of sunshine, but with a delay of approximately one month. This offset is well known and called seasonal lag. While not showing anything exciting so far, Figures 2.11a and 2.11b still helped us to detect the expected.
40 (cid:4) Interactive Visual Data Analysis 1000 100 10 1 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 (a)Cloud cover value frequencies in Rostock. 1000 100 10 1 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 (b)Cloud cover value frequencies in Dresden. Figure2.12 Histograms of the distribution of cloud cover values. This gives credence to the validity of the data and the expressiveness of the visual representation. But we can also discover unexpected things. Looking at Figure 2.11c, the average daily cloud cover (with 0/80 and 80/80 meaning clear sky and fully covered, respectively) seems almost evenly distributed over the course of the year at a first glance, with just a slight tendency of denser sky cover in the winter months. But if you look closely at the chart, you might be able to spot a horizontal line that is lighter and less covered with dots than all others. Could it be that there are unusually few occurrences of a particular cloud cover value? As we reason about this question in more detail, we change our task from exploring seasonal trends in the data to describing the unexpected finding. Providing a description of the finding may help those who were unable to discern it on their own: The thin horizontal line is located a little above the 50/80 mark for an average cloud cover of 52/80. Next, we want to find out whether our finding is by itself significant, or if we just interpret too much into the noise among the data. This implies a slight shift in our task from describing the unexpected finding to explaining it. To this end, we study the frequency distribution of cloud cover values, that is, we look at how frequently individual values appear in the data. Again, we approach our analytic task with visual means, more concretely withhistogramsasshowninFigure2.12.Ahistogramshowsalongitshorizontal axis the individual data values whose frequencies are of interest to us. For each of the 81 possible cloud cover values from 0 through 80, there is a vertical bar whose height encodes how often a value occurs in the data. The histogram clearly shows that values of 52/80 and also of 12/80 are much rarer than all others. Hence, our observation was not a visual illusion, but is backed by the data. Finally, we want to confirm our finding and show that it is not specific to the time series at hand. For this purpose, we look for similar patterns in the
Criteria, Factors, and Models (cid:4) 41 time series from a different weather station as shown in Figure 2.12b. And indeed, we can see a similar phenomenon. The pattern does not only re-occur for our initial observation at 52/80 and 12/80, but it even seems to affect the data more broadly at 2/80, 22/80, 32/80, 42/80, and so forth. At this point, we can only speculate about the reasons behind this pattern. For a satisfactory explanation, further investigations would be necessary, which suggest that visualdataanalysisisoftenanopen-endedprocesswithmanytwistsandturns involving many analytic tasks. With the above example, we end the discussion on influencing factors. Next, we continue with conceptual considerations regarding the fundamental processes of interactive visual data analysis. 2.3 PROCESS MODELS In the previous sections, we have introduced criteria and influencing factors. The criteria help us to capture a notion of the quality of analysis solutions, whereas the influencing factors tell us what we need to consider in order to obtain high-quality results. What we have not talked about so far is the actual solution itself. How does it come about, how does it look like, and how is it employed? All these questions will be dealt with in this section. Inthefollowing,wewillshedlightontheprocessesthatthedifferentactors beinginvolvedinthecontextofinteractivevisualdataanalysishavetodealwith. In the first place, there are designers who are responsible for conceptualizing data analysis solutions. They need to know about the design approach of how to get from an application problem to an actual solution for that problem. Secondly, there are the developers who engineer and implement the solution to createarunningsystem.Developersneedaconceptualblueprintoftheprocess of transforming data artifacts through several stages into interactive visual representations. Finally, target users actually analyze data. In this regard, it is of interest how utilizing data analysis solutions leads to new insights. Understanding this aspect will enable us to deploy and embed our solutions successfully into real-world problem-solving workflows. Followingtheabovelinesofthinking,wewillnextstudyabstractmodelsof interactivevisualdataanalysisfromadesignperspective,adata-transformation perspective, and a knowledge-generation perspective. 2.3.1 Design Designing interactive visual data analysis solutions is a challenging endeavor. The key difficulty is to come up with an appropriate ensemble of visual, interactive,andcomputationalmeansthatactuallyhelpusersinaccomplishing application-specific analysis problems. To cope with this difficulty, it makes sense to follow a well-structured design process called the nested model [Mun09; Mey+15]. The model consists
42 (cid:4) Interactive Visual Data Analysis Domain Characterization Data and Task Abstraction Design Conceptualization Solution Implementation Figure2.13 Four nested levels outline how to design interactive visual data analysis solutions. Adapted from [Mun09]. offournestedlevelsthatdescribethewayfromadomainproblemtoanactual implementation of a solution. Here, we describe a variant of the nested model that has been slightly adapted, mainly to be consistent with the terminology used in this book. The model’s levels are illustrated in Figure 2.13 and are detailed below. Domain Characterization First, the visualization designers must familiar- ize themselves with the application domain. This primarily concerns understandingtheusersandtheirdomain-specificproblems.Moreover,it is necessary to characterize the context of the data analysis as discussed in Section 2.2.3. What are the typical workflows and what tools are employed? How does the working environment look like? It is mandatory that these questions be answered in cooperation with domain experts. Data and Task Abstraction Once the application domain is characterized, the next step is to abstract the essential data and tasks based on the notions introduced in Sections 2.2.1 and 2.2.2. This is to focus thedesign onconceptuallyrelevantaspects,ratherthandomain-specificdetails.For example, instead of speaking of biomedical signaling pathways, we would saywearedealingwithtime-varyingattributesingraphsT×R→A.The diffuse application problem of trying to understand signaling pathways, for example, would be translated to explore recurring pattern of peaks visually and confirm them computationally. The abstract description of data and tasks will then inform the design conceptualization on the next level. Design Conceptualization This level is about defining the appropriate means that constitute the overall solution. As discussed earlier, the means can be of visual, interactive, or computational nature. That said, the design conceptualization involves finding expressive visual encodings, useful interactive tools, and potent computing procedures. Thanks to the abstract data and task description established in the preceding step, we can identify approaches that have previously proven
Criteria, Factors, and Models (cid:4) 43 successful, and if necessary, adapt them to the application domain. Only if no prior art exists is it necessary to develop entirely new approaches. As we will see in Chapters 3 to 5, the design space for visual, interactive, and computational approaches is considerably large. This implies that substantial expertise is necessary in order to make appropriate design decisions that match the data, the tasks, and the context. Solution Implementation Finally, the conceptualized design has to be brought to life. To this end, the building blocks identified in the previous level are implemented as concrete algorithms. They perform all the com- putations and transformations that are necessary for the data analysis. Moreover, the algorithms provide parameters that enable us to flexibly adjust the data analysis. Following the nested model, as outlined above, has several advantages. First, the domain characterization suggests a user-centered design strategy where the needs of the users are central to later design decisions. Second, the abstraction of data and tasks bridges the gap between the different vocabu- laries of domain experts and the designer. The abstraction further allows us to categorize application-specific problems into classes of data analysis prob- lems, which in turn simplifies the selection and the development of reusable approaches. Finally, the separation of concerns between conceptualization and implementation is beneficial because it allows the designer to concentrate on identifying or devising appropriate techniques, and the developer to focus on efficient algorithms. Asanothermajoradvantage,thenestedmodelfacilitatestestingthevalidity of a data analysis solution. Why is this important? The problem is that the design process is prone to incorrect design decisions. Being aware of the potential threats will help us to carefully check and critically question the design choices we make. Next, we briefly describe what can compromise the validity of our design on the different levels. • Inaccurate domain characterization. The application domain has not been characterized accurately. For example, the identified problems are not actual problems of the target users, or their workflows and working environment deviates from what was understood. • Incorrect data and task abstraction. The abstraction fails to extract the correct descriptions of the data and the tasks. The data may contain different information than expected. The tasks are not appropriate to successfully approach the application problem. • Inadequate design conceptualization. The devised techniques are not adequate. The visual encoding might not be expressive or effective. The interaction techniques could not match the environment. Computational results could be invalid due to the unfitness of analytical procedures.
44 (cid:4) Interactive Visual Data Analysis • Inefficient solution implementation. The solution does not run efficiently. This could be due to the time complexity of the involved algorithms or their inefficient implementation. Moreover, the memory footprint could be too large for the available resources. It is important to realize that invalid design decisions on the upper levels typicallyleadtoacascadeofnecessarychangesatthelowerlevels.Forexample, a misunderstood application problem can void all later design efforts. On the other hand, an issue at the lower levels can often be remedied with moderate effort, for example, the implementation could be improved by replacing a computationally expensive exact calculation with a sufficient approximation. In summary, the nested model describes the fundamental design process and where problems could occur along the way from the application problem totheimplementationofalgorithms.Next,wewillswitchourperspectivefrom the overall design to the actual process of transforming data into images. 2.3.2 DataTransformation In this section, we are interested in what actually happens to the data when becoming expressive visual representations. Certainly, we begin with raw data as input and seek images1 as output. But how do we continue from there? What are the steps to be performed in order to transform data into images? From a most abstract point of view, we can think of this process as a parametric transformation v that takes data and parameters as its input and generates an image as its result [JMG07]: v :D×P→I By invoking I = v(D,P), some data D ∈ D are transformed to an image I ∈ I, where P ∈ P is a parameterization that controls the transformation. Let us next dig a bit deeper into what v does internally and what changes D will undergo. Models that can help us in this regard are the visualization pipeline [HM90] and the data state reference model [CR98]. They conceptually model the data-to-image transformation by defining different data stages and different types of operators. Data Stages From bits and bytes to images, data exist in various states. In order to abstract from particular state details, it makes sense to consider four basic data stages (note the difference between state and stage): • data values, • analytical abstractions, 1Forbrevity,weuseimage todenoteanyformofinteractivevisualrepresentation.
Criteria, Factors, and Models (cid:4) 45 • visual abstractions, and • image data. The data values are typically raw and unprocessed digital pieces of infor- mation.Analyticalabstractionsarewell-structureddatameaningfullyenriched withderivedcharacteristics.Thisincludesdatatables,hierarchicallystructured levels of detail, and higher-order abstractions such as classifications or clusters. Visual abstractions model the visual appearance of the data by means of geometric primitives and corresponding visual attributes such as fill color or stroke style. Finally, image data describe the colored pixels to be shown on the output device. Thedifferentdata stagesareillustrated asboxesin Figure2.14. Thecircles inthefigurerepresentoperatorsthatdotheactualdataprocessingasdescribed next. Preprocessing Mapping Rendering Operators Operators Operators Data Analytical Visual Image Values Abstractions Abstractions Data Value Analytical Visual Image Operators Operators Operators Operators Data-oriented Graphics-oriented Figure 2.14 Data-oriented and graphics-oriented stages and operators. Adapted from [Chi00]. Operators An operator processes some type of input in order to produce some type of output. In the light of the above data stages, we can distinguish two categories of operators, transformation operators and stage operators. As illustrated at the top of Figure 2.14, transformation operators take data from one stage and generate data of another stage. That is, the type of input an operator accepts and the type of output it produces belong to different stages. The data transformation happens in three fundamental steps: preprocessing, mapping, and rendering. These three steps form the so-called visualization pipeline. Along the pipeline, data-oriented operators carry over their results to graphics-oriented operators, as indicated by different colors in Figure 2.14. For example, relevant wind features are extracted from radar data (preprocessing),thencolor-codedontoacartographicterrainmodel(mapping), and then finally displayed on a stereoscopic projector (rendering).
46 (cid:4) Interactive Visual Data Analysis For stage operators, as illustrated at the bottom of Figure 2.14, the type of input and output are within the same stage (not necessarily of the same type). Operators that handle data values are usually responsible for data cleansing and format conversions. At the stage of analytical abstractions, further data processingcantakeplace.Forexample,metricsmaybecalculatedtodetermine thequalityofclusters,orahierarchicalstructureofdifferentlevelsofdetailmay be established. Stage operators on visual abstractions support the adaptation of the visual representation, for example, by re-arranging visual elements or by reducing their numbers so that they fit the display. Finally, image operators may modify the graphical output. They can blur certain parts of the image to attenuate them, or enhance contrast and lightness of other parts for emphasis. Withthehelpofoperatorsanddatastages,wecannowdefinetheinternals of our previously sketched mapping v :D×P→I as a network of interlinked processing steps that transform data from one state to another across several stages as illustrated in Figure 2.15. It is this network that needs to be set up when designing interactive visual analysis solutions. As mentioned earlier, parameters are used to adjust and control the data transformation.Whilenotbeingdirectlyvisibleinourexample,theparameters are essentially part of the operators. Each operator in the network can make parameters available. These can then be set interactively by the user via a suitable user interface, or they can be set automatically based on the output of operators of the network. Defining reasonable default parameterizations is also a task to be dealt with during the design phase. Finally, note that the presented model is not restricted to a single input being transformed to a single output. In practice, it is not uncommon to deal withseveraldatasources,andthedataoftencontainsomuchinformationthat multiple images are needed to represent them in an understandable way. In our example in Figure 2.15, D consists of two data sources and I comprises actually two visual representations. Data Values Analytical Abstract. Visual Abstractions Image Data Operator State Data-oriented Graphics-oriented Figure 2.15 A network of operators describes the data’s transformation through several stages from data values to image data.
Criteria, Factors, and Models (cid:4) 47 Action Analysis Hypothesis Solution Knowledge Insight Knowledge Finding generation Exploration loop Verification loop loop Figure2.16 Knowledge generation model. Adapted from [Sac+14]. We have learned how data analysis solutions can be modeled as operator networks that transform data to interactive visual representations. In Chap- ters 3 to 5 of this book, we will learn more about concrete visual designs, interactive techniques, and computational procedures. For now, let us make a switch from the internals of the data transformation to studying how using interactive visual data analysis solutions leads to new insight and knowledge. 2.3.3 KnowledgeGeneration Developing useful data analysis solutions is essentially a human-driven effort. The transformation from data to expressive images takes place in the machine. Finally, it is again the human’s turn to make sense of the depicted data. What we are actually interested in is the gain in knowledge generated by interactive visual data analysis. This gain does not pop up all of a sudden, but is the outcome of an iterative process [vWij06]. Here, we cannot delve into the intricate mechanisms of the human brain, which, as already mentioned, are not yet fully understood. Instead, we con- sider the knowledge-generation model of visual analytics, which conceptually describes the interplay of the human and the analysis solution [Sac+14]. TheanalysissolutionisshownasasingleblockontheleftsideofFigure2.16. Despite being depicted here rather abstractly, the solution internally consists of all the interactively parameterizable data abstractions, processing steps, and visual encodings as outlined in the previous section. In this section, we want to concentrate on the model’s human part as illustrated on the right side of Figure 2.16. The human is connected to the analysissolutionviathreeloops.Theseloopssuggestthatknowledgegeneration takes place at different levels of sophistication. The first loop is the exploration loop, which primarily supports exploratory goals. The loop starts with observing the output generated by the analysis solution.Interestingfindingsmadeduringtheobservationwillprompttheuser to take actions. Interesting findings can be trends or recurring patterns, but also the inability to detect anything useful can trigger actions. Actions can be understood abstractly as adjustments of the data transformation process
48 (cid:4) Interactive Visual Data Analysis in terms of configuring the network of operators or altering the operators’ parameters. At some point, when enough findings have been accumulated, the knowledge-generation process will transition into the verification loop. The verification loop models the confirmatory phase of the data analysis. It starts with describing and explaining the findings to constitute insight in the senseofameaningfulinterpretationintheapplicationdomain.Theapplication domain is also the source of the hypotheses to be tested. As we gain more and more insight, new hypotheses can be formed or existing ones be confirmed or rejected. This involves carrying out actions to collect sufficient evidence. For example, additional calculations can be performed to check whether they produce similar results, or alternative visual representations can be generated to see whether they support the same conclusions. Finally,thethirdloopistheactualknowledge-generationloop.Atthispoint, we are on the verge of turning the accumulated insight into new knowledge of the application domain. Making this final step usually requires consultation withdomainexpertstodeterminewhethersufficientlystrongevidenceexiststo trust the analysis results. If this is not the case, either additional insight must be gained from the data, or new or altered hypotheses need to be established and tested. The three loops of exploration, verification, and knowledge generation give usbutanideaofhowinteractivevisualrepresentationsofdataaretransformed into something trustworthy and valuable. In practice, the reasoning process is rather spontaneous and does not follow strict deterministic rules. During the analysis, the human constantly switches between the three loops as new findings and insight lead to the formulation of new questions. For example, new knowledge may give rise to new hypotheses to be verified. In turn, the rejection of a hypothesis might initiate further rounds of exploratory activities. Withtheknowledge-generationmodelasoutlinedabove,wehavecompleted our study of the processes behind interactive visual data analysis. The three perspectives we have taken, the design perspective, the data-transformation perspective,andtheknowledge-generationperspective,provideuswithasound overview of the interplay of human and computer efforts when it comes to setting up, implementing, and using data analysis solutions. The variety of topics touched upon in this section suggests that experts from several fields should work together in order to make the data analysis a success, including experts from cognitive sciences, visualization, human-computer interaction, data mining, and from the problem domain itself. 2.4 SUMMARY The development of interactive visual data analysis solutions is a challenging endeavor. It requires observing several aspects. In this section, we focused on criteria, influencing factors, and process models. Below, we briefly summarize the key messages of this chapter.
Criteria, Factors, and Models (cid:4) 49 Criteria We established three fundamental criteria that interactive visual data analysis solutions should satisfy. The expressiveness criterion tells us that interactive visual representations must communicate the relevant information and must afford the necessary actions to generate knowledge. The effectiveness criterion demands that analysis solutions be aligned with the human visual and motor systems in order to be able to extract information and perform interactions effectively. Finally, the efficiency criterion suggests balancing benefits and cost of the data analysis. Influencing Factors In order to develop solutions that meet the aforemen- tioned criteria, we must take several influencing factors into account. First of all, the analysis is influenced by the data we are dealing with. We discussed several characteristics of data. At the level of data values, we distinguished data domains with different scales. Entire datasets are characterized by the data structure, the data space (with dimensions and attributes as dependent and independent variables, respectively), the data size, and the data scope. We also considered additional meta-data that describe the data’s evolution. Anotherimportantinfluencingfactoristhetaskstobeaccomplishedduring the data analysis. We characterized tasks by four facets: goals, questions, targets,andmeans.Thegoalcaptureswhatthegeneralpurposeoftheanalysis is. The questions describe concretely what should be answered by the analysis. The targets tell us where in the data we should focus our analysis effort. The means suggest that tasks can be accomplished in various ways by visual, interactive, or computational methods, or combinations of them. Finally, the user and the technology are important influencing factors. With regard to the users, we must observe general human factors, individ- ual backgrounds and expertise, the application domain, and whether users work individually or collaboratively. Regarding the technology, we need to considertheavailablecomputationalresources,displaydevices,andinteraction modalities. Process Models Three fundamental processes are at work in the context of interactive visual data analysis: the design process, the data-transformation process, and the knowledge-generation process. The design process can be modeled along four nested levels, in which domainproblemsarecollected,abstractdataandtaskdescriptionsarederived, appropriate visual, interactive, and computational means are composed, and suitable algorithms are implemented. The data-transformation process describes the way of the data from their raw form to interactive visual representations. Along the path, the data pass the stages of input data, analytical abstractions, visual abstractions and image data, the former ones being data-oriented, the latter ones being graphics- oriented. A network of operators is responsible for the actual data processing and data transformations.
50 (cid:4) Interactive Visual Data Analysis Finally, we introduced a model that abstractly outlines the knowledge- generation process by means of three intertwined loops. These loops subsume exploratory and confirmatory analysis activities culminating in the creation of new domain knowledge. In summary, this chapter presented the fundamental know-how that is necessary to develop interactive visual data analysis solutions. In the following chapters, we will describe in detail visual, interactive, and computational approaches that are useful for analytic purposes. FURTHER READING General Literature: [Mac86] • [vWij06] • [War12] • [Gua13] • [Mun14] Visual Analysis Tasks: [Vic99] • [Sch+13a] • [Bre16] • [KK17] • [LTM18]
3 CHAPTER Visualization Methods and Techniques CONTENTS 3.1 Visual Encoding and Presentation .......................... 54 3.1.1 Encoding Data Values .............................. 54 3.1.2 Presentation ......................................... 62 3.2 Multivariate Data Visualization ............................ 67 3.2.1 Table-based Visualization ........................... 67 3.2.2 Combined Bivariate Visualization .................. 69 3.2.3 Polyline-based Visualization ........................ 71 3.2.4 Glyph-based Visualization .......................... 73 3.2.5 Pixel-based Visualization ........................... 75 3.2.6 Nested Visualization ................................ 77 3.3 Visualization of Temporal Data ............................ 82 3.3.1 Time and Temporal Data ........................... 82 3.3.2 Visualization Techniques ............................ 86 3.4 Visualization of Geo-spatial Data .......................... 95 3.4.1 Geographic Space and Geo-spatial Data ............ 96 3.4.2 General Visualization Strategies .................... 99 3.4.3 Visualizing Spatio-temporal Data .................. 106 3.5 Graph Visualization ......................................... 111 3.5.1 Graph Data ......................................... 111 3.5.2 Basic Visual Representations ....................... 113 3.5.3 Visualizing Multi-faceted Graphs ................... 118 3.6 Summary .................................................... 124 V ISUAL representations of data are at the heart of this chapter. Theoret- ically, there are myriad ways of mapping data to visual representations. Some will lead to nice depictions of the data and might even have some aesthetic value, whereas others might be less successful. 51
52 (cid:4) Interactive Visual Data Analysis This suggests that the visual mapping is most crucial. Different mapping strategiesleadtodistinctvisualrepresentationsthatcommunicatethefeatures of the data quite differently [Han09]: “Therepresentationeffect:Humanperformance variesenormously(10–100:1)withdifferentrepresentations.” Hanrahan,2009 Therepresentationeffect isillustratedinFigure3.1.Itshowsthreeplots,all visualize the same data, a daily time series of the number of people diagnosed with an influenza-related illnesses. Yet, the three visual representations were generated using different visualization techniques and different parameteriza- tions. Figure 3.1a shows a linear plot. We can clearly identify some peaks in time and periods of moderate and low number of cases. It seems there is no clear trend over time. But are there any cyclic patterns? To answer this question, a different visualization technique can be employed: a spiral plot. The spiral plot in Figure 3.1b uses a cycle length of 32 days. From this figure, no cyclic patterns can be discerned. So let us try a different parameterization. In Figure 3.1c, the cycle length has been set to 28 days, which amount to exactly four weeks. As the weekdays are aligned now, we can see a clear cyclic pattern: At the beginning of a week more people are diagnosed and hardly any diagnoses are reported on weekends. This pattern is impossible to discern from the linear plot. On the other hand, the exact peak times are not as easy to identify in the spiral plot. By this example with quite simple data and rather basic visualization techniques, we have seen how important the design of visual representations is. Depending on what features of the data are to be communicated to answer which questions (e.g., Where are the peaks? Or, do cyclic patterns exist?), different options may be appropriate. In this chapter, we are interested in visual representations that fulfill the criteria of expressiveness, effectiveness, and efficiency as introduced in the previous chapter. To this end, it is important to learn basic visual building blocks and understand how to compile them into visual data representations. The two key concerns when designing a visual representation is to decide how to encode data visually and how to present the data meaningfully to the user. The fundamental ideas behind visual encoding and presentation will be described in Section 3.1. Once a suitable design has been devised, it can be implemented as a visu- alization technique. Visualization techniques typically make some assumptions with respect to the data for which they are applicable. Parameters may be available to fine-tune the visual representation within reasonable bounds.
Visualization Methods and Techniques (cid:4) 53 (a)Line plot. (b)Spiral plot (cycle length 32 days). (c)Spiral plot (cycle length 28 days). Figure3.1 Illustration of the effect of different visual representations. Because there are different classes of data, there are different visualization techniques, of which this chapter offers a data-oriented overview. First, we discuss basic techniques for visualizing multivariate data in Section 3.2. Then our view will be extended in Sections 3.3 and 3.4 to techniques for temporal data and geo-spatial data, which require the communication of a temporal and a spatial frame of reference, respectively. Graph data define relations among data elements, which calls for dedicated visualization techniques as we will see in Section 3.5. In short, we broaden our view step-by-step from basic visual encodings to the visualization of multivariate data attributes A, to techniques for data with temporal context T and spatial context S, to relations among the data R. This also includes different combinations of the individual aspects.
54 (cid:4) Interactive Visual Data Analysis 3.1 VISUAL ENCODING AND PRESENTATION In1967,basedonanextensiveanalysisofvisualrepresentationsincartography, Jacques Bertin introduced the idea of data graphics expressed by marks and visual variables [Ber67; Ber83]. Marks serve as the carriers of information. They are distinguished by their dimensionality. There are 0D points, 1D lines, 2D areas, and 3D bodies. Marks are the basic building blocks of visual representations. The actual information is conveyed via visual variables such as position, shape, or hue, which control the marks’ visual appearance. In other words, visualizingdatameanscreatinggraphicalprimitivesandspecifyingtheirvisual appearance according to the underlying data values. This concept of data graphics is still the fundamental basis of visualization. 3.1.1 EncodingDataValues Letusdiscussthevisualencodingwithvisualvariablesinmoredetail.Avisual variable can be understood as a graphical property that can be varied within a reasonable range. Different variations of a visual variable can be perceived as different. For example, we can discern position, shape, or hue of a mark. Originally introduced by Bertin, the concept of visual variables has later been refined and extended in several ways [Mac86; Mac95]. Figure 3.2 illustrates a selection of visual variables commonly applied today. Position Lightness Length Saturation Area Hue Angle Shape Figure3.2 Visual variables. With the help of visual variables, we can define the visual encoding, that is, how the data are represented visually. In this regard, two questions must be addressed: • What to map? • How to map?
Visualization Methods and Techniques (cid:4) 55 Quantitative Data Ordinal Data Nominal Data Position Position Position Length Lightness Shape Angle Saturation Hue Area Hue Lightness Lightness Length Saturation Saturation Angle Length Hue Area Angle Shape Shape Area Figure3.3 Effectiveness ranking of visual variables. Adapted from [MW14]. WhattoMap? First,wemustdecidewhichdatavariablesaretobemappedontowhichvisual variables. For answering this question, we need to consider the analysis task at hand, in particular the task’s target as discussed in Section 2.2.2. It tells us which data variables are deemed relevant and hence should be mapped onto the most effective visual variables. The question that remains is which visual variables are the most effective? As we will see next, the answer depends on both, human perception and the data domains introduced in Section 2.2.1. With the help of perceptual studies, researchers found that visual variables are differently effective for nominal, ordinal, and quantitative data [CM84; HB10]. Visual variables that allow us to make precise distinctions are particu- larly suited to encode nominal data. Visual variables that facilitate a visual ordering are well suited for ordinal data. For quantitative data, we need visual variables that support the estimation of proportions or differences. An effectiveness ranking of visual variables is illustrated in Figure 3.3. Interestingly, encoding to position is effective for all data domains. On the other hand, a visual variable’s rank may change drastically across the different data domains, as it is for instance the case for hue and shape, which are best applied for nominal data but are less or not effective for ordinal and quantitative data. HowtoMap? Once we have decided what to map, we must think about how the data are to be mapped onto a visual variable. As already indicated in Chapter 2, the task and the data at hand influence the mapping strategy. Figure 2.8 back on page 29 already illustrated the utility of different color mappings for different tasks. In the following, we will briefly look at further ways of tuning the visual mapping to the task and the data. For the sake of brevity, we will concentrate on the use of colors. Identifyingandlocatingdatavaluesonthedisplayarefundamentalanalysis tasks. In order to support identification tasks, perceptual color maps should be
56 (cid:4) Interactive Visual Data Analysis Individual Values Classes Identify Locate Figure3.4 Color maps for identifying and locating values and classes. 2015 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2015 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2014 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2014 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2013 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2013 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec (a)Color coding for identification tasks. (b)Color coding for location tasks. Figure 3.5 Applying the color maps from Figure 3.4 to temperature data. Adapted from bl.ocks.org/mbostock/4063318. used. Such color maps guarantee that perceived color differences are propor- tional to the underlying differences in the data [BRT95]. Ideally, there should be a clearly perceptible color for each value in the data. Certainly, the degree to which this ideal can be reached is limited by the output device and human perception. Forlocationtasks,thegoalistodeterminewhereinthevisualrepresentation a certain value or subrange of interest is located. This is supported by color maps that let the relevant data stand out. Again, the characteristics of human perception need to be considered. Particularly helpful are visual encodings that are pre-attentive [HE12]. For example, the relevant data are encoded with a signal color, whereas less-relevant parts of the data are visualized with dimmed colors. Another important factor relates to the question whether the analysis targets individual data values or classes of values. Continuous color maps are a sensible option for individual values. Segmented color maps are suitable for classes of values. Figure 3.4 provides a schematic illustration with color maps for identifying and locating individual values and classes of them. In Figure 3.5, we applied two of these color maps for the visualization of three years of daily temperatures. Figure 3.5a uses the continuous color map to supporttheidentificationofdatavalues,whereasFigure3.5busesasegmented color map to support the locating of a particular class of values, days with temperatures between ten and twelve degrees Celsius in our case. What we can see from these figures is how substantially different the data look like when using different color maps. What we can’t see is how the mapping works internally. This will be explained next.
Visualization Methods and Techniques (cid:4) 57 Data range d min d val d max Normalized range 0 t 1 Visual range v v v min val max Figure3.6 Basic mapping of a data variable onto a visual variable. Typically, the entire data range from the minimum value to the maximum valueismappedontotheentirespectrumofavisualvariable.Alinearmapping as expressed by the following formulas is common to assign a visual value v (a color in our case) to a data value d . In a first step, it is practical to val val calculate a normalized data value t∈[0,1]: d −d t= val min d −d max min This normalized t is then used in a convex combination to compute the actual visual representative v from the visual range [v ,v ]: val min max v =(1−t)·v +t·v val min max Figure 3.6 illustrates the basic mapping of a quantitative variable onto a continuous color map. Note that for qualitative data, it is necessary to first map categorical or ordinal values to numbers, for which dedicated methods exist [Ros+04]. The distinction between quantitative and qualitative data is not the only factor that influences the mapping strategy. In the following, we will illustrate this with additional examples. Asafirstexampleofadata-dependentadaptation,letusconsiderthescales showninFigure 3.7a.Scalesplayanimportantrolewhenitcomestointerpret- ingvisualrepresentations,thatis,whentheuserperformstheinversemapping of color to data. Plainly using the data’s actual minimum and maximum often leads to situations where the interpretation is unnecessarily complicated. As can be seen in our figure, expanding the mapping range [d ,d ] to multi- min max ples of two or powers of ten can significantly ease the interpretation [JTS08]. Several algorithms exist to create similar results [TLH10]. In addition to the data’s minimum and maximum, also the magnitude spanned by the value range influences the visual mapping. The basic mapping introduced before works well in many cases. However, for large value ranges that cover several magnitudes and include small as well as large values, a linear mapping could be ineffective. In such cases, it makes sense to use a non- linear mapping. Figure 3.7b compares a linear-scale mapping with a log-scale mapping. For the logarithmic variant, the calculation of t changes as follows: log(d −d ) t= val min log(d −d ) max min
58 (cid:4) Interactive Visual Data Analysis 294 300 10000 10000 400 400 197.8 200 8000 1000 320 320 280 101.6 100 6000 100 240 5.4 0 4000 10 160 120 -90.8 -100 2000 1 80 30 -187 -200 0 0 0 0 (a)Value range expansion. (b)Logarithmic mapping. (c)Box-Whisker mapping. Figure3.7 Enhanced data-dependent visual mapping. Variable A 0 20 Data ranges Variable B 10 50 Combined color map 0 50 Figure3.8 Combined color map for comparing two data variables. In situations where the analysis is focused on semantically meaningful ranges of values, it can even make sense to go beyond linear or logarithmic mapping.Figure3.7cshowsabasicmappingincontrasttoamappingthathas been adapted based on the data’s underlying distribution. In the example, the distribution is approximated via quartiles and inter-quartile ranges in the data as indicated by the box plot [Tuk77]. As a result, we obtain a visual mapping that emphasizes unusually low and high data values and outliers with shades of blue and red, whereas common values are represented by neutral white. As a last example, let us consider the visual mapping when the task is to compare data, for example, two data variables or data given at two time steps. The challenge for the visual mapping lies in balancing two conflicting needs. Ontheonehand,theindividualsetsbeingcomparedshouldbedistinguishable and values should be identifiable. This calls for a separate visual encoding per set being compared. On the other hand, the sets must be comparable. However, this is usually not the case when using separate visual encodings. Instead, a common encoding must be employed for all data, so that the same data value, no matter with which data variable or time step it is associated, is always visualized with the same visual stimulus. However, this could lead to local data variations being washed out in the visual representation. One way to address these conflicting needs is to combine multiple color maps into a dedicated comparison map [TFS08b]. Figure 3.8 illustrates this
Visualization Methods and Techniques (cid:4) 59 for comparing two data variables A and B. Variable A has lower values and is mapped using variations of green, whereas variable B has larger values represented by variations of blue. Shades of gray indicate the range of values thatbothvariableshaveincommon.Withthehelpofthecombinedcolormap, comparison is possible, while individual values are still distinguishable. Insummary,weseethatthedecisiononhowtomapdatavaluesontovisual variables must be done with care. What has been said here about color coding applies analogously to other visual variables. More details about color coding in general and specifically for different tasks and data characteristics can be found in the further readings at the end of this chapter. In the following, we continue with advanced mapping strategies in which multiple visual variables are employed. UsingMultipleVisualVariables A 1-to-1 visual mapping is the standard approach to visual encoding: A single data variable is mapped onto a single visual variable. Yet, it is also possible to utilize n visual variables simultaneously. There are two variants of this approach. A 1-to-n visual mapping takes advantage of the combined power of n visual variables for the encoding a single data variable. When multiple data variables are to be visualized, an n-to-n visual mapping has to be designed. Note that n-to-1 visual mapping makes no sense, because it would no longer be possible to unambiguously interpret a visual stimulus. 1-to-n Visual Mapping In general, we can use several visual variables to encode a data value. For example, we can set a mark’s position, size, and color according to one and the same data value. This typically makes the value easier to recognize. A particularly clever approach of combining n = 2 visual variables is two-tone coloring [Sai+05]. The goal of two-tone coloring is to achieve precise value recognition while keeping space demands low. Figure 3.9 illustrates the problem.Whenvisualizingdataasaplot,valuescanbereadprecisely.However, a certain amount of vertical display space is needed in order to make data values easily recognizable. Less display space is needed when the data are visualized via color coding. Yet, the data values are more difficult to read. With a continuous color map, it takes some time to figure out which value is associated with a color. With the segmented color map, only discrete classes of values can be identified. Let’s look at how two-tone coloring deals with these issues. The basic idea is to encode a data value with two colors and combine this withalengthencoding.Asshownintheenlargedviewontheright-handsideof Figure 3.9, a data value is visualized by two adjacent colors from a segmented color map, orange and red for the particular data value in our example. The colors tell us at a glance where in the value range the specific data value is
60 (cid:4) Interactive Visual Data Analysis 100 100 0 2/3 ≈ 83 0 20 40 60 80100 1/3 75 0 25 50 75 100 Figure3.9 Two-tone coloring explained. Adapted from [JTS08]. Figure3.10 Two-tone visualization of 20 years of daily temperatures. located. It is between 75 and 100. The precise value is encoded by varying the proportion of the colors, that is, how many pixels are colored with the first color and how many with the second color. In the example, 1/3 of the pixels are red and 2/3 are orange. This means the encoded value is one-third between 75 and 100, which is approximately 83. Figure 3.10 illustrates that the two-tone approach has several advantages. First, thanks to the combined use of color and length as visual variables, less display space is necessary, which allows us to overview more data. Second, colors make it easy to locate data of interest. Third, it is even possible to rather accurately identify individual values. In sum, a quite efficient visual encoding.
Visualization Methods and Techniques (cid:4) 61 05 05 150 10 0 50 0 50 (a)Mapping to position. (b)Mapping to area. 5 5 0 150 0 150 A B 10 10 C 2500 2500 0 0 0 50 0 50 (c)Mapping to color. (d)Mapping to shape. Figure3.11 Visual encoding via position, area, color, and shape. n-to-n Visual Mapping We already mentioned that it is often necessary to visualize more than one data variable. In such cases, the visual encoding can utilize several visual variables simultaneously. This shall be illustrated by the example of a scatter plot. A scatter plot consists of two orthogonally aligned axes that represent the valuerangesoftwodatavariables.Dotsareplacedinthespacespannedbythe axes in order to visualize the data elements. Conceptually, this corresponds to a mapping of data to position. A first data variable is mapped with respect to the horizontal x-axis, and a second variable with respect to the vertical y-axis. Figure 3.11a shows a basic example.
62 (cid:4) Interactive Visual Data Analysis In order to encode additional data variables, we can now employ additional visual variables. For example, we can vary the area of the dots to encode a quantitative data variable as in Figure 3.11b. With larger dots, it is also possible to use colors to visualize an ordinal data variable, which is illustrated in Figure 3.11c. We can even think of replacing the dots with different shapes to visualize a nominal data variable. In sum, Figure 3.11d now represents five data variables: two are mapped to position, one to area, one to color, and a fifth to shape. With each variable being added to the visual mapping, the richness of the visual representation is increased. Theoretically, we could add yet another visualvariable,forexample,bytexturingtheshapes.However,fromapractical point of view, there are limits. While a rich visual mapping opens up the possibility to make a wider range of analytic discoveries, the downside is that the mental effort required to digest the visual representation increases as well. Therefore, it is really important to balance the visual mapping according to the task and the data. When designing a visualization, the following quote can be useful to keep in mind [dSai39]: “Inanythingatall,perfectionisfinallyattainednotwhenthereisnolonger anythingtoadd,butwhenthereisnolongeranythingtotakeaway[..]” deSaint-Exupéry,1939 In fact, the visual encoding is one of the most crucial steps when designing visualdataanalysissolutions,becauseithasconsiderableimpactonexpressive- ness, effectiveness, and efficiency. However, a good visual encoding is only half the story. Of equal importance is an appropriate presentation of the encoding, as we will see in the next section. 3.1.2 Presentation The visual encoding produces marks whose graphical appearance meaningfully reflectsthedata.Themarksarecompiledintoviewstobedisplayedtotheuser. The question now is how to present the marks and views in such a way that thevisualizeddataareeasytounderstand.Inthefollowing,wewilldiscusskey questions that need to be taken into account when designing the presentation. Shouldthemarksbepresentedin2Dor3D?Whendealingwithlargenumbers of data elements, should we display the marks for all of them or should we concentrate on subsets of interest? And, how can we present multiple views each dedicated to showing a particular aspect, perspective, or facet of the data?
Visualization Methods and Techniques (cid:4) 63 PresentingMarksin2Dor3D The first design decision is whether to place the information-bearing marks in a two-dimensional (2D) or a three-dimensional (3D) presentation space. A 2D presentation space has two independent axes: the horizontal x-axis and the vertical y-axis. A 3D presentation space has an additional third z-axis. Both 2D and 3D representations have different pros and cons. Two- dimensionalvisualizationsarearguablymoreabstractandeasiertoexplore.On the other hand, human perception is naturally tuned to the three-dimensional world around us. Moreover, the third dimension can also serve as a carrier of additional information. However, the third dimension also introduces difficul- ties that are less problematic or do not even occur in 2D, such as occlusions and perspective distortions. There is no definite answer whether to use 2D or 3D. The decision has to be made carefully on a case-by-case basis, taking into account several factors, including the number of marks to be arranged, the analysis task, and the available display technology. Figure 3.12 shows an example 3D terrain visualization with a 2D overview map. We will return to the implications of 2D and 3D representations later in Section 3.4 when we talk about geographic visualization in more detail. Figure3.12 Terrain visualization with overview+detail. Courtesy of Martin Röhlig. PresentingAllDataorDataofInterest In order to facilitate a comprehensive understanding of the data being studied, two essential communicative goals must be supported by the presentation: 1. Convey an overall picture 2. Provide details where necessary
64 (cid:4) Interactive Visual Data Analysis The overall picture allows users to observe global patterns and general properties of the data. The details enable users to study local particularities of the data. For data of small or moderate size, overview and details for all data can be presented in a single view. However, with larger data, this becomes more and more difficult. For large data, conveying an overall picture usually requires neglecting details and giving preference to the presentation of the data as a whole. When details are to be presented, it is usually necessary to sacrifice completeness and concentrate on selected data. A basic strategy to address this problem is to distinguish between data of interest and less relevant data. The idea is that marks encoding the data of interest will be presented in full detail, while the others are reduced or simplified. Two approaches support this strategy: overview+detail and focus+context [CKB08]. Overview+Detail presents an abstracted overview of all data in one view and selected details in separate views. There can be a small overview being superimposed on a large detail view, as commonly seen for geo- graphic maps. Alternatively, a large overview can be superimposed by multiple detailed views. In Figure 3.12, the main view shows a detailed representation of the 3D terrain for a selected region, while a smaller window provides an abstracted 2D overview and highlights the region whose details are currently visible. Focus+Context integratesfocuseddetailswithinaglobalcontextinasingle view.Theintegrationcanbeachievedindifferentways[Hau06].Aclassic approach is to use distortions to magnify the focus, scale down the context,andcreateasmoothtransitioninbetween.Fish-eyedistortionis a prominent example [LA94]. Figure 3.13 illustrates a fish-eye distortion being applied to the rows of a table-based visualization of the Iris flower dataset.Ascanbeseen,thedistortionmakesindividualrowsinthefocus region easier to read and even allows text labels to be displayed. (a)Regular visualization. (b)Focus+context distortion of rows. Figure3.13 Illustration of focus+context for a table-based visualization of the Iris flower dataset. Focused rows are magnified to accommodate labels.
Visualization Methods and Techniques (cid:4) 65 Both overview+detail and focus+context are viable approaches to present- ing the overall picture and details of larger data. For overview+detail, the user has to establish links between the separate views mentally. The integrative approach of focus+context relieves the user of this task. However, interpreting a densely packed focus+context presentation might require some training, especially when distortions are involved. PresentingMultipleViewsinSpaceorTime When the data to be analyzed become more complex, it is no longer feasible to indiscriminately present each and every aspect of the data in a single view. When we reach this point, it makes sense to create several dedicated visual representations, each focused on communicating a particular aspect or facet of the data. The question is how several such views can be presented to the user in order to convey a comprehensive picture? There are two basic answers to this question: The views are presented either in parallel side-by-side or sequentially one after the other. Conceptually, this corresponds to an arrangement of views in space or a sequencing of views in time. What this means concretely will be described next. Arranging Views in Space Showing several faceted visual representations in parallel side-by-side is commonly referred to as multiple views visualiza- tion [WWK00]. Later, the term coordinated has been prepended to indicate that the views are not used in isolation but in concert [Rob07]. In particular, coordination means that interactions performed in one view are automatically propagated to all other views. An example of three different views for visualizing the same multivariate dataisshowninFigure3.14.Detailsaboutthedepictedvisualizationtechniques will be given very soon in Section 3.2. How multiple views can be employed for graphvisualizationwillbeexplainedattheendofthischapterinSection3.5.3. For the time being, we are interested in the spatial arrangement of views. Therearetwoextremalpositionswhenitcomestodefiningthespatialarrange- ment.Ontheonehand,onecoulduseafixedandprovablyefficientarrangement designedbyanexpert.Ontheotherhand,userscouldbeprovidedwiththefull flexibility to arrange views arbitrarily. Both extremes have their advantages and disadvantages, and both are actually applied in practice. An interesting intermediate alternative is to allow flexibility only within certain limits. For example, it can make sense to prohibit partial overlap of views. In other words, a view should be either visible or not. This can be achieved by partitioning of the available screen space into regions, each of which may contain one or more views. While views are allowed to be flexibly resized and moved from one region to another, the overall arrangement is constrained to remain a partition, which is free of partial overlap. Arranging multiple views on a single display is usually a task that can be accomplished with little or moderate effort. However, when views have to be
66 (cid:4) Interactive Visual Data Analysis Figure3.14 Multiple coordinated views for analyzing multivariate data. arranged on multiple displays, the costs for defining a suitable spatial layout increase significantly. In such scenarios, users should be supported by means of algorithms that automatically distribute and lay out views on the available displays. Later in Chapter 6, we will describe such an automatic arrangement approach in detail. Sequencing Views in Time The alternative to arranging views in space is to sequence them in time. That is, instead of showing multiple views simultaneously,theyarepresentedstep-by-step,oneaftertheother.Depending on how many views are shown to the user per time unit, a sequencing of views may generate the impression of a slide show or an animation. The advantage of sequencing views in time is that each view can fully utilize the display space. There is no need to divide the space among views. Obviously, sequencing views in time is particularly suited to convey temporal characteristics of data. It can also be helpful to take the user on a journey from one data facet to another. However, presenting views in quick succession to the user also has some limitations.Forexample,itcouldbedifficulttomakesenseofalltheinformation provided during a sequence of views. Especially when sequences take a long time, users may be unable to follow and could drown in an indigestible flood of visual representations. Therefore, it is mandatory to provide interactive controls to pause, slow down, reverse, and advance the presentation. In summary, this section dealt with the basics of visual encoding. We discussed how data can be mapped to visual variables of marks, and how the marks can be presented to the user. With the general aspects of visualization being clear now, we will next study how specific techniques utilize visual variables and arrange visual marks for visualizing different kinds of data. We
Visualization Methods and Techniques (cid:4) 67 will start with techniques for multivariate data, that is, data with several data attributes. 3.2 MULTIVARIATE DATA VISUALIZATION We have already seen several visualization examples in this chapter that are univariate in the sense that they show the data values of only one dependent data variable, for instance, the line plot and the spiral plot in Figure 3.1 or the temperature representation in Figure 3.10. However, inspecting only one dependent variable is often not enough. In many application contexts, it is necessary to discern and understand the interrelations and dependencies between multiple or even all of the dependent variables. To this end, we need multivariate visualization techniques. Multivariate data visualization concentrates on the depiction of the depen- dent variables of a dataset, that is, the attributes A. In the following, we will introduce fundamental classes of multivariate visualization techniques. Each class is characterized by its own fundamental visualization strategy. We will consider: • table-based visualizations, • combined bivariate visualizations, • polyline-based visualizations, • glyph-based visualizations, • pixel-based visualizations, and • nested visualizations. Independent variables, including time T and space S, and relationships R between data elements will be taken into account in later sections of this chapter. 3.2.1 Table-basedVisualization As detailed in Section 2.2.1, multivariate data are generally modeled as data tables where columns accommodate attributes, and rows represent data tuples. It is common to represent such data using tabular spreadsheets, which show data values as text in the spreadsheet cells. As such, spreadsheets are well- suited for reading and editing data values precisely. However, understanding the multivariate relations of the data is hardly possible. Moreover, the textual representationofdatavaluesrequiresconsiderabledisplayspacepercell,which limits the number of data tuples that can be shown. The simple, yet very effective idea of table-based visualization is to retain the tabular layout of spreadsheets, but to replace the textual representation of
68 (cid:4) Interactive Visual Data Analysis Figure3.15 Two-tone colored table-based visualization of the Cars dataset. Figure3.16 Table Lens with textual labels for focused data tuples. data values by a visual representation. A visual representation will not only maketheinterpretationofthedatamucheasier,itwillalsorequirelessdisplay space. Depending on the data domain of an attribute, different visual encodings are practical. A universally applied strategy is to color-code the table cells. An alternative for quantitative data is to embed bars into the table cells and to vary the bars’ length depending on the data values. It is also possible to combine length and color analogous to the two-tone approach introduced in Figure 3.9 back on page 60. An example of a table-based visualization with two-tone coloring is given in Figure 3.15. The image shows seven properties of about 400 cars in the Cars dataset sorted by miles per gallon (MPG). As we can see, table-based visualization can provide us with a general overview of the distribution of multivariate data. On the other hand, while we gain an overview, we lose details: The exact data values are no longer available as textual labels. The Table Lens is a classic focus+context technique to cope with this problem [RC94]. For a focused subset of the data, the Table Lens shows larger rows and inserts exact values in textual form. The remainder of the table, that is, the context remains unchanged. Figure 3.16 shows an example visualization of demographic data of 111 countries. The height of table rows is varied by means of a fish-eye transformation function so as to create a smooth transition between focus and context.
Visualization Methods and Techniques (cid:4) 69 A table-based visualization is also compatible with the common operations available with spreadsheets. One can reorder the table columns and sort the rows according to attribute values. While this is quite helpful for studying individualattributes,findingmultivariaterelationshipsisstillnotan easy task. The data must be sorted successively on a column-by-column basis, and the obtained results have to be integrated mentally by the user. To reduce the required effort, it makes sense to incorporate methods that sort rows based on multivariate similarities. One such method will be presented in Section 5.4.2 of Chapter 5, which is dedicated to automatic analysis support. 3.2.2 CombinedBivariateVisualization An alternative to table-based approaches is to visualize multivariate data by combining several bivariate displays. To this end, individual bivariate displays are created for all pairs of attributes (a ,a ):a ,a ∈A,i=6 j. The bivariate i j i j displays are then combined to facilitate an overview of the entire data. The combination can be a spatial arrangement or a temporal sequence of views. Spatial Arrangement Among the earliest and most widely used combi- nations of bivariate displays is the scatter plot matrix [Cle93]. For m attributes, the matrix is composed of m2−m individual scatter plots, as illustrated in Figure 3.17. There are m−1 plots per row and per column. Each scatter plot consists of two orthogonal axes that represent two attributes, and dots visualize the data tuples with respect to the two data attributes. The scatter plot at (i,j) visualizes the attributes a and a . The scatter plot at (j,i) shows the same data but with the i j axes swapped. There is usually no scatter plot at (i,i), because it hardly makes sense to plot attribute a against itself. Instead, the space is often i used to show the value distribution of a or simply just a label. i The rows and columns of a scatter plot matrix can be re-arranged to support different tasks. For example, it can make sense to bring relevant attributes to the top left or to show correlated attributes next to each other. Temporal Sequencing Instead of using a spatial arrangement as a matrix, it is also possible to create an animated sequence of scatter plots, which is known as the Grand Tour [Asi85]. The idea is to show the individual scatterplotsoneaftertheother.Thesequenceofplotsiscarefullychosen sothatthemostimportantmultivariaterelationshipsarerevealedduring the animation. When working with combined bivariate visualizations, it is important to realize that each bivariate display communicates the data values only with respect to two attributes. Hence, finding an outlier or a correlation in either bivariate display does not tell us whether the finding is indeed a multivariate
70 (cid:4) Interactive Visual Data Analysis Figure3.17 9×9 scatter plot matrix of meteorological data. Color is used to ease the recognition of data variables. feature.Inordertohelpusersdrawtherightconclusions,additionalmechanisms need to be integrated. Onesuchmechanismisinteractivebrushing & linking.Itenablestheuserto mark dots in a bivariate display, and in this way to select data tuples. All the dots that represent data values of the selected tuples are then automatically highlighted in all bivariate displays. So, if the user marks an outlier in one displayandseesthatthesubsequentlyhighlighteddotsareoutliersintheother displays as well, then it can be assumed that indeed a multivariate outlier has been found. More details about data selection and visual highlighting are provided in Section 4.4 of Chapter 4.
Visualization Methods and Techniques (cid:4) 71 3.2.3 Polyline-basedVisualization A difficulty with combined bivariate visualizations is that the connection between the individual displays has to be established by the observer mentally. That is, as the eyes move from one bivariate display to the next, the observer hastokeeptrackofthevisiteddotsinordertoformacompleteunderstanding of data tuples. Visualization techniques based on polylines aim to tackle this difficulty. The basic strategy is to create m axes, one for each attribute, and n polylines, one for each data tuple. The polyline of an m-variate data tuple is constructed as follows. For each attribute value of the data tuple, a position is computed at the corresponding attribute axis. The m positions that we obtain are then connected to form the polyline that represents the entire tuple. Thequestionthatremainsishowtheaxesarearrangedonthedisplay.Par- allel coordinates plots are known for their parallel arrangement of axes [Ins09]. For radar charts, the axes are not parallel, but emanate from a central point. Figure 3.18 illustrates both the parallel and star-shaped arrangement of axes for the same multivariate data. Axes can also be arranged in 3D, and in Section 3.3, we will see that dedicated arrangements exist for temporal data. (a)Parallel coordinates plot. (b)Radar chart. Figure3.18 Visualization with polylines across parallel and star-shaped axes. It is worth mentioning that the order in which polylines connect the axes is important, because patterns in the data can be best interpreted between neighboring axes. Let us look at some of the visual patterns that are created by polyline-style visual representations. Consider the pairs of parallel axes in Figure 3.19. The visualization shows several distinct patterns synthetically created for the purpose of illustration. The first pair of axes shows a positive correlation. How a negative correlation looks like can be seen in the second pair of axes. The third and fourth pair show a peak and a valley, respectively. Finally, there are a few local outliers in the right-most pair of axes. It is interesting to think about how the same data would look like when being visualizedasindividualscatterplots.Ifyouwanttocheckifwhatyouimagined is correct, Figure 3.20 provides the answer.
72 (cid:4) Interactive Visual Data Analysis 100 100 200 100 100 200 100 90 90 180 90 30 90 190 90 45 80 250 80 160 80 22 05 80 11 78 00 80 40 70 200 70 140 70 15 70 160 70 35 10 60 60 120 60 60 150 60 30 5 50 150 50 100 50 0 50 140 50 25 40 40 80 40 -5 40 130 40 20 30 100 30 60 30 -- 11 50 30 11 12 00 30 15 20 50 20 40 20 -20 20 100 20 10 10 10 20 10 -25 10 90 10 5 0 -30 80 Reference Posi�ve Reference Nega�ve Reference Peak Reference Valley Reference Outlier Figure3.19 Visual patterns between pairs of parallel axes. Posi�ve Nega�ve Peak Valley Outlier 200 200 1122 0505 0000 11111 02468 6800000 00 -- 1111223 - 5005050 505 111111111 123456789 000000000 1223344 5050505 50 40 -20 100 10 0 20 -- 32 05 89 00 5 102030405060708090100 102030405060708090100 102030405060708090100 102030405060708090100 102030405060708090100 Reference Reference Reference Reference Reference Figure3.20 The same data as in Figure 3.19 visualized as scatter plots. Axes-basedVisualization When thinking more deeply about scatter plots, scatter plot matrices, and parallel coordinates plots it becomes clear that they all follow the same under- lying strategy: They are all based on axes with respect to which data points are projected [CvW11]. Many other types of diagrammatic representations follow this strategy [Har96]. As such, axes-based visualization is a universal concept that is applicable to a wide range of multivariate analysis questions. A difficulty though is that tuples with the same data values are projected to the same positions. This leads to over-plotting. The problem is that we typicallycannottellhowmanytuplesarerepresentedbyalinesegment(ordot or pixel in general). This problem can be remedied in different ways. A simple option is to use transparency. Another option is to incorporate additional visual cues to visualize the frequency of data values. Histograms are quite usefulinthisregard.Theycanbeeasilyattachedtotheaxesofscatterplotsor parallel coordinates plots. Figure 3.21 shows an example. Later in Chapter 5, we will return to the problem of reducing over-plotting and clutter in visual representations by means of additional automatic computations.
Visualization Methods and Techniques (cid:4) 73 Figure3.21 Parallel coordinates with histograms showing demographic data. 3.2.4 Glyph-basedVisualization Axes-basedvisualization,asdiscussedbefore,emphasizesrelationshipsbetween two attributes. In contrast, glyph-based visualization aims to emphasize indi- vidual data elements. To this end, data elements are visualized as small self-contained graphical objects called glyphs. Each glyph visually encodes a multivariate data element, more precisely, it encodes the attribute values associatedwithadataelement.Settingupaglyph-basedvisualizationrequires three steps: 1. Glyph design 2. Attribute-to-glyph mapping 3. Glyph placement Glyph Design The design of glyphs is an intricate process [Bor+13]. The particular challenge is to encode several data attributes, although there is only very little display space available per glyph. Severalquestionsneedtobeanswered:Howmanyattributesshouldaglyph represent? How many individual values should be discernible per attribute? Which visual variables should be used? How should the visual encodings be intertwined and the overall glyph be compiled? When answering these questions, it must be ensured that a glyph can be easily perceived as a whole, and that multiple glyphs can be easily separated from each other. Only then can a glyph-based visualization be interpreted as individual data elements to be studied as such and compared to others. Figure 3.22 illustrates a couple of classic glyph designs. The autoglyph in Figure 3.22a consists of a grid of color-coded cells, one cell per attribute. Stick figures as in Figure 3.22b vary the length, thickness, color, and angle of limbs in order to visualize data. The Chernoff faces in Figure 3.22c encode data by varying the features of a human face, such as the expression of the mouth or the eyes or the shape and size of the nose.
74 (cid:4) Interactive Visual Data Analysis (a)Autoglyph. (b)Stick figures. (c)Chernoff faces. Figure3.22 Examples of classic glyphs for visualization. Good Regular Bad Figure3.23 Corn glyph for representing six ordinal data values. Attribute-to-Glyph Mapping The second step is to carefully decide which data attributes should be visualized with which visual encoding offered by a glyph. We already discussed this mapping problem in general in Section 3.1.1. For the case of glyph-based visualization, there is one aspect that needs to be considered in particular: Usually, glyphs can encode only a few distinct values due to their limited size. As a consequence, it can be necessary to reduce the attributes’ value ranges. For example, a continuous attribute may need to be discretized to only a few values before it can be mapped to a glyph. What this means shall be illustrated by a glyph for visualizing factors influencing maize harvest. The design is based on three pictorial corncobs: a particularly well-developed exemplar, a regular cob, and a withered cob as shown in Figure 3.23. These three depictions represent three ordinal values: good, regular, bad. By cutting the corn glyph into segments, it is possible to encode several data values. However, any attribute that we would like to map to a segment of the corn glyph must first be made compatible with the ordinal scalethattheglyphcandisplay.Thiscanbedoneinaclassificationstepwhere numeric data values are assigned to classes. Glyph Placement Finally, the placement of glyphs is important. Placing glyphs on grids is useful in general. Figure 3.24a provides an example with data about the resistance of bacteria against eight antibiotics. The binary data (resistent, non-resistent) are represented as a grid of autoglyphs, where black
Visualization Methods and Techniques (cid:4) 75 (a)Autoglyphs as grid (aka. shape coding). (b)Corn glyphs on a map. Reprinted from [SM00]. Reprinted from [NSS05]. Figure3.24 Examples of different placements of glyphs. means resistant and white means non-resistant. Such representations provide a nice overview of the data, which makes it possible to spot patterns, as for example, the glyphs showing no resistance at all (all cells are white) or glyphs showing a T-shape. Glyphs can also be placed more flexibly according to data criteria. For example, the glyph placement can be computed based on a semantic grouping of data elements [War02]. If spatial dependencies exist in the data, glyphs are typically arranged so as to communicate these dependencies. Figure 3.24b showsanexamplewithcornglyphsplacedonamap[NSS05].Theuseofglyphs for data with spatial dependencies will be discussed further in Section 3.4 3.2.5 Pixel-basedVisualization In contrast to glyph-based visualization, pixel-based visualization appears to be a minimalistic approach. The key idea is as simple as to create a representation where each pixel visualizes exactly one data value by its pixel color [KK94]. Yet, what appears to be minimalistic is quite the opposite. Pixel- based representations are very compact, which makes it possible to display millions of data values. Three design questions need to be answered for pixel-based multivariate data visualization: • How should individual data values be mapped to color? • Where should the pixels for multiple attributes be located? • How should the individual pixels be arranged? The first question is particularly important. A pixel is the smallest possible graphical primitive on a computer display. In fact, a pixel is so small that identifyingitscolorcanbedifficult.Moreover,apixelistypicallynotperceived
76 (cid:4) Interactive Visual Data Analysis Avg. temp Min. temp Max. temp Wind speed Pressure Precip. January February March April May June July August September October November December 1893 2000 1893 2000 1893 2000 1893 2000 1893 2000 1893 2000 -21.4 -26.8 -16.6 0.0 9554.0 0.0 -10.3 -14.5 -6.3 1.1 9747.5 0.5 0.9 -2.2 4.0 2.2 9941.1 1.1 16.5 11.9 22.3 7.0 10123.7 5.4 23.1 16.7 30.7 16.2 10267.4 55.6 29.6 21.5 39.1 25.3 10411.0 105.7 Very low Low Mean High Very high Figure 3.25 Pixel-based visualization of daily values of six meteorological attributes collected for more than hundred years in the city of Potsdam. Courtesy of Thomas Nocke. in isolation, but together with its surrounding context. As a consequence, our perception is influenced by contrast effects. Therefore, pixel-based approaches should employ color maps with well-discriminable colors. The second question is about deciding where the pixels of a particular attributeshouldbeshown.Acommonapproachistopartitionthedisplayinto regions, each being associated with one attribute. Figure 3.25 demonstrates this for meteorological data with 6 attributes. For each attribute, there is a separate rectangular region filled with colored pixels. Finally,thethirdquestionregardsthearrangementofpixelswithinaregion. In general, there are different options for arranging the pixels: • Element-wise: The pixels are simply arranged in the same order as data tuples appear in the dataset. • Pre-determined: The pixel arrangement is determined by an independent variable of the dataset. For example, temporal dependencies can be revealed by arranging pixels with respect to time. • Attribute-based: The pixels are sorted according to the data values of a particular attribute. Multivariate correlations can be revealed by such attribute-based arrangements.
Visualization Methods and Techniques (cid:4) 77 Selec�on Year Quarters Months Weeks Days Figure 3.26 Exploding a pixel-based visualization by applying a stepwise separation of a time-oriented recursion pattern. Adapted from [LS07]. • Query-based: The pixels are arranged according to the distance of their associated data tuple to a query specification. This way, relevant values (with respect to the query) are grouped and appear at the front of the arrangement. OurexampleinFigure3.25usesapre-determinedarrangementwithrespect totime.Wecaneasilyseethatthefirstthreeattributesarestronglyinfluenced by the seasons of the year. The fourth and fifth attributes show a dependency on seasons as well, though less pronounced. The last attribute seems to be independent of time. Apparently,usingpixelasinformation-bearingprimitivesisverywellsuited to create overviews of multivariate data. Yet, details are not so easy to access. One way to remedy this is to use exploded views, which are widely applied in the context of technical drawings. The idea is to add details to complex graphics by separating individual parts, showing these parts by enlarged views, and enriching them with annotations. As illustrated in Figure 3.26, the idea of exploded views can also be applied to pixel-based visualization [LS07]. In the example,auser-selectedpartofarecursivetime-orientedarrangementofpixels isexploded.Asteppedanimationshowsasequenceof“explosions”thattransfer the compact and dense pixel selection into a calendar-style representation. Details are now much easier to discern. 3.2.6 NestedVisualization The final class of techniques for multivariate data visualization to be described in this section is nested visualization. The previous example from Figure 3.26 already involved a nested recursive arrangement of pixels, whereas the other visualization techniques discussed so far had a flat arrangement. The basic idea of nested visualization is to divide the attribute space into subsets and to spatially nest the subsets on the display. This involves two steps:
78 (cid:4) Interactive Visual Data Analysis 1. Definition of attribute subsets and nesting order 2. Embedding of subsets within subsets Definition of Attribute Subsets and Nesting Order Inthefirststep,three aspectshavetobeconsidered.First,thedatadomainoftheattributesmustbe madecompatiblefornesting. Nested visualizations require nominal, ordinal,or discrete data domains of small cardinality. Otherwise, a nesting is impractical or cannot be carried out at all. Second,theattributesubsetshavetobespecified.Allsubsetsmusthavethe same size. Typically, there are not more than three attributes per subset. It is importanttonotethatdifferentsubsetsleadtodifferentvisualrepresentations, which in turn, communicate different aspects of the data. Third, the nesting order of the subsets must be defined. In other words, it has to be declared which subset should be nested into which other subset. The nesting order determines which attributes are primary in the visual representationandwhicharesubordinate.Usually,thenestingisrathershallow with hardly more than four levels. The reason is that the deeper the nesting, the harder it is to interpret the visual representation. Embedding of Subsets within Subsets Once the attribute subsets and their nesting order have been set up, the next question is how to embed them spatially on the display? Usually, the embedding depends on the size of the attribute subsets. Different techniques exist for subsets with one, two, and three attributes. Mosaic plots are suited for subsets with only one element, for example (a ),(a ),.... For the first step, the display area is split along the horizontal 1 2 axis into rectangular segments. The number of segments corresponds to the numberofdistinctvaluesofa ’sdomainandthesizeofthesegmentsrepresents 1 the frequency of the values in a ’s value range. For the second step, each of 1 the rectangular segments is split along the vertical axis with respect to a . 2 The procedure of splitting along the horizontal and vertical axes continues for all attributes in the nesting order. At the end, the rectangular segments of a mosaic plot represent the value distribution of the data. Figure 3.27 shows an example with the distribution of surviving passengers of the Titanic with respect to class and sex. Ifthenestingorderconsistsofpairsofattributes(a ,a ),(a ,a ),...,then 1 2 3 4 the embedding scheme of dimensional stacking can be used [LWW90]. The basic idea is to embed grids within grids. The dimensional stacking starts with dividing the display into a grid of |a |×|a | uniformly sized cells, where 1 2 |a | denotes the number of distinct values in a ’s domain. The next step is i i to subdivide each grid cell into a new grid of dimensionality |a |×|a |. The 3 4 procedure continues for all pairs of attributes. In order to actually visualize multivariate data, the grid cells are color-coded according to the frequency of data tuples that exhibit the value combination of the corresponding cell.
Visualization Methods and Techniques (cid:4) 79 ts1 dn2 dr3 werC Sex Male Female ssalC oN seY oN seY oN seY oN seY devivruS Figure3.27 Mosaic plot visualizing the survival of passengers of the Titanic. As an example of dimensional stacking, Figure 3.28 visualizes data about resistance to antibiotics as mentioned earlier. The data consist of eight binary attributes indicating non-resistance or resistance. We can easily see that most of the cells are empty, meaning that no data tuples exist that exhibit the correspondingresistancepattern.Wecanalsoeasilyidentifythefrequentvalue combinations indicated in red and the most frequent resistance pattern in the top left corner, which represents no resistance at all. Withmosaicplotsanddimensionalstackingwehavediscussedstrategiesfor attributesubsetswithoneandtwoattributes,respectively.Iftheattributesub- sets are triples, the worlds-within-worlds approach can be used for embedding data into a 3D display space [FB90]. A nesting order with triples could look like this: (a ,a ,a ),(a ,a ,a ),.... The three attributes of the first subset 1 2 3 4 5 6 definethethreeaxesofa3Dcoordinatesystem.Withinthiscoordinatesystem, a particular point is fixed by user selection. This point serves as the origin for embedding a new 3D coordinate system spanned by the second triple of attributes. The process of fixing a point and embedding a coordinate system is repeated for all triples. The coordinate system defined in the last step serves to display the actual data values of the attribute subset at the end of the nesting order. In other words, only three attributes are visualized directly, while the values of the
80 (cid:4) Interactive Visual Data Analysis a a a a a 7 n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. 8 6 4 2 a non-res. resistant non-res. resistant non-res. resistant non-res. resistant 5 a non-resistant resistant non-resistant resistant 3 a non-resistant resistant 1 Figure3.28 Dimensional stacking of eight binary data attributes. Adapted from [SM00]. remaining attributes are expressed by the nesting process. As such, the worlds- within-worldsapproachrepresentsonlyaparticularpartofthedataasspecified by the sequence of coordinate origins. From the previous paragraphs we see that nested visualization of multi- variate data is a powerful approach. However, the visual representations are not trivial to construct, and hence, require some training in order to be able to interpret them. Summary With the description of nested techniques, we conclude this section on visu- alization of multivariate data. As we discussed quite different visualization strategies for multivariate data, let us briefly summarize them. • Table-based visualization transforms common spreadsheets into visual representations. Sorting the table rows allows us to detect multivariate correlations, outliers, and clusters. A focus+context display of the table rows makes it possible to identify individual data values. • Combined bivariate visualization is suited to communicate two- dimensional value distributions, bivariate correlations, clusters, and out- liers. Multivariate relationships can be explored with interactive linking & brushing. Additional visual encodings can be used to represent the frequency of data elements as well.
Visualization Methods and Techniques (cid:4) 81 • Polyline-based visualization shows data elements as polylines across axes. The exploration of two-dimensional value distributions between two neighboring axes is well supported. The analysis of all bivariate relationships in the data requires rearranging the axes. Data frequencies can be visualized by embedding histograms. • Glyph-based visualization encodes data tuples as small glyphs. The goal is to provide an overview of the data, while details are typically omitted. As such, glyphs facilitate a rough estimation and comparison of the properties of data elements. Glyphs are also suitable for visualization of data in a spatial frame of reference. • Pixel-based visualization approaches encode each data value by a single color-coded pixel. Thanks to this very space-efficient encoding, even very large datasets can be visualized. Yet, details cannot be discerned easily, unless additional interactive or visual means are employed. • Nested visualization is based on a recursive embedding of attribute subsets on the display. The resulting visual representations are very well suited to communicate frequencies of the different value combinations in the data. Yet, the interpretation of nested visualizations is not easy, particularly when the nesting is deeper. Intermsofthebasicvisualdesignoptionsdiscussedinthebeginningofthis chapter, we can conclude the following. The described visualization strategies for multivariate data typically use spatial arrangements in 2D. There are techniques that use 3D spatial arrangements, such as the worlds-within-worlds approach, and also techniques that generate an animated sequence of views, such as the grand tour, but in general 2D spatial arrangements are much more widely used. Already at this point, we can further conclude that visualization requires interactiontotakefulladvantageofthepowerofvisualrepresentations.Interac- tionallowsus,forexample,toidentifydataondemandortoexplorealternative correlations through rearrangements. In fact, interaction is so important that the entire Chapter 4 is dedicated to this topic. Moreover, we have mentioned that certain visualization strategies can only be applied when the data domain contains not too many values. We also briefly mentioned the problem of over-plotting when there are very many data elementstobevisualized.Inthesecases,additionalcomputationsarenecessary to condense the data before they can be transformed into meaningful images. We will discuss this topic in more depth in Chapter 5. All in all, this section on multivariate data visualization focused on the dependentvariables,theattributesA.Inthenexttwosections,wewillconsider the independent variables time T and space S.
82 (cid:4) Interactive Visual Data Analysis 3.3 VISUALIZATION OF TEMPORAL DATA Time is an exceptional dimension. Virtually everything around us is governed by the steady progress of time. So it comes as no surprise that much of the data that people seek to understand are connected to time. In this section, we add the dimension of time T to our considerations. In other words, we are interested in methods and techniques for visualizing time and temporal data, where the latter primarily means communicating the dependency of data attributes on time T → A. As we will see in a moment, time and data that depend on time are quite special and require dedicated techniques for their visualization. 3.3.1 TimeandTemporalData Time is not just another data attribute. Time has several properties that need to be taken into account when visualizing data that are connected to time. In the pages to come, we will briefly characterize time and temporal data. CharacterizingTime Philosophers have pondered the concept of time for ages. Here we want to concentrate on aspects of time that are relevant for the visual analysis of temporal data. Before going into the details of time, there is a more general aspect to be mentioned: Analog to the different data domains discussed in Section 2.2.1, the time domain can be characterized as ordinal, discrete, or continuous. For ordinal time, only equality and partial order relations are defined. For discrete time,amappingexitsfromthetimedomaintothesetofintegers,whichmakes it possible to measure distances in time. Continuous time conceptually maps tothesetofrealnumbers.Assuch,continuoustimeisdense,which,ingeneral, complicates the data handling and visualization. Let us next look at the specifics of time in more detail. In particular, we will deal with four aspects: primitives, arrangement, granularity, and structure of time. Primitives In the first place, we have to think about temporal primitives. Temporal primitives serve as anchors to pinpoint certain events or phenomena in time. Two different types of temporal primitives can be distinguished: instants and intervals. A time instant is a singular point in time. An instant is assumed to have no temporal extent. An example of a time instant would be 19:30 o’clock, and we could use it, for example, to agree on a specific time to meet for dinner with friends. A time interval is a temporal primitive with an extent. An interval can be defined explicitly by two time instants: the interval’s start and end. Alter-
Visualization Methods and Techniques (cid:4) 83 A equals(A, B) B A A before(A, B) meets(A, B) B B A A A equals(A, B) overlaps(A, B) starts(A, B) B B B A A A before(A, B) during(A, B) finishes(A, B) B B B (a)Instant relations. (b)Interval relations. Adapted from [All83]. Figure3.29 Temporal relations for time instants and time intervals. natively, an interval can be defined with a start instant and a duration. For example, when we mark the dinner in our electronic calendar, we may reserve a slot that starts at 19:30 and lasts for 2 hours. The distinction of temporal primitives is important, because different rela- tions may exist between instants and intervals. As illustrated in Figure 3.29, particularly the relation between intervals can be versatile [All83]. Because understanding temporal relations is a relevant analysis objective, visual repre- sentations should be designed so as to enable the observer to identify them. Arrangement The arrangement of time basically describes how temporal primitive are laid out in time. The arrangement can be linear or cyclic. In correspondence with our natural perception of past, present, and future, time can be considered as a linear arrangement of temporal primitives. For linear time, we can clearly denominate a temporal primitive as being before or after another one. We may also quantify how much time has elapsed. Cyclic time is based on recurring temporal primitives. Many phenomena follow the natural cycles on Earth, such as the seasons of the year or the hours oftheday.Forcyclictime,anyprimitiveisprecededandsucceededatthesame time by any other primitive. For example, winter comes before summer, but winter also succeeds summer. A common data analysis objective for unknown data is to figure out whether there are recurring cycles in the data or not. Granularity Time is usually measured with respect to a smallest possible unit. If there is only the smallest possible unit, we can say that the time domain has a single granularity only. The granularity of time is also closely related to the size of temporal data. The finer the granularity is, the larger we can expect the data to be. A typical example would be simulation data that are given at sub-second granularity. Inmanyapplications,itispracticaltoconsidertimewithrespecttomultiple granularities, where smaller units are compiled into larger ones. Human-made
84 (cid:4) Interactive Visual Data Analysis calendar systems are an example for such multi-granularity time models. They can serve as a natural scaffold for multi-scale data analysis. Studying temporaldataatafinergranularitymayrevealsubtledetails,whereasacoarser granularity would convey overall trends or rough estimates. Structure Finally, the structure of time is relevant [Fra98]. Our regular understanding of time is that it progresses in an ordered fashion. There is a single thread of time on which things happen one after the other, and what has happened cannot be changed or undone. If the exact path through time is unknown, for example, in planning and predictionscenarios,abranching timestructureunfolds.Eachbranchdescribes a possible development in time, of which only one will eventually become true. If multiple paths through time co-exist in parallel, the conceptual structure of time is denoted as multiple perspectives. This type of structure helps, for example, when analyzing eyewitness reports, where each person has their own perspective on the observed reality. It is obvious that branching time and multiple perspectives require more resources for their visualization than ordered time, simply because more information needs to be encoded visually. The uncertainty inherent in the different time structures has to be communicated as well. As we see, time is more than a simple linear succession of consecutive moments. It makes a difference if we are dealing with instants or intervals in time, if time is linear or cyclic, if a single or multiple granularities are given, or if time runs in an ordered, branching, or parallel fashion. All of these aspects need to be considered when designing visual representations of time and temporal data. A schematic overview of the discussed aspects is given in Figure 3.30. After this brief characterization of time itself, we will next take a closer look at how data can be linked to time. TIME Primitives Instant Interval Arrangement Linear Cyclic Granularity Single Multiple Structure Ordered Branching Multiple perspectives Figure3.30 Aspects of time to be considered when visualizing temporal data.
Visualization Methods and Techniques (cid:4) 85 CharacterizingTemporalData In general, temporal data are data with references to time. Depending on whether the values in the data vary with respect to time T, the data are denotedastemporal ornon-temporal.Ifadatasetasawholechangesovertime, that is, there are several versions V of the data, then the data are denoted as dynamic. If the dataset is fixed, meaning there is only one version, it is said to be static. These denominations lead to a categorization of different types of data: Static, non-temporal data do not vary over time at all. The well-known Iris flower dataset is an example of data that are completely independent from time. Static, temporal data can be considered a historical view of how an observed phenomenon evolved during a certain period of time. Common time series are a typical example. They contain time-varying data values, but the dataset itself does not change after it has been created. Dynamic, non-temporal data are data that change over time, but only a single snapshot of time is available. In other words, we have a continuous stream of data, but without a history. Such data can often be found in monitoring scenarios, for example, when the current state of some machinery is continuously visualized in a control room. Dynamic, temporal data are data whose values vary over time, and the state of the dataset changes over time as well. Such bi-temporally depen- dent data constantly evolve and also maintain a history. An example is meteorological data, which contain time-dependent measurements and are updated regularly as new measurements become available. An illustration of the different types of temporal dependencies of data is given in Figure 3.31. Non-temporal data can be visualized with general techniques for multivariate data as introduced in the previous section. A peculiarity of dynamic data is that the visualization has to keep up with the changes in the data. If dynamic data additionally preserve a history (dynamic, temporal data), often only a small time window of it can be visualized due to the flood of information that needs to be processed. Itshouldbementionedthatthedistinctionofthedifferenttypesoftemporal dependenciesofdataisinspiredbydatabaseterminology[Ste98].Inthecontext ofvisualization,suchaclearnotationhasnotyetgainedwidespreadacceptance. Hence, one may find a mix of alternative terms, such as time-varying data, time-series data, or dynamic data, but most of the time the data are in fact static, temporal data. In the following, we will concentrate on the visualization of such static, temporal data, because this is the class of data that is most relevant in many application domains.
86 (cid:4) Interactive Visual Data Analysis laropmet-non laropmet static dynamic T T t t 0 0 V V now now T T t t 2 2 t t 1 1 t t 0 V 0 V now now Figure3.31 Types of data with references to time. Adapted from [Ste98]. 3.3.2 VisualizationTechniques In general, any visual variable can be used to encode time and its associated temporal data, where the visual design choices apply as introduced in Sec- tion 3.1. A key characteristic of visualization techniques for temporal data is if the visual representation is static or dynamic. A static representation does not change over time. It remains fixed and providesanoverviewofthetimeaxisinasingleimage.Themostcommonvisual encoding is based on mapping time to space, which means that the temporal primitivesareplacedatdifferentlocationsonthedisplay.Aprominentexample of a static time-to-space visualization is small multiples [Tuf83]. The idea is to visualize the data as a series of miniaturized, high-density, non-overlapping views that are arranged according to time. The views are of equal size and use the same visual encoding, yet each view shows the data for a different temporal primitive. Figure 3.32 illustrates the small-multiples approach. The figure makes clear that static representations can provide a nice overview of the data. In contrast to static representations, a dynamic representation changes over time. This corresponds to a mapping of time to time, more precisely, of time in the data to time of the presentation (or wall-clock time). In other words, the temporal data are shown as an animated sequence of individual images. Dynamic representations can communicate the time-varying behavior of the data very well. However, at any time during a dynamic representation only a single moment of the temporal data is visible, and it is immediately overwritten by the next moment. This can make it more challenging to spot details and to get an overview of the data, particularly when the data are
Visualization Methods and Techniques (cid:4) 87 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Figure3.32 Small multiples visualization of the number of people diagnosed with problems of the upper respiratory tract. larger. Therefore, dynamic representations are usually equipped with controls to pause, rewind, speed up, or slow down the representation. Knowing about the fundamental distinction between static and dynamic representations, we next introduce concrete visualization techniques for time and temporal data. Because time and temporal data are important in so many applications domains, there exist a plethora of techniques to visualize them. A comprehensive survey is available in specialized literature [Aig+11]. Here, we can only describe a few selected examples, and because this book is a print medium, we concentrate on static representations. To cover a broad range of techniques nonetheless, the examples are organized according to the key aspects of time as discussed before. First, we deal with techniques for time instants and time intervals and then for linear and cyclic time. We will illustrate the utility of multiple granularities and finally take the structure of time into account. RepresentingInstantsandIntervals A key difference when visualizing temporal data is whether we are concerned withinstantsorintervalsintime.Instantscorrespondtopoints.Theybearonly asinglepieceoftemporalinformation.Intervals,ontheotherhand,correspond to ranges. They bear two pieces of information (start and end, or start and duration). Let us next look at two visualization techniques, one that is suited for instants and one for intervals. The Time Wheel Intheprevioussectiononthevisualizationofmultivariate data, we introduced axes-based techniques with different layouts of axes and different types of data representations between pairs of axes. The Time Wheel is an axes-based technique particularly for multivariate temporal data [TAS04]. It uses a central time axis around which several attribute axes are arranged in a radial fashion as shown in Figure 3.33.
88 (cid:4) Interactive Visual Data Analysis Figure3.33 Time Wheel visualization of human health data. The actual data representation is done via lines that connect an instant of the time axis to its corresponding data values at the attribute axes. To this end, time and attribute values, which may be continuous, are projected to their associated axes and a line is drawn between them. As such, the Time Wheel implements a visual encoding based on the screen positions of the lines’ start and end points at the axes. The Time Wheel in Figure 3.33 visualizes temporal data that contain the number of people diagnosed with certain health problems. The time axis uses dates as time instants. Each of the attribute axes shows a different diagnosis. As can be seen, the top and bottom axes can be best interpreted with respect to time. By rotating the attribute axes around the central time axis, it is possible to bring any pair of attributes into this focal position. In its depicted state,theTimeWheelshowsmoderatelylowvaluesthroughouttime,butthere are also dates with unusually high numbers of diagnosed people. Triangular Model The triangular model is a technique particularly for visu- alizing intervals [Kul06]. It is based on two coordinate axes, the horizontal one representing time and the vertical one representing duration. In the triangular model, an interval is represented as a dot with two attached arms. The dot is placed so that the arms connect the time axis exactly at the start and the end of the represented interval. The point’s height corresponds to the interval’s duration. Figure 3.34 provides a schematic illustration. The triangular model is useful when it comes to reasoning about prop- erties and the relationships of multiple intervals, because it generates easily distinguishable visual patterns for all possible interval relations. There is even
Visualization Methods and Techniques (cid:4) 89 I 1 I 2 I 4 I 3 I 3 I I I 2 1 5 I 5 1 2 3 4 5 6 7 8 9 Time Time notiaruD 10 I 4 5 0 1 2 3 4 5 6 7 8 9 (a)Standard interval representation. (b)Intervals in the triangular model. Figure 3.34 Visual representation of intervals using the triangular model. Adapted from [Qia+12]. room for visualizing data that might be associated with the intervals. The dot-based encoding would allow for resizing or coloring the dots based on some attribute values. Yet, the triangular model is only of limited use for multivariate attributes. LinearandCyclicVisualization Different analytic questions can be answered with linear and cyclic visu- alizations of time. Temporal trends can be communicated well with linear representations. Recurring patterns, on the other hand, are easier to spot with cyclic representations. Below are three example techniques, the first has a linear time axis, the second a cyclic time axis, and the third combines linear and cyclic encoding. Stream Graph A stream graph is a technique for visualizing multivariate temporal data with a linear arrangement of time. As in the previous two examples, time is shown along the horizontal display axis from left to right. The multivariate data attributes are visualized as stacked streams, there is one stream for each attribute. The actual visual encoding is based on varying the thickness of the streams along the horizontal axis. That is, the vertical height of a stream at a particular horizontal position represents the underlying data value at the corresponding time. Various alternatives exist for ordering the streams and shaping the overall stack of streams. As illustrated in Figure 3.35, a stream graph provides a nice overview of thedataevolution.Ontopofthat,astreamgraphundeniablyhasanaesthetic value, which is the reason why it has gained widespread popularity [BW08b]. A downside is that individual streams can be difficult to interpret, because we intuitively perceive a stream’s thickness as being perpendicular to its flow, whereas the actual visual encoding is always done with respect to the vertical axis irrespective of the ups and downs of the streams.
90 (cid:4) Interactive Visual Data Analysis Figure 3.35 Stream graph with randomly generated data. Adapted from bl.ocks.org/mbostock/4060954. Spiral Representation In case cyclic temporal behavior is central to the analysis task, a spiral representation of the data can be created [TS08]. The basic idea is to map the time axis to the shape of a spiral along which the time-dependent data values are shown. The cycle length determines how many values are shown per spiral loop. The data values can be visualized in different ways, for example, by color-coding or by bars of varying length. We already saw a color-coded spiral at the beginning of this chapter in Figure 3.1. Another example is depicted in Figure 3.36. It shows four years of daily temperature data encoded with the two-tone pseudo-coloring technique. The spiral nicely reflects the recurring seasonal pattern of lower temperatures (blues and greens) in winter and higher temperatures in summer (reds and oranges). Figure3.36 Spiral display with four years of daily temperatures in Rostock.
Visualization Methods and Techniques (cid:4) 91 Such cyclic characteristics become visible, only if the cycle length of the spiral matches the length of the recurring patterns in the data. Particularly for data with unknown periodicities it is therefore necessary to enable the user to interactively search for suitable cycle lengths. How the user can be guided in the exploration process will be illustrated later in Section 6.2 of Chapter 6 on advanced visual analysis methods. Inadditiontocyclicpatterns,thespiralcanalsocommunicatelinearaspects of the data. By looking along a line from the center to the outer loop of the spiral, we can see how the data evolves from one cycle to the next. In our example from Figure 3.36, we can easily compare summers and winters from the past (inner loops) with the current year (outer loop). Cycle Plot The cycle plot is a technique particularly designed for the com- bined visualization of linear and cyclic components of temporal data [Cle93]. Thebasicideaistoshowthecycliccomponentasalineplotintowhichseveral smaller plots are embedded to visualize the linear component. As such, the cycle plot is a kind of nested visualization. For the purpose of illustration, Figure 3.37 compares a standard line plot against a cycle plot. The line plot is shown as a reference at the top. The actual cycle plot is depicted at the bottom. Its horizontal axis encodes the 100 80 60 40 20 0 1st Week 2nd Week 3rd Week 4th Week 100 80 60 40 20 0 Mon Tue Wed Thu Fri Sat Sun Figure3.37 Comparisonofaregularlineplot(top)andacycleplot(bottom).
92 (cid:4) Interactive Visual Data Analysis days of the week. The dashed line visualizes the average values per day to form the overall representation of the weekly cycle. Per day of the week, we see a smaller plot embedded into the main chart. Each of these smaller plots represents the linear development over four weeks. The cycle plot allows us to see things at a glance that are not so easy to derivefromastandardlineplot.Forexample,itisimmediatelyclearthatdata values are generally lower on Saturdays and Sundays, and that data values have increased on Mondays, whereas Tuesdays are in a decline. ConsideringGranularity Severalofourpreviousexamplesvisualizedtemporaldataatthegranularityof days.Yet,temporaldatacanbegivenatdifferentgranularities:years,quarters, months,weeks,days,hours,minutes,seconds,orevensmallertimeunits.Next, we look at a visualization technique that considers multiple granularities to create a multi-scale representation of temporal data. Cluster and Calendar-based Visualization In order to understand how certain resources (human, energy, computing time, etc.) are utilized over time, itmakessensetocollectconsumptiondataandvisualizethem.Oftensuchdata are measured at fine granularity, say minutes or hours. Hence, consumption data can quickly grow to a size that poses a considerable challenge. Cluster and calendar based visualization addresses this problem by imple- menting a multi-granularity visualization [vWvS99]. The approach starts out with representing the daily evolution of the consumption. To this end, a classic line plot is used. As shown in the right part of Figure 3.38, the line plot has a granularity finer than hours along the horizontal axis. The vertical axis represents the resource consumption, in our case the number of employees. This kind of representation reveals patterns with respect to the time of the day. But are the patterns the same for all days, or are there characteristic patterns for different days of the year? To answer this question, the data are clustered. Details on clustering as a generaldataabstractionstrategywillbediscussedinSection5.4.2ofChapter5. For the time being, we only need to know that days are grouped with respect to the similarity of the daily consumption curve. A calendar representation as in the left part of Figure 3.38 is then used to visualize cluster affiliation. In other words, the days of the calendar are color-coded according to the cluster to which a day belongs. With this combined representation of calendar and line plot, the user can see the typical pattern of workdays (orange), days that deviatefromthisregularpattern,forexampleaseriesofFridaysduringsummer vacation (green), and when in the year unusual behavior can be observed, for example, on December 5 or December 31.
Visualization Methods and Techniques (cid:4) 93 Figure 3.38 Cluster- and calendar-based visualization of temporal data. Reprinted from [vWvS99]. VisualizingTimewithaBranchingStructure Last but not least, we said that the structure of time needs to be taken into account. All the examples presented before assume an ordered structure of time. That is, the data are understood as a single unique series of values. Branching time and multiple perspectives are different in that there can be variability in when things happen(ed) in time. Next, we look at a technique that supports the visualization of such variable time structures and also the temporal uncertainty they involve. Planning Lines Particularly in the context of scheduling, it is not always certain when activities start or end. The Planning Lines technique has been designed to support planning and clearly communicate the involved uncertain- ties[Aig+05].Forthispurpose,horizontalglyphsencodesixpiecesoftemporal information: the earliest and latest start of an activity, the earliest and latest end of it, and its maximum and minimum duration. A detailed view of a glyph is provided in the upper right box in Figure 3.39. For the actual planning, multiple glyphs can be placed on a calendar tablature where several activities are allowed to form a branching structure or run in parallel. Dependencies between activities are indicated via links. More precisely, if the start of an activity depends on the successful completion of another activity, the two activities are connected by a link. The links can split and also join paths through time. How a hypothetic planning scenario could look like is illustrated in Figure 3.39.
94 (cid:4) Interactive Visual Data Analysis Excavation Max / min duration Foundation Activity Walls / Ceilings Earliest / latest start Earliest / latest end Windows / Doors Roof Screed Mar 26, 2018 Apr 2, 2018 Apr 9, 2018 Apr 16, 2018 Apr 23, 2018 Week 13 Week 14 Week 15 Week 16 Week 17 Figure3.39 Visualization of uncertain time intervals for planning purposes. Adapted from [Aig+05]. Note that the Planning Lines technique is primarily for visualizing the structure of time and uncertainties, and not so much for the temporal data associated with branching time. In fact, visualizing branching time structures togetherwithmultivariatetemporaldataisachallengingproblemthatrequires further research to solve it. The Planning Lines are the last example in our series of visualization techniques for time and temporal data. The previous pages could cover only a fraction of the rich body of existing work. A comprehensive overview with many more examples is provided by the TimeViz Browser. TheTimeVizBrowser There is a great number of valuable approaches for visualizing time and associated data. A problem is how to find a technique that fits a user’s particular needs. The TimeViz Browser aims to provide a solution to this problem. The idea behind the TimeViz Browser is to provide in a single place a greater overview of what visualizations are possible. The TimeViz Browser is an illustrated survey with a searching and filtering function that allows users to narrow down the scope of techniques according to their needs. To reach a wide audience, the TimeViz Browser is available as a web site accessible at http://browser.timeviz.net. It enables practitioners and researchers to explore, investigate, and compare more than a hundred techniques. The design of the TimeViz Browser is shown in Figure 3.40. The main view consists of thumbnails and provides a compact visual summary of the available visualization techniques. Selecting a technique opens up the detail view that offers a brief abstract for the technique, a larger figure, and a list of relevant publications. The filter interface to the left covers several criteria, for example,
Visualization Methods and Techniques (cid:4) 95 Figure3.40 The TimeViz Browser provides an illustrated overview of more than a hundred techniques for visualizing time and temporal data. if a technique is suited for instants or intervals, or for linear or cyclic time. Using the filters it is possible to narrow down the collection of thumbnails presented in the main view, for example, to focus on techniques for a cyclic time arrangement of temporal intervals. The TimeViz Browser also offers a filter that is related to an aspect that we have left out of consideration so far: the presence or absence of spatial dependenciesinthedata.Inotherwords,inadditiontotemporaldependencies of data, there can also be a dependency of data on space S →A, often on the geographic space. This brings us to the next section, where we will discuss the visualization of spatial and also spatio-temporal data in more detail. 3.4 VISUALIZATION OF GEO-SPATIAL DATA Similartotherolethattimeplaysforunderstandingdata,thespaceintowhich the data are embedded is crucial for gaining insight. Given the ubiquity of modern sensory technology, it can be taken for granted that a considerable amountofthedatacollectedtodayareofspatialnature,typicallyofgeo-spatial nature. In this section, we are going to visualize data with a geo-spatial frame of reference. Our primary interest will be the communication of the dependency of the data attributes on the geographic space S →A. Tightly connected to this dependency are two of the analytic questions discussed in Section 2.2.2: What is the value at a given position (identification task), and where are the positions with a given value (location task)? Yet, before we introduce visualization methods that help us answer these questions, we will first look at the aspects being specific to geographic space and geo-spatial data.
96 (cid:4) Interactive Visual Data Analysis 3.4.1 GeographicSpaceandGeo-spatialData Geo-spatial data are data that are tagged with a position in geographic space. The probably most well-known geo-spatial data are weather data including measurements of temperature, precipitation, wind speed, and also wind direc- tion. Moreover, traffic data and demographic data are associated with the geographic space around us. The aspects that need to be considered when visualizing geographic space and geo-spatial data will be discussed in the following. CharacterizingGeographicSpace Geographic space is special in that it addresses the spherical shape of planet Earth.Here,weconsidertwoimportantaspectsthatcharacterizethegeographic space: dimensionality and scale. Dimensionality The geographic space is three-dimensional. A position in geographic space is typically defined by three coordinates: latitude, longitude, and elevation. The latitude is the angle between the equator and the poles. The latitude is 0° at the equator, +90° at the north pole, and −90° at the south pole. The longitude is the angle with respect to the prime meridian in Greenwich. Eastward angles have a positive sign, whereas westward angles are negative. The maximum absolute value of the longitude is 180°. Because the surface of the Earth is not perfectly planar, elevation is used to measure the distance of a geographic position above or below the sea level. Depending on the application, the two coordinates latitude and longitude may be fully sufficient for the visual analysis of geo-spatial data. For example, visual representations for weather forecasts typically are based exclusively on plotting the data with respect to latitude and longitude. Yet, there are also applications where the elevation part is crucial. Air traffic planning and analysis immediately comes to mind as an example. Scale In terms of scale, the geographic space is continuous and infinite with respecttoresolution.Essentialfordatacollection,processing,andvisualization is the definition of spatial units at a certain scale. In the simplest case, the spatial units are points given at a certain spatial scale. Yet, the spatial units may also be given as complex regions at multiple scales, such as federal states, districts, and zip-code regions. In such cases, the spatial units are typically organized in a hierarchical structure (similar to what we said about temporal granularity in the previous section). Figure 3.41 shows the German federal stateMecklenburg-Vorpommerncomposedofdifferentspatialunitsatdifferent scales. The spatial scale significantly affects the results of the visual data anal- ysis. Relationships detected at one scale may not be observable at another scale[And+10].Hence,determininganadequatespatialscalethatcorresponds
Visualization Methods and Techniques (cid:4) 97 Figure3.41 Spatialregionsatdifferentscales:Federalstate,districts,zip-code regions. to the subject under consideration is crucial. For example, investigating the trafficataspecificcrossingrequiresadifferentspatialresolutionthanexploring major routes between districts. Often,findingthespatialscalethatbestmatchesthetaskathandisatrial- and-error procedure. It may even be necessary to create further spatial scales by subsuming or subdividing spatial units. Coarser scales can be derived from the original scale by means of a suitable aggregation strategy. This includes the application of aggregation functions such as average, sum, or count. For thecreationoffinerscales,asuitabledistributionstrategyisrequiredtoassign data values to the newly specified sub-regions. Usually, additional context information is necessary to arrive at semantically meaningful aggregations and distributions. Both dimensionality and spatial scale are important characteristics of the geographic space. They have a decisive influence on the results that can be achieved with visual data analysis. Let us next take a closer look at what characterizes geo-spatial data.
98 (cid:4) Interactive Visual Data Analysis (a)Points. (b)Lines. (c)Areas. (d)Volumes. Figure3.42 Geo-spatial data can refer to different spatial units. CharacterizingGeo-spatialData Geo-spatial data (also called spatial data or geographic data) have explicit referencestothegeographicspace.Inotherwords,thedataareassociatedwith spatial units. As illustrated in Figure 3.42, there are four types of spatial units: points, lines, areas and volumes. For example, data collected at a measuring station are associated with the very position of the station. Highway traffic data would refer to lines. Census data typically refer to discrete administrative spatial areas. Weather phenomena such as thunderstorms are an example where geo-spatial data have a volumetric anchor in space. It is the existence of a relationship between the data and spatial units in geographic space that characterizes geo-spatial data in the first place. The relationship itself is further characterized by the first law of geography as formulated by geographer and cartographer Waldo R. Tobler [Tob70]: “[...]everythingisrelatedtoeverythingelse,butnearthingsaremorerelated thandistantthings.” Tobler,1970 This law has immediate consequences for the visualization of geo-spatial data. The law’s first part implies that geo-spatial data can be interpolated and extrapolated. The second part tells us about the weighting of near and distant data points. Pictorially speaking, the visual representation of a geo-spatial data value can spread in a certain neighborhood. The scope of the data, as explained in Section 2.2.1 of Chapter 2, defines how far the data may spread. At this point, we know about the specific character of the geographic space and geo-spatial data. In addition to these specifics, the characteristics of multivariate data apply as described in Section 2.2.1. Next, we will introduce strategies for visualizing the geographic space and the associated data.
Visualization Methods and Techniques (cid:4) 99 3.4.2 GeneralVisualizationStrategies Visualizing geo-spatial data means showing univariate or multivariate data attributesAinrelationtothegeographicspaceS.Thisallowsustoinvestigate spatial dependencies S×A. If the data have an additional temporal dimension T, a spatio-temporal visualization can be created to support the analysis of spatial and temporal dependencies S×T ×A. A prerequisite for geo-spatial data visualization is the representation of the geographic space. RepresentationofGeographicSpace Geographic space is typically represented by 2D maps. Maps are universally useful.Theyallowustodevelopanunderstandingoftheworldanditsnaturalor manmadephenomena.Cartographyisthedisciplinedealingwiththeproduction of maps. The cartographic literature provides a wide range of design concepts, guidelines and conventions for creating maps [Mac95]. One such convention is to use blue colors for water, green for lowlands, and brown and white for mountains and their icy caps. Anessentialstepwhencreatingamapistoprojectthesphericalgeographic coordinatestotheplanarcanvas.Therearevariousprojectionmethods,someof whichevendatebacktoancienttimes.Thedifficultythatallprojectionmethods have to deal with is to find a trade-off between preserving several properties of the Earth’s surface, including area, shape, direction, and distance. For example, the Equirectangular projection, attributed to Marinus of Tyre, who lived around 70–130 CE, preserves distances. The famous Mercator projection, which preserves angles and shapes, was invented in 1569. Even today, new projection methods are devised, for example the Natural Earth projection by Tom Patterson from 2011. Figure 3.43 illustrates these three projections. A particularly interesting class of projections is myriahedral projections, which (a)Equirectangular. (b)Mercator. (c)Natural Earth. Figure 3.43 Different map projections preserve different spatial proper- ties. Adapted from bl.ocks.org/mbostock/3757119, .../3757132, and .../4479477.
100 (cid:4) Interactive Visual Data Analysis Figure3.44 Myriahedral projections of the Earth. Adapted with permission by Nicolas Belmonte from philogb.github.io/page/myriahedral/. unfold the globe by cutting and folding the faces of polyhedra [vWij08; BW18]. The three examples in Figure 3.44 illustrate the new perspective on planet Earth offered by these projections. Another important step when creating maps is cartographic generaliza- tion. It addresses the issues of spatial scale as indicated before. Cartographic generalization aims at reducing the complexity of the visual representation of spatial features by depicting them at lower resolution. The difficulty is to preserve both the geometric characteristics (graphic generalization) and the essential data characteristics (semantic generalization). Figure 3.45 gives an example where the map of Mecklenburg-Vorpommern, as shown earlier, is displayed at different resolutions. The map data have been reduced with the classic line generalization algorithm by Visvalingam and Whyatt [VW93]. At 50% reduced data size, the visual representation of the map is almost identical to the original data. At 10%, the reduced map resolution is fairly obvious, but still the major features of the map can be discerned. (a)Original data 100%. (b)Data reduced to 50%. (c)Data reduced to 10%. Figure 3.45 Map representation at different resolutions. Generated with mapshaper.org. While2-dimensionalmapsarepowerfulingeneral,therearealsoapplication scenarioswhereitisnecessarytofaithfullyrepresentthe3-dimensionalfeatures of the geographic space. Sophisticated terrain rendering techniques have been developed to create almost photo-realistic 3D images of the Earth’s surface. Terrain rendering is a complicated matter in itself and involves several steps including the generation of 3D meshes, definition and selection of appropriate resolutions, hardware-accelerated shading and texturing, and advanced post-
Visualization Methods and Techniques (cid:4) 101 4392m 0m Figure3.46 Terrain rendering of the Puget Sound region. Courtesy of Steve Dübel. processing.Here,wewillnotgointothedetailsofterrainrendering,butinstead refer the interested reader to the dedicated literature on the topic [Ruz+12]. An example of a 3D terrain rendering of the Puget Sound region is shown in Figure 3.46. Color is used to visualize the terrain elevation. In the following pages, we will see further examples, where geo-spatial data are visualized in 3D scenes. Yet, before that, we will introduce basic visualization strategies for geo-spatial data. VisualizationofGeo-spatialData Theessentialtaskwhenvisualizinggeo-spatialdataistoconveytherelationship between the geographic space S and the data attributes A. In particular, we will concentrate on two key questions: First, how to combine S and A visually, and second, whether to draw S and A in 2D or 3D? Direct and Indirect Visualization There are two baseline options for the visual combination of S and A. We can directly show the geo-spatial data withinthepresentationofthegeographicspace.Thisshallbedenotedasdirect visualization. Alternatively, we can pursue an indirect visualization. In this case, the geo-spatial data and the geographic space are depicted in separate views, and their connection is communicated indirectly via visual cues. Direct Visualization For direct visualization, geo-spatial data are embed- ded directly into the visual representation of the geographic space. A prominent example is choropleth maps, for which the spatial units of the geographic space are color-coded according to their associated data. While such choropleth maps are intuitive and easily understood, they usually depict only one or two geo-spatial attributes. Multivariate geo-spatial dependencies can be communicated by placing glyphs, as described in Section 3.2.4, directly on the map. To this end, suitable glyph positions must be determined. Ideally, glyphs should be locateddirectlyattheirassociatedspatialunit.Atthesametime,glyphs
102 (cid:4) Interactive Visual Data Analysis (a)Straightforward placement. (b)Overlap-optimized placement. Figure3.47 Reducing overlap of stream graph glyphs on a map. should neither overlap each other nor occlude important geographic features.Theserequirementsmakeglyphplacementanon-trivialproblem whose solution may involve sophisticated optimization algorithms [FS04]. Figure 3.47 illustrates the difference between a straightforward and an overlap-optimized placement of stream graph glyphs. Overall, direct visualization allows us to easily identify the data values at certain positions and to locate positions with certain data values. However, with an increasing number of spatial units and data values, direct visualization reaches its limits, because it becomes increasingly difficulttoembedallgeo-spatialdatadirectlyintothemap.Thisiswhere indirect visualization can help. Indirect Visualization For indirect visualization, geo-spatial data and geo- graphic space are represented in distinct views that are linked through visual cues. For example, Figure 3.48a shows a choropleth map and a parallel coordinates plot. The map visualizes a single attribute by color for the entire geographic space. The parallel coordinates plot shows all fiveattributesforalldatatuples.Yet,theconnectionbetweengeographic space and the multivariate data is possible only for a single location, which is marked in the map via cross-hairs, and whose associated data tuple is highlighted in red in the parallel coordinates view. One way to increase the number of locations for which multivariate geo-spatial data can be visualized is to use geo-spatial probes [But+08]. The goal of probes is to achieve a more flexible combination of the map and several views showing the geo-spatial data. To this end, the user can place a number of probes on the map. For each probe, a separate view is created that shows the data associated with the space around the probe. The connection between the probed spatial locations and the data views is established via visual links. A probe-based indirect visualization is illustrated in Figure 3.48b.
Visualization Methods and Techniques (cid:4) 103 (a) Univariate choropleth map plus (b) Flexible visualization via probes. multivariate parallel coordinates plot. Courtesy of Thomas Butkiewicz. Figure3.48 Indirect visualization of geo-spatial data. A major advantage of indirect visualizations is that we can show larger amounts of geo-spatial data. Yet, the user must mentally connect two or more views to explore the data’s spatial dependencies. In fact, both direct and indirect visualization are viable options for com- municating geo-spatial data. When the spatial aspect S is in the focus, direct visualization tends to be better suited. When the data attributes A play a stronger role, indirect visualization can be more practical. Let us next continue with the question of whether geographic space and geo-spatial data should be depicted in 2D or 3D. 2Dand3DVisualization Wealreadydiscussedthegeneraldesigndecisionofusing2Dor3Dpresentations in Section 3.1.2. When visualizing geo-spatial data, this decision is more intricate, because we have to distinguish between the presentation of the geo-spatial data A and the presentation of the geographic space S [Düb+14]. A systematic view on this concern is given in Figure 3.49. The vertical axis schematically depicts the difference between 2D and 3D presentations of the geo-spatial data, while the horizontal axis compares 2D and 3D presentations of the geographic space (map or terrain). The four possible combinations of 2D and 3D can be characterized as follows. 2D Data Visualization on 2D Map Showing both data and space in 2D is the quasi-standard for geo-visualization. Many established techniques
104 (cid:4) Interactive Visual Data Analysis D2 D3 2D 3D A atad latiaps-oeG Geographic space S Figure3.49 Systematic view of 2D and 3D representations of geo-spatial data and geographic space. Adapted from [Düb+14]. belong to this category, including choropleth maps, contour maps, dot maps, flow maps, cartograms, or glyph maps, as shown earlier in Fig- ure3.47.Thelevelofvisualabstractionachievedbypurely2Dapproaches usually makes the resulting images easy to interpret. 2D Data Visualization in 3D Terrain In this category, 2D and 3D com- ponents are combined. A 3D terrain rendering represents the geographic space in its full extent. The geo-spatial data are mapped onto the 3D terrain via 2D billboards or 2D textures, as already illustrated in Fig- ure 3.46. This makes it possible to study how the data behave in relation to specific landscape characteristics, such as mountains or valleys. 3D Data Visualization on 2D Map For this category, the geographic space is abstracted as a 2D map, whereas 3D graphical elements are used for the data. In a sense, the third dimension is exploited for a richer depiction of the data, for example for showing more data values, providing clearer arrangements, or incorporating more dimensions, for example the dimension of time, as we will see a little later. 3D Data Visualization in 3D Terrain If three-dimensional spatial rela- tionships play an important role for the data analysis, it makes sense to showbothdataandspacein3D.AnexampleisgiveninFigure3.50.The visualization shows how an aircraft decreases its speed (color changes fromdarkbluetobrightblue)whilemaneuveringthroughthemountains near Sion airport during the approach.
Visualization Methods and Techniques (cid:4) 105 Figure3.50 3D visualization of the trajectory of an aircraft approaching Sion airport. Courtesy of Steve Dübel. In general, both 2D and 3D representations are suited for the visualization ofgeographicspaceandgeo-spatialdata.Whichcombinationtochoosedepends again on the specific data and tasks. In the case that 3D components are involved in the visualization, it is definitely necessary to address the problems due to occlusion and perspective distortion. In the first place, it is important to make the user aware of the fact that information may be obscured. This awareness enables the user to act and resolve potential problems by changing the viewpoint on the 3D scene. Visibility widgets as illustrated in Figure 3.51 can help the user in this regard [RS17]. The panoramic view in the bottom right corner allows the user to look with an extremely wide angle onto the 3D scene. The central circularlensvisualizesthedataotherwisehiddenonthebacksideoftheterrain. The color-coded bands at the bottom and to the left indicate where and at Figure3.51 Visibility widgets help users identify obscured information in 3D geo-visualizations. Courtesy of Martin Röhlig.
106 (cid:4) Interactive Visual Data Analysis what distance the majority of obscured information is located. While such sophisticatedtoolsmightnotalwaysbeavailabletotheuser,itisimportantto provide at least rudimentary support for an effective exploration of 3D visual representations. In summary, direct and indirect visualization, as well as 2D and 3D visu- alization, are the basic options when designing visual representations of geo- spatial data. Next, we will put these basic strategies to use for the combined visualization of data in space and time. 3.4.3 VisualizingSpatio-temporalData So far, we considered the visualization of geo-spatial data and temporal data separately.Yet,spaceandtimearetightlyconnected.Spatialphenomenaoften include a temporal aspect, and evolving phenomena are often embedded in space. This leads us to the question: how to visualize spatio-temporal data? Spatio-temporal data have references to both space S and time T, a fact that makes the visual data analysis more difficult. It is no longer sufficient to understand in isolation how data values are distributed in space S → A and how they evolve over time T → A. It is rather necessary to combine both perspectives to enable a comprehensive understanding of spatio-temporal dependencies S×T →A. Designing corresponding visual representations is challenging. Particularly for the case of spatial-temporal data visualization, it isimportanttobalancethedesigner’swishtocommunicatealotofinformation and the users’ capacity to conceive it. Probably the most widely used solution is to show an animated map where the visualization changes with each frame. We also mentioned small multiples as a suitable approach and gave an example in Figure 3.32 on page 87. In general, we could combine and link two dedicated views, one visualizing the temporalaspectofthedataandtheothershowingthespatialaspect.However, in all these cases, it would be up to the user to mentally integrate S and T. In the following, we want to illustrate how 3D data visualization on 2D maps can help us obtain visual representations where S and T are already integrated.Ourfirstexamplewillbeaboutthevisualizationofspatio-temporal movement data. The second example is concerned with the visualization of spatio-temporal health data. StackingMovementTrajectoriesin3D Movement data capture how some observed objects move through space and time. Here, we consider movements where each data point is defined as a tuple with latitude and longitude coordinates, a timestamp, and several data attributes, including speed, acceleration, or sinuosity. The standard way of looking at such data is to study 2D maps where movement trajectories are visualized as 2D paths. Figure 3.52a shows an example with trajectories of cars moving along roads. Color is used to encode
Visualization Methods and Techniques (cid:4) 107 (a)2D map with 2D paths. (b)2D map with stacked 3D bands. Figure3.52 Visualization of movement trajectories. the cars’ speed. This basic form of representation suffers from overplotting which makes it difficult or even impossible to see any details of the spatio- temporal behavior of individual trajectories. Atthispoint,thethirddisplaydimensioncanhelpusuntanglethesituation. As illustrated in Figure 3.52b, the simple, yet powerful idea is to tilt the map in 3D and to stack 3D trajectory bands along the third dimension, where each band is assigned a separate z-coordinate [Tom+12]. The3Dtrajectoryvisualizationhastwoadvantages.First,wegetacompre- hensiveoverviewofalltrajectories.Second,itisnowpossibletoinvestigateeach individual trajectory in detail. The spatial aspect can be studied in relation to the base map. For the selected trajectory, a dynamic auxiliary map layer is displayed. It eases the interpretation of the spatial embedding, particularly for the data at the top of the stack. Moreover, the bands in the stack can be grouped according to their similarity in terms of their shape. This way, we can see quite well where in space the data exhibit certain characteristics, for example, where the speeds are higher (green) and where they are lower (red). But what about the when as captured by the temporal aspect of the data? Again, the third dimension comes to our help. In contrast to 2D paths, the 3D bands offer enough space to include arrows that indicate the direction of the movement. This allows us to see the temporal sequence of places visited during a movement. Moreover, the stacking order can be altered with respect to time. Trajectories from the past could be put at the bottom of the stack, whereas the most recent trajectories could go to the top, or vice versa. Such a temporal ordering of the trajectory bands can reveal long-term changes in the overall movement data.
108 (cid:4) Interactive Visual Data Analysis Both the arrows in the bands and the ordering of the stack allow us to draw qualitative conclusions about temporal dependencies. We can tell that one data point is temporally before or after another one. But quantitative statements about the temporal distance between data points cannot be made. In Section 4.6.3 of Chapter 4, we will learn how an interactive lens technique can help us remedy this problem for movement data. Next, we will continue with an approach where time is granted more visual prominence by design. 3DSpace-timeCubeVisualization Under the assumption that the geographic space is defined by two coordinates, spatio-temporal data can be mapped directly to the three display dimensions. This general visualization approach is called a space-time cube. The x-axis and the y-axis of the cube are used to map the two spatial coordinates, the third z-axis shows time. The space-time cube (STC) has been developed long before visualization became an independent field of research [Häg70]. The primary purpose of STC visualizations is to show the paths that objects have taken through space and time [Kra03]. In the following, we will use the STC concept to visualize spatio-temporalhealthdata.Thespatialframeofreferencewillbevisualizedas a basic 2D boundary map. The actual data attributes, in our case the number of people diagnosed with certain diseases, will be visualized via different 3D graphical representations. (a)Pencil glyphs for linear trends. (b)Helix glyphs for cyclic patterns. Figure3.53 Visualizing spatio-temporal data using 3D glyphs on a 2D map. 3D Glyphs in a Space-time Cube Let us start with 3D glyphs. As in 2D, glyphs are created and placed at a suitable location for each spatial unit of the map. Figure 3.53 shows 3D pencil glyphs and 3D helix glyphs [TSS05]. Both types of glyphs are oriented along the z-axis, which represents time. Colored glyph segments visualize individual data values. Pencil glyphs facilitate the visual analysis of linear temporal trends. Map- ping different attributes to the different faces of the pencil allows us to see correlations in the evolution of multiple data attributes. The example in Fig- ure 3.53a shows the monthly number of people with different infections of the upper and lower respiratory tracts.
Visualization Methods and Techniques (cid:4) 109 Helixglyphsemphasizecyclicpatternsinthedata.Ahelixglyphisbasically a 3D band that winds up along the z-axis. Sub-bands visualize different data attributes. Figure 3.53b shows the same data as Figure 3.53a. Each helix glyph is configured to show 12 months per helix loop with the seasonal peaks currently facing toward the viewer. Both pencil and helix glyphs have the advantage that spatial and temporal aspects are shown within a single STC image, which allow us to study mul- tivariate spatio-temporal dependencies. Yet, while the temporal evolution of the data is nicely visualized along the individual glyphs, the spatial evolution is more difficult to extract. Because of the empty space between glyphs, the user has to mentally link the information displayed in one glyph to what is shown in another glyph. How an alternative visual representation can resolve this problem will be explained next. 3D Wall in a Space-time Cube The question is, how can we avoid gaps in thevisualrepresentationofthedatasothatthespatialcharacteristicsareeasier to interpret? An interesting solution is to create a non-planar slice through the three-dimensional space-time continuum onto which the spatial-temporal data can be projected [TS12]. The creation of the slice is based on three steps as shown in Figure 3.54: 1. Define topological path through neighborhood graph 2. Construct geometrical path through spatial units 3. Extrude slice for data visualization In the first step, a topological path has to be defined through the neigh- borhood graph of the spatial units. The topological path guarantees that the data visualization is free of gaps. In a second step, the topological path is transformed into a geometrical path. This path is constructed in the x-y-plane taking into account the geographic characteristics of the spatial units. A good geometricalpathshouldhavelowcurvatureandfollowthespatialunits’shapes without passing through other territories. Finally, the geometrical path is Time Space (a)Topological path. (b)Geometrical path. (c)Extruded slice. Figure3.54 Creation of a non-planar 3D slice through space-time.
110 (cid:4) Interactive Visual Data Analysis Figure3.55 Spatial-temporal visualization along a wall on a map. extruded in the direction of the z-axis to form a non-planar slice with a wall- like 3D shape. This wall acts as a kind of canvas onto which we can project visual representations of the spatio-temporal data. Figure 3.55 shows an example with the health data that we have already visualized via glyphs earlier. Per wall segment, colors visualize the number of sick people for 36 months of observation. As there are no gaps in the visualization, it is now easier to follow the data’s evolution through space and time.However,wearerestrictedtothepathdefinedinthefirstplace.Therefore, itisimportanttoprovideflexiblemeanstoadjustthepathinteractivelyorbased on the characteristics of the data. For example, data-driven paths could be definedbypursuingagradientdescentalongspatio-temporaltrendsinthedata. In conclusion, we see that designing visualizations of spatial and spatio- temporaldatainvolvesnumerousdesignchoiceswhichallmustbeweighedand decided carefully. The primary goal must be to clearly represent the spatial dependencies of the data. This goal can be achieved on the basis of suitable 2D map or 3D terrain representations. The actual data representation can be doneinavarietyofways,wheredifferentcombinationsof2Dand3Dgraphical elements are possible. Conceptually, the data can be visualized directly in the spatial frame of reference or indirectly as a dedicated view that is linked with the map or the terrain. When the dimension of time is added to the visualization, matters become still more complex. We have seen that using the third display dimension is a viable option to integrate time. Yet, when time, space, and multivariate data attributes are to be analyzed in concert, we usually have to make compromises. In our last example, we used a wall-like 3D visualization without gaps to ease the spatial interpretation of the data, but the solution shows only a single data attribute. 3D glyphs are capable of visualizing multiple spatio-temporal attributes, but some information may be invisible at the glyphs’ back face. Becauseitisnecessarytomakecompromises,acomprehensivevisualanalysisof
Visualization Methods and Techniques (cid:4) 111 spatio-temporaldatashouldideallyprovidetheuserwithdifferentvisualization techniques and easy ways to configure them. Next, we will learn that the visualization of graphs, which typically do not have a natural embedding in space or time, is a considerable design challenge as well. 3.5 GRAPH VISUALIZATION In the previous sections, we have learned how to visualize multivariate data attributes A, the temporal context T, and the spatial frame of reference S. Last but not least, this section will deal with the visualization of relations R among the data. Following the same pattern as in the previous sections, we will first look at the specifics of the underlying data model and then introduce dedicated visualization solutions and illustrate them with selected examples. 3.5.1 GraphData The data model that interests us now is graphs. A graph G=(V,E) consists of a set of vertices V (or nodes) and a set of undirected or directed edges E (or links) between the nodes. Roughly speaking, nodes represent pieces of data, whereas edges correspond to the relations among the data. We can also say that the edges define a structure among the data. NetworksandTrees Graphs may come in a variety of different forms. Depending on the charac- teristics of the structure of a graph as defined by its set of edges, one can distinguish different classes of graphs. If there are no particular constraints imposed on the graph, it is commonly called a network. As such, networks capture the intuitive notion of binary relations between entities. Social net- worksandcomputernetworksareprominentexamples.Butalsotransportation systems and biological systems are often modeled as networks. Incontrasttobasicnetworks,atreeisagraphthatobeysspecificconstraints. In particular, a tree is a connected, acyclic graph. Connected means that for any two nodes in the graph a path exists connecting the two nodes. Acyclic means that the graph does not contain paths where start node and end node are the same. Moreover, being an acyclic graph also implies that the path that exists between any two nodes is unique. As in nature, a tree’s root and leaves are special. A tree with a designated root node r ∈V is called a rooted tree. The root serves as the center of the tree from which edges are directed away describing cascades of parent-child relations. A node that has no edges to child-nodes, but only a single edge to its parent-node is called a leaf. A managerial hierarchy of a company is an example for a rooted tree with the CEO as the root, and team leaders being the leaves.
112 (cid:4) Interactive Visual Data Analysis Time (a)Structure (b)Attributes (c)Time (d)Space (e)Groups Figure 3.56 Facets to be considered when visualizing graphs. Adapted from [HSS15]. Networks and trees are the most commonly encountered graphs in the context of visualization. There are other, less common graphs. For example, in bipartitegraphs,thenodescanbepartitionedintotwosets,andedgesrunonly between the two sets, not within the sets. For another example, hypergraphs are graphs where an edge is allowed to connect more than two nodes. Many more types of graphs are known in the literature [BLS99; GYZ14]. Yet, in this book, we want to focus on the visualization of networks and trees. FacetsofGraphs Figure 3.56 provides an overview of the facets that are most relevant in the contextofgraphvisualization. Thestructure inFigure3.56a isaprimaryfacet. Yet, additional facets can also play a role when analyzing graph data. These additional facets lead to distinct types of graph data: Multivariate graphs have additional data attributes associated with their nodes and/or edges as illustrated in Figure 3.56b. The data attributes provide additional information about nodes and edges, for example, importance or weight. Dynamic graphs vary over time. As shown in Figure 3.56c, for each point in time, the set of nodes and the set of edges can be different. Between subsequent time steps, nodes and edges may continue to exist, leave the graph or enter it. Spatial graphs are associated with coordinates in space as in Figure 3.56c. The coordinates define the layout of nodes and sometimes even the routing of the edges. A network of flights connecting airports is an example of a spatial graph. Compound graphs partition the nodes and the edges into groups that are typically nested hierarchically, which is depicted in Figure 3.56e. The groups can be expanded or collapsed to create views of the graph with different levels of abstraction.
Visualization Methods and Techniques (cid:4) 113 As mentioned before, the structure of graphs is fundamental, while attributes, time, space, and groups may provide additional context infor- mation to complete our understanding of the data. Yet, these additional facets also make the visualization more difficult to design. Therefore, we will start simple and introduce basic visual representations for the structure of graphs in the next section. Later in Section 3.5.3, we will discuss how the additional facets can be incorporated. 3.5.2 BasicVisualRepresentations There are three fundamental categories of graph representations: node-link representations, matrixrepresentations, andimplicit representations. Addition- ally, there are hybrid representations, which are basically combinations of the fundamental representational paradigms. Node-linkRepresentations Node-link representations (or simply node-link diagrams) depict nodes as dots and the edges between them as lines or arcs. This representation is by far the most common one for networks and trees. A key question is, where should nodes and edges be drawn? Solving this problem is the task of graph layout algorithms [Bat+99; Tam13]. The literature defines a number of conventions, aesthetic criteria, and constraints that graph layouts should obey and fulfill. For example, nodes shouldnotoverlapandthenumberofedgecrossingsshouldbeminimal.Curved edges may be aesthetically more pleasing than straight lines or orthogonal edges. In general, it is not possible to observe all of these requirements, as they may be contradictory to each other. Therefore, the graph layout problem is typically formulated as an optimization task. Depending on the degree of freedom that an algorithm has to find suitable node positions, three classes of layouts can be distinguished [SS06]. There are: • free layouts, • fixed layouts, and • stylized layouts. Forafreelayout,therearenoparticularconstraintswithrespecttothenode positions. Such layouts are typically generated with force-directed approaches. Thegoalistoplaceconnectednodesneartoeachotherwithoutclutteringthem inasinglespot.Tothisend,force-directedalgorithmssimulaterepulsiveforces between all nodes and attractive forces between adjacent nodes [FR91]. With an adequate configuration and a suitable number of iterations, the simulation should converge in an equilibrium that represents the final layout. In contrast to free layouts, fixed layouts are based entirely on predefined node positions. Only the edges can be routed flexibly by the visualization
114 (cid:4) Interactive Visual Data Analysis Figure 3.57 Node-link diagram of flights connecting US airports. Created with gephi.org. method. Fixed layouts are typically encountered when the graph is embedded into some spatial frame of reference. Figure 3.57 shows an example where nodes are placed according to the positions of US airports. Inbetweenfreeandfixedlayouts,therearetheso-calledstylizedlayouts.On theonehand,theyarenotasrestrictedasfixedlayouts,andontheotherhand, they are not entirely free. A typical scenario is that possible node positions are confined to a certain predefined scheme. For example, the nodes may be required to be located on a circle or be arranged in an axis-aligned fashion. Tree layouts often aim to place the nodes of individual hierarchical levels on distinct horizontal or vertical lines. Once a layout has been computed, it can be post-processed to improve its appearance and readability. For example, the graphical representations of the nodes may overlap, because many layout algorithms neglect the fact that space is needed to actually draw a node. Therefore, node overlap removal is a commonly applied post-processing step [Nac+16]. Another option is to route edges in compact bundles. The bundling of geometrical primitives is a general approach to de-clutter visual representations. It will be explained in more detail in Section 5.1.2 of Chapter 5. Overall, node-link representations are well suited to display graph nodes and edges in a balanced way. Their widespread use testifies to the universal utility of node-link diagrams. Yet, for graphs with very many edges, node- link representations can get cluttered so much that they resemble hairballs prohibiting any visual analysis. Moreover, the time complexity of optimizing the layout can be high. Matrix representations deal time complexity for space complexity and avoid the edge clutter as we will see next.
Visualization Methods and Techniques (cid:4) 115 To morF 1 2 3 4 5 6 7 8 4 1 8 2 2 5 3 4 7 5 1 6 6 3 7 8 Figure 3.58 Node-link diagram and corresponding matrix representation. Adapted from [vHSD09]. MatrixRepresentations Matrix representations are a graphical interpretation of a graph’s adjacency matrix. There is exactly one row and one column for each node v ∈ V. If there exists an edge (v ,v ) ∈ E between the i-th and the j-th node, the i j corresponding matrix cell at position [i,j] (and [j,i] for undirected graphs) is marked. Otherwise, the cell remains empty. The marks placed in the cells can also be utilized to visualize the edge weight or other edge attributes, for example, by varying the color or the size of the marks. Figure 3.58 depicts a simple node-link diagram and a corresponding matrix representation. As we can see, a matrix is visually focused on the graph’s edges, whereas nodes are merely represented as labels. This makes it possible to detect structural patterns such as those illustrated in Figure 3.59. However, such patterns appear only if the nodes are ordered appropriately along the horizontal and vertical axis. There are many ways to order the nodes of matrices [Beh+16]. A simple approach is to sort them according to a given 1 2 3 4 5 1 1 2 3 4 5 1 . 1 . 5 X 2 2 5 2 X 3 . 4 . 4 3 5 4 3 1 2 3 4 5 1 2 3 4 5 3 3 1 1 2 1 2 1 3 4 3 4 4 4 2 2 5 5 5 5 Figure3.59 Graph patterns represented as matrices and node-link diagrams. Adapted from [SM07].
116 (cid:4) Interactive Visual Data Analysis M MMMl ML l MeC Mt MmmmMo..h C oGGGC F M tla M meo ee tl oh haB iiiC eB hm C lll ...C u MG nu mllla T e To THC P PeB lC .o eee Sa eB o lnl BGm thM hc Nru hpu o oP.ea o G JG Tl Fh um cm nnn Pra WWec pa M Ita h o aA e ee e oS om q M .l rPn na anCu E aOe c Ze r eah oE ooB VF r LMp .r IaB mac LC p oGeF nb Vna niBbn u gv s u luau t l t ooh pp nMro Gr eain a vs rnB me rrr at aho tB oruMlFa a aDCC so a �l m muaat df oe aC u a aJeo dn u pz lB s ffl se mr me m aJ ee mmoro o pgn e nmi ujp brer e ta eesh t mr n ta gv bhhl ue su vo ad lsf o r sra ru peo rtr Mr ebr os ia lu .lvdh l hn cae e yas je eye bb a u amv vee db doo a � o� n d aea aav aaaa J g aal c ee rii dehe ceo l rr lu teub e yel rj rsm s v llii iir r rorii r e r ioo ii ��� ai ii i iiii ir ii nn oi on uo on edd nne n uhe nnn aaaul l il g cL l cuu la s ecs ca nne ee e nn ee rr r rrr ueee t tll l ee i l rl ii i Rd hddo pnuu n nndn nu12 12ee eee e ee ee eee eee ee eeee eTa a i y yy yc ss ssss sr rr rr r tt tttt tt fl ll dn pyr tad he ncuu em en i cr eeoe do n ed rr rat scnr li irs ae L � mn n emh soe oo ae ois ee ti s ae osre hatl rc l snc n yi n ure u g tu dl egl e rT �o a n etv d br m ee r cl a R nn cenee e o el lree te rii a eo s ms 12em eb pe Pl iru yrn ra urua u nooe rieni dl�e h Ihres nts i nM ea urst t l rfsf nneaey uvl ll ma a rreed ip Hsem a i ar i ni B ide Pe ie e e ce uee o� rpp oe scna aTr ee pa naeeru e aen te fflG mu aat itt a ly Br� a V12l r ane ie auttrn l loa . ... ..tu eb hut er e vn imm nee iaa oiu lt Mrra lhhf st iel h o o u o mml e eee eee gnn ln io rn ...le dd nplh h�e ni ee vr l i na udr io o utpo bm cs l sg ivlm e eeegov lq b e ttm i rrc j rm mmm mmeb pbh Gu uuue nhb vo oaa yll ez mc r pus jus anu b du ooo o ooer ndt av ra aaaal lll yii ja a v liuee aaan a l lllloo ooooo ohhhhh s oaaaa pn hhe a.rr ee oa aaa arr l WWr cri MM MM MM MMM M MMMM MMMooual l t iOi NsGG GG GGGDA VBB BBBB BB B CC CCCCC CC CCCC PPPEE TTT ZF FFF SSF LLLJ JJJI M M MM l MLl Me CM tM m mm M o. .h CoG GG CF Mtl a Mm e oee tl o hha Bi iiC e B hm Cl ll . .. C uMG nu ml ll aT eT oT HC PPeB lC.o e ee Sa e Bo lnl B Gmth Mh c Nruh p uooP. eao G JG TlF h um cmn nn P raWW ecp aM I ta h oa Aee eeo S omq M .lr P nna an Cu E a Oe cZ erea hoE ooB VF rL M p. r I aB mac LC po GeF nbV n an i B bnu gv sul uau tltooh pp nM roGr e ai nav srn B mer rr ataho tBo r uM lFa aa D CCso a� l mmu aat df oea Cuaa J eo dn upzlB s ffls em rmem a J ee mm or o op g ne nm i uj p bre re t aees h tm rn ta g v bhhlu es uvoa dl sfo rsr ar u p e o rt r Mreb ro s ia l u .lv dh lh n c a eey asj eeye bb au amvv ee d bd oo a� o� n da eaa ava aa aJ gaal ce er iideh ec eol rr l uteu b ey elr j rs m sv lliii ir rror iir e ri ooi i ��� aii ii ii i iir iin n oi on uo o n edd nnen uh en nna a au lli l g c Ll cu ula se csca nn ee ee n ne err rr rru ee et tl llee ilrl ii iRd hdd op nuu nn ndn n u12 12e ee eee ee ee eee e e eee e eee e Taa i yyyyc ssss sss rr rr rr ttt tt t t tflll (a)Ordered by name. dn p yr ta d he n cuueme ni c re eoed on ed r rrat s c nr liir sa eL� mnn e mhso eooae o is ee t is a e osr e ha ltr cls nc n yin ur e u g tu d leg le r T� oane tv d brem er cla Rnnce nee eoe l lre e te riia eo sm s12 eme bp el Pi r uy r nra ur ua un oo er ie nid l �eh Ih re snt si nMe a urs t tlr fsf nn e aey uvl l lm aa r r ee di pH se m ai arin i Bi dePe i ee ecue ee o �r pp oes c na aT ree pan ae er u eae n te fflG muaat itt aly B r� a V12l ran ei eau ttr nl lo a. .. . ..t ub e hu t er evn i mm ne ei aao iu lt Mr ral h hf stie lh o ou omml e ee e ee eg nnl nior n.. .le ddn pl hh� en ie e vr li n au dr i oo utpo bm cs l sg i lvme ee ego vl q be t tmi rr c jr e bm mm m mm p bh G uuuu e nh bv oo aay lle zmc rpusj us an u bdu o ooo e oo r ndt av r aa aa al ll ly iij a avl iu e ea aa nal ll ll oo ooo ooo hh h hhs oa aa a pn hh ea . rree oaaa a alrr WW r criM MM M MM M M MM M MM M MM M oo ua l l ti Oi N sG GGG G GGD AV B BB BBBB B B CCCCC C CCCC C CC PPPEE TT TZFFF F S SF LL LJ J JJ I MM M Ml MLlM e C Mt Mm m m M o. . h Co G GG C FM tlaM me oe et lo h haB i iiC eBh m Cl ll. . . C uMG n um l ll aT eTo THC P P eBl C .o e ee Sae B ol nl B Gmth Mh cN ru h pu o oP .e aoG JG T lF hum cmn nn Pr a WWe cpa M Ita ho aAee ee o So mqM .lrP n n a a nCu E a Oec Z er ea hoE ooB V Fr LM p. r IaB ma c LC po G eFnb Vn an iB bnu g v s ulua utl t ooh pp nM ro Gr e ai nav srn B m er rr at aho tBoru M lFa aaDCC so a � lm mu a atdf oe a CuaaJ e o dn u pzlB sffls emr m ema Jee mm or o op gn en m iuj pb re re t ae esh tm rnt ag vbhh luesuvoa dl sfo r sr ar u pe ort r Mr eb ros ia lu .l vdh lhn ca e e yas je eye b b au am vve e d bd oo a�o �n da eaa ava aa aJ g aa l ceerii de hec eol r r l ut eu b eye l rjr sms vll iiiir r ror iir e r io oi i � �� a ii iiii iii r iin no i onu oo nedd nn e nuh enn n a aaul lil gc Ll cu ula s ec s ca nnee e en ne err rr rru ee e tt ll lee il rl i iiRdh dd opn uu nn ndnn u12 12 ee ee e eee ee e ee ee e ee ee ee eT a aiy yy yc s sss sss rr rr rr ttt tt tt tf l ll (b)Ordered by frequency. dnp y r ta dh e ncu u em en ic r e eo ed oned r rr a ts cn rl i irsa eL�m nne m hs oe oo ae o i s eet isaeo sr eh al tr cls nc n y inu r e ug t u dl e glerT �o ane t vdb remer c la R nn ce nee e o e ll re et e ri i aeo sms 12 eme b p elP i ru yr nr aur u au n oo er ien idl� eh Ihr e sn ts i nMea urs t tlrf sf nn e aey uvl l lm a a rr ee di pH sem ai ar i niB i d ePe i e e ecue ee o �r ppo esc naa T re epa n aee rue ae n te fflG m u aa tit ta ly Br� aV12 l ra n ei eau t tr n l lo a .... .. t ub e hu t ere v n i mmn eeia aoi ult Mrr al hh f s t iel ho ou omml eeee ee eg n nl nio rn ...l edd n pl hh�en iee vrli n au dr io o ut pobm cs ls gil vme eeego vlq be tt mi rr cj re bmmmm mm pbhGu u uu enh bv o o aayll e z m crpu s jusa n ub du oo oo eoo rn dt av r aaaaa l lll yii j aav l iu ee aaa n al llll oo o oo oo o hhhhh so aaa apn hh ea. rr ee o aa a aa lrr WW rcr i MMMM MM MM MMMMMMM MMoo ua llt i Oi Ns GG GG GG G DA VB B BB BBB B B CCCCC CCCC C CCC PPP EE TTT ZFF F FSSF LLL J JJJ I (c)Ordered by community. Figure 3.60 Differently ordered matrix representations of the same data. Adapted from bost.ocks.org/mike/miserables/. nodeattribute.Morecomplexapproachesclusterthenodesbasedonstructural properties of the graph so that similar rows and columns are placed next to eachother.Asfindingtheright orderingthatrevealsthecharacteristicfeatures of the graph remains challenging, interactive rearrangement is often helpful. Figure 3.60 illustrates the impact that the ordering has. All three matrices showthesameco-occurrencegraphoftheplayLesMisérables,whichwealready visualized in the introduction of this book in Figure 1.1 on page 7. Different shadesofgreenvisualizethenumberofco-occurrencesoftwocharacters,where darker greens represented larger numbers. The shades of blue in the matrix diagonal visualize the overall frequency of a character’s occurrences. Three different ordering procedures are depicted. In Figure 3.60a, rows and columns are simply ordered by the name of the characters. The matrix in Figure 3.60b has been ordered according to the characters’ occurrence frequency. Finally, Figure 3.60c shows the matrix ordered by the result of a community detection algorithm. As we can see, the more sophisticated the ordering procedure is, the clearer are the patterns in the matrix representation. Taken together, the main benefit of matrix representations is their focus on the graph’s edges, whereas the nodes are merely indicated as labels along the rows and columns. The visual mapping of the graph data is straightforward. While there is no need for layout optimization, matrix ordering is an intricate problem. It should further be noted that a matrix’s drawing space grows quadratically with the number of nodes. ImplicitRepresentations Matrix and node-link representations as described before have in common that they visualize the relations in the data, that is, the edges by means of dedicated graphical primitives (marked cells or links). In this sense, we can call these representations explicit.
Visualization Methods and Techniques (cid:4) 117 1 4 2 1 2 4 4 5 1 5 1 2 5 3 2 3 4 5 3 3 (a)Node-link. (b)Inclusion. (c)Overlap. (d)Adjacency. Figure3.61 Node-link representation compared to implicit representations. Adapted from [SHS11]. In contrast to that, implicit representations lack explicitly drawn edges. Instead,relationsbetweennodesareencodedimplicitlybytherelativeposition of the nodes. However, this strategy is applicable only if the graph structure followssomeregularities.Generalnetworkscanhardlybevisualizedbyimplicit approaches. Yet, implicit representations are very well suited for trees with their regular parent-child relations. There are three options for the implicit encoding of parent-child relations. The encoding can be based on inclusion, overlap, or adjacency as illustrated in Figure 3.61. For inclusion, the child nodes are included in the parent node. For overlap, the children merely overlap their parent. For adjacency, children and parent are placed next to each other. The absence of explicitly drawn edges obviously puts a visual emphasis on the nodes. In fact, implicit representations often aim to be space-filling, meaning that they strive to place the graphical primitives representing the nodes as tightly as possible in the available drawing space. Depending on the applied layout strategy, this can be fast and simple, or time-consuming and complex. Examples of implicit representations are abundant [SHS11]. Figure 3.62 showsafewselectedtechniques,allvisualizingthesameclassificationhierarchy ofmammalswithabout3,000nodes.The2DsquarifiedtreemapinFigure3.62a is based on inclusion [BHvW00]. The information pyramids in Figure 3.62b use adjacency to encode the parent-child relations by stacking the levels of the hierarchy along the z-axis [AWP97]. The 3D sunburst in Figure 3.62c uses adjacency as well, but now the levels are organized as a kind of shells around the central root node [SHS11]. For many more examples, the reader is referred to the website http://treevis.net, which collects all kinds of tree visualization techniques. In summary, the upside of implicit graph representations is their efficient usageofdrawingspaceforthenodeprimitives.Adrawbackisthatoverplotting can hamper perception if nodes are not carefully laid out. Moreover, the directionality of edges may get lost, depending on the chosen layout strategy. Overlap can easily show directionality, as either node u overlaps node v or the other way around. However, adjacency is symmetric and thus requires additional conventions such as drawing from top to bottom or from left to right in order to encode the directionality.
118 (cid:4) Interactive Visual Data Analysis (a)Squarified Treemap. (b)Information pyramids. (c)3D sunburst. Figure3.62 Implicit visualizations of a classification hierarchy. Software cour- tesy of Steffen Hadlak. HybridRepresentations Node-link, matrix, and implicit representations are suited for different graph data.Node-linkdiagramsaregoodforsparsenetworks,whichhaveamoderate number of edges. Dense networks with many edges are best visualized using a matrix. Trees, as we just said, are nicely represented by implicit approaches. But what if our graph has sparse and dense parts and includes tree-like sub-structures at the same time? This is where hybrid representations can help. They combine different representational paradigms. The idea is to mix representations, where possible, inordertoutilizethepositiveaspectsandcompensateindividualdisadvantages. Yet,thisalsoaddsanewquestion:Whichpartofagraphshouldberepresented in which way? Some hybrid representations leave the decision completely to the user, who then has to style the different parts of the graph via interaction. Yet, it is also possible to use methods to detect densely connected, sparse, or tree-like sub-structures in graphs and to assign representational paradigms automatically [AMA07]. AparticularlyniceexampleofahybridtechniqueistheNodeTrix[HFM07]. As the name suggests, it is a combination of node-link and a matrix repre- sentation. Figure 3.63 shows an example of the Les Misérables co-occurrence network, where the dense communities are visualized as sub-matrices, whereas connections between these communities are depicted as curved links. To sum up, node-link, matrix, implicit and hybrid representations are the fundamental options that visualization designers have at their disposal when creating images of graphs. In the next section, we will look at advanced graph visualization approaches taking into account additional data facets. 3.5.3 VisualizingMulti-facetedGraphs With the visualization approaches described above, we are able to represent the nodes and edges that make up a graph’s relational structure. However,
Visualization Methods and Techniques (cid:4) 119 CMU - Roth et al. Bederson et al. PARC Eick et al. Shneiderman et al. Berkeley Plaisant et al. Figure 3.63 NodeTrix visualization of a co-author network. Adapted with permission by Jean-Daniel Fekete from www.aviz.fr/Research/Nodetrix. we also mentioned additional facets that might be relevant when analyzing graphs. In particular, we named multivariate data attributes, temporal and spatial references, as well as hierarchical groupings of nodes and edges. This section deals with incorporating these additional facets into the visualization of a graph. Multi-faceted graph visualization is a research topic on its own [HSS15]. The design challenge lies in finding a balanced visual representation of the involved facets that meets the analytic or communicative goals of the user. In order to arrive at a well-balanced representation, it makes sense to follow a two-step design procedure. First, a base representation has to be defined for the primary graph facet whose depiction governs the overall display. Second, the additional facet(s) will be incorporated into the base representation. For example, if we want to visualize the graph structure together with multivariate attributes, we could choose a node-link diagram as the base representation for the structure. The visual representation of multivariate attributes could be incorporated by varying color, size, or shape of the nodes and the color, width, or dash pattern of the links. Alternatively, if our focus is moreontheattributesandlessonthestructure,wecouldchooseatable-based visual representation for multivariate data elements. The structural aspect could then be incorporated by drawing additional links between the rows of the table. In general, analogously to what has been said about the combination of views in Section 3.1.2, a visual combination of different data facets can be performed in two different ways. On the one hand, a temporal composition can be implemented by utilizing display time to show one facet after the other. On the other hand, the combination of facets can be realized through a spatial composition, which utilizes the available display space. Thebasicoptionsforspatialcompositionarejuxtaposition,superimposition, and nesting. Figure 3.64 illustrates these options for the case of combining
120 (cid:4) Interactive Visual Data Analysis (a)Juxtaposition. (b)Superimposition. (c)Nesting. Figure 3.64 Spatial composition of graph facets in a single representation. Adapted from [HSS15]. structure and geo-spatial context. For juxtaposition, structure and geo-spatial context are shown side-by-side. This results in a balanced view of both facets. However,itisnecessarytolinkthefacetsviaadditionalvisualcues,forexample, by arcs as in Figure 3.64a. For superimposition, the facets are drawn on top of eachother.IntheexampleinFigure3.64b,thegeo-spatialcontextisgoverning the display of the structure in that the geographic regions prescribe the layout of the graph nodes. This is also the case for nesting, where the nested facet has to obey the positioningandspaceconstraintsofthebaserepresentationintowhichwenest. Depending on which facet is nested into which, the visual results can differ significantly. For example, both instances of nesting in Figure 3.64c show the samedata,butconveydifferentinformation.Nestinggeo-spacewithinstructure (left) tells us which geographic region belongs to a given node, whereas nesting structurewithingeo-space(right)communicateswhichnodesbelongtoagiven region. Letusnextlookattwoconcretesolutionsthatdealwithmulti-facetedgraph visualization. First, we will consider the visualization of dynamic spatial trees asa3Dlayeredmaprepresentation.Thesecondsolutionvisualizesmultivariate compound graphs in a multiple-views system. 3DLayeredMapVisualizationofDynamicSpatialTrees As defined earlier, dynamic graphs change over time T, and spatial graphs are connectedtospatialreferencesS.Similartotemporaldataingeneral,dynamic graphs are usually analyzed with respect to their evolution over time. Which nodes and edges enter or leave the graph, and which parts remain persistent over time? When dealing with spatial graphs, the analytic interest is focused on the interplay between the graph structure and the spatial context. Next,wewanttodemonstratehowtreestructurescanbevisualizedtogether with their temporal and spatial facets. The example we are going to discuss is based on a map display of the spatial frame of reference. The tree structures will be nested into the regions of the map. Superimposition of several map layers is then used to communicate the temporal evolution of the data.
Visualization Methods and Techniques (cid:4) 121 Figure3.65 Map with tree layouts embedded into selected regions. Spatial Nesting of Tree Structures As just said, we start with a display of the spatial facet as a map. The next step is to embed the structural aspect of the trees into the map regions. For the later integration of the temporal facet, it is necessary to use a layout algorithm that assigns fixed positions to the tree nodes. Moreover, the algorithm must be able to adapt the layout to the irregular shape and size of the geographic regions. Both requirements can be fulfilled by combining a point-based layout with a skeleton-based region subdivision [Had+10]. Figure 3.65 shows an example result. We can see how the trees fit nicely into the different map regions. Temporal Layering and Difference Encoding In order to visualize time, a separatemaplayeriscreatedpertimestep.Followingtheideaofthespace-time cube introduced in Section 3.4.3, the map is tilted into 3D and the layers are stacked along the third display dimension. Figure 3.66 shows three such layers. We can now easily see the structures per time step. Yet, what can be more difficult to detect are the changes between the indi- vidual layers. Therefore, the visualization includes an explicit visual encoding of differences. Red and blue spikes indicate where nodes left or entered the trees. Moreover, orange and brighter blue lines connect those nodes for which the associated data values change significantly. Spikes and lines are always perpendicular to the map layers thanks to the fixed node positions of the tree layout.
122 (cid:4) Interactive Visual Data Analysis Figure 3.66 Three map layers visualize the data of three consecutive time steps. Spikes and lines indicate differences between the layers. Note that the static figure cannot reproduce the experience of a live 3D visualization due to the lack of dynamic motion, which is an important depth cue.Becauseourhumanperceptionisverywelltunedtointerpreting3Dscenes, the actual association of nodes and edges with layers, spikes, and lines can be quite easily interpreted with the running system. But still, there is a clear limitation of the presented approach: Only a few time steps can be visualized simultaneously. If the number of time steps increases, alternative means must be applied. One option would be to scroll through time, which corresponds to a mapping of the time in the data to the display time. In general, multi-faceted graph visualization becomes more difficult when the graph itself is large and when the involved facets are extensive as well. In the next section, we illustrate how a multiple-views approach can help in coping with this challenge. Multi-viewVisualizationofMultivariateCompoundGraphs As the number of nodes and edges in a graph increases, it becomes more and more difficult to show all of them in a visualization. In such cases, it makes sense to consider groups of nodes and edges as in compound graphs. A compound graph G = (V,E,H) is a graph where a hierarchical grouping is given as a rooted tree H. The set of leaves of H corresponds to the nodes V of G. Non-leaves of H correspond to the groups and are sometimes called meta-nodes or cluster nodes. Groups can be expanded or collapsed in order to study a graph at different levels of abstraction. A multivariate compound graph G=(V,E,H,A) is a graph whose nodes V and edges E have additional data attributes A. The central objective when analyzing multivariate compound graphs is to understand the relationship of structural properties of the graph and the
Visualization Methods and Techniques (cid:4) 123 attributes and groups associated with it. For example, given a subset of nodes being similar in their attributes, do they exhibit similar structural properties? Or, given a certain sub-structure of the graph, do the nodes in that sub- structure exhibit similar attribute values? Or, are the nodes in one group similar to the ones of another group? To answer these and similar questions, multiple coordinated views can be employed as in the graph visualizations system CGV [TAS09]. From a conceptual perspective, the system solves the problem of multi-faceted graph visualization via juxtaposition of facets, while individual views may work with superimposition. Multiple Views Figure 3.67 shows eight different views as provided by the system. The graph structure (V and E) is visualized in the matrix at the left and in the node-link diagram in the center. Selected attributes of A are encoded in these views by varying colors and sizes. A dedicated multivariate representation of the data attributes is offered by the parallel coordinates plot at the bottom. The density of nodes in the graph layout is visualized in a splat view in the bottom-right corner. What remains to be visualized is the hierarchical organization H of the graph. This is done in the three views at the top. The graphical hierarchy view (top right) shows nodes as colored triangles. This nicely visualizes the sizes of the groups defined by H. The textual tree view (top middle) enables users to read the labels associated with the groups. The 3D tree representation on the left is called Magic Eye View [KS99]. It shows the compound hierarchy Figure3.67 Multiple coordinated views for multi-faced graph visualization.
124 (cid:4) Interactive Visual Data Analysis explicitly as red dots and blue links projected onto a hemisphere. Additional orange arcs span the hemisphere to visualize selected links of E in relation to H. Finally, a property view in the bottom-left corner lists the attributes A associated with a selected node or edge. As we can see in this example (and in Figure 1.3 on page 10), there are quite different representations of one and the same graph data. Yet, each view focuses on different facets of the graph. Coordinated Highlighting In order to make sense of such a multiple-views visualization, it is obviously necessary to mentally connect the different repre- sentations. This can be supported by coordinated highlighting, which means thatthefocusedgraphelementisconsistentlymarkedinallviews.Forexample, the node with the label “albert, einstein, life” is marked with a circle in the node-link diagram and with a blue background in the tree view. The parallel coordinates plot visualizes the polyline of the node in red, rather than the regular black. The graphical hierarchy view, the matrix view, and the splat view show crosshairs to mark the node. Changing the focus in either view will change the marks in all views simultaneously. Additionally, the blue line in the graphical hierarchy view on the top indicates the current cut through the grouping hierarchy. When the user descends or ascends in the hierarchy by expanding or collapsing groups, the blue line will automatically adjust its shape to make clear to the user how deep in the graph the ongoing analysis takes place. The different views of the graph together with the coordinated highlighting make it possible to study even large multivariate graphs. In general, we can conclude that multi-view visualizations are a good solution when the data are complex in the sense that they contain a diverse set of facets. With the two examples of multi-faceted graph visualization, the section on how to represent relations among the data comes to an end. We have learned thatdedicatedmethodsexisttovisualizedifferentclassesofgraphs.Visualizing multi-faceted graphs requires diverse and rather complex representations and considerable design effort is necessary to create them. In Chapter 5, we will see how graph visualization in general can be advanced further with the help of automatic analytical computations. 3.6 SUMMARY This chapter explained how data can be mapped to visual representations for interactive visual analysis. At the heart of visualization are two fundamental steps, the visual encoding and the visual arrangement. The visual encoding is about mapping data to graphical marks and visual variables. The visual arrangement describes the layout of the data-representing graphical marks in one or multiple views. For the most part of this chapter, we described how the fundamental visualization steps are implemented in the context of different classes of data.
Visualization Methods and Techniques (cid:4) 125 In fact, a key message of this chapter is that the characteristics of the data are crucialfordesigningtheirvisualization.Wedealtwithseveraldataaspectsthat need to be considered: Are the data related to time T, are the data embedded in the geographic space S, or do the data contain relations R to be exposed in the visualization? These aspects can also occur in combination. In particular, we discussed the visualization of: • Multivariate data A • Temporal data T ×A • Geo-spatial data S×A • Spatio-temporal data S×T ×A • Graphs R • Multi-faceted graphs R×T ×S×A Aswehaveseen,themoreaspectsareinvolved,themorewecanclaimthat designing expressive and effective visual representations is challenging. And therearestillmoreaspectsworthbeingconsideredinthecontextofinteractive visual data analysis. Additional Visualization Aspects What we have not addressed in this chapter is the aspect of data quality. In fact, most of the data in the wild are “dirty”. By dirty we mean there are missing values, there are uncertain values, there are inconsistencies, and there are errors [GS06]. These problems add a whole new layer to the visualization design. For example, the visualization of uncertain geo-spatial data not only involves the aspects of space S and data A, but also the aspect of uncertainty U. Considering uncertainty U leads to new questions one may want to investigate. Here is an incomplete list: • A→U – What data are uncertain? • T →A×U – How do data values and uncertainty evolve over time? • T ×S →U – Where in time and space are the data uncertain? • R→U – Which structural relations are uncertain? • ... Uncertaintyisbutoneaspectamongothersworthbeingintegratedintothe visualization. Another source of information is provenance P. This not only includes data provenance, which explains how the data came about, but also insight provenance, which explains how insights have been generated [Rag+16]. It is obvious that the inclusion of provenance P expands the space of possible analytic questions to an even greater extent.
126 (cid:4) Interactive Visual Data Analysis Prioritized Multi-aspect Visualization With such a multitude of aspects having an impact on the visualization design, a simple combination of visual- ization techniques is often not successful. Instead, it is necessary to prioritize certain aspects and adjust the visualization design accordingly [Düb+17]. To this end, scalable visual representations with different levels of sophisti- cation must be developed for each involved aspect. For example, the spatial aspect could be represented in full detail using sophisticated terrain rendering, or as a contour representation that only indicates the main features of the spatial frame of reference. If space plays an important role in an analysis scenario, the sophisticated terrain rendering is employed. If space plays a minor role, the graphically less-expensive contour representation can be used. Making such importance-driven design decisions for all involved aspects leads to a prioritized multi-aspect visualization. However, this is a topic that requires much more research to arrive at a mature state with a comprehensive consideration of all relevant analysis aspects. Future work in this direction can draw inspiration from a wealth of available visualization techniques, some of which are collected in visualizations surveys. Visualization Surveys Since the foundation of visualization as a field in computer science, a variety of visual representations have been developed for all kinds of data. Here, we could only describe the fundamental concepts and procedures and illustrate them with selected examples. Many more techniques are described in the visualization literature. To help potential users find visual solutions to their data analysis prob- lems, it makes sense to catalogue the available visualization techniques. We already mentioned browser.timeviz.net and treevis.net, which provide comprehensive lists of techniques for temporal data and tree data, respec- tively. The InfoVis Wiki curates a compilation of such interactive online sur- veys at infovis-wiki.net/wiki/Interactive_Online_Surveys, including, for instance, surveys on: • Text visualization: textvis.lnu.se • Biological data visualization: biovis.lnu.se • Financial data visualization: financevis.net • Visualization of scientific literature and patents: paperviz.org • Visualization of dynamic graphs: dynamicgraphs.fbeck.com Often, these online resources are based on books or state-of-the-art reports that study specific visualization issues in greater detail [ML17]. These works should provide the interested reader with plenty of material to delve further into the world of visual data representations. At this point, we close the chapter on visualization. In the next chapter, we will focus on the role of interaction in the context of visual data analysis.
Visualization Methods and Techniques (cid:4) 127 FURTHER READING General Literature: [Spe07] • [War12] • [WGK15] Color Coding: [BRT95] • [HB03] • [ZH16] Visualization of Temporal Data: [Aig+11] • [Wil11] • [Bac+17] Visualization of Geo-spatial Data: [Mac95] • [AA06] • [And+13] Graph Visualization: [vLan+11] • [KPW14] • [Nob+19]
4 CHAPTER Interacting with Visualizations CONTENTS 4.1 Human in the Loop ......................................... 131 4.1.1 Interaction Intents and Action Patterns ............ 132 4.1.2 The Action Cycle ................................... 135 4.2 Requirements for Efficient Interaction ...................... 136 4.2.1 Interaction Costs .................................... 136 4.2.2 Directness of Interaction ............................ 138 4.2.3 Design Guidelines ................................... 143 4.3 Basic Operations for Interaction ............................ 144 4.3.1 Taking Action ....................................... 144 4.3.2 Generating Feedback ................................ 146 4.4 Interactive Selection and Accentuation ..................... 148 4.4.1 Specifying Selections ................................ 149 4.4.2 Visual Emphasis and Attenuation .................. 153 4.4.3 Enhanced Selection Support ........................ 156 4.5 Navigating Zoomable Visualizations ........................ 159 4.5.1 Basics and Conceptual Considerations ............. 160 4.5.2 Visual Interface and Interaction .................... 162 4.5.3 Interaction Aids and Visual Cues ................... 164 4.5.4 Beyond Zooming in Two Dimensions ............... 168 4.6 Interactive Lenses ........................................... 173 4.6.1 Conceptual Model ................................... 173 4.6.2 Adjustable Properties ............................... 176 4.6.3 Lenses in Action .................................... 178 4.7 Interactive Visual Comparison .............................. 184 4.7.1 Basics and Requirements ........................... 184 4.7.2 Naturally Inspired Comparison ..................... 186 4.7.3 Reducing Comparison Costs ........................ 190 4.8 Interaction Beyond Mouse and Keyboard .................. 194 4.8.1 Touching Visualizations ............................. 194 129
130 (cid:4) Interactive Visual Data Analysis 4.8.2 Interacting with Tangibles .......................... 197 4.8.3 Moving the Body to Explore Visualizations ........ 202 4.9 Summary .................................................... 204 V ISUALIZATION techniques provide us with expressive visual repre- sentations of data. As humans, we can interpret and make sense of the visual representations and draw conclusions about the underlying phenomena. Yet, if we look only passively at visual representations, we waste much of the potential of visual data analysis. Ideally, we would like to actively engage in a dialogwiththedata.Thisincludesgeneratingdifferentviews,studyingspecific details, and fine-tuning the visual encoding. All these activities are enabled through interaction. This chapter elaborates on many different forms of interaction for visual data analysis. When discussing interaction, four key aspects are relevant: the human, the tasks, the data, and the technology. Visual representations are studied and interpreted by human users. Based on their impressions, they will interact with the visualization system. The interactions are typically related to analytic tasks to be accomplished with respect to the data being studied. Technology is the mediator in the process of interactive visual analysis. It displays visual output and accepts the user’s input. Wrapped into a single sentence, this means: The user solves analytic tasks on data using technology. This chapter touches upon all of these four aspects. The chapter’s first part willfocusonthehuman’sroleininteractivevisualdataanalysis.InSection4.1, we motivate the human-in-the-loop approach of interactive visual reasoning and consider conceptual perspectives on humans interacting with visualization systems.Therequirementstobefulfilledinordertoarriveatusefulandusable interactionforvisualdataanalysiswillbediscussedinSection4.2.Throughout this chapter, we will keep a keen eye on human aspects. For the chapter’s second part, our attention will shift to the tasks to be carried out interactively and to the data on which they operate. We start with basic low-level interactive operations in Section 4.3. Section 4.4 deals with interactive selection and visual accentuation as fundamental tasks in visual data analysis. In Section 4.5, we continue with multi-scale exploration and navigation in zoomable visualizations. Section 4.6 explains how flexible and light-weight adjustments of visual representations can be made via interactive lenses. As an example of a more complex analytic task, Section 4.7 focuses on supporting visual comparison with naturally inspired interaction. As we discuss interaction for these different tasks, we will also be considering the data aspect. We will see that interaction, very much as the visualization, has tobedesignedaccordingtothedataathand.Thischapterincludesinteraction techniques for multivariate data, temporal data, spatio-temporal data, and graph data.
Interacting with Visualizations (cid:4) 131 Thethirdandlastpartofthischapterisdedicatedtothetechnologyaspect. Section 4.8 goes beyond traditional paradigms and sheds some light on novel ways of interacting with visualizations. As we will see, modern technologies such as touch and tangible interaction or large high-resolution displays open up new possibilities for interactive visual data analysis. Yet, new designs are necessary to make the best of what the new technologies offer. 4.1 HUMAN IN THE LOOP As indicated, we will begin this chapter with some general thoughts on inter- action for visual data analysis and the role of the human within this process. Let us start with the purpose of interaction. Is interaction necessary at all, can’t we let the computer do all the work? Well, if the analytic problem can be formalized precisely and its solution be calculated exactly, then certainly no interaction is needed. Typically, however, data analysis activities are not so simple. Jacques Bertin made this clear even before visualization existed as a field [Ber81]: “Agraphicisnot‘drawn’onceandforall;itis‘constructed’andreconstructed untilitrevealsalltherelationshipsconstitutedbytheinterplayofthedata.The bestgraphicoperationsarethosecarriedoutbythedecision-makerhimself.” Bertin,1981 Bertin’sassessmentstillholdstruetoday.Analyticgoalsareoftenill-defined andexploratoryinnature.Sometimesitisevenunclearwhatthedesiredresults are or how they should look like. In Chapter 3, we have seen many different visualization techniques. Deciding for suitable ones and configuring them appropriately require human expertise. It is interaction that enables the user to experiment with different visual encodings and to look at the data from different perspectives. Interaction is not only helpful for specifying how to visualize data, but also for selecting what to visualize. Data often capture complex phenomena as an interplayofthousandsofmulti-facetedpiecesofinformation.Thehumanmind, however, can digest only a limited amount of information at a time. Therefore, the data analysis must be divided into meaningful chunks. It is interaction that enables the user to define such chunks, to navigate between them, and to combine them in order to form a comprehensive understanding [Spe07]. In fact, interaction is not only for pragmatic purposes bringing the user closer to a goal. Interaction also serves epistemic purposes helping the user form and scaffold a better mental model of the problem being investigated and a better understanding of the tools being used [KM94].
132 (cid:4) Interactive Visual Data Analysis In summary, visualization helps us see things that are otherwise not visible, and interaction allows us to do things that we would otherwise not be able to do. While visual representations may provoke curiosity, interaction provides the means to satisfy it. 4.1.1 InteractionIntentsandActionPatterns Interaction is clearly important for visual data analysis. But what exactly actuates users to interact and what interactions are common? To answer these questions, next we will look into a high-level categorization of interaction intents and a more fine granular list of action patterns. InteractionIntents Interaction intents capture why users interact with visual representations. Sevenbroadcategories canbe identified [Yi+07].Without goingintotoomuch detail, next we will describe the basic idea behind each category. Mark something as interesting. When users identify something interest- inginavisualrepresentation,theytypicallyhavetheintenttomarkitfor furtherinvestigation.Markingscanbetransient tohighlightintermediate findings, or permanent to memorize important analysis results over a longer time. Show me something else. Forlargeandcomplexdata,itisoftenimpossible to visualize all information in a single view. To develop a comprehensive understanding, users have to explore different parts of the data and experiment with different combinations of variables to be shown in the visualization. Show me a different arrangement. Generating different visual arrange- ments allows users to study data from various perspectives to obtain different insights. For example, arranging data according to time can reveal trends, while attribute-driven layouts may be better suited to communicate data distribution. Show me a different representation. The visual encoding is decisive for what can be derived from a visual data representation. Therefore, users want to adapt the visual encoding to suit their needs, be it to carry out different analysis tasks, or to confirm a hypothesis generated from one visual encoding by checking it against an alternative one. Show me more or less detail. As in real life, users want to look at certain things in detail. On the other hand, users need an overview to keep themselves oriented. During data exploration, the level of detail needs to be adjusted constantly to satisfy the conflicting demands of studying subtleties and seeing the big picture.
Interacting with Visualizations (cid:4) 133 Show me something conditionally. It makes sense to restrict the visual- izationtoshowonlythosedatathatadheretocertainconditionsorsearch criteria that are particularly relevant to the task at hand. Interactively filtering out or attenuating irrelevant data clears the view and allows users to focus on their task. Show me related things. When an interesting finding has been made, a logical next step is to ask whether similar or related discoveries can be made in other parts of the data. To find, compare, and evaluate such relations, users wish to be connected to them on demand, for example by means of visual links to the related data. These seven categories cover interactions that are particularly relevant for visual data analysis. Next, we describe two additional categories of more general intents, which are nonetheless quite relevant for visual data analysis as well. Let me go back to where I’ve been. Because visual data analysis is an exploratory process, it is usually necessary to try out new views on the data and experiment with alternative what-if scenarios. If exploratory actions do not yield the desired results, users intend to return to a previous state. A history mechanism can keep track of the interaction and allows users to undo and redo operations. Let me change the interface. In addition to tuning the visual representa- tion to the data and tasks at hand, users also want to adjust the overall visualanalysissystem.Thisincludesadaptingtheuserinterface(e.g.,the arrangement of windows or the items in toolbars), and also the general management of system resources (e.g., display resolution and amount of memory to be used). The presented interaction intents summarize the reasons for an active participation of users in the visual data analysis. The next section focuses on more concrete action patterns. ActionPatterns An alternative perspective on interaction is offered by action patterns [SP13]. They are more oriented toward what users actually do to support the creation of insight during visual data analysis scenarios. Two types of action patterns can be distinguished. An action pattern can be: • unipolar or • bipolar. Unipolar patterns describe human actions that are performed in only one direction. For these actions, there is no natural opposite action. If semantically
134 (cid:4) Interactive Visual Data Analysis TABLE4.1 Examples of unipolar human action patterns [SP13]. Pattern Description Arranging changes ordering, either spatially or temporally Assigning binds features or values to be encoded Blending fuses visual representations together to form one entity Comparing determines similarities or differences Drilling brings out and displays interior, deep information Filtering displays subsets obeying certain criteria Navigating moves on, through, and around the data Selecting focuses on or chooses either individuals or groups TABLE4.2 Examples of bipolar human action patterns [SP13]. Pattern Description Collapsing/ fold in and compact visual items, or oppositely, fold them Expanding out or make them more diffuse Composing/ assembleandjointogethertocreateholisticrepresentations, Decomposing or oppositely, break up into separate components Linking/ establish relationships or associations, or oppositely, disso- Unlinking ciate and disconnect relationships Storing/ put aside for later use, or oppositely, bring stored items Retrieving back into usage meaningful at all, unipolar action can only be reversed by a generic undo operation. Selecting, navigating, and comparing are examples of unipolar action patterns. Bipolar patterns are characterized by the existence of two actions, where one action is the natural opposite of the other. Together, both actions can be used to make progress and to return to previous states. An example of a bipolar pattern is collapsing/expanding. The natural opposite action for collapsing multiple data elements into a single representative is expanding the representative to reveal its individual members. Therearemanymoreunipolarandbipolaractionpatternsbeingrelevantin thecontextofinteractivevisualdataanalysis.Tables4.1and4.2listprominent examples. We will return to some of them in particular in dedicated sections. In Section 4.4, we will look at selecting and filtering in more detail. Questions of navigating and drilling will be addressed when we talk about zoomable visualizations in Section 4.5. In Section 4.6, we will blend alternative visual representationsbymeansofinteractivelenses.Comparingasaquiteinteresting action pattern will be discussed in Section 4.7. At this point it should be clear that interaction is a multi-faceted concept forgivingmeaningtowhatisperceived,forcollectingrelevantinformation,and forextractingandstoringinterestingfindings.Inotherwords,thehumanisnot
Interacting with Visualizations (cid:4) 135 only a passive onlooker, but an active participant in a dynamic process. How this process can be modeled and understood conceptually will be discussed next. 4.1.2 TheActionCycle Fromaconceptualperspective,theideaofthehuman-in-the-loopcanbenicely explainedwithNorman’saction cycle.Theactioncycleisageneralmodelthat describes interaction via several stages of action [Nor88; Nor13]. As shown in Figure 4.1, the human and the system are connected via two phases: an execution phase and an evaluation phase. The execution phase is related to performing the interaction, whereas the evaluation phase is concerned with interpreting the visual response generated by the system. Goal EVALUATION PHASE Visual Evaluate Interpret Perceive Response HUMAN SYSTEM Intend Plan Execute EXECUTION PHASE Figure4.1 Stages of action forming the action cycle. Adapted from [Nor13]. Stages of Action Figure 4.1 suggests that a motivating goal is required before the human engages in some data analysis activities. The action cycle actually starts with intending some change that aims at making progress toward the analysis goal. The second stage consists of forming an action plan to satisfy the intent. The plan is then executed physically in the third stage. The system will then process the user’s request and generate a visual response to be presented to the user. This is where the evaluation phase starts.Thenewvisualrepresentationisperceived,andsubsequentlyaconscious interpretation takes place. In the final evaluation stage, the result of the interaction is compared to the original intent. If there is a mismatch of result and intent, the action cycle has to be re-run. Certainly, the user has to change the course of action to generate an alternative result. For our explanation, the action cycle ran only once. In practice, though, the action cycle is executed many more times. This is only partly due to discrepanciesthatmayoccurbetweentheintentandtheresultofaninteraction. A far more important reason for interaction is the dynamics of data analysis activities.Asindicatedearlier,interestsmayvarygreatlyashumansinvestigate
136 (cid:4) Interactive Visual Data Analysis visual representations of data. Spotting something interesting may raise the intent to study it in detail. Seeing the details may provoke curiosity to look at a different visual encoding. Doing so may lead to new insights or questions thatmayturnthedataanalysisinyetanotherdirectioncreatingyetnewideas and objectives. Levels of Interaction Depending on the number of runs typically necessary to achieve a goal, one can distinguish different levels of interaction: low-level interaction, intermediate-level interaction, and high-level interaction. At a low level, interaction is concerned with mapping the fundamental degrees of freedom offered by input devices to basic operations of pointing at andmanipulatinggraphicalobjects.Wemaythinkofsuchlow-leveloperations as the interaction alphabet or syntax. At an intermediate level, basic low-level operations are combined to form semantically meaningful data analysis activities. These include navigation in the data, adjusting the visual encoding, or filtering with respect to criteria of interest. Intermediate-level interactions can be understood as a kind of interaction vocabulary. Similartohowlow-leveloperationsarecombinedtocarryoutintermediate- level activities, so are the intermediate-level activities a precursor to high-level problem solving. At this level, interaction is considered more broadly as a catalyst for analytic thinking and discovery. The interaction vocabulary is employed to form longer action sequences that support the formation, refinement, and falsification of hypotheses. This includes setting data into relation, extracting high-level features, and organizing the derived knowledge artifacts in an analytic visual-interactive workspace. As we see, there is a cascade where high-level interaction builds upon intermediate-level interaction, which in turn is based on low-level interaction. Attheveryheartofthiscascadeistheactioncycle.So,thesuccessofinteraction essentially depends on designing the action cycle appropriately. To this end, we need to know more about the requirements for interaction in the context of visual data analysis. 4.2 REQUIREMENTS FOR EFFICIENT INTERACTION For visual data analysis to be productive, interactions must be designed prop- erly.Thissectiondealswiththecostsofinteractionanddiscussesrequirements that should be met to allow users to accomplish their analysis goals effortlessly. 4.2.1 InteractionCosts As a matter of fact, the phases of the action cycle occasion costs [Lam08]. This is the reason why they are also denoted as the gulf of execution (the intend,
Interacting with Visualizations (cid:4) 137 plan, and execute stages) and the gulf of evaluation (the perceive, interpret, and evaluate stages) [Nor88; Nor13]. The costs can be physical or mental. Physical Costs are related to performing physical actions, such as moving the fingers or forearms to control the mouse, and to physically sensing the visual response with the eyes. These are largely visceral activities, which happen without the human paying special attention to them. Physical costs can be significant when engaging in visual data analysis activities. Much of the interaction is of exploratory nature. For such trial-and-error procedures, many repetitive actions can be necessary. For example, functionality may be hidden in cascaded menu structures, which require long and accurate, and hence costly, pointer movements. Exploring the data typically requires constant navigation from one point of interest to another. During the course of an interactive data analysis, pointer mileage and click counts can accumulate and interaction can become a strain. Inasimilarvein,perceivingthevisualfeedbackresultingfrominteraction can be costly. If an interaction leads to much change in the visual representation, the eyes have to sense much information and transmit it to the brain. If visual changes are distributed across the display, the eye muscles will have to work a lot to capture each individual change. Mental Costs pertain to the stages of intending, planning, interpreting, and evaluating interaction. At these stages, conscious or subconscious activities require the user pay attention to the interaction. Just like the physical costs, the mental costs can be considerable. Visual- ization tools are often rich in functionality, and there are various ways to accomplish a task. How does a user know which graphical objects afford what actions upon the visualization? The more options there are, the more difficult it is to identify them, mentally weigh them, and make a decision for one or the other alternative. Similarly, the visual response can be costly to interpret and evaluate. It is not easy to contrast a refreshed visual representation against how it haslookedbeforetheinteraction.Thedifficultyliesincomparingagainst animagethatisnolongerbeingdisplayed,butinsteadisonlytransiently present in the human’s short-term memory. On top of that, visual data representations are usually packed with information, which further complicates the evaluation. If the entire layout of the visual representation changes due to interaction, the user may not be able to follow up and comprehend the effect. On the other hand, the visual response could be hardly visible, because it affects only a few pixels on the screen. This could leave the user wondering if the interaction is ineffective or if the system was not properly notified of the interaction intent.
138 (cid:4) Interactive Visual Data Analysis In the light of interaction costs, we may wonder if we should really try to tackle each and every data analysis problem by means of interaction? No! While interaction is a powerful tool, it is not a silver bullet. Interaction can be a burden when seemingly simple tasks are cumbersome to accomplish due to bad interaction design. Moreover, users may feel uncomfortable with being responsible for even the most basic parameter settings of a visualization. Another problem can be the arbitrariness of visual representations generated through interactive adjustments. It is no longer clear if a feature visible on the screen actually corresponds to a finding in the data or if it is just an artifact caused by an inappropriately set parameter. This points us to think of interaction in a less-is-more way. The system should be responsible for doing its best to relieve the user of unnecessary work. Only as a last resort should input be requested from the user. Fromthepreviousparagraphs,wecanconcludethatitisessentialtonarrow the gulf of execution and the gulf of evaluation: Each stage of the action cycle should require as little human effort as possible. Or put differently, interaction has to be designed such that its costs are minimal. In the following, we will see that the directness of interaction is an important factor in this regard. 4.2.2 DirectnessofInteraction The directness with which interaction is carried out largely determines how smoothly and efficiently the action cycle can run and how deeply the user can immerse in an interactive dialog with the data. The importance of what is called direct manipulation has been recognized quite early in human-computer interaction research. Hutchins and colleagues advocate direct manipulation and use visual data analysis as their motivating example [HHN85]: “Areweanalyzingdata?Thenweshouldbemanipulatingthedatathemselves; orifwearedesigningananalysisofdata,weshouldbemanipulatingthe analyticstructuresthemselves.” Hutchinsetal.,1985 Nowadays, direct manipulation is the preferred paradigm for interacting withdataandtheirvisualrepresentations.Thebasicideabehinddirectmanip- ulationisthattheuseroperatesdirectlyonthevisualrepresentationofthedata using physical actions, and the system provides feedback immediately. Accord- ing to its classic definition, a direct manipulation interface is characterized by three key properties [SP09]:
Interacting with Visualizations (cid:4) 139 • Objects and actions of interest are presented continuously using mean- ingful visual metaphors. • The user’s requests are expressed through physical actions, rather than complex syntax. • Actions are rapid, incremental, and reversible, and their effect is imme- diately visible. Thesepropertieshaveanimmediateconsequenceforthedesignofinteractive visualizations.Avisualrepresentationisnolongeronlyameanstocommunicate data to the human. Additionally, a visual representation has to provide the meanstoenablethehumantointeractwiththedataandthesystemingeneral. Knowing the basic idea behind direct manipulation, we can next study the question: What does directness actually mean? SeparationasanInverseMeasureofDirectness In order to develop an understanding of directness, it is helpful to look at directness from an opposite point of view. In fact, directness is inversely proportionaltothedegreeofseparationofthehuman’sactionandthesystems’s response. In other words, we do not achieve directness if there is a high degree of separation. Different types of separation can be detrimental to directness. There are: • conceptual separation, • spatial separation, and • temporal separation. Conceptual Separation relates to the different models involved when inter- actingforvisualdataanalysis.Therearetheuser’smentalmodel,thesystem’s implementation model, and the interface’s represented model [CRC07]. The mental model comprises the analytic problem being dealt with and the con- cept a user has about the system. It abstracts from details and focuses on goal-relevant aspects. Visualization software adheres to an implementation model. This is a formal model full of technical details, algorithmic conventions, and parameterized procedures. The mental model and the implementation model exhibit a large conceptual separation, as illustrated schematically in Figure 4.2. Therefore, there is the third model, the represented model. This model captures what the user can actually see and interact with on the display. The closer the represented model is to the mental model, the more directly can users interact.
140 (cid:4) Interactive Visual Data Analysis MENTAL REPRESENTED IMPLEMENTATION MODEL MODELS MODEL Reflects user’s vision Reflects technology Better Worse Figure 4.2 Conceptual separation across different models. Adapted from [CRC07]. Spatial Separation concerns distances to be covered during the interaction. Large distances can increase interaction costs considerably. Costs accrue when users have to move the pointer across large distances in order to execute certain actions. Spatial separation is also problematic when the eyes have to switch frequently between different parts of the screen when evaluating the system’s response. Consider, for example, the scattered data visualization and thecorrespondinggraphicaluserinterfaceinFigure4.3.Theuserinteractswith the controls to the right, whereas the visual response becomes visible in the main view. The problem is that action and effect are spatially separated. This can make it more difficult to understand the action-effect causality. The user may have to shift the attention back and forth between the user interface and the main view several times to comprehend how certain parameters influence the visual representation of the data. Figure4.3 Spatial separation between the graphical user interface (right) and the visual representation in the main view (center). Temporal Separation is about the latency between the user’s action and the system’s visual response. Ideally, a response should be provided within 50–100ms [Shn94; Spe07]. As a matter of fact, if the latency is too high, the efficiency of interactive visual exploration degrades [LH14]. However, the computations involved in processing the visualization transformation and generating the visual feedback can take a considerable amount of time. For
Interacting with Visualizations (cid:4) 141 example, the visualization in Figure 4.3 consists of scattered data points, their corresponding Voronoi diagram, and a smooth coloring in the background. Withrespecttolatency,theindividualdatapointscanberenderedvirtuallyat no time cost. Even the Voronoi diagram can be computed quite quickly given the fact that there are only ten data points to be processed by a Θ(nlogn) algorithm.Forthesmoothcoloring,however,acolorvalueneedstobecomputed for every pixel of the main window. Already for a moderately sized window with a 1,280×1,024 resolution, 1,310,720 pixels need to be colored. On a 4K display, it will be 8,294,400 pixels. This can cause a noticeable delay leading to the adverse effects of efficiency degradation. All three types of separation impair directness and hinder smooth and efficient visual data analysis. Therefore, reducing separation should be a top priority throughout the visualization development process. This concerns both the design and the implementation. Interaction and visual feedback should be designed so as to minimize conceptual and spatial separation. Implementation- wise, algorithmic efficiency is important to reduce temporal separation. ScenariosofDifferentDirectness We previously discussed directness on a theoretical level. But how does direct- ness manifest in practice? Next, we sketch five scenarios illustrating different degrees of directness (or separation) when interacting with a visualization. For these scenarios, we assume a user has spotted an interesting group of nodes in a graph visualization and wants to zoom in to look at them in detail. There are several alternatives to accomplish this by interacting with the system. 1. Source Code Editing On the implementation level, a visual represen- tation is defined by source code. Only a few lines of code need to be edited to set the visualization view to where the nodes of interest are located. The altered code is compiled and run to see if the visual result is as expected. If not, the procedure is re-run until the user is satisfied. Changing code lines, re-compiling, and test-running the visualization is the least direct form of interaction, as it exhibits large conceptual, spatial, and temporal separation. 2. Scripting Commands Alternatively, the visualization may offer a script- ing interface allowing the user to enter commands to zoom the view. Once issued, the commands take effect immediately while the visualiza- tion is running. In this scenario, no separate compilation is necessary, which reduces the temporal separation. But still the interaction is rather indirect, and several commands may be necessary before the view fits as desired. 3. Graphical Interface Thefieldofviewisdisplayedinagraphicalinterface alongsidethevisualization.Standardcontrolssuchasbuttonsandsliders
142 (cid:4) Interactive Visual Data Analysis allow the user to easily shift the view and control its zoom factor. Any changes are immediately reflected in the graphical interface and the visualization. Given the fact that the graphical interface represent the view status and at the same time serves to manipulate it, the conceptual gap is narrowed. Yet, the interaction (with the controls) and the visual feedback (in the graph visualization) are still spatially separated. 4. Direct Manipulation The user zooms the view directly by drawing an elastic rectangle around the nodes to be inspected in detail. This is a rather simple press-drag-release operation when using the mouse. During the interaction, visual feedback constantly indicates the frame that will make up the new view once the mouse button is released. Any necessary fine-tuning can be done using the mouse wheel. In this scenario, the manipulation of the view takes place directly in the visualization. There is no longer a spatial separation between the interaction and the visual feedback. Or is there? 5. Direct Touch Indeed there remains some degree of separation. The inter- action is carried out with the mouse, whereas the feedback is shown on the screen. To obtain a yet higher degree of directness, the interaction can alternatively be carried out using touch input on the display. The basic principle of sketching an elastic rectangle to specify a new view is maintained, but the action is performed using a finger directly on the display. Now, the interaction takes place exactly where the visual feedback is shown. A truly direct way of zooming in on a node-link diagram. Thesefivescenariosillustratedifferentdegreesofdirectnesswheninteracting with a visualization. Certainly, our list is not exhaustive, but it does contain some general ideas on how interaction can work in practice. From the examples, we may be inclined to think that direct touch is the best solution. But that is not quite true, we have to differentiate. Each scenario involves its own benefits and drawbacks. Direct touch, for example, tightly links interaction and visual feedback. However, the hand being used for the touch interaction may occlude the user’s view on important information. Scripting commands, for another example, can be difficult to specify for novice or casual users. Yet, expert users may be faster and more precise issuing commands than using a graphical form of specification. Moreover, commands are easily reproducible. Even the least direct scenario of modifying lines of source code has some benefits. It enables a visualization developer to test- drive several alternative visual representations without the need to set up and implementdedicatedinteractionmechanisms.Thiscanbehelpfulwhenrapidly prototyping visualization software. From our discussion we can see that it is important to carefully weigh the means of interaction that a visualization should exhibit. It can very well be
Interacting with Visualizations (cid:4) 143 that several alternatives are necessary to be able to adapt the interaction to different user audiences. In any case, we need to design the interaction such that it is useful and usable. The next section provides some guidelines that can help us achieve this goal. 4.2.3 DesignGuidelines Usability and user experience are important factors for the development of usable and useful interaction. They subsume several objective and subjective quality criteria for interaction, including efficiency, predictability, consistency, customizability, satisfaction, engagement, responsiveness, and task confor- mance, to name only a few. GeneralRules Guidelines can inform the design of efficient interaction. Here, we list the golden rules by Shneiderman and Plaisant [SP09]. They are not specifically related to visual data analysis, but still offer valuable advice. 1. Striveforconsistency.Consistentactionsshouldberequiredinsimilar contexts and be responded to consistently. 2. Cater to universal usability.Thesystemshouldbeusablefornovices, casual users, and experts alike. 3. Offer informative feedback. For each possible action, there should be informative feedback appropriate to its importance. 4. Design dialogs to yield closure. Action sequences should have a well-defined beginning, middle, and end. 5. Prevent errors. The system should prevent serious errors and be able to recover from minor problems. 6. Permiteasy reversalof actions.Actionsshouldbereversibletoallow for undoing accidental actions and to encourage exploration. 7. Support internal locus of control. Users should be given the feeling that they are in charge, not the computer. 8. Reduce short-term memory load. Short-term memory load should be limited to seven plus minus two chunks of information. FluidInteraction Inthecontextofvisualdataanalysis,onecanoperationalizetheaforementioned general rules to define what is called fluid interaction [Elm+11]. The idea of fluid interaction includes three guiding principles:
144 (cid:4) Interactive Visual Data Analysis • Promote flow • Support direct manipulation • Minimize the gulfs of execution and evaluation We already talked about direct manipulation and minimizing the costs of interaction. The first principle, promote flow, though is worth a few words of explanation. By “promoting flow” it is meant that interaction should be designed so that users can totally immerse in the data analysis activities. This requires balancing the difficulty of analytic tasks and the skills of the user, giving the user a sense of control over what is going on, having the right tools available at the right time, and providing an overall rewarding experience. Heer and Shneiderman underline the significance of the fluent, direct, and human-centered character of interaction in visualization [HS12]: “Tobemosteffective,visualanalyticstoolsmustsupportthefluentandflexible useofvisualizationsatratesresonantwiththepaceofhumanthought.” HeerandShneiderman,2012 With this statement, we end the first part of this chapter, in which we considered interaction mostly at a conceptual level, focusing on the role of the human-in-the-loop. The upcoming second part will be more concrete in terms of interaction tasks performed when analyzing different data. Based on a brief look at low-level interaction in the next section, later sections will introduce specific interaction concepts for interactive visual data analysis. 4.3 BASIC OPERATIONS FOR INTERACTION We already mentioned that visual analysis involves high-level interaction and builds upon intermediate-level interaction techniques, which in turn are based on low-level operations. These basic operations are the subject of this section, which is intended to bridge our previous conceptual considerations and the interaction techniques described in the sections to come. As visual data analysis is largely based on interacting with visual repre- sentations, we will focus on fundamentals of graphical interaction, that is, the procedure of performing physical actions in the real world that change virtual graphical objects on a computer display.
Interacting with Visualizations (cid:4) 145 OUT OF RANGE POINTING MANIPULATING Exit Release 0 1 2 Enter Press Move Move Move Figure4.4 Three-state model of graphical input. Adapted from [Bux90]. 4.3.1 TakingAction When interacting with visual representations, there are two basic questions. We have to define, first, where our interaction should take effect and, second, what the effect should be. Consequently, we distinguish two basic operations: • point and • manipulate. Pointing allows us to define which graphical objects we want to interact with and the manipulation defines what should happen with the objects. For example,wemaypointatanodeinanode-link-diagramtomarkitasrelevant, orwepointatasliderhandleandthenmanipulateittoadjustafilterthreshold. Physically, pointing and manipulation can be carried out in different ways. The necessary actions are typically related to movements that are tracked with an interaction device. Moving our hand to control a computer mouse is a prominent example. Moving our fingers allows us to press down a button or rotate the mouse wheel. We are also used to moving fingers and hand across our mobile phone’s surface. Conceptually, graphical interaction can be described with a three-state model[Bux90].Figure4.4showsthestatesasboxesandthetransitionsbetween them as arrows. Consider state 1 first. This is the state we are in when moving the mouse to point at something on the screen. As soon as we press a button, we transition from state 1 to state 2. This is how manipulations are triggered. For example, we can move the mouse to drag a filter slider. Upon releasing the button, we will return to state 1. If the mouse is out of reach, for example, if we move it beyond our application window or lift it from the table, we fall back to state 0, in which movements of the input device do not have an effect. Note that the previous paragraph describes only one simplified instance of the three-state model. Different configurations exist for different input modali- ties. For example, touch interaction does not support state 1 out of the box. Once our finger is on the surface, we immediately begin manipulating what is underourfingertip.Cleverinteractiondesignandadditionaltiminginformation are necessary to compensate for the missing state. On the other hand, a mouse with multiple buttons will lead to additional states and transitions. These can be exploited to provide more ways of interaction, but also complicate design and use.
146 (cid:4) Interactive Visual Data Analysis ModesofInteraction With the three-state model, we now have a conceptual understanding of how interaction is performed on a low level. We can further utilize the model to distinguish two characteristic modes of interaction. There are: • discrete (or stepped) interaction and • continuous interaction. Triggering a short state change can be considered a discrete interaction. Examples would be pressing and immediately releasing the mouse button or briefly tapping on a touch-enabled surface. In both cases, we temporarily enter state 2 to trigger a manipulation. We may have clicked a button to change the visual encoding or tapped a data element to mark it. Discrete interaction is most useful for occasionally making a selection when there are only few alternatives to choose from. However, in visualization scenarios, we often face very many alternatives. This not only concerns the many data elements that we may want to study in detail. There are also many parameters with potentially large value ranges to control the visualization transformation. Exploring the data and browsing different parameterization in a purely discrete fashion would certainly be cumbersome. This is where continuous interaction comes into play. For continuous inter- action, we remain in a manipulating state for a longer time while continuously movingtheinputdevice.Forexample,whenadjustingafilterslider,eachincre- mentalcursormovementresultsinaseparatevisualresponse.Theadvantageis that a larger space of options can be scanned in a short period of time using a singlecontinuousgesture.Thismakescontinuousinteractionparticularlyuseful for exploratory visual data analysis, where testing many what-if alternatives is a common task. 4.3.2 GeneratingFeedback So far, we have considered the fundamental low-level actions that users may perform. Yet, to complete the action cycle, a user request always requires a visualresponsebythesystem.Fromasystemperspective,twooperationsneed to be carried out: • update and • refresh. The update operation is concerned with changing the internal state of the visualization based on the action performed by the user. The refresh operation, on the other hand, is responsible for presenting a new visual representation that reflects the internal change.
Interacting with Visualizations (cid:4) 147 Perceive Manipulate Refresh Update VIEW MODEL CONTROLLER Figure4.5 Model-view-controller pattern. Again, a conceptual point of view can help us clarify the update and refresh operations. This time, we refer to the model-view-controller (MVC) pattern [KP88]. MVC is a software design pattern for graphical user interfaces. Figure 4.5 illustrates a variant simplified for the context of this text. In our case, the model consists of the data and all parameters of the visualization transformation. The views correspond to visual representations of the data and auxiliary information. The controllers conceptually capture all different ways of interaction. As we see from the figure, once the user performs some manipulation, the controller sends an update request to the model. This update can be as simple as a mode switch, but may also be as complex as performing some analytical processing of the entire data. Once the model has updated its internal state, the views are notified to refresh themselves to reflect the new model state. Again this could require a simple repaint using different colors, but also a complete rebuild of the visual data representation. In terms of temporal separation of action and response as discussed earlier in Section 4.2.2, both update and refresh are critical. Ideally, they run so fast that the user has the feeling of having a system that reacts immediately. TypesofVisualFeedback Update and refresh are the operations that generate the visual feedback. But how can the feedback itself be characterized? Analog to the modes of interaction, we can distinguish two types of visual feedback: • static feedback and • animated feedback. For static feedback, the system creates a single new visual representation to replace the old one. The new image immediately corresponds to the system’s new state. Such an abrupt visual change can be useful to draw the user’s attention toward the significant differences caused by the interaction. However, there is also a drawback: With static feedback, it might be difficult to com- prehend how the new visual representation has evolved from the old one. For example, when switching to an alternative graph layout algorithm, the graph nodes may be located at completely different positions. In such situations, users will have a hard time maintaining their mental model of the data.
148 (cid:4) Interactive Visual Data Analysis With animated feedback, the visual representation changes gradually. The system generates a series of responses to smoothly interpolate the display from its current state to the new one. Each intermediate step represents only an incremental update, which makes it easier for users to follow the changes and tounderstandtheeffectoftheiractions.Forourpreviousexampleofswitching the graph layout, nodes can be moved smoothly from their original locations to the new ones. However, a difficulty with animated feedback is that it takes timetocompleteandhencedelaystheactioncycle.Therefore,animatedvisual feedback has to be designed with care, but it is worth the effort. To summarize, pointing and manipulation are the basic operations to be carried out by the user. As we have learned, interactions can be discrete or continuous,wherethelatterisparticularlyusefulinthecontextofvisualization. Updating the data model and refreshing the visualization views are the basic operations to be carried out by the system in order to create visual feedback. The feedback can be static or animated, where the latter can help users better understand the effect of their actions. Together, the basic operations as described in this section are the fundamental building blocks for interactive graphical systems. In the following sections, we add the necessary semantics to create useful interaction techniques for visual data analysis. Next, in Section 4.4, we start with interactive selection as a preparatory step for many other operations such as highlighting, filtering, or even modifying data. How data can be explored interactively at multiple scales will be explained later in Section 4.5. 4.4 INTERACTIVE SELECTION AND ACCENTUATION From our regular work with computers, we know that selecting something on thedisplayisafundamentalandfrequentlyusedactionpattern.Inthecontext of visual data analysis, interactive selection takes a similarly central role. In fact, selection is a door opener for visual data analysis. It allows us to divide the analysis into smaller manageable subtasks simply by marking parts of the data to be relevant to the question at hand. Conceptually, selection can be described as follows. Assume we want to analyze a visual representation of some dataset V(D) that contains a lot of information. As we cannot digest all information at once, we successively concentrate on different subsets of the data. Let D+ ⊂D denote what we are currentlyinterestedin.ThedataD− =D\D+ arenotinourfocus,andhence, are considered less relevant for the time being. D+ and D− are constantly in flux as the user’s attention and interests change during the data analysis. The distinction of D+ and D− implies two things. First, interactive visual analysisrequiresmeanstoenableuserstospecifywhatisrelevanttothem.This aspectwillbediscussednextinSection4.4.1.Second,thevisualrepresentation of the data must be adapted in such a way that the relevant data stand out. Corresponding accentuation strategies will be introduced later in Section 4.4.2.
Interacting with Visualizations (cid:4) 149 4.4.1 SpecifyingSelections There are two points of departure for specifying a selection: Data can be selected based on their location on the display and based on the actual data values.Intheformercase,theselectiontakesplacedirectlyinthevisualization. This is also known as brushing [BC87; MW95]. In the latter case, selection criteria are defined on the data-level using dedicated controls, which is also referred to as dynamic querying [AS94; Shn94]. Next, we take a closer look at the two alternatives. InteractiveBrushing Interactive brushing works by marking a part of the visual representation and the data shown in that part get selected. With interactive brushing it is possibletospecifythreecategoriesofselections:pointselection,rangeselection, and composite selection. Point Selection A very basic form of brushing is to point at a location of interest followed by a discrete interaction, such as a mouse click or a tap on the screen. But how do we know which data reside at the selected point? Toanswerthisquestion,thevisualizationtransformationhastobeinverted. Thisisalsocalledpicking.Whilethevisualizationtransformationmapsdatato graphics, picking reverses this mapping in order to get from graphics to data. Depending on the underlying graphics, different methods can be employed for picking, including lookup-buffers, geometrical tests, or plain mathematical calculations. Range Selection There are two classic techniques for marking entire 2D ranges in a visualization: rubberband selection and lasso selection. Both are continuous interactions that employ a press-drag-release gesture for defining the selection. The rubberband selection uses an elastic shape, for example a rectangle, which makes it easy to mark orthogonal parts of the visualization. If the layout of the data is rather irregular, a free-form lasso selection is more practical. It offers more flexibility, but comes at higher interaction costs, because more (and more accurate) physical movements of the pointer are necessary. Rubberband and lasso define a geometric shape (rectangle or free-form shape) based on which the actual selection is made. There are two alternatives for that. For data to be selected, they must either be included in or intersect with the shape. Inclusion and intersection have different characteristics in terms of accuracy and effort. Which alternative is more suitable depends on the graphical objects to be selected. For smaller regular objects like the nodes in the graph layout in Figure 4.6a, inclusion is practical. We can easily and quite accurately include them in a rectangle or lasso. In contrast, larger irregular objects such as the geographic regions of the map in Figure 4.6b
150 (cid:4) Interactive Visual Data Analysis (a)Selection by inclusion. (b)Selection by intersection. Figure4.6 Rubberband selection for marking multiple data elements. are difficult to mark without adding too many unwanted objects. In this case, an intersection-based mechanism is more practical because it is easier to just touch the data we want to mark. Enclosing the data entirely is not necessary. With the techniques described so far, we can select the data from one particular point or area. But how can we select data from multiple origins? Marking them with a lasso selection is cumbersome at best, if not impossible. Therefore, we need interactions that enable us to combine and edit selections. Composite Selection There are various ways to compose multiple select operations. In fact, with 524,288, the number of theoretically possible com- binations of add, remove, subtract, intersect, union operations on selections is extremely high [Wil96]. Therefore, it makes sense to follow established standards. Multiple selections can typically be composed using modifier keys. Holding down the control key will usually toggle the data’s selection state. That is, unselected data become selected and vice versa. When data are arranged in a linear order, as for example top to bottom for the trajectories in Figure 4.7, thenthecontrolandshiftkeyscanbeusedincombinationtoselectmultiple subranges. Similar variations of the selection behavior can be designed for area selec- tions via rubberband or lasso. The shift and control keys can be used to add and remove data to and from a selection, respectively. Holding both keys simultaneously enables the creation of intersections. In summary, interactive brushing together with the options for composing selections enables us to specify our interest directly in the visualization. The advantage is that we can easily select what we see. On the other hand, the selection is based solely on the data’s visual arrangement. This can lead to situations were selections are unnecessarily complex to define. Therefore, it makes sense to complement interactive brushing with dynamic queries.
Interacting with Visualizations (cid:4) 151 1 2 SHIFT 3 CTRL 4 SHIFT Figure4.7 Four steps of selecting multiple trajectories using modifier keys. DynamicQueries Data-based selection via dynamic queries is helpful when visually based selec- tion is impractical. Consider, for example, the trajectory visualization in Figure 4.8a and suppose we are interested in selecting the low-speed segments in red and yellow. Brushing them is hardly possible. Rubberband selection is infeasible due to the irregular arrangement of the segments, and lasso selection would require a costly circumvention of the high-speed segments. Composite selection would be possible, but it would take very many individual operations. So why not select low-speed segments based on the speed values directly? To this end, we need a dedicated representation of the value range of the speed attribute and a mechanism that allows us to mark what we are interested in. Interactive Legends Interactive legends are a helpful tool for specifying dynamic queries [RLP10]. The example legend shown in Figure 4.8b consists of multiple sections, each is associated with a specific data interval. Users can simply click on a section to (un)select all trajectory segments that fall into the corresponding data interval. In our example, the segments with high speeds have been unselected to concentrate on the data where movements are slow. This would have been very hard to accomplish with interactive brushing, whereas querying via the legend took but a few clicks. Query Sliders Dynamic queries can also be done in a continuous fashion. For an illustration, let’s assume we want to select the red high-degree nodes from the graph visualization in Figure 4.9a. As we can see, the graph layout does not correspond to the characteristics based on which we want to select, which rules out interactive brushing as a suitable selection method. Moreover, we are not quite sure about what ‘high-degree’ means precisely and need a bit of flexibility to experiment with potential thresholds.
152 (cid:4) Interactive Visual Data Analysis (a)How to select slow-speed segments? (b)Select via an interactive legend! Figure4.8 Selecting segments based on their color, which represents speed. (a)How to select high-degree nodes? (b)Select via slider handles! Figure4.9 Selecting nodes based on their data attributes. This is where query sliders enter the stage. They are commonly applied in visual analysis scenarios [Eic94]. The exemplars in Figure 4.9 consist of a scale representing the value range of the node degree and two handles enabling the user to specify, in a continuous fashion, a data interval of interest. In Figure 4.9b, we have adjusted the slider so as to select all nodes with a degree between 23 and 29. The advantage of dynamic query sliders is obvious: We can precisely define our interest based on the characteristics of the data. Observing the visual representation while specifying the selection further enables us to evaluate on the fly if the result suits the task at hand. Adding more handles allows for selecting multiple distinct data ranges. It is even possible to combine multiple sliders to create sophisticated multi-criteria selections. However, a disadvantage of dynamic queries is that the interaction no longer takes place directly in the visual representation of the data, but with a dedicatedinterface,alegendoraslider.Thismayleadtospatialandconceptual
Interacting with Visualizations (cid:4) 153 separation. Therefore, it is important to strive for a tight integration of the selection interface and the visualization. In our previous examples, color has been used as the link that connects interface and visualization. This makes it easier to comprehend how a selection at the interface will affect the visual representation. The interactive mechanisms described above enable users to specify their interest in the data. The next step is to adapt the visualization such that relevant data stand out, allowing the user to fully concentrate on them. We havealreadyseenseveralexamplesofhowthiscanbedoneinthefiguresofthis section. Next, we will systematically discuss different strategies of emphasizing or attenuating certain elements of a visualization. 4.4.2 VisualEmphasisandAttenuation Given a visual representation of the data V(D) and a selection of relevant data D+, the question that we deal with now is how to visually distinguish D+ from the rest of the data D−? What is needed are additional encodings that either visually emphasize V+ or visually attenuate V− certain parts of the data. An alternative is to suppress parts of the data altogether. UsingD+,D,D− andV+,V,V− asaconceptualbasis,onecanimplement different strategies: Highlighting Highlighting emphasizes the data of interest V+(D+) in con- trast to a regular visualization of less-relevant data V(D−). Dimming Through dimming, we can attenuate less-relevant data V−(D−), while data of interest are visualized regularly V(D+). Filtering For filtering, only the relevant data are visualized regularly V(D+), whereas all other data are omitted. A decision for either strategy should be made carefully. As a general rule of thumb, the visual feedback should be effective, but with only minimal side effect on the regular visualization. There are two important influencing factors: the frequency of change of the selection and the size of the selection. Highlighting and dimming contrast the regular visualization V against an alternative encoding (either V+ or V−). If D+ can be assumed to be small, as for example when hovering the cursor over individual elements of a visualization, then highlighting is a sensible choice. If D− is small, as for example when we want to get rid of a few outliers, then dimming makes sense. Filtering effectively clears the view and allows users to fully focus on D+. A disadvantage, though, is that the user is no longer aware of the presence of D−. Therefore, filtering is typically applied only when a selection is stable for a certain subtask during the visual analysis. Theoretically, further strategies are possible. For example, one could redundantly emphasize relevant data and dim or filter less-relevant data
154 (cid:4) Interactive Visual Data Analysis REGULAR VISUALIZATION V(D) D– D+ D– HIGHLIGHTING V+(D+) + V(D–) DIMMING V(D+) + V–(D– ) FILTERING V(D+) Figure4.10 Strategies for visual emphasis of relevant data and attenuation of less-relevant data. (V+(D+)+V−(D−) or V+(D+), respectively). While this would generate a stronger contrast, the regular encoding V would not be used anymore. As this violates the rule of thumb not to interfere too much with the visualization, such redundant encodings are hardly applied in practice. Let us next illustrate our conceptual considerations with the example in Figure 4.10. Our starting point is a simple visual encoding that shows a time series as bars of varying size in neutral gray, our V. In order to emphasize and attenuate selected parts of the data, we vary color in this example. For V+, we use a dedicated highlighting color that can be easily distinguished from the neutral gray. Dimming is accomplished by using a lighter gray for V−, which blends well with the white background. Note that filtering is indicated via dashed outlines, even though the data are actually invisible. Figure 4.10 shows but a simplified example to illustrate how selected data canbeaccentuated.Inreal-worldapplications,findingsuitablevisualencodings isconsiderablymoredifficult.Usually,theregularencodingV alreadyoccupies much of the visual resources (e.g., color, size, position) for the purpose of effective and efficient visualization of the data. The difficulty is to come up with adequate V+ and V− to achieve emphasis and attenuation. The actual design challenge is that V+ and V− should not (or only mini- mally) interfere with V, but need to be sufficiently expressive to contrast D+ against D−. This design challenge has to be solved depending on application needs. A general strategy though is to use visual variables that are not yet used for the regular visualization. If this is impractical or impossible, it can even be necessary to embed additional graphical elements, such as outlines or halos into the visual representation. Of course this has to be done sparingly to avoid cluttering the visual representation of the data. Figure 4.11 provides a few more practical examples for visual emphasis and attenuation for graph visualization. Figure 4.11a shows a node-link diagram where node size and color encode certain node attributes. We want to concen-
Interacting with Visualizations (cid:4) 155 (a)Original visual representation. (b)Highlighting by encircling nodes. (c)Dimming nodes and edges. (d)Filtering nodes and edges. Figure4.11 Visual feedback for selections in visual representations of graphs. trate on the four larger red-orange nodes. It is not possible to highlight the nodes using a special color, because the nodes are already color-coded. Instead, Figure 4.11b shows the selected nodes highlighted via additional graphical primitives, circles in this case. While the nodes are now clearly marked as relevant, our attention may still be distracted by the presence of too many nodes and edges. Therefore, let us further draw the user’s attention to the relevant informa- tion by dimming all unselected nodes and their incident edges. In Figure 4.11c, the data of interest stand out clearly now, and it is easier to see how they are interconnected. For an even more focused view, we can filter all irrelevant information as in Figure 4.11d. While this removes any distraction, it also obliterates awareness of the filtered information. Looking at the figures, one can easily imagine alternative solutions, and indeed the design space for visual emphasis and attenuation is large [Hal+16]. Any visual variable can theoretically be used as a visual cue, including color,
156 (cid:4) Interactive Visual Data Analysis size, position, texture, blur, and even pulsing animations or blinking [WH04]. Particularly promising are visual cues that are perceived pre-attentively, which means they immediately draw our attention [HE12]. Deciding for either of these options is not an easy task, even for the binary distinction of D+ vs. D− discussed so far. Matters quickly become complicated when it comes to encoding multiple selections simultaneously in a single visualization image. In conclusion, the discussion in this section makes clear that expressive visualaccentuationisasimportantaseffectivemeansforinteractivelyspecifying selections. 4.4.3 EnhancedSelectionSupport So far, we have discussed basic means of interactive selection. In the following, we will introduce methods to enhance the work with selections in visual analysisscenarios.Smoothbrushing willallowustogobeyondbinaryselections. Brushing & linking helps us propagate selections across multiple visualization views. We will further see how automatic methods can be used to reduce the costs of interactive selection. SmoothBrushing Up to now, our selections define a binary distinction of data into selected and unselected. Smooth brushing is a concept that breaks the barrier of discrete binary selection [MW95; DH02a]. The idea is to assign to each data point a continuous selectedness from the interval [0..1]. The extremal values 0 and 1 stand for unselected and selected, respectively. Data points with a selectedness greater than 0 and smaller than 1 are somewhat selected. In other words, we are working with a kind of fuzzy selection. Obviously, a fuzzy selection requires dedicated interaction and visual feed- back. Let us illustrate this with the parallel coordinates plots in Figure 4.12. As with regular brushing, the user marks the data range to be assigned a selectedness of 1. This is shown in Figure 4.12a. For smooth brushing, the selectedness automatically spreads beyond the initially defined range until it gradually fades out to zero, as shown in Figure 4.12b. The automatic gradual spread is key to capturing features that by nature are not crisp and clear, and hence, costly to define. In order to provide visual feedback for smooth selections, it makes sense to consider selectedness as an additional attribute to be visualized alongside the data. The binary selection in Figure 4.12a shows selected and unselected data tuples as black and gray lines, respectively. The smooth selection in Figure 4.12b encodes selectedness by varying the lightness of the gray tones. This allows us to see even subtle differences in selectedness.
Interacting with Visualizations (cid:4) 157 (a)Binary selection in the range. (b)Fuzzy selection beyond the range. Figure4.12 Brushing a range (red) of an axis for binary and fuzzy selection. Brushing&Linking Regardlessofwhetherweuseregularorsmoothbrushing,itisalwaysexecuted on a single visual representation. However, visual data analysis often requires workingwithmultiplerepresentations.Thequestionthatarisesishowselections can be propagated across several visualization views? Consider for example the graph visualization system in Figure 4.13. It consists of four views: a textual tree view to the left, a hierarchy view on the top, a node-link view in the center, and a parallel coordinates view at the bottom. Each view allows us to select data that interests us. But how can the selectionbekeptconsistentacrossallviews?Manuallyreplicatingtheselection in the other views is certainly impractical. What is needed is a concept to automatically link all views. Or, as Buja and colleagues put it [Buj+91]: “Multipleviews,however,shouldnotberegardedinisolation.Theyneedtobe linkedsothattheinformationcontainedinindividualviewscanbeintegrated intoacoherentimageofthedataasawhole.” Bujaetal.,1991 Brushing & linking (or focusing and linking) provide the answer to the aforementioned questions [BC87; Buj+91]. Brushing is what the user does manually. The linking is done automatically by the system. The basis for the linking is a dedicated selection model where views are registered. As all views share the same model, a selection made in one view is automatically available inallotherviews.Theonlyresponsibilityoftheindividualviewsistorepresent the selection visually. For the example in Figure 4.13, the tree view shows a bluelabelbackgroundforselectednodes,thehierarchyviewusesdarkoutlines, the node-link view encircles selected nodes, and the parallel coordinates view dims data that are not selected.
158 (cid:4) Interactive Visual Data Analysis Figure4.13 Brushing & linking in a multiple-views graph visualization. So, with brushing & linking, we only need to perform the selection in one view instead of several. Moreover, we can revise the selection in any view, no matter where the selection has been triggered, and the selection consistency is maintained automatically. This gives us much flexibility in specifying our interest in selected parts of the data. AutomaticSelectionSupport Interactive selection can particularly benefit from integrating automatic meth- ods that take over otherwise manual selection steps. The general idea is that only an initial mark has to be placed by the user. From there, an automatic procedure completes the selection. We have already seen instances of this idea in the previous sections. For smooth brushing, selectedness is automatically spread beyond the initially brushed data interval. For brushing & linking, selections are automatically propagated across multiple visualization views. Developing dedicated selection algorithms is sensible if manual selections areverycostly.Thisisthecase,forexample,whenselectingclustersin3Dpoint clouds. Purely interactive selection in 3D point clouds is notoriously tedious and time-consuming work. Automatic selection procedures can facilitate this work[Yu+12].Basedonasimple2Dlassoselection,analgorithmautomatically expands the selection along analytically determined structures in the 3D point cloud. This effectively relieves the user of many incremental and costly 3D selection steps. The idea of automatic selection support can be generalized in various ways[HAW08].Forthispurpose,itiscommontotransformgraphicalselections intoabstractmodelsthatdescribeselectionrulesorconstraintsratherthanthe selection state of individual data elements [Che04]. Such declarative models
Interacting with Visualizations (cid:4) 159 help us apply the same selection to different datasets, maintain selections even when the data vary over time, or derive relaxed selections when broadening the view on the data is necessary. With this outlook on automatic support, we end the section on interactive selection and accentuation. We have learned that being able to mark and emphasize data according to varying interest is of fundamental importance for visual data analysis. In the next section, we will see that it is just as important to be able to examine data at varying scales, especially when dealing with large amounts of data. 4.5 NAVIGATING ZOOMABLE VISUALIZATIONS Data are usually explored, because it is not clear upfront where interesting patterns are located in the data. Interactive selection as described in the previous section enables us to investigate different parts of the data. Yet, this is only one part of the data-exploration equation. Asecondreasonfordataexplorationisthatitisinmanycasesalsounclear how deeply valuable information is buried in the data. This makes it necessary to analyze the data at different scales, a concern that is particularly relevant for larger data that span several magnitudes. An interactive solution to this problem is multi-scale data exploration via zoomable interfaces [Bed11]. Zoomable visualizations enable the user to explore different parts of the data at different scales. Shneiderman formulated the visual information seeking mantra as a fundamental guiding principle for studying data at varying scales [Shn96]: “Overviewfirst,zoomandfilter,thendetails-on-demand.” Shneiderman,1996 The mantra suggests to start from an overview. The overview offers a big picture by visualizing as much data as possible. Yet it is only a coarse big picture,asthereisnospacefordetails.Detailedinformationcanbegatheredby zoomingintosubsetswithlessdata.Lessdatameansmorespacefordetailsand closer inspection. With a zoomable visualization, the user can freely explore information at variable scales. The findings made during the exploration are compiled into a comprehensive understanding, very much like putting together the pieces of a puzzle. Superficially, zooming sounds very much like successively focusing on data subsets D+ as discussed in the previous section. The Shneiderman mantra, though, explicitly distinguishes between zoom and filter, since there is a big
160 (cid:4) Interactive Visual Data Analysis REGULAR VISUALIZATION V(D) D– D+ D– FILTERING V(D+) D+ ZOOMING V=(D+) Figure4.14 Using V= to scale relevant data to fit the display space. difference in how D+ is represented visually: Zooming works with a visual encoding V= that is capable of scaling the graphical objects representing D+, whereas with filtering, the focused subset is visualized only regularly V(D+). To illustrate the difference, Figure 4.14 compares a filtered view to a zoomed view.Filteringandzoomingaresimilarinthatnoirrelevantdataaredisplayed. Yet, for zooming V=(D+), the relevant data are scaled up so as to fit the available display space, the bars are enlarged and distributed evenly across the horizontal axis. Obviously, the size of D+ determines the scale of the visual representation. Discarding only a small part of the data such that |D+| ∼ |D| will lead to only a marginal change in scale. Focusing on a very small portion of the data |D+| (cid:28) |D| will bring the data analysis to a much finer scale. At any time, though, the user will see only a particular subset at a particular detailedness. It is the flexible interactive adjustment of focus (which part) and scale (which granularity) that enables users to engage in multi-scale data exploration. 4.5.1 BasicsandConceptualConsiderations Conceptually, zoomable visualizations build upon the notions of a world, a viewport defined in the world, and a screen. The world corresponds to the spatial arrangement of the visualized data. The viewport acts as a window into the world, and as such, determines what information is to be projected onto the screen. By moving the viewport, different parts of the world can be made visible. By resizing the viewport, one can adjust how much of the world is shown. The duo of move and resize operations of the viewport is commonly denoted as pan and zoom. Figure 4.15 illustrates a basic visualization world and how three different viewports lead to three different representations on the screen. AttheveryheartofzoomablevisualizationsisthefunctionV= thatdefines how the data representation gets adjusted when the scale is changed. This function can be implemented in different ways to accomplish:
Interacting with Visualizations (cid:4) 161 Viewport WORLD Viewport Viewport SCREEN SCREEN SCREEN setanidrooc neercS setanidrooc dlroW Figure4.15 Illustration of the conceptual model of zoomable interfaces. • geometric zooming or • semantic zooming. For geometric zooming, scale adjustments are considered at a geometric level only. That is, the visual representation is scaled according to the defined viewport. This can be done with a basic mathematical projection of the graphical objects. Semantic zooming goes beyond graphical scaling and allows for any kind of adjustments of the visualization depending on the scale. The additional semantics can be very valuable for visual data analysis. A simple example from graph visualization can illustrate how dramatic the difference between a purely geometric zoom and a semantically enhanced zoom can be. Figure 4.16 contains a sequence of three zoom operations toward acentralnodeofagraph(shownindarkgreen).Thezoomispurelygeometric: Everything becomes bigger as we zoom in. However, this does not help us much from a data analysis perspective. Figure 4.17 shows a semantically enhanced zoom. The scaling takes effect only on the nodes’ positions, but deliberately not on their size. The benefit of not scaling node sizes is that dense parts of the graph layout are untangled, creating an unobstructed view on the edges between the nodes. There is a second reason for keeping node size constant. In our case, node size encodes the node degree. To ensure a consistent interpretation of the data, the node size should not be changed during zoom operations. Since many visualizations use size as a visual variable to encode the data, it is important to pay close attention to what should and should not be scaled when zooming. The basic ingredients of zoomable visualizations are clear now. Next, we will look at the visual interface and the interaction that facilitate multi-scale data exploration.
162 (cid:4) Interactive Visual Data Analysis Figure4.16 Geometric zooming of a node-link visualization. Figure4.17 Semantically enhanced zooming of a node-link visualization. 4.5.2 VisualInterfaceandInteraction At any time, a zoomable visualization shows only a particular view of the data. Therefore, zoomable visualizations have to be designed such that users can orient themselves and take navigation steps easily, allowing full concentration on the data analysis objectives. This requires a suitable visual interface as outlined below. The interface should support the user in coping with three questions in particular [Spe07]: • Where am I? • Where can I go? • How do I get there? Where Am I? To facilitate the data exploration, it is essential to clearly communicate where the current view is located in the global context. Scroll barsasillustratedatthebottomandtotherightof Figure4.18indicatewhere in x-direction and y-direction the current view is located in the world. From the size of the scrollbars, we can further infer how much of the visualization is covered by the current view. Scroll bars require relatively little display space, but it takes some mental effort to extract the information they bear.
Interacting with Visualizations (cid:4) 163 Figure4.18 A zoomable graph visualization and its controls. Theoverview+detailconcept,asalreadydescribedinSection3.1.2,allocates more visual resources to convey information more explicitly. As can be seen in the left part of Figure 4.18, an overview shows a miniature version of the visualizationinadedicatedwindow.Withintheoverview,thecurrentviewport ismarkedwitharedrectangle.Thismakesitquiteeasytointerpretthecurrent view’s location and scale in relation to the entire visualization. However, a disadvantage of embedded overviews is that they obscure parts of the actual visualization. Where Can I Go? Similar to how scrollbars and overviews communicate locationandscaleofthecurrentview,theyindicatewhereonecouldpotentially go. The free space left and right as well as above and below the scrollbars and in the overview stand for parts of the data that can be accessed. Yet, indicating where one could potentially go is only a first step. As a second step, it makes sense to hint at where one could usefully go. The more information about the world is provided, the easier it is for the user to decide on the destination to explore next. Therefore, overviews often embed a highly abstracted visual representation of the data. In our case, the overview depicts the global graph layout only via tiny colored dots. But even though there are neither edges nor any details in the overview, we can still see accumulations of nodes that may be worth visiting. The size of these accumulations suggests to us at which scale we should look at them. How Do I Get There? Knowing where to go, the next question is how to actually get there? Getting to a new view may be accomplished by moving and scaling the current view, or by defining a completely new one. Conceptually, there are two basic options for users to actually carry out these operations. The first is to interact directly on the view. The second is to manipulate graphical objects of the interface. Table 4.3 lists some common
164 (cid:4) Interactive Visual Data Analysis TABLE4.3 Interactions for moving, scaling, and defining a zoomed view. Direct interaction on the view Move view Drag world in view, also known as panning. Scale view Mouse wheel, mouse drag, or pinch gesture on view. Define view Draw elastic rectangle in view. Manipulation of interface objects Move view Drag scrollbars or red frame in overview. Scale view Drag edges of scrollbars or red frame in overview. Define view Draw elastic rectangle in overview. implementations for both options. Interacting directly on the view is good for making smaller changes to explore the immediate vicinity of the current view. However, substantial changes are more difficult to make because they would require many repetitive interactions. This is where the manipulation of interface objects can be more practical. Yet, it is important to realize that the precision with which can be interacted is limited. This is due to the fact thattheinterfaceobjectsrepresenttheentirevisualizationspaceinarelatively small display space. In summary we see that a suitable interactive visual interface is indispens- able for multi-scale data exploration with zoomable visualizations. Yet, it does not stop there. Additional interaction aids and visual cues can further improve the utility of zooming and facilitate particular analysis tasks. 4.5.3 InteractionAidsandVisualCues With the techniques introduced so far, the navigation in the data is based solelyonthevisuallayoutofthedata.Forexample,whenwepaninanode-link diagram, we will get to see the data that is spatially near in terms of the graph layout. But what if the data analysis involves questions regarding the neighborhood in terms of the graph structure. The structural neighbors are not necessarily in the vicinity of the current view, but might be anywhere in the graph layout. The question is where are they and how can we get there? Another common task is to look for data that are similar to those shown in the current view. Again, the similar data are not necessarily spatially near the current view, but could be in a completely different part of the visualization. Again we ask ourselves, where should we go and how can we get there? Both scenarios outlined above have in common that the data we wish to inspect may not be visible on the screen. Therefore, our first goal is to make users aware of off-screen data. As a second goal, we want to enable users to actually navigate to these data. Third, we are interested in making the view change understandable. Next, we will see how off-screen visualization, navigation shortcuts, and animated feedback can help us achieve these goals.
Interacting with Visualizations (cid:4) 165 Arrow Halo Wedge Proxy Enriched wedge neercs-ffO neercs-nO Figure4.19 Visualcuesforpointingtooff-screendata.Adaptedfrom[GST13]. Off-screenVisualization Off-screen visualization is a form of focus+context representation where the zoomed view (the focus) is enhanced with additional context information. The idea is to embed additional visual cues into the display to make residue of otherwise invisible data elements visible. Figure 4.19 illustrates examples of visual cues for off-screen nodes (dashed lines) in a node-link diagram. From left to right, there are five alternative visual cues. An arrow is a most simple mark to point at off-screen data. While arrows only indicate a direction, halos and wedges visualize direction plus distance [BR03; Gus+08]. By mentally completing the halo to a full circle, we can infer the full circle’s center and so the location of an off-screen node. With a similar mental effort, we can determine a node at a wedge’s tip. Proxiesaimatrepresentingtheactualoff-screendatamorefaithfully[FD13]. Note how the proxy in Figure 4.19 allows us to see the off-screen node itself and also its connection to the visible part of the graph layout, rather than indicating direction and distance. Enriched wedges strive to balance several communicativegoals[GST13].Theyvisualizedirection,distance,andproperties of the off-screen object. Even additional meta-information can be embedded, for example, to explain why a node is considered relevant. Apparently, pointing to off-screen data has to be done sparingly to avoid cluttering the display. Therefore, an off-screen visualization is typically backed byamechanismthatautomaticallyinferswhichpartsofthedataarepotentially relevant.Suchmechanismscanbebasedondegree-of-interest(DoI)functionsor analyticalcalculationsonthedata.MoreinformationonDoI-basedexploration of graphs can be found in Section 5.2.1. NavigationShortcuts The next question to be addressed is how to get to data that are far from the current view. While basic pan and zoom interactions can of course be used for this purpose, we are now interested in making the navigation more cost-efficient. This can be achieved by means of navigation shortcuts.
166 (cid:4) Interactive Visual Data Analysis Off-screen nodes Proxy nodes Radar Screen Figure4.20 Bring & go with radar view and proxy nodes. A navigation shortcut has a target in the data and is further associated with a viewport. The viewport is typically centered on the target, and its extent defines how much context around the target will be shown. In order to actually take advantage of navigation shortcuts, we need to make them visible. A particularly useful approach is to employ the off-screen visualizationsdescribedbefore.Theonlythingthatneedstobedoneistoturn the otherwise passive visual cues into active interface elements that can be interacted with. Navigating to an off-screen target is then as easy as clicking or tapping a visual cue. The idea of combining off-screen visualization with navigation shortcuts is also known as Bring & Go. It is a very effective means to reduce navigation costs when exploring large data [Mos+09; TAS09]. Let us briefly illustrate the bring and the go part with an example. Figure4.20showsagraphvisualizationwithmanynodesoutsidethecurrent view. A so-called radar tool brings in the otherwise invisible graph nodes by placing proxy nodes at the screen boundaries. The proxies act as visual cues and also serve as navigation shortcuts. In order to go to a particular node, the user simply activates its associated proxy. Using navigation shortcut like this is much easier than repeatedly perform- ing pan and zoom operations. Note, however, that navigation shortcuts only work when potential targets are known. For exploring uncharted terrain, the basic pan and zoom operations are still important. We are now familiar with several interactions that allow us to navigate from one portion of the data to another. Closing the action cycle, we will next consider animated visual feedback to make transitions between different views easily comprehensible.
Interacting with Visualizations (cid:4) 167 AnimatedViewTransitions Themostsimpleformofvisualfeedbackistoinstantlyrefreshthevisualization once a new viewport is set. This corresponds to static visual feedback as introduced in Section 4.3.2. For small incremental changes of the viewport, as they occur when panning the view directly or when dragging a scrollbar, this type of visual feedback is a suitable option. However, when the viewport has changed substantially, for example, after activating a navigation shortcut, simply replacing the old view with the new one may confuse the user. The problem is that the user has no chance of creating a mental connection between old and new view. This problem can be addressed by providing animated visual feedback. A smoothly animated transition makes it possible for users to comprehend how one view evolves into another, and hence, to stay oriented within the data. Smooth viewport animations are based on the following general idea: First, zoom out from the current view, second, pan toward the new view’s location, and third, zoom in to the reach the new view’s scale [vWN04]. But instead of taking these steps one after the other, they are smoothly intertwined. It turns out that the math involved in a smooth viewport transition is not trivial. The interested reader is referred to the excellent mathematical derivation in the original paper [vWN04]. Here, we can only sketch the basic idea and illustrate it with an example. For this purpose, let us take a look at a sequence of snapshots of a zoom animation. Figure 4.21 provides an overview with all intermediate viewports marked as gray rectangles. The animation starts with the smallest viewport in thebottom-leftpartofthevisualization.Thedestinationisasmallsubgraphat thetop-right.Inthefirstphaseoftheanimation,wearetakentoazoomed-out Destination Origin Viewports Figure4.21 Viewports during an animated transition.
168 (cid:4) Interactive Visual Data Analysis Origin Destination Figure4.22 Snapshots of the viewport animation outlined in Figure 4.21. view. Then, the view is smoothly panned and zoomed toward the destination. TheresultshowninFigure4.22isnotonlyaneye-pleasinganimation,butalso helps users understand the view transition. Try for yourself to comprehend it without the overview and the intermediate steps of the animation. Looking at the origin and the destination alone, it is hardly possible to reach the same level of understanding as when following the animation. We now have reached the point where the action cycle is closed. We have described several interactive mechanisms and corresponding visual means that allow us to explore data at varying scales. Common to all our previous considerations is that we worked with a two-dimensional zoom, which is a perfect match for the many two-dimensional visualization techniques in existence. In the following, though, we will see that one-dimensional or n- dimensional zooming is also important in data analysis scenarios. 4.5.4 BeyondZoominginTwoDimensions Multi-scale exploration via zooming as described above inherently relies on a two-dimensional visual representation of the data. What we actually explore is the 2D view space. Yet, there are situations in which it makes sense to make the exploration more independent of the view space, that is, to provide specific pan and zoom functionality depending on the data characteristics. In this section, we will look at the particular case of multi-scale exploration of univariate (1D) and multivariate (nD) data by the example of time series. Exploring1DTimeSeries Understanding data in their temporal context is a particularly important analysis objective [Aig+11]. As we explore time series, we need to flexibly adjust where to look in time and also at how much we look. Range sliders are commonly applied to support one-dimensional navigation along the time axis. A range slider consists of a scale representing the time domain and two handles that define the time period to be visualized. Figure 4.23 shows a range slider in combination with a spiral visualization. Theslider’shandlescanbeadjustedtonarroworwidenthetimeperiodvisible
Interacting with Visualizations (cid:4) 169 Figure 4.23 A range slider controls the time period mapped to a spiral visualization of the daily average temperature for the city of Rostock. at the spiral. This corresponds to zooming in and out. A rather wide period facilitates an overview of the data (left spiral), whereas a rather narrow period allows us to see details (right spiral). Moving through time is possible by draggingtherangemarkedinbetweenthetwosliderhandles.Thiscorresponds to a pan operation. As the user manipulates the slider, different or more or less time steps are visualized, while the general spiral layout is maintained. It is left as an exercise to the reader to imagine what visual results a normal 2D zoom would produce. Inourexample,zoomandpanaredeliberatelyone-dimensional.Bymaking the zoom independent from the 2D visual representation and hooking it up more tightly to the dimension of time, it is possible to define a time period of interest more directly. However, there is a problem with sliders when used for navigating in time. Time-orienteddataoftencontainthousandsoftimesteps.Thetimeseriesfrom Figure4.23consistsofabout25,000daysworthofdata.Wewouldneedaslider that has a width of 25,000 pixels to be able to guarantee that any date in the range can be accessed. If we had the 25,000 pixels, each pixel would represent exactly one date. Typically, however, we do not have so many pixels. More realistic are scenarios where our slider has a width of 1,000 pixels, meaning that 25,000 dates are mapped to only 1,000 pixels. So, by moving a slider handle by one pixel, we will not get to the next day, but to the next 25th day, essentially skipping 24 dates in between. There is no way we can access any of these 24 dates by direct manipulation of the slider. In short, when the data dimension we wish to navigate with a slider is reasonably large, some values could be inaccessible. This problem can be tackled by considering multiple scales not only for the visualization, but also for the interaction. Multi-scaleInput The question is how to explore a larger range of values quickly and precisely by using a single continuous interaction gesture? With standard controls we
170 (cid:4) Interactive Visual Data Analysis (a)Regular range slider with global scale. (b)A slider with increased precision is dynamically added to the range slider. Figure4.24 Adjusting a time period at different input scales. can either be fast (cover large distances) or precise (get to an exact spot), but not both. The reason is that the scale at which the interaction takes place is fixed. The time slider from Figure 4.23 facilitates quickly browsing through thousands of days of data. Yet, how can we navigate with precision, say to an exact date? Interacting at dynamically changing scales is an answer to this question. Addingdynamicscalingtotheinputequationallowsustobeswiftandaccurate at the same time. At a coarser scale, large distances can be covered, while at finer scales, interaction precision is increased. We will illustrate this with a simple example. Let’s assume our data domain covers the dates between January 1, 2000 and December 31, 2010. The goal is to focus on the subrange from August 8, 2006 to October 8, 2010. Figure 4.24a shows a regular range slider, where the upper limit has already been set as intended. In order to set the lower limit, the user performs a continuous gesture as illustrated in Figure 4.24b. The gesture starts by grabbing the left slider handle, which is then dragged coarsely to the desired lower limit. As the exact date cannot be accessed directly, the user carries out a downward movement orthogonaltotheslider.Thistriggersthedynamicappearanceofanon-demand slider, where the gesture continues with a horizontal movement. Theimportantdetailaboutthenewslideristhatitoffersahigherprecision. This is accomplished by mapping a smaller range of values to the slider. It covers only a local interval around the value where the cursor left the regular slider. Thanks to the increased precision, the new slider’s handle can be moved exactly to the desired date. As this happens, the lower limit at the regular slider is updated automatically. Once the exact date has been reached, the gesture can be released, which dismisses the on-demand slider. As we can see, a two-scale mechanism enables us to interact coarsely (with theregularslider)andprecisely(withtheon-demandslider)inonegesture.By
Interacting with Visualizations (cid:4) 171 incorporating additional increased-precision sliders, it would also be possible to interact at more than two scales. Such a general approach would be useful for exploring very large time series. The example in this section started out with the goal to adjust a time period to specific limits. Some readers may argue that knowing the limits in advance, we could have set them simply by using the keyboard or a calendar widget. Yet, when exploring unknown data, it is often unclear where to look for interesting findings. They appear and disappear as we form and falsify hypotheses about the data that unfold before us. It can very well be that we spotsomethingpromisingwhilemovingtheslider.Thismaychangeourcourse of action and we decide on the fly that studying a different subrange could be more rewarding. This is a major advantage of continuous interaction: With a single gesture, we can dynamically explore the data at flexible pace and precision. Such a fluid exploration is hardly possible by querying the data in a discrete fashion, be it via keyboard input or a calendar widget. In the next section, we will continue to explore zoom concepts for data exploration. Yet, we will no longer navigate along the dimension of time alone, but rather expand zooming and panning to any dimension in the data. ZoomingMultipleDataVariables Previously, we considered sliders as useful elements of the graphical interface. Next,wewillseehowatightintegrationofslidersandvisualizationcansupport multi-scale exploration of nD time series. Let us illustrate this for the example of axes-based visualizations. As you may recall from Section 3.2.3, the basic visual component of axes-based visualizations are axes, each being associated with a specific data variable and arranged according to a specific layout. The actual data representation is achieved by lines or dots placed between pairs of axes. With this general approach, a variety of visualizations can be generated, including parallel coordinates plots, scatter plot matrices, line charts, and TimeWheels. An example of a TimeWheel is shown in Figure 4.25a. The central axis represents time, whereas the axes in the periphery represent time-dependent data attributes. Colored lines connect the time axis to all dependent attribute axes. Different colors are used to differentiate the attributes. The question we want to investigate is how can users flexibly explore not only time, but also the time dependent attributes via nD panning and zooming? An answer is to make the axes of the TimeWheel interactive [TAS04]. As axesalreadyrepresentrangesofobservablevalues,weonlyneedtoincorporate facilities that allow us to adjust what and how much of the value ranges gets visualized. To this end, a slider is integrated with the axis as shown in Figure 4.26. The slider body and its two handles to the left and the right can be manipulated directly via drag gestures as usual. Adjusting the slider has two complementary effects. First, the slider body marks the subrange of interest D+ = [D ,D ] within the global value low high
172 (cid:4) Interactive Visual Data Analysis (a)Plain non-interactive axes. (b)Axes with integrated sliders. Figure4.25 Integrated sliders for nD pan and zoom in the TimeWheel. Value range D D min Subrange of interest max D D low high D D low high Visual mapping range Figure4.26 Integrated range slider for per-axis pan and zoom. range D =[D ,D ]. Second, the actual visualization mapping of the axis min max is altered. Instead of showing the global min-max range, only the subrange of interest is visible. Note that the subrange extends over the entire length of the axis. This way, we obtain a per-axis pan and zoom functionality. Narrowing the slider allows us to look closer at details, whereas widening the slider will take us back to a broader view. Moving the slider will bring us to a different part of the value range. An example with four zoomable axes is shown in Figure 4.25b. The time axis has been zoomed to the first half of 1999, the two greenish axes have been set so as to get rid of outliers, and the red axis focuses on a user-chosen value range. With these per-axis zoom interactions, we conclude our excursion into the area of zoomable visualizations. As we have seen, there exists a rich set of interactive tools and corresponding visual cues for comprehensive multi-scale dataexploration.Inthenextsection,wewillstudyinteractivelensesasanother
Interacting with Visualizations (cid:4) 173 powerfulconceptforinteractivedataexploration.Incontrasttopanandzoom, which typically affect the visualization globally, interactive lenses are tools for lightweight local adjustments of the visualizations. 4.6 INTERACTIVE LENSES Theinteractiontechniquesdiscussedintheprevioussectionenableustoexplore different parts of the data, that is, to change what is shown on the screen. Anotheraspectofvisualdataexplorationistoexperimentwithdifferentvisual encodings of the data, that is, to change how the data are visualized. In a sense, we broaden our view of interaction from exploration of the data space to exploration of the visualization space. This includes adjusting the mapping of data values to visual variables and the arrangement of visual marks on the display as explained in Chapter 3. Thestandardwayofsupportingexploratoryvisualizationadjustmentsisto provideagraphicalinterfacewithallkindsofcontrolcomponents.InFigure4.3 back on page 140, we already saw such an interface. Altering parameters in the interface leads to a global and permanent change in the visualization. For example, when we switch the color scale, our visualization will be freshly painted overall. An elegant alternative to the aforementioned standard approach are inter- active lenses [Tom+17]. Lenses are lightweight exploration tools that can be added to a visualization on demand. Lenses can be used for various visualiza- tion adjustments, be it to encode data differently, to reconfigure the data’s visualarrangement,tofilterdataaccordingtocertainconditions,ortoconnect related findings. A key characteristic is that lenses produce local and transient changes in the visualization. That is, the visual representation is adjusted only inselectedpartsanditsoriginalstateisrestoredoncethelensisdismissed.For example, a lens could be used to enhance the color coding to inspect details of local accumulation of data elements. 4.6.1 ConceptualModel AschematicdepictionofaninteractivevisualizationlensisgiveninFigure4.27. A lens is defined by its position, size, shape and orientation and divides the visualrepresentationintoan interiorand anexterior part. Conceptually,lenses combine two interactions in a single tool: (i) interactive selection and (ii) adjustment of the visualization. A corresponding model can be defined based on the visualization pipeline. As you may remember from Chapter 2, the visualization pipeline describes how data are transformed to a visualization image via analytical and visual abstractions. A visualization lens can be understood as an additional lens pipeline that is attached to a standard visualization pipeline as illustrated in Figure 4.28. The standard pipeline (bottom) produces a regular visualization. The lens pipeline (top) implements a lens function that generates a lens effect. There
174 (cid:4) Interactive Visual Data Analysis Position VISUALIZATION Regular representation e Adjusted representation z Si Orientation LENS Exterior Shape Border Interior Figure4.27 Schema of an interactive lens. Adapted from [Tom+17]. Lens function 01100011 01110100 Selection Join 01000001 01101110 01101010 01100001 Figure 4.28 Model of a lens pipeline attached to a standard visualization. Adapted from [Tom+17]. are two points of information exchange between the standard pipeline and the lens pipeline. The first is a selection. It defines what is to be processed by the lens function. The second is a join, which specifies how the result of the lens function is to be integrated back into the standard pipeline. Next, we will describe these main ingredients of lenses in more detail. The Selection The selection corresponds to the content shown underneath the lens. Any type of content that is available along the visualization pipeline canbeselected.Alenscandirectlyselectpixelsfromtheimagespace.Content from other stages of the visualization pipeline can be selected as well, for example,asetof2Dor3Dgraphicalobjects,agroupofdataelements,arange of values, or any combination thereof. Typically, the selection will be a proper subset of the data that is signifi- cantlysmallerthantheoriginaldata.Thisallowsalenstoperformcalculations that would take too long for the entire dataset or would not be possible at all. As we will see a little later in this section, some lenses install mechanisms that automatically restrict the selection to maintain their operability.
Interacting with Visualizations (cid:4) 175 Christian Vincent Anja Thomas Arvid Grit Olivia Vincent (a)Alteration. (b)Suppression. (c)Enrichment. Figure4.29 Fundamental effects of lens functions. The Lens Function While the selection specifies what is to be affected, the lens function defines how the visualization is modified. For example, when our goal is to re-color parts of the visualization, the lens function may achieve this effect by altering selected graphical objects. A selection that contains raw data opens up the possibility to create an entirely different visual representation, but restricted to the selected values. The output generated by the lens function will lead to an alternative or modified visualization. In general, the goal is to improve the visualization with respect to the task at hand. To this end, a lens function can alter existing content, suppress irrelevant content, or enrich with new content. Figure 4.29 illustrates the different options. In Figure 4.29a, the visual encoding inside the lensisalteredtoemphasizethesmalldots.Incontrast,thelensinFigure4.29b suppresses the small dots deemed less relevant. Finally, Figure 4.29c shows a lens that enriches the visualization with labels for dots within the lens interior [BRL09]. The lens function usually depends on parameters that control the lens effect. A magnifying lens, for example, may expose the magnification factor as a parameter. A filtering lens may be parameterized by a threshold to control the amount of data to be filtered out. In general, a lens may be parameterized by an alpha value used for blending lens and visualization when both are joined. The Join To create the final image, the lens effect needs to be joined with the base visualization. Traditionally, the visible effect of a lens is confined to the lens interior. In the context of visualization, though, it can be practical to allow lenses to affect the visualization beyond their interior or even show their effect separately. Yet,mostlensesfollowthemetaphorofconventionallenseswherethevisual effect manifests exclusively in the lens interior. This can be accomplished with the following generic three-step procedure. First, render the base visualization,
176 (cid:4) Interactive Visual Data Analysis optionally sparing the interior of the lens. Second, fetch the lens effect to the lensinterior,optionallyblendingwiththebasevisualization.Third,incorporate suitable visual feedback and optional user control elements. In our examples, a thick outline makes clear to the user that a lens is in operation. Conceptually, the join can be done at any stage of the pipeline. If the join takes place at the early stages of the visualization pipeline, the visual effect may go beyond the lens. For example, a lens may adjust the positions of selected nodes in a node-link diagram. As a side effect, the incident edges of the altered nodes will take different routes as well, which in turn introduces (limited) visual change into the base visualization outside the lens. In summary, selection, lens function, and join describe the key components of lenses. Conceptually modeling lenses as secondary visualization pipelines makes it not only possible to use multiple lenses in the same visualization, but also to combine different types of lenses to create composite lens effects. Later in Section 4.6.3 we will see an example. Next, our attention shall first be drawn to the properties of lenses and means of adjusting them. 4.6.2 AdjustableProperties From the perspective of a user who is actually working with a lens, two questions are relevant: What properties of lenses exist and how can they be adjusted to suit the user’s data analysis objectives. LensProperties Looking at lenses, their geometric properties are the first to catch the eye. Position and size of a lens are most relevant. They determine where and to which extent a lens takes effect. Another prominent property is the lens shape. Followingtheclassicprototypeofreallenses,manyvirtuallensesareofcircular shape as shown in Figure 4.30a. Rectangular lenses are common as well. For non-circular lenses, also the orientation is relevant. Orientable lenses can be better adapted to the underlying data as illustrated in Figure 4.30b. In addition to the geometric properties, there are the parameters that control the inner workings of lenses. We already mentioned magnification factors and filter thresholds as examples of lens parameters. In general, lens parameters are often used to balance the strength of the lens effect, where strength can have different meanings, for example, how much more detail is added, how much irrelevant data are suppressed, or how substantially the base visualization gets altered. InteractiveandAutomaticAdjustmentofLenses A great deal of the flexibility attributed to lenses pertains to the possibility to adjust them interactively via direct manipulation. Complementary automatic mechanisms may support the adjustment of lenses.
Interacting with Visualizations (cid:4) 177 (a)Circular. (b)Rectangular orientable. (c)Content-adaptive shape. Figure4.30 Lenses with different shapes and orientation. (a)Move and resize. (b)Adjust parameters. Figure4.31 Direct manipulation of lenses. Direct manipulation as shown in Figure 4.31a is the preferred way of interactively adjusting position and size. Both properties can also be set automatically.Forexample,onecanpositionalensautomaticallyatinteresting data tuples. Automatic adjustments of the lens size can help to cope with the computationalcostsforproducingthelenseffectandalsothecognitivecostsfor makingsenseofit.AnexampleisthelabelinglensfromourearlierFigure4.29c. Whenbeingmovedintodensepartsofthedata,thelensautomaticallyreduces its size to limit the number of labels. This way, the lens can keep the labels readable and the algorithmic runtime low [BRL09]. Interactively switching between different lens shapes is possible, but rather uncommon. More interesting are lenses that adapt their shape automatically based on characteristics of the data [Pin+12]. Such self-adapting lenses are particularly useful in cases where the lens effect needs to be confined to complicatedgeometricfeaturesinthevisualization.Asanexample,Figure4.30c illustrates a lens that has adjusted itself to a cluster of data elements. Finally, we need to think about the internal parameters of lenses. For occasional adjustments, it is sufficient to rely on standard widgets. Parameters
178 (cid:4) Interactive Visual Data Analysis that need to be fine-tuned more frequently are preferably adjustable via dedicatedmechanisms.Figure4.31bshowsanexamplewherecustominterfaces elements are provided directly at the lens [KRD14]. There are also lenses that can adjust their parameters automatically to the data. An example is to tune a sampling rate parameter to the data density underneath the lens [ED06]. As we see, the adjustable properties of lenses (geometry plus parameters) make them very flexible data exploration tools. Lenses can be easily controlled via direct manipulation or through automatic procedures that adapt the lens totheunderlyingdata.Howlensescanbeappliedtoactuallyaccomplishvisual analysis tasks will be demonstrated next. 4.6.3 LensesinAction So far we have considered lenses on a rather abstract conceptual level. In this section, we will illustrate the versatility and utility of interactive lenses in the context of visual data analysis scenarios. We will consider four practical problems and corresponding lenses to solve them. We will start with the quite common task of studying specific details in the visualization. Second, we will explain how lenses can support the exploration of structural relationships in graphs.Addingspaceandtime,thethirdlenswillhelpusunderstandtemporal aspects of geo-spatial movement trajectories. Finally, we will make a step from altering the visualization to altering the actual data with an edit lens. ExploringDetailswithaFish-eyeLens Magnifying glasses have an ancient history as tools allowing us to look at details that cannot be seen with the human eye alone. On a computer screen, interactive magnifying lenses serve the same purpose: They are positioned on the screen where a more detailed view is needed. The lens will then transform the content underneath it according to a mathematical specification. A prominent example of such a mathematical specification is the fish-eye distortion [SB94]. It gradually pushes content from the lens center outward. As shown in Figure 4.32, this effectively magnifies the content near the mouse cursor and allows us to see the details there. Because the fish-eye distortion smoothly embeds the details within the global context, it is conceptually a form of focus+context representation as introduced in Section 3.1.2. The fish-eye lens already demonstrates the utility of lenses for altering a visualization in a dynamic and lightweight manner. Next, we will elaborate on lenses that are particularly useful for exploring structures in graph data. ExploringStructuralRelationshipswithGraphLenses When exploring graph data, structural relationships play an important role. Node-link diagrams, as introduced in Section 3.5 allow us to see how nodes are
Interacting with Visualizations (cid:4) 179 (a)Regular map visualization. (b)Details magnified with a fish-eye lens. Figure4.32 Magnifying details in a map visualization with a fish-eye lens. connected and if there are any communities or clusters. Next, we will apply graph lenses to enhance exploratory work with node-link diagrams [Tom+06]. Let us take a look at the zoomed-in graph visualization in Figure 4.33a. We are interested in the edges that connect the node in the center. As we can see, edge clutter is a problem in our example. There are many edges and we cannot really say which edges do actually hook up to our node of interest and which are just passing by. An interactive lens can help us out. In Figure 4.33b, we use a local-edge lens to clear the visualization of irrelevant edges. The lens suppresses edges that do not connect to nodes inside the lens. We can now easily see that our node of interest has seven incident edges. What we cannot see, though, are the adjacent neighbor nodes, which are beyond the current view. We could pan and zoom to each one of them, but this could take a while and we cannot really be sure what we will find. Using a lens can be more efficient in this situation. This time, we apply a bring-neighbors lens. As its name suggests, the lens will bring to us the neighbors of the nodes inside the lens. As the lens is moved toward the node of interest, its neighbors will be gradually drawn toward the lens. When the lens is exactly on top of the node, all its neighbors will be inside the lens as we can see in Figure 4.33c. The lens effectively produces a local overview of the neighborhood of the nodes covered by the lens. There is no need to manually visit the neighbors, the lens brings them in for us to inspect them. The bring-neighbors lens works well when the neighbors are evenly dis- tributed across the graph layout. However, if this is not the case, we could end up with the majority of the nodes occluding each other at the lens center. To solve this problem, we can exploit the fact that lens effects can be combined to create composite lenses. In our case, we combine the bring-neighbors effect with a fish-eye distor- tion. The combined effect will bring in the neighbors, but those that would accumulate too tightly at the lens center will be pushed outward to loosen
180 (cid:4) Interactive Visual Data Analysis (a)Node-link diagram without lens. (b)Local-edge lens. (c)Bring-neighbors lens. (d)Composite lens. Figure4.33 Graph lenses for exploring structural relationships. the clutter. Figure 4.33d shows the result. Looking closely, you will realize that the figure actually shows a composite lens that combines all three effects mentioned before: local-edge effect plus bring-neighbors effect plus fish-eye effect. We have just seen lenses in action for the specific task of exploring graph data.Thelenseshelpedustidyupedgeclutterandpeekatneighborhoodsthat are otherwise not visible at a glance. Next we will present a lens particularly designed for exploring temporal dependencies of movement data.
Interacting with Visualizations (cid:4) 181 Interior Scale Links Ring Lens Figure4.34 A lens to query temporal characteristics of movement data. ExploringTemporalAspectsofMovementDatawithaLens BackinSection3.4.3,wediscussedthechallengeofvisualizing spatio-temporal data. Showing space, time, and data attributes simultaneously at full detail is typically difficult. A more practical solution is to depict two aspects in full detail and add the third aspect interactively on demand for a selected part of the data. Here, we will enhance a visualization that focuses on space and attributes with an interactive lens to integrate the temporal aspect on the fly. Our point of departure is a 2D visualization of spatio-temporal movement data as shown in Figure 4.34. A map provides the spatial context. Trajectories of moving cars are visualized as lines, where colors encode speed. What we can see from this kind of representation is where cars drive at certain speeds. The time lens will help us to also see when [Tom+12]. The lens effect is shown in an auxiliary circular display to the left in Figure 4.34. The interior shows a scaled copy of the trajectory points selected withthelens.Thetime-dependentspeedattributeisvisualizedinthehistogram ring aroundtheinterior.Ourparticularexamplerevealsthatmovementsinthe selected region occur mostly around 9–10 and 18–19 o’clock, with the speeds being evenly distributed. Moreover, links connect the points in the interior with a finer-grained minute scale. This allows us to see additional details. For example, the trajectory marked in gray represents a movement that took place around 18:15 o’clock as indicated by the gray links accumulating at minute 15 of the 18th hour. The insights gained with the time lens could not be obtained with the plain trajectory visualization alone. This demonstrates quite nicely how useful lenses can be when it comes to accessing additional information and details of complex data on demand. All described lenses, the time lens, the graph lenses, and the fish-eye lens help users explore the data. Next, we go one step further and study a lens that supports editing the data.
182 (cid:4) Interactive Visual Data Analysis Figure4.35 Orthogonal node-link diagram of a biological network. EmployingaLensforSemi-automaticGraphEditing During the exploration of data, one may stumble upon findings that make it necessary to correct the data, for example, to insert missing elements, update erroneous data values, or delete obvious outliers. The goal in this section is to demonstrate how a lens can support such an on-the-fly data editing. Our particular example is about editing graphs. Suppose you have to insert a node with say a dozen of edges into a network such as the one shown in Figure 4.35. It is obvious that positioning the node and routing each of the edges by hand would be tedious work and take a lot of time. A graph layout algorithm could do the math and compute a high quality layout. But most algorithms would recompute the layout globally, which would harm the mental map that we or others might already have about the data. What we need is a tool that lets us edit the data locally without intensive manual labor and no global changes of the layout. Again,weemployalenstoaddressthisproblem.Fittingtoitspurpose,the lens is called edit lens [Gla+14]. It supports three fundamental edit operations: insert, update, and delete. Figure 4.36 illustrates the lens being used for inserting,updating,anddeletingagraphnode.Theonlymanualworkrequired is to place the lens within the layout to specify where an edit operation is to take effect. In a sense, the adjustable lens acts as a coarse human-specified solution to be refined with automatic methods.
Interacting with Visualizations (cid:4) 183 Delete Update Insert (a)Place lens to insert. (b)Adjust lens to update. (c)Flick lens to delete. Figure4.36 Editing using the edit lens. Adapted from [Gla+14]. The automatic part consists of two steps. The first step determines a suitable unoccupied area in the lens interior where the edited item can be placed.Second,theprecisespotwithinthatunoccupiedareaiscomputedbased on heuristics for different graph aesthetics criteria, such as maximum distance to other nodes, short overall edge length, or low number of edge bends. During the editing, the user is free to move and resize the lens and to choose a different heuristic. The lens will compute and suggest suitable node positions and edge routes on the fly. Only if the user agrees with a suggested solution is the result of the edit operation committed to the data. Tosummarize,theeditlenssimplifiesfullymanualeditingtosemi-automatic editing.Thiseaseseditingoperationsconsiderably,becausetheuseronlyneeds to define a coarse region interactively, rather than precise positions or routes. The algorithmic part of the lens computes suggestions for precise solutions, which the user can customize on-the-fly. Finally, the lens is integrated into the regular visualization so that data analysis and data editing can go hand in hand. The edit lens concludes our journey into the world of interactive lenses in the context of visualization. In addition to the conceptual ideas behind lenses, we have described several exemplars of lenses for a number of different tasks, including looking at details, exploring graphs, incorporating temporal information, and even data editing. For more information on the described lenses, the interested reader is referred to the list of references collected at the end of this chapter. This section discussed lenses as a versatile interactive approach to support data exploration and analysis. The focus was on one tool for many different tasks.In thenextsection,wewill fliptheperspective. The focuswill beonone taskforwhichacomprehensivesetofinteractiontechniqueswillbeintroduced. The particular task we will be dealing with is visual comparison.
184 (cid:4) Interactive Visual Data Analysis 4.7 INTERACTIVE VISUAL COMPARISON Visual comparison takes a central role during data analysis activities. By inter- actively comparing different parts of the data, users may formulate, confirm, fine-tune, or reject initial hypotheses, draw corresponding conclusions, and thus can gain a better understanding of the data. Elementary comparison is often a predecessor to a more in-depth data analysis.Forexample,wecancomparesuccessivedatavaluesinatimeseriesof stock prices to identify trends. Comparing the trends enables us to find groups with similar trend behaviors. Still more insight can be gained by comparing the groups, for example, to study if certain behaviors occur in specific periods of the fiscal year. It is also common to derive quantitative statements about the compared data to capture their degree of relatedness. A corresponding notion is that of similarity (or dissimilarity), which plays an important role in many higher-level knowledge generation activities. Thissectiondealswithdedicatedinteractiontechniquesspecificallydesigned to support visual comparison tasks. But before we look at these techniques in detail, we need to understand what comparison is and how visual comparison takes place. 4.7.1 BasicsandRequirements What precisely do we mean by comparison? Given individual data values p and q (or sets of values P and Q), comparison tasks are defined as the search for a relation r such that p r q (or P r Q). When comparing numerical values, order relations r ∈ {<,≤,=,≥,>} are of great practical relevance. Specific relations exist for comparing temporal data (e.g., before, during, after) and spatial data (e.g., inside, overlap, touch). There are three fundamental visual designs specifically for comparison tasks: juxtaposition, superposition, and explicit encoding [Gle+11]. Figure 4.37 provides a comparison (pun intended). Juxtaposition shows the data side- by-side, that is, in separate spaces. In contrast, superposition stacks data in a unified visual space. Explicit encoding means calculating and visualizing numeric differences of the data being compared. However, comparing data visually without dedicated interaction support is typically non-trivial. For example, suppose you have spotted two interesting JUXTAPOSITION SUPERPOSITION EXPLICIT ENCODING Figure4.37 Visual designs for comparison tasks.
Interacting with Visualizations (cid:4) 185 patterns in different parts of a color-coded spreadsheet visualization. For comparing them, you first have to visit one pattern and memorize it. Then you have to navigate to the second pattern and compare that to the stored mental image of the first one. This procedure is inefficient, because it requires you to scroll over and over again, and is also error-prone, because the actual comparison is carried out based on a mental image from your short-term memory. The previous statements suggest that visual comparison involves multiple actions working in concert to reach a higher level of analytic thinking. From an interaction perspective, visual comparison is a procedure that comprises three phases: 1. Select the information to be compared. 2. Arrange the selected information for comparison. 3. Compare the arranged information visually. From these phases, we can infer important requirements. First, a set of comparison candidates has to be selected and maintained interactively. The number of candidates is usually small because our visual working memory is limited [PW06]. Yet, data may enter or exit the set of candidates on the fly as user interests change during the data investigation. Second, the data to be compared have to be rearranged dynamically to facilitate their comparison. This is necessary because many standard visualiza- tionsareoblivioustocomparisontasksandarrangethedataaccordingtosome fixed layout algorithm or some naturally given mapping such as geographic positions.Asaconsequence,theremightbelargergapsbetweenthedatatobe compared, which make comparisons more difficult. The eyes have to look back and forth between different parts of the display frequently. Moreover, when studying larger data with zoomable visualization interfaces as introduced in Section 4.5, it is not guaranteed that all relevant data are visible at all. Many manualnavigationstepsmightbenecessaryinordertosuccessfullyaccomplish a comparison task. At the same time, the short-term memory has to store not only the locations of the data, but also their visual representations. Finally, the actual comparison is performed. Juxtaposition, superposition, and explicit encoding form the visual basis for the comparison. However, it is not clear upfront which is the best strategy for the data at hand. Therefore, the user should be able to interactively choose and parameterize comparison strategies as needed. In summary, we see that visual comparison is a highly dynamic procedure. Interactiontechniquesareneededtoflexiblylinkthephasesofcomparisonand to account for the changing interests during comparative analytic activities. In the following, we introduce dedicated interaction designs for visual comparison. NextinSection4.7.2,ourmainprioritywillbenaturalnessofvisualcomparison. Later in Section 4.7.3, we will additionally consider aspects of cost efficiency.
186 (cid:4) Interactive Visual Data Analysis (a)Side-by-side. (b)Shine-through. (c)Folding. Figure4.38 Natural behavior of people comparing information on paper. 4.7.2 NaturallyInspiredComparison Naturalinteractionisathemeofinteractionresearchthatfocusesonenhancing interactive experiences by drawing inspiration from natural human behavior. Following this thinking, we first observe how people compare information natu- rallyandthendeveloptechniquesthatmimicpeople’snaturalbehavior[TFJ12]. Figure 4.38 illustrates the three basic strategies that can be observed when humans compare information printed on paper: (a) Side-by-side comparison: Sheets of paper are moved on a table until they are arranged side-by-side to facilitate comparison. (b) Shine-through comparison: Sheets of paper are stacked and held against the light to let information shine through and blend. (c) Folding comparison: Sheets of paper are stacked and individual sheets are folded back and forth to look at them in quick succession. Our goal is to replicate these natural strategies. To this end, we have to design corresponding virtual counterparts for the involved visual components and interactive procedures. Intermsofthevisualcomponents,weneedavirtualcomparisonworkspace and a virtual equivalent for sheets of paper. The workspace will be a zoomable visualization space based on the ideas discussed in Section 4.5. Sheets of paper are replicated as visualization views that reside in the zoomable space. Within the zoom space, views can be moved freely in analogy to arranging pieces of paper on a table. How the visual components are put to use for naturally inspired comparison will be explained in the following. SelectingWhattoCompare The comparison procedure starts with the selection of data to be compared. When users spot something interesting for comparison, they can simply mark
Interacting with Visualizations (cid:4) 187 Sub-views Main view Origin highlight Figure4.39 Creating sub-views for comparison. A red frame indicates where the left sub-view has been detached from the main view. it with an elastic rectangle. The system then creates a new view corresponding to the marked region, a sub-view of the entire visual representation so to say. Once created, sub-views exist as independent views in the visualization space. The real-world analog would be to create a copy and cut it out to contain only the information of interest. Yet, in the real world, we would lose the connection between the cutout and its origin. On the computer, the parent- child relationship of views can be preserved in a view hierarchy. As a result, users no longer need to mentally keep track of what they want to compare becausethisinformationisnowexternalizedintheformofdynamicallycreated views collected in a view hierarchy. The view hierarchy further makes it possible to embed visual cues for highlighting a sub-view’s origin on demand. Figure 4.39 illustrates this for a matrix visualization from which two sub-matrices have been created. As the user points at the left one, a red frame indicates where the sub-matrix has been extracted from its parent matrix in the background. ArrangingforComparison The second comparison phase consists of arranging the views to be compared. Using simple drag gestures, views can be arranged in the visualization space similar to shifting paper on a table. In the real world, people use edges of papers or patterns on the table surface to guide the arrangement. In the virtual world, snapping can be used to assist in arranging views. Snapping automatically aligns views with respect to certain features in the visualization. This way, costly pixel-precise adjustments can for the most part be avoided. In our example from Figure 4.39, snapping helps to maintain the alignment of matrix cells.
188 (cid:4) Interactive Visual Data Analysis Side-by-side B A B A A B Shine-through A B B AB AB AB Folding A B B A A A Figure4.40 Overview of natural interaction techniques for visual comparison. Adapted from [TFJ12]. ComparingViews Phase three is the actual comparison. Here, we take a simplified perspective on the underlying problem and abstract from the specific details of the data being compared. Chapter 3 made clear that plenty of visualization techniques are available to generate expressive visual representations of data. Under this assumption, we can simply resort to our views and sub-views as the objects to be compared visually. Next, we introduce the virtual analogs for the natural comparison strategies: side-by-side, shine-through, and folding. They are also summarized in Figure 4.40. Side-by-side Comparison Side-by-sidearrangementsprovideuswithacom- plete sight of the data, which is helpful as an overview. On the other hand, when comparing details, resolving spatial references among the views requires some cognitive effort. In particular, we have to move our eyes to check if a feature in one view can be found in the very same spot in another view. For example, when we compare matrix cells at position [i,j], we can follow along the i-th row easily from the left sub-matrix to the right. However, as we do so, we have to take care to stop correctly at the j-th column, which may be doable for our small example, but would certainly require some counting for a larger matrix. For more complex visualizations, the eyes might need to move to and fro multiple times to be sure of looking at the same reference spot.
Interacting with Visualizations (cid:4) 189 Folding point P Candidate points P • Folding origin O Folding axis O A Folding anchor A Figure4.41 Folding geometry. Adapted from [TFJ12]. Shine-through Comparison As with natural comparison of paper, an alter- nativeistoletviewsoverlap.Inreality,peopleoftenpurposelystackpaperson top of each other in order to create a unified comparison space. While spatial references are aligned then, mechanisms are needed to look through or behind the paper sheets. One way to resolve the occlusion is to hold the paper against the light. The degree to which information shines through can be controlled by altering the viewing angle with respect to the light source. On the computer, shine-through comparison can be realized via alpha- blending. That is, views are made partially translucent, where the level of transparencycanbevariedbytheuser.Shine-throughmakesiteasytocompare shapes and sizes in the graphical depiction of the data. On the other hand, blending the views implies that we mix colors as well, which is hindering the comparison of color-coded visual representations. Using shine-through, it can also be difficult to figure out which view contributes its data to a particular feature in the blended image. Or put differently,shine-throughfavorsamergedviewonthedataatthecostoflosing separabilityofindividualdataelements.Thefoldinginteractiondescribednext addresses this aspect. Folding Comparison Folding paper back and forth allows us to compare information shown on different sheets. The same is possible with our visu- alization views: To uncover occluded information, the user can fold away or peel off views as if they were virtual paper. The folding resolves the occlu- sion temporarily, while otherwise keeping the views in place to preserve their arrangement. In a data analysis setting, the fold should appear directly where the user’s focus is, which is typically the location of the pointer cursor P. Knowing P, a heuristic can be applied to determine from a set of candidate points a folding origin O and a folding anchor A. For real paper, the folding origin O corresponds to the spot where we would grab a page for folding it. The anchor A represents the fixture around which the paper is folded, such as a staple or the binding. Finally, the folding axis is constructed as a line originating at P and being perpendicular to the line PA as illustrated in Figure 4.41. The folded visualization view can be rendered in different styles. As shown in Figure 4.42, the styles are to vary in naturalness, information-richness, and degree of occlusion. For example, the visual effect resembling natural paper
190 (cid:4) Interactive Visual Data Analysis Information-rich Natural Occlusion-free Figure 4.42 Information-rich, natural, and occlusion-free folding styles. Adapted from [TFJ12]. folding leaves the folded backside blank. A more information-rich style would enhancethebacksidewithadditionalinformation.Anocclusion-freestylecould be restricted to only showing a subtle shading gradient. Foranaturalfeelingandrealisticappearance,thefoldingisanimatedusing a simple physically based spring-mass system. The animation starts with a springforcesmoothlypullingthefoldfromtheoriginO tothepointerposition P. When the user adjusts the pointer, P is updated so that the fold smoothly follows up. When the user releases the fold, the spring force is inverted to pull the fold back from the pointer P to the folding origin O, effectively unfolding the view. With the animated visual feedback, the virtual folding interaction for comparison tasks is complete. It is time for a recap. We studied a comprehensive repertoire of techniques for interactive visual comparison. By dynamically extracting sub-views, it is possibletoflexiblydefinewhatshouldbecompared.Side-by-side,shine-through, and folding interaction enable users to carry out the actual comparison in differentways.Thenaturalinspirationbehindtheinteractiondesigncontributes to making the comparison intuitive and easy. However, a pressing question remains: Under which circumstances can we apply which technique most effectively?Findinganswerstothisquestionrequiresfurtherextensiveresearch and elaborate user studies. So far, we left an important advantage of interactive systems (compared to paper) out of consideration: the ability to do automatic computations, for example, to calculate differences and show them directly. In the next section, we will continue with interaction specifically for visual comparison. However, our focus will shift from naturalness to reducing the interaction costs for comparing data, also by including automatic calculations. 4.7.3 ReducingComparisonCosts Before taking measures to reduce costs, we need to know where they accrue. In Section 4.2.1, we learned that interaction costs in general can be attributed to executing the interaction and to evaluating the visual representation. For comparison tasks, the following specific costs can be identified:
Interacting with Visualizations (cid:4) 191 • Costs for selecting the data to be compared • Costs for carrying out the comparison • Costs for understanding the data in context To reduce these specific comparison costs, we will now combine several of thetechniquespresentedintheprevioussections,includingautomaticselection mechanisms from Section 4.4, off-screen visualization and navigation shortcuts from Section 4.5, and the idea of local and lightweight adjustments as we know themfromthelensesfromSection4.6.Let’sstartwithreducingselectioncosts. Semi-automaticSelectionoftheDatatobeCompared For printed non-interactive data visualizations, much of the costs for com- parison are related to memorizing where relevant data are located and what characteristics they exhibit. On a computer, interactive selection enables the user to mark and highlight interesting data (see Section 4.4), which effectively off-loads otherwise mental effort to the machine. As user interests change frequently during data exploration, it makes sense to invest in reducing the selection costs by integrating automatic mechanisms. The idea is to reduce the manual selection of n data elements (or data subsets) to be compared to only a single manual selection plus an automatic selection of the remaining n−1 elements (or subsets). Theutilityofthisidealargelydependsondefiningasemanticallymeaningful automatic selection. A sensible approach in the context of comparison tasks is to rank the data according to their similarity (or dissimilarity). In other words, when the user selects a first data element, the n−1 most similar (or dissimilar) elements are added to the selection automatically. There are a number of similarity measures for different types of data that can be used for this purpose. Euclidean or Manhattan distance work well for numerical data, Edit or Hamming distance operate on string data. Categorical data require dedicated distance measures [BCK08]. In cases where the similarity of complex data subsets must be captured, it makes sense to extend to multi-dimensional or subspace measures [Tat+12]. As an alternative to similarity-based automatic selection, one could con- sider mechanisms that traverse the internal (graph) structure of the data or utilize flexible degree-of-interest (DoI) functions, as for example explained in Section 5.2.1 of Chapter 5. Irrespective of the method being employed to drive the automatic selection, the benefit for the user is significant: A single click (or tap) is enough to create a selection of n comparison candidates. This corresponds to a reduction of the costs from O(n) down to O(1). Of course, additional manual steps may be taken to complement or refine the automatic selection.
192 (cid:4) Interactive Visual Data Analysis DynamicRearrangementandVisualCuesforComparison Once data have been selected for comparison, the next step is to compare them in detail. However, typically it will take a number of navigational steps until we have acquired enough information to draw a comparative conclusion. The goal is to reduce the time-consuming navigation between the data to be compared. Instead of us collecting the required information, the system should automatically bring it to where we need it. To this end, the data have to be rearranged dynamically. The basic idea is to create a juxtaposition arrangement to facilitate the comparison on the fly. One option to do this is to form a ring, a so-called CompaRing [Tom16]. As shown in Figure 4.43, the CompaRing is a circular arrangement of slots to be filled with the data for comparison, regions of a choropleth map in our case. When the CompaRing is activated, the previously selecteddataaredynamicallyrelocatedfromtheiroriginalpositiontotheslots. With all relevant data being now displayed at the ring, the comparison can be carried out more directly and more easily. However, the relocated data are now detached from their original spatial context,whichcouldbedetrimentaltootherdataanalysisobjectives.Therefore, indicator arcs point in the direction where a slot’s data are originally located. Wide arcs (max. 90◦) stand for far-away origins, whereas narrow arcs (min. 10◦) suggest the origin is close. Theindicatorarcscanfurtherbeexploitedtoaugmentthecomparisonwith additional details. In order to make even subtle variations visible, the pairwise differences between a selected slot and all other slots of the CompaRing are calculated and color-coded into the indicator arcs. In Figure 4.43, a diverging red-blue color scale indicates negative and positive deviations from the slot under the mouse pointer. ThekeybenefitoftheCompaRingisthatitisnolongernecessarytocollect and memorize data characteristics, as the CompaRing brings the required information to us. Moreover, the explicit visual encoding of calculated local differences grants us insight into details that are not evident in the base visualization. This naturally reduces the costs for comparison tasks. UnderstandingDatainContextwithNavigationShortcuts So far, the CompaRing facilitates the plain comparison of the selected data. Formoreprofoundinsight,wealsoneedtounderstandthedataintheirspatial context. This still requires us to navigate to individual places in the data manually. In order to reduce the cost for navigation, the CompaRing picks up the idea of navigation shortcuts as introduced in Section 4.5.3. Each slot of the CompaRing serves as a trigger for a smooth animation that takes the user (and the CompaRing) to the corresponding data’s original position. Manual navigation steps are thus reduced to a minimum. On top of that, using the navigation shortcuts in combination with similarity-based automatic selection as described earlier enables a whole new
Interacting with Visualizations (cid:4) 193 Slot Indicator Arc Data Minimum Maximum Negative Positive CompaRing Difference Figure4.43 Relocating selected regions to form a ring for easier comparison. The map background has been desaturated for the purpose of illustration. typeofdata-drivenbring & go navigation.Wecanuseanavigationshortcutto go to the context of some data of interest. If we find some interesting data in the context, we can mark them and the automatic selection will bring related data to the CompaRing and hence to our attention. Each of the newly brought data elements can then be used to go to yet another destination to continue the data exploration. The interesting thing about this type of navigation is the combined use of semantic (data similarity) and spatial (data context) relationships as paths through the data. This concludes the section on interactive visual comparison. We learned thatvisualcomparisonisahigh-levelactivitythatinvolvesseveralintermediate steps.Inthefirstpartofthissection,wereplicatedhumancomparisonbehavior on the computer to facilitate natural comparative data analysis. Secondly, we presented several strategies to reduce comparison costs by complementing interactive methods with automatic mechanisms. As a result, we obtain a comprehensive repertoire of dedicated techniques for visual comparison. A take-home message of the described techniques is that visual comparison requiresahighdegreeofflexibilitywhenitcomestoselectingdata,rearranging their visual representation, and actually collating them. In this section and for the most part of this chapter, we have focused on specific data analysis tasks that are to be accomplished visually and interactively by a human user in one way or the other. However, we largely ignored the question regarding the technology used for interaction and visual output. We implicitly assumed to be working with a normal display, mouse,
194 (cid:4) Interactive Visual Data Analysis and keyboard. In the next section, we will leave these behind and study how interactivedataanalysiscanbecarriedoutinalternativedisplayenvironments and with different interaction modalities. 4.8 INTERACTION BEYOND MOUSE AND KEYBOARD What most of the existing visualization and interaction approaches have in common is that they are targeted for regular desktop workplaces. Yet, already in 1985, researchers recognized the importance of considering new technologies for the interface between humans and computers [HHN85]: “Butifwerestrictourselvestoonlybuildinganinterfacethatallowsustodo thingswecanalreadydoandtothinkinwayswealreadythink,wewillmiss themostexcitingpotentialofnewtechnology:toprovidenewwaystothinkof andtointeractwithadomain.” Hutchinsetal.,1985 EGA, MCGA, and VGA graphics cards and the computer mouse were the new technologies at that time. Today, new display technologies are related to largedisplaywallsorsmallmobiledisplays,bothwithhigh-resolutiongraphical output. New form factors and the increased pixel density made it necessary to adapt existing visualization approaches or to devise new ones. Modern interactiontechnologies,suchasmulti-touchinteractionortangibleinteraction haveconsiderablybroadenedthespectrumofwhatispossibleand,atthesame time, created a need for rethinking existing visualization solutions with regard to interaction. In this section, we will shed some light on how interactive visualization solutionscanworkwithmoderninputmodalitiesandoutputenvironments.We willstartwithbasictouchinteraction.Fromthere,wecontinuewithinteraction for tangible visualization views. Finally, we illustrate how proxemic interaction can facilitate visualization on a large high-resolution display wall. 4.8.1 TouchingVisualizations Touch interaction has become very popular, especially for hand-held devices. Bytouchingthevisualizationdirectlyunderourfingertips,alsotheexploration and analysis of data seems to be particularly well supported and promising. With touch interaction, direct manipulation becomes truly direct because the interaction takes place where the visualization is shown: on the display. Yet, in order to actually obtain directness, we have to address a few challenges.First,wehavetodealwiththefactthatonlytwostatesofBuxton’s three-statemodelofgraphicalinput(seeSection4.3.1)areavailablewhenusing
Interacting with Visualizations (cid:4) 195 current touch technology. That is, there is no way we can hover a visualization as we can do with a mouse. We can either touch it or not. Second, touch interaction is less precise. While practiced users can position a mouse pointer at a pixel-precise location, we are typically unable to touch down exactly at a certain pixel. This is due to our limited motor skills and the fact that our fingertips are several times larger than a pixel. Moreover, as the hover state is lacking, there is no way of correcting the initial touch point before triggering an action. Once the finger is down on the display surface, the touch is registered. Another problem is the occlusion that is introduced when parts of our hand or arm cover information on the display. Moreover, dealing with multiple touches is non-trivial from an interaction design perspective. An in-depth discussion of all of the aforementioned challenges of touch interaction for visualizationisbeyondthescopeofthischapter.Formoredetails,theinterested reader is referred to the further readings collected at the end of this chapter. Here, we would rather briefly demonstrate the differences of mouse-based and touch-based interaction for a simple visual representation of time-oriented data, a SpiraClock visualization. Touch-enabledSpiraClockVisualization The SpiraClock is a technique for visualizing collections of temporal events, suchasthe personalagendaor busschedules [DH02b].The SpiraClock’s hands display the current time, whereas its interior provides a spiral view onto the time to come. Along the spiral, future events are marked as spiral segments, each spiral cycle represents an hour in the future. Figure 4.44 shows a simple example with the current time being almost 3:55 o’clock. In 5 minutes, the next appointment will start and it will take 15 minutes. Following the spiral inward, we see that after a half-hour break there will be another appointment lasting 25 minutes. Still further inward we see more future events. As time goes by, events gradually move outward and eventually exit the spiral, while future events enter the spiral at the center. InordertoenableuserstoexploreeventsintimeandadjusttheSpiraClock, it makes sense to support the following basic interactions: • Navigate in time • Adjust future view • Query details The question that interests us is how these actions can be carried out with the classic mouse and alternatively with touch interaction. Let us first look at the mouse-based approach and then at the necessary changes to create a touch-enabled SpiraClock.
196 (cid:4) Interactive Visual Data Analysis Figure4.44 Visualizing future appointments with a SpiraClock. Mouse Interaction For navigation in time, users can temporarily set a different time by rotating the clock hands via drag gestures. The view into the future can be narrowed or widened by dragging the spiral. Dragging toward the center will reduce the number of cycles, dragging outward to the rim will bring more cycles to the SpiraClock’s interior. Finally, textual details can be queried simply by hovering an appointment, which will display a tooltip with the corresponding information. Touch Interaction In order to take advantage of touch interaction, the mouse-based design has to be cast into a touch-based one. On first sight, the transition from mouse to touch seems trivial: Instead of operating the mouse, fingers will carry out the drag gestures directly on the display. Yet, our users may soon stumble upon the difficulty of grabbing the clock hands due to the limited touch precision. One way to address the precision issue would be to enlarge the clock hands. Yet,thickerclockhandsnotonlylookawkward,theyalsoincreaseocclusion.An alternative is to decouple graphics and interaction [Con+08]. That is, regular clockhandsareusedfordrawingtheSpiraClockandaseparate,slightlythicker invisible geometry for handling the interaction. This way, the SpiraClock can maintain its appearance, while the clock hands are easier to pick thanks to their invisibly increased thickness. Unfortunately, the enlarged interaction geometry of the clock hands makes it more difficult to perform a drag gesture on the clock interior, as we might accidentally touch the hands. To solve this conflict, we can use a dedicated
Interacting with Visualizations (cid:4) 197 TABLE4.4 Mouse-based vs. touch-based interaction for the SpiraClock Action Mouse Touch Navigate Drag gesture on Drag gesture on in time clock hands enlarged clock hands Adjust Drag gesture on Pinch gesture future view spiral interior anywhere Query Hover appointments Tap on appointments details touch gesture: the pinch gesture. While clock hands are rotated with a single finger, manipulations of the spiral will be performed by a two-finger pinch gesture.Pinchingisthequasi-standardforscalinginteractionsontouchdevices. In our case, we scale the number of spiral cycles as soon as a second finger touches the display. Finally, we need to design the interaction to query details about the displayed events. As hovering is incompatible with touch interaction, we have to resort to a different approach. This time, we use a simple tap gesture, which corresponds to just a brief touch. When an event is tapped, a tooltip will display its details. A second tap on the same event (or on the background) will dismiss the tooltip. But what about the increased thickness of the clock hands? Doesn’t it lead to problems when the clock hands overlap with events? Yes, indeed. But the tap gesture is a discrete interaction, whereas the dragging of clock hands is continuous. This means, the tap is a very short interaction that is easily detected. Once we know that a tap occurred, we simply ignore the clock hands and consider only the events as potential interaction targets. Aswehaveseen,alreadyforthethreesimpleinteractionsoftheSpiraClock, we had to carefully think about the transition from mouse to touch interaction. Table 4.4 provides a comparison of both designs. We cannot claim that either of the designs is better or even optimal. Yet, they served well the purpose of demonstrating a few issues when designing touch interaction for visual representations of data. In the next section, we will continue with modern ways of exploring data. Yet, we will go one step further from fingers touching visualizations on a display to tangible interaction with the display. 4.8.2 InteractingwithTangibles Tangibleuserinterfacesisabroadfieldofresearch[SH10].Thegoaloftangible interaction is to narrow the gap between the virtual world on the computer and the real world in which the interaction takes place. The link between both worlds is physical objects, so-called tangibles. Physical manipulations
198 (cid:4) Interactive Visual Data Analysis of tangibles are transferred to virtual objects on the computer. In this sense, mouse-based interaction is already a form of tangible interaction, yet a rather indirect one. More direct tangible interaction can be achieved by using tangibles such as discs or cubes directly on horizontal touch-sensitive surface displays. To minimizeocclusion,thetangiblesaretypicallymadefromtranslucentmaterials, such as acrylic glass or foil. Various physical gestures can be performed with tangibles. Tangibles can be placed and moved across the display. Additionally, they can be rotated or positioned with different sides facing upwards. These tangible interactions expand the possibilities for interactive data exploration on touch-sensitive displays. Yet, interaction and visualizations remain in the two-dimensional horizontal plane of the display. Next, we present an approach that extends tangible interaction to the three-dimensional space above the display to provide enhanced visualization and interaction functionality. TangibleVisualizationViews Let us first introduce the basic setup. As before, a horizontal surface display will serve to visualize data and to receive touch input from the user. What we add to this basic setup are so-called tangible views [Spi+10]. Tangible views are lightweight “devices” that act as additional displays in the space on or above the horizontal surface. In a most simple instantiation, a tangible view can be a piece of cardboard onto which a projector transmits visualization content. In this case, the tangible view is passive. A tangible view can also be active, in which case it is capable of displaying graphical content on its own, for example, a tablet device. A key characteristic of tangible views is that they are spatially aware. Throughconstanttracking,thesystemalwaysknowsatangibleview’sposition and orientation. This opens up whole new possibilities for interaction as illustrated in Figure 4.45. The extended capabilities of tangible views include basictranslationandrotationinthreedimensionsaswellasgesturesofflipping, tilting, and shaking. By providing tangible views that are distinguishable by shape or appearance it is possible to create an interaction toolbox, where users can infer interaction functionality from the look of a tangible view. Multiple tangible views can be used simultaneously for advanced interaction and for adding display space for visualization purposes. Yet, offering extended interaction capabilities is only one part of the story. The second part is to utilize them to create a semantically meaningful interac- tion vocabulary for visualization scenarios. This has to be done depending on the characteristics of the data and their visual representation, and in line with the tasks to be carried out. Next, we take a closer look at two selected applications of tangible views. In the first example, we will use a tangible view as a magnifying lens for a scatterplot visualization. For the second example, we apply two tangible views
Interacting with Visualizations (cid:4) 199 TRANSLATE ROTATE GESTURE Vertical Horizontal Vertical Horizontal Tilt Shake FREEZE APPEARANCE GESTURE Vertical Horizontal Color Shape Flip Wipe Figure4.45 Extendedinteractionwithtangibleviews.Adaptedfrom[Spi+10]. to visually compare graph matrices. For both examples, we have already seen non-tangible implementations earlier in this chapter. It will now be interesting to see how tangible interaction creates a more physical data exploration experience. ATangibleMagnifyingLens For our first example, the main visualization is a scatter plot. In order to de-clutter dense parts of the plot, we can use a magnifying lens as described in Section 4.6.3. In a standard mouse-plus-keyboard setting, the lens can be moved across the visualization by drag gestures to define where it should take effect. The degree of magnification is typically adjustable via standard or custom-made sliders. Now let’s make the virtual lens tangible and truly direct. To this end, a circular tangible view is inserted into the space above the surface display as illustrated in Figure 4.46. The tangible view’s horizontal position determines whichpartofthevisualizationistobemagnified(dashedcircleonthesurface). The actual lens effect is projected onto the tangible view. This already yields a very tight coupling of display and interaction. The extended interaction vocabulary also makes it possible to control the magnification factor in a tangible way. There are several options how this can be implemented. The tangible view can be raised and lowered along the vertical axis to increase and decrease magnification. Figure 4.46 illustrates a second alternative: The tangible view is rotated around the vertical axis while a circular gauge visualizes the current magnification factor. As a result, we obtain a tangible magnifying lens that can be adjusted by physical manipulation, and the visual feedback is immediately visible where the interaction takes place. In this first example, one or two hands control a single tangible view. Next, we add a second tangible view and operate both simultaneously to accomplish comparison tasks.
200 (cid:4) Interactive Visual Data Analysis Figure4.46 A circular tangible lens for magnification purposes. Figure4.47 Comparing matrix data with two tangible views. TangibleVisualComparison Now, the base visualization on the surface shows a graph. It is represented in matrix form as described in Section 3.5.2 of Chapter 3. We will be using two rectangular tangible views as depicted in Figure 4.47. In order to select a sub-matrix to be compared, the user moves a tangible view horizontally above the surface. To fix the selection, a freeze gesture is carried out. This involves no more than swiping the thumb through a particular spot at the border of the tangible view. Once frozen, a tangible view maintains its visualization content, effectively ignoring any horizontal movements. This allows the user to arrange both tangible views side-by-side for closer inspection and comparison. Once the views are sufficiently close to each other, the system recognizes the user’s comparison intent and adds an explicit encoding of the overall aggregated similarity of the matrices to the tangible views. In our case, the green frame suggests that the matrices are quite similar to each other.
Interacting with Visualizations (cid:4) 201 (a)Parallel coordinates. (b)Node-link diagram. (c)Space-time cube. Figure4.48 Tangible views for different visualizations. In order to continue the visual comparison elsewhere in the data, the tangible views need to be awakened from their frozen state. A natural gesture to stir up a stationary system is to shake it. This is what the user does to un-freeze the tangible views: Simply shake them horizontally and start over with selecting different parts of the matrix. In summary we see that tangible views offer novel and interesting ways of interacting with visual representations of data. This is not only the case for magnification and comparison tasks, but for a broad range of visualization problems.Figure4.48illustratesfurtherapplicationsexamples.InFigure4.48a, the tangible view can be raised or lowered to control the sampling rate with whichthemultivariateparallelcoordinatesplotissampled.Raisingandlowering thetangibleviewabovethenode-linkdiagraminFigure4.48benablestheuser to access different levels of abstraction of a hierarchical graph. The tangible view in Figure 4.48c serves as a slice in a space-time cube visualization of spatio-temporal data. From a conceptual perspective, tangible views advance interaction and visualization in several ways. Tangible views integrate display and interaction device, allowing us to interact directly with the visualization. They also extend common two-dimensional interaction by tangible three-dimensional interaction above a horizontal base visualization. The resulting enhanced interaction and the extended physical display space create a tangible experience of otherwise purely virtual actions. Results of controlled user studies suggest that tangible spatial interaction is indeed a promising alternative when working in and with layered zoomable information spaces, which are common in visualization scenarios [SMD12; Spi+14]. Yet, developing an enhanced interaction and providing evidence of its extended expressive power is only a beginning. It remains to be investigated what will be the most suitable interaction designs for a broad range of data analysis scenarios, taking into account different types of data and different user tasks.
202 (cid:4) Interactive Visual Data Analysis Another interesting observation is that tangible views, touch-sensitive devices, and the classic mouse, all require us to use our hands for interaction. Next, we will look at a hands-free form of interaction where a visualization is controlled by physical body movements in front of a large display wall. 4.8.3 MovingtheBodytoExploreVisualizations Sofar,wehaveconsideredvisualizationonregulardisplays,horizontalsurfaces, and tangible views. These and other output devices with conventional pixel resolution are typically limited in the amount of information that can be displayed. Thanks to technological advances, large high-resolution displays are now becoming available to a broader range of users. The larger physical size and the increased pixel resolution offered by such displays have obvious advantages for visualization applications. Particularly in light of big data, being able to visualize much more information is an exciting prospect. Yet, with larger size and more pixels there also come new challenges. It is no longer feasible to interact with a mouse or touch alone, because it is simply too difficult, if not impossible to reach across the entire display. Therefore, new solutions are needed to support the visualization and interaction on large high-resolution displays. In this section, we look at a scenario where physical body movements in front of a display wall support the exploration of a graph with multiple levels of detail [Leh+11]. VisualizingaGraphonaDisplayWall ThesetupweareaddressingisatileddisplaywallasshowninFigure4.49.The wall consists of 24 individual displays covering an area of 3.7 m × 2.0 m with a total resolution of 11,520 × 4,800 which amounts to 55 million pixels. The data we want to explore is a graph that is described at three levels of detail. The visualization is based on a node-link representation which is augmented with textual labels and hulls for grouping. Individual nodes can be expanded or collapsed in order to get a finer or coarser view on the graph, respectively. On a desktop, expand and collapse operations are typically carried out by clicking or tapping. In our scenario, this is obviously impractical due to the large distances that would need to be covered. InteractingbyBodyMovements An alternative is to control the displayed level of detail by the user’s physical movement in front of the large display wall. This requires a tracking system to be set up to acquire information about the user’s position and orientation (6 degrees of freedom). Theuser’spositioncanthenbeexploitedtoadjustthelevelofdetailglobally. To this end, the space in front of the display wall is sub-divided into zones with increasing distance to the display, as illustrated in Figure 4.50a. Each
Interacting with Visualizations (cid:4) 203 Figure 4.49 Graph exploration on a large high-resolution display wall. Reprinted from [Tom15]. zone corresponds to a level of detail. When the user moves into a zone closer to the display, the graph is visualized at greater detail. Stepping backward into zones farther away from the display will lead to a coarser representation. This approach is also dubbed proxemic interaction [BMG10]. It is inspired by natural human behavior: Humans typically step up to the object of interest to study it in detail and step back to obtain an overview. Withthezone-basedinteraction,userscancontrolthelevelofdetailglobally. Local adjustments require a way to point at where the level of detail should be changed. One option to do this is to show more details exactly where the user is looking on the display. Based on the tracking information (position (a)Zones for global control. (b)Gaze plus lens for local control. Figure4.50 Interacting by physical movements. Adapted from [Leh+11].
204 (cid:4) Interactive Visual Data Analysis and orientation), the user’s gaze direction can be estimated. With the help of dedicatedeye-tracking,theprecisionoftheestimationcanfurtherbeimproved. At the spot being looked at, an interactive lens is embedded into the regular graphvisualizationasillustratedinFigure4.50b.Nodesthatareinsidethelens are automatically expanded to reveal more detailed information. By moving the head, the user can quickly scan the graph for details. Filtering the tracking input and smoothly animating node expand and collapse operations help to avoid flickering caused by natural head tremor and to maintain a reasonably stable visualization. The user’s physical movements can be used not only to control the level of detail, but also to derive a suitable layout for the labels shown in the graph visualization. For a user standing close to the display, the visualization will show more and smaller labels. When looking from a greater distance, the user will see fewer, but larger labels for groups of nodes. This way, the costs for producing the visualization and for comprehending it can be balanced. The results of a pilot study suggest that interaction through physical movement in front of a display wall can indeed be a valuable alternative in cases where classic means of interaction fail [Leh+11]. Physical movement not only better matches the scale of the display, it also is in line with natural interaction with real-world objects. Interacting via zones was reported as the approach that is easier to use, but on the other hand, the lens offered more control over where increased detail is to be shown. Here,weconsideredtherelativelysimpletaskofadjustingthelevelofdetail of a graph visualization. But physical movement has also proved useful in otherscenarios.Zoomingtasks,forexample,canbesupportedwellbyphysical navigation [Jak+13]. Other research results indicate that physical navigation can also be beneficial for higher-level analytic sensemaking [AN13]. In this last section of the chapter on interaction, we have focused on new technologies for interactive visual data analysis. We made a transition from mouse interaction to touch interaction and enhanced the latter with tangible interaction. Finally, we have illustrated how physical movements can be exploited to control visualizations. The presented techniques illustrate the prospect of utilizing modern technologies, but more research is necessary to make them reliable and evolve them to mature and tested ways of interacting with data and their visual representations. 4.9 SUMMARY The goal of visualization is to support people in forming mental models of otherwise difficult-to-grasp subjects, such as massive data, complex models, or dynamic systems. The term forming implies that visual output is not the end product of visualization. It is rather the human-in-the-loop process of interactively exploring the data and adjusting their visual representation that enables us to gain insight. This chapter elaborated on this very process.
Interacting with Visualizations (cid:4) 205 Welookedatthetopicfromdifferentangles,includingaspectsofthehuman user who interacts, the interaction tasks, the data being interacted with, and the technology used for interacting. In the first part of this chapter, we dealt with the question of why interaction is needed in visual data analysis scenarios. In the middle part, we discussed what users can actually accomplish with general interaction concepts and concrete interaction techniques. Finally, in the last part, we considered how interaction can be carried out using modern interaction modalities. Throughout the chapter, many examples illustrated how interaction can support data analysis activities. ConcludingRemarks Useful and usable interaction techniques are the result of careful consideration of the human user, the analytic tasks to be accomplished, the characteristics of the data, and the technological environment in which the analysis takes place. Several books would be needed to cover the design space for interaction comprehensively. Nonetheless, the following paragraphs will provide some high-level remarks on the design of interaction for visual data analysis. Duality of Input and Output A fundamental requirement is to consider the duality of visual output and interactive input right from the beginning. Hutchins and colleagues put it this way [HHN85]: “[...]thenatureoftherelationshipbetweeninputandoutputlanguagemust besuchthatanoutputexpressioncanserveasacomponentofaninput expression.” Hutchinsetal.,1985 Appliedtothecontextofthisbook,thismeansthatwhateverwedisplayin a visualization will most certainly be relevant for interaction as well. Whatever we desire to input into the system will typically require a suitable visual representation for direct manipulation. Adding interaction as an afterthought to a visualization is likely to cause trouble and increased development costs. Mapping Tasks to Interactions Given the complexity of exploratory and analytic activities and the wealth of interaction modalities, finding a good mapping of tasks to concrete interactions is a non-trivial endeavor. Two requirements are particularly important: Interactions must be conflict-free and should be cost-efficient. It is absolutely necessary that interactions be conflict-free, meaning that an interaction must be associated with a unique task. To this end, interactions
206 (cid:4) Interactive Visual Data Analysis must be unambiguously distinguishable. A distinction is typically made at the level of gestures, such as hovering, clicking or double-clicking. Also the space where the interaction takes place can be a differentiating factor. That is, we use the same gesture, but perform them in distinct spatial regions. In this case, clicking a map, clicking a data item, and clicking the background are all different interactions. Moreover, the order or timing of operations could be considered, but this complicates the interaction enormously. Speaking of complicated interaction, the second requirement is about interaction costs: Interaction should be cost-efficient. Each interaction costs. The more often an interaction is used, the higher the accumulated costs. Therefore, the mapping of tasks to interactions should consider a ranking of the relevance or frequency of tasks and an estimation of the costs of the available interactions. Based on that, a reasonably balanced mapping can be established. Frequent tasks should be mapped to interactions with the lowest costs. Only for infrequent tasks is it acceptable to use interactions with higher costs. InteractiveplusAutomatic Directlyrelatedtothecostaspectofinteraction is the question regarding what should be done interactively and what can be accomplished by automatic means. But this is not a question of interactive versus automatic means, rather it is a call for interactive plus automatic means. This chapter proposed several ideas of how interaction can be eased by integrating automatic mechanisms. After all, it is the responsibility of the system (and the designer of the system beforehand) to provide the information needed in a particular situation. Whenever we make functionality available via interaction we should carefully think about how interaction costs can be reduced by integrating automatic assistance. With these concluding remarks, we close the chapter on interaction. As we have seen, interaction is an integral component of visual data analysis approaches. In the next chapter, we will see that analyzing large data addi- tionally requires the integration of computational analytic components. FURTHER READING General Literature: [Dix+04] • [SP09] • [Tom15] • [SP16] • [DP20] Zoomable Visualizations: [BH94] • [FB95] • [Fur97] • [Bed11] Lenses for Visualization: [Bie+93] • [TFS08a] • [Tom+17] Visual Comparison: [Gle+11] • [vLan18] • [Gle18] Beyond Mouse and Keyboard: [Lee+12] • [Ise+13] • [JD13] • [Mar+18]
5 CHAPTER Automatic Analysis Support CONTENTS 5.1 Decluttering Visual Representations ........................ 209 5.1.1 Computing and Visualizing Density ................ 209 5.1.2 Bundling Geometrical Primitives ................... 212 5.2 Focusing on Relevant Data ................................. 214 5.2.1 Degree of Interest ................................... 214 5.2.2 Feature-based Visual Analysis ...................... 220 5.2.3 Analyzing Features of Chaotic Movement .......... 224 5.3 Abstracting Data ........................................... 231 5.3.1 Sampling and Aggregation .......................... 231 5.3.2 Exploring Multi-scale Data Abstractions ........... 233 5.4 Grouping Similar Data Elements ........................... 238 5.4.1 Classification ........................................ 239 5.4.2 Data Clustering ..................................... 243 5.4.3 Clustering Multivariate Dynamic Graphs .......... 250 5.5 Reducing Dimensionality ................................... 257 5.5.1 Principal Component Analysis ...................... 258 5.5.2 Visual Data Analysis with Principal Components .. 260 5.6 Summary .................................................... 263 IN THE PREVIOUS chapters we described fundamental approaches to interactive visual data analysis. In light of the potentially complex and very large datasets we are facing today, it is hardly possible to indiscriminately visualize and interact with all data. Our visual representations would simply be overcrowded and interaction would be cumbersome and costly. This is why computational analysis support is necessary. Visual analytics pioneer Daniel Keim puts it this way [Kei+06]: 207
208 (cid:4) Interactive Visual Data Analysis “Thevisualanalyticsprocesscomprisestheapplicationofautomaticanalysis methodsbeforeandaftertheinteractivevisualrepresentationisused.This isprimarilyduetothefactthatcurrentandespeciallyfuturedatasetsare complexontheonehandandtoolargetobevisualizedinastraightforward mannerontheotherhand.” Keimetal.,2006 The use of automatic analysis methods aims at extracting essential data characteristics. Showing key characteristics instead of the original data values reduces the complexity of visual representations and facilitates an initial overview of the data. Deeper insight can then be achieved by combining visualanalysis,interactivequerying,andfurtherautomaticcomputations.This procedure is nicely reflected in the visual analytics mantra, which we already mentioned in the introduction [Kei+06]: “AnalyseFirst– ShowtheImportant– Zoom,FilterandAnalyseFurther– DetailsonDemand” Keimetal.,2006 Theprimarygoalofthischapteristoprovideanoverviewofcomputational approaches that can support the analysis of large and complex data. While we will focus on the analyze step, we will also see that it is actually the tight interplay of automatic, visual, and interactive means that really advances the way data can be investigated and understood. Each section of this chapter will briefly explain a basic strategy for the analysis step and one or two selected techniques that implement the strategy. A common theme will be the reduction of complexity to make the visual analysis easier or enable it at all. What gets reduced will differ from section to section. In Section 5.1, our objective will be to reduce the complexity in visual representations. Section 5.2 considers the extraction of data and features of interest to narrow down the analysis on the parts that are relevant. How complexity can be reduced in the data space will be described in Sections 5.3 to 5.5. Section 5.3 is dedicated to the reduction of the cardinality of the data domain via data abstraction methods. In Section 5.4, we will see that grouping similar data is a powerful strategy to reduce the number of data
Automatic Analysis Support (cid:4) 209 elements. Finally, Section 5.5 will explain dimensionality reduction as a way to focus the analysis on the key information-bearing data variables. Let us now discuss the sketched options in more depth. We will start with methods that aim at reducing the complexity in visual representations. 5.1 DECLUTTERING VISUAL REPRESENTATIONS Over-plotting and visual clutter are common problems when large volumes of data are visualized. A first and important strategy facilitating the analysis of large data is to employ methods that can declutter the visual representation. We consider two basic approaches: computing and visualizing density as well as bundling of geometrical primitives. 5.1.1 ComputingandVisualizingDensity Density-based representations aim to communicate the distribution of data, rather than showing individual data values. The basic idea is to calculate how many data values fall within certain intervals. Alternatively, one can calculate how many graphical objects are within certain regions of the display. Both alternatives will next be illustrated by two well-known examples: con- tinuous scatter plots, which are based on data density, and outlier-preserving focus+context visualization, which is based on visual density. Data Density Traditional scatter plots visualize data elements as dots. For verylargedata,thiscanresultinsevereover-plotting,whichmakesitimpossible to discern how much data are represented by a dot. Continuous scatter plots solve this problem by visualizing a continuous density function [BW08a]. For this purpose, a mapping is performed from the data domain to the spatial domain that is spanned by the two axes of the scatter plot. While being conceptually continuous, the density is typically (a)Traditional scatter plot. (b)Continuous scatter plot. Figure5.1 Comparison of traditional scatter plot and continuous scatter plot. Reprinted from [BW08a].
210 (cid:4) Interactive Visual Data Analysis approximated by counting data elements in discrete intervals. The continuous visualization is then obtained by interpolation. Figure 5.1 compares a traditional scatter plot based on discrete dots with a continuous scatter plot based on data density. Both plots show the same “blunt-fin” dataset. With the density-based visualization, internal structures in the data are much easier to discern. VisualDensity Incontrasttoscatterplots,parallelcoordinateplotsvisualize data elements as polylines across parallel axes. For large datasets, a large number of polylines needs to be drawn, which can substantially clutter the visual representation. Again, a density-based approach can be employed to reduce visual clutter. Yet, now we are interested in the visual density of the graphics [NH06]. By computing the visual density, it is possible to accentuate not only general trends, but also details such as outliers. The rest of the data can be attenuated. Figure 5.2 schematically depicts how the visual density in parallel coordi- nates can be determined and how the visualization can be adapted accordingly. For the purpose of illustration, we start out in Figure 5.2a with the most basic (and unusual) case of only two parallel axes. In a first step, the axes are subdivided into bins b , where i denotes the i,j axis and j the bin per axis. The second step is to count how many lines emanate from the bins of one axis and arrive at the bins of the other axis. These pairwise frequencies are stored in a so-called bin map. We can see in Figure5.2bthatabinmapisagridofcells,whereeachcellrepresentsapairof bins.Forexample,therearefourlinesbetweenbinsb andb .Consequently, 1,1 2,2 the corresponding cell in the bin map contains the number 4. Repeating the counting for all pairs of bins completes the bin map. Thethirdstepistocategorizethecellsofthebinmapasbelongingtotrends ordetails,whichisillustratedinFigure5.2c.Thresholds,outlierdetection,and clustering methods can be involved in the categorization [NH06]. In the final step,trendsanddetailsaredrawntothedisplay,butdifferentlyso.Figure5.2d illustrates that the trends are represented in a graphically aggregated fashion, whereas the details get rendered as individual lines. b b b 5 1 b 5 1 b b 1,4 2,4 1,4 1,4 1,4 2,4 b 6 b 6 b b 1,3 1,3 b b 1,3 2,3 b 1 b 1 1,3 2,3 b b 1,2 1,2 b b 1,2 2,2 b 4 b 4 1,2 2,2 1,1 1,1 b b b b 1,1 2,1 b b b b b b b b 1,1 2,1 2,1 2,2 2,3 2,4 2,1 2,2 2,3 2,4 (a)Binned axes. (b)Bin map. (c)Categorization. (d)Drawing. Figure5.2 Procedure of determining visual density in parallel coordinates.
Automatic Analysis Support (cid:4) 211 (a)Standard parallel coordinates plot. (b)Density-based visualization. Figure5.3 Visualization of more than three million data elements. Reprinted from [NH06]. For general parallel coordinates plots with m axes, the above procedure is repeated for each pair of neighboring axes. As a result, m−1 bin maps are obtained based on which the visualization complexity is reduced globally. Figure 5.3 demonstrates the benefit of the described approach. A classic parallel coordinates plot is shown in Figure 5.3a. Due to clutter, it does not revealmuchofthedata’sinnerstructure.Thedensity-basedapproachisshown in Figure 5.3b. The general trends are visualized as parallelograms of different shades of green, where strong trends use brighter greens and weaker trends use darker greens. Data elements that have been categorized as details are explicitly visualized as green polylines. With the density-based visualization, trends and details can be seen much better. By interactively adjusting the number of bins and the thresholds for cat- egorizing trends and details, it is possible to further abstract or elaborate on the visualized structures. Moreover, red polylines can be superimposed on the visualization to facilitate comparing user-selected data against the automatically determined trends. This is illustrated in Figure 5.4. We have seen that density-based methods, no matter if they operate on data values or on graphical objects, are very well suited to declutter visual representations.Inthenextsection,wediscussanotherapproachthataddresses the problem of visual clutter, this time, by bundling geometrical primitives.
212 (cid:4) Interactive Visual Data Analysis Figure5.4 User-selecteddatainredcomparedagainstgeneraltrendsingreen. Courtesy of Helwig Hauser. 5.1.2 BundlingGeometricalPrimitives As we know from the previous section, visual representations that use lines or line segments as their basic geometrical primitives are prone to severe clutter. Many such visualizations exit, for example, the parallel coordinates plot as described before, node-link diagrams of graphs, or trails of movement data. In this section, we focus on decluttering such visual representations by summarizing geometrical line primitives into so-called bundles. As before, the goal is to highlight the fundamental structures in the visual representation. The general procedure of bundling is outlined in Figure 5.5. The starting point is line primitives. They are the basis for the specification of paths that consist of the lines’ original vertices plus additional control points. Such paths can be flexibly adjusted in their course in order to form visual bundles. To this end, it is necessary to decide which paths should belong to a bundle and how the individual paths should be transformed. Both decisions can be made based on either an explicit or an implicit bundling definition. Lines Paths Bundles Image Defining Paths Bundling Drawing Explicit Definition Implicit Definition Figure5.5 General procedure of bundling. Inspired by [LHT17].
Automatic Analysis Support (cid:4) 213 (a)Conventional representation. (b)Hierarchical edge bundling [Hol06]. Figure5.6 Visualizationofdependenciesinasoftwareclasshierarchy.Adapted from bl.ocks.org/mbostock/4341134. Explicit definitions resort to existing criteria. For example, paths can be bundled along given hierarchical structures [Hol06]. In parallel coordinate plots, polylines can be bundled according to pre-calculated clusters [Hei+12]. Bundling based on such predefined criteria produces predictable layouts, but it is less flexible in terms of bundling control. Implicit definitions make use of appropriate similarity metrics. Paths that are similar will be transformed to bundles. A prominent example is force- directed bundling methods [HvW09]. Compatibility criteria define which paths are eligible for bundling and the actual bundling transformation is done via spring forces between the control points of compatible paths. The implicit bundling strategy is more flexible, since it is not necessary to address pre- defined constraints. It can be adjusted easily, and bundles will be updated on the fly. In the last step, the bundles are visualized. There are two options: drawing individualcurvesordrawingentirebundlesascompactgeometricalshapes.The important thing is to ensure that bundles are recognizable and distinguishable. Thiscanbeachievedbyasuitablevisualencoding.Often,bundlesareassociated with a unique color. The visual encoding can be enhanced by shading the bundles based on a pseudo-surface that spans a bundle [LHT17]. Blending can be applied to resolve occlusion problems when bundles overlap. Figure 5.6 illustrates the positive effect of bundling. The treemap in the background visualizes the class hierarchy of a software framework. Links between the nodes of the treemap indicate dependencies between classes. The darker a link is, the larger are the classes involved in a dependency. As can be seeninFigure5.6a,manylinksleadtosubstantialclutter.Bytransformingthe links into curved bundles, in this case based on the explicitly given hierarchy, the visualization can be streamlined. Figure 5.6b reveals the major relations among the class dependencies. In summary, this section discussed methods to facilitate the analysis of large datasets by reducing the complexity in visual representations. In the following sections, we will continue with reducing the complexity in the data.
214 (cid:4) Interactive Visual Data Analysis 5.2 FOCUSING ON RELEVANT DATA A common and widely used strategy in terms of data reduction is to focus the visual analysis on data of interest. But what are the data of interest and how can a distinction between interesting and less-relevant data be made? Of course,userscouldinteractivelyselectthedatatheydeemimportant.Forlarge datasets, however, a purely manual procedure can be time-consuming and error-prone. How automatic computations can help in determining relevant data will be the topic of this section. The basic idea is to let users specify (not select) their interests and to employ automatic computations to select those parts of the data that match the specification. We discuss two concepts that implement this general idea. First, we study the concept of degree of interest and how it can help us to narrow down the analysis on relevant data. Second, we present the concept of feature-based visual analysis, which is about the specification and automatic extraction of meaningful data characteristics. 5.2.1 DegreeofInterest The degree of interest (DoI) is a concept to capture the relevance of data for solving an analysis task. The degree of interest can be expressed by DoI functions.ADoIfunctionassignstoeachdataelementarelevancevalue.Using a suitable threshold, it is then possible to distinguish between relevant and less-relevant data. The relevant data correspond to the target of an analysis task. In Section 2.2.2, we explained that particularly the targeted data need to bevisualizedfaithfully.Incontrasttothat,less-relevantdatacanbedimmedor even omitted. This substantially reduces the complexity of the visual analysis. BasicApproach Already in the 1980s, Furnas introduced a DoI function to express the degree of interest for the nodes of static hierarchies [Fur86]. The basic idea is that the degree of interest of a node n depends on the distance of n to a focus i i node n and an a priori interest of n . For the purpose of illustration, we set f i up a DoI function doi(n ,n ) as follows: i f doi(n ,n )=dist(n ,n )+api(n ) i f i f i Thefocusnoden isassumedtobeintheuser’scenterofattention,dist(n ,n ) f i f is the length of the shortest path between n and n , and api(n ) describes the i f i a priori importance of n . In the context of hierarchical data, it makes sense i to define api(n ) as the level at which n can be found in the hierarchy, that i i is, the distance of n to the hierarchy’s root. i Figure 5.7 illustrates how the DoI function assigns a relevance value to each node. In Figure 5.7a, we can see the dist(n ,n ) term, to which the i f api(n )termisaddedinFigure5.7b.AsourDoIfunctionisbasedondistances, i
Automatic Analysis Support (cid:4) 215 0 2 0 2 0 r = 3 1 3 3 1 1 4 4 2 1 r = 5 24 4 4 0 2 26 6 6 2 4 2 r = 7 3 5 1 1 1 3 8 4 4 4 3 4 6 6 2 2 2 4 10 10 6 6 6 4 (a)Distances to focus node. (b)Adding node levels. (c)Extracted subtrees. Figure5.7 Illustration of Furnas’ DoI function. lower values stand for higher degrees of interest. A user-defined threshold r determines where to draw the line between relevant and non-relevant nodes. Depending on the threshold used, relevant sub-structures of different size are automatically extracted from the hierarchy. Figure 5.7c provides examples for three different thresholds r =3, r =5, and r =7. Furnas’ idea of a DoI function is well established. It has been extended in different ways, for example, to handle multiple focus nodes [HC04] or to deal with general graphs [vHP09]. Most of the time, however, DoI functions are pre-defined and cannot be altered by the user. This is in contrast to the necessity of adapting the visual analysis to changing tasks and requirements. Next, we look into a more flexible approach that allows the user to construct DoI functions on demand. FlexibleConstructionofDoIFunctions The construction of a DoI function requires a modular design of the involved components[Abe+14].Withoutgoingintotoomuchdetailabouttheunderlying formalism, setting up a DoI function can be done in three steps: 1. Definition of relevance components 2. Combination of relevance components 3. Specification of relevance propagation With the first step, the user defines relevance components that compute a relevance value per data element. It makes sense to work with normalized values,where0signifiesnorelevanceand1isthehighestrelevance.Anexample ofarelevancecomponentwouldbeaGaussianfunctionthatproducesasmooth decline of relevance with increasing distance to a particular value of interest. It can make sense to define several components to capture different aspects of relevance. For example, one component could consider spatial proximity, another component could account for temporal dependencies. Also structural
216 (cid:4) Interactive Visual Data Analysis properties can be taken into account, as in the previous example, where we used distances in a hierarchical structure. ThesecondstepistoconstructacomprehensiveDoIfunctionbycombining the relevance components. Conceptually, a combination of relevance compo- nents can be modeled as a function that takes two or more relevance values and returns a combined relevance value. Furnas’ DoI function simply adds the components. Yet, more advanced combinations are possible. A common example is the combination as a weighted sum. It allows for balancing the influence of components: Heavily weighted components influence the overall resultmorethanlightlyweightedones.Furthermore,minormaxcombinations can be applied. A min combination requires all involved components to return high relevance in order for the combination to return a high relevance as well. A max combination works the other way around: If either component returns a high relevance, the overall combination will yield high relevance. We can now flexibly construct component combinations to compute the relevance of data elements. However, a relevance value characterizes only an individual data element. The immediate context of the data elements is not taken into account. For example in Figure 5.7c, a subtree with three nodes is extracted for r =3. Yet, structural properties of the focal node are lost. The subtree does not tell us whether the focal node has siblings or child nodes. Choosing a higher threshold of r =7 adds too much context information than would be needed specifically for the focal node. Only if the threshold is set to r = 5 is the user’s interest in siblings and child nodes properly reflected. However, finding such an appropriate threshold is notoriously difficult. Therefore, the third and last step is to decide on a propagation method that distributes relevance in the vicinity of high-relevance data elements. The propagation can be performed in different ways. Structural propagation distributesrelevancealongtheedgesinagraph,temporalpropagationconsiders neighboring time intervals, and spatial propagation spreads relevance in a spatial neighborhood. By specifying different types of propagation, the user can trigger the extraction of different context information. Taken together, users can now specify what data should be considered as relevant.Theflexiblecombinationofrelevancecomponentsmakesitpossibleto specify which data aspects should be taken into account and how they should impact the final decision. With propagation, it is possible to steer how high relevance can diffuse into neighboring data. Sofar,thedescribedprocedureprovidesbutanabstractconceptualscheme. Inordertomakeitapplicabletorealanalysisproblems,weneedasuitableuser interface that supports the three aforementioned steps. Such a user interface typically requires a design that is tailored to the data being studied. Next, we will consider this aspect in more depth by the example of an interface for DoI-based visual analysis of dynamic graphs.
Automatic Analysis Support (cid:4) 217 DoI-basedVisualAnalysisofDynamicGraphs Dynamic graphs are typically large and complex, which makes their visual analysis difficult. Consider, for instance, dynamically changing co-author net- works, where nodes represent authors, and edges exist between authors that published together. Node attributes provide additional information about the authors, for example, the number of publications. Edge attributes, such as the number of joint publications, characterize the co-authorships. Figure5.8showsfiveyearsofsuchadynamicco-authornetworkasextracted fromtheDBLPcomputersciencebibliographydatabase.Fromthevisualization, we can very clearly see how computer science has gained in importance as an academic discipline over the years. But we can also see that the network structuredrownsinafloodofnewauthorsandco-authorrelationships.Already fortheyear1990,thevisualizationissosaturatedthatnostructuralinformation can be discerned from it. 1970 1975 1980 1985 1990 Figure5.8 Five years of a dynamic co-author network extracted from DBLP. Courtesy of Steffen Hadlak. Inthefollowing,wewilldemonstratehowtheflexibleDoI-basedmechanism outlined before can help us in analyzing large dynamic graphs [Abe+14]. The graph that we will be dealing with covers 22 years of the DBLP database from 1990 to 2011 with an overall number of 914,492 nodes and 3,802,317 edges. The DoI-based visual analysis starts with the formulation of some interest in this big network. Let’s assume the user is interested in the top authors. Now, what makes an author a top author? Obviously the number of publications of an author plays a role. Moreover, top authors are likely to have many co-authors with whom
218 (cid:4) Interactive Visual Data Analysis Figure5.9 DoI specification via a nested graphical user interface. Reprinted from [Abe+14]. theypublishedtheirmanypaperstogether.So,tworelevancecomponentsneed to be defined: one based on the number of publications, the other based on the number of co-authors, where we would like to prioritize the number of publications a little more. Finally, it must be decided how the relevance values should be propagated. In order to enable users to specify interactively what we described verbally so far, a suitable graphical user interface is needed. Figure 5.9 shows an interface that is based on nested frames. The innermost frames represent our tworelevancecomponents.Theyaredesignedashistograms.Sigmoidfunctions maphighnumbersofpublicationsandco-authorstohighrelevancevalues.The inner components are encompassed in an intermediate frame representing a weighted sum. Sliders at the components facilitate the adjustments of weights to give the number of publications a slightly higher impact. Finally, the outermost frame represents the selected propagation method, in our case a structural propagation with a certain drop-off. Thanks to the tight integration of aggregated visual representations and user controls, the described interface makes it easy to extend and adjust the DoI specification as necessary. The next question is how the DoI approach can help us reduce complexity and support the visual analysis of our large dynamic graph? In Figure 5.10, we can see that the DoI interface is but a part of a larger framework. In addition to the DoI interface (a), there are a panel with graph statistics (b), a list of all authors and their co-author relationsships for manual selection (c), controls to adjust visualization thresholds (d), a time line with the aggregated relevance per year (e), and a node-link representation of the graph (f). The node-link representation visualizes the graph in a DoI-based fashion. Nodes of interest, that is, nodes with a relevance above the user-defined threshold are represented in full detail as individual dots. Subgraphs that are
Automatic Analysis Support (cid:4) 219 b c f a d e Figure 5.10 DoI-based visualization of the 2007 DBLP co-author network. The graphical interface controls (a)–(d) allow users to specify their interest in the data. The visualizations (e) and (f) show the relevant data and their associated relevance values. Reprinted from [Abe+14]. Number of nodes few intermediate many low medium high low medium high low medium high Edge density Figure5.11 Glyph design for representing collapsed subgraphs. Adapted from [Abe+14]. not of interest are collapsed and summarized by glyphs. The glyphs visualize two properties of the collapsed subgraphs: their number of nodes and their edge density. As illustrated in Figure 5.11, the number of nodes is encoded by the angle of the arc sector at the top of a glyph, whereas the edge density is mapped to the radius of the semicircle at the bottom. Throughout the node-link representation, the relevance values are encoded with variations of green, where darker green stands for higher relevance. Glyphs and edges leading to glyphs are shown in gray to indicate that they are less relevant. Comparing the representation of the network of 1990 in Figure 5.8 to the one of 2007 in Figure 5.10 makes immediately clear how the concept of degree of interest helps us to focus the analysis on relevant data. The DoI-based visualization facilitates a quick overview of those graph elements that are of
220 (cid:4) Interactive Visual Data Analysis particular interest to the user. In our case, the top authors are emphasized, while less-relevant data are represented in an aggregated fashion to provide some context. Here, we discussed the specific example of large dynamic graphs. Yet, the generalideaofdegreeofinterestcanbeappliedtootherdataclassesaswell.Its most important characteristic is that we can distinguish between relevant and less-relevant data. This allows for emphasizing those data elements that match the interests of the user, while the others are summarized or even omitted. Inthenextsection,wecontinuewithanotherapproachfornarrowingdown the analysis on relevant data, more precisely, on features. 5.2.2 Feature-basedVisualAnalysis The goal of feature-based visual analysis is to automatically extract features that capture meaningful data characteristics [RPS01]. The visual analysis will then concentrate on the features, rather than on individual data elements. This approach has two key benefits. First, we can reach a higher level of analytic abstraction. And second, the visual representations are clearer and less cluttered, because usually the number of features is much smaller than the number of data elements. The basic procedure of feature-based visual analysis consists of three steps: 1. Feature specification 2. Feature extraction 3. Feature visualization The first step is concerned with the specification of criteria that define interesting features. Based on this specification, features are automatically extracted from the data in the second step. This also involves the tracking of features over time and the detection of events in the evolution of features. The third step is to visualize extracted features and detected events. Next, we will explain these fundamental steps in more detail and illustrate them in the context of visual analysis of time-varying reaction-diffusion systems. FeatureSpecification The first step is the specification of meaningful criteria that characterize the features. What meaningful criteria are depends strongly on the data to be analyzed. For certain types of data, there are tailored feature definitions. For example,inflowvisualization,wherefeature-basedvisualanalysishasitsroots, features describe critical points, vortices, or shock waves [Pos+03]. However, such a priori definitions do not always exist. For simulations of reaction- diffusion systems, for example, domain experts are interested in features that describe 3D regions with high concentrations of certain particles. However, the
Automatic Analysis Support (cid:4) 221 (a)Specification of thresholds. (b)Formal feature definition. Figure5.12 Feature specification with an interactive interface. Courtesy of Christian Eichner. thresholds of what should be considered a particularly high concentration is not known beforehand, but varies depending on the type of particles and the system being studied. In such cases, features can be specified interactively. Forinteractivefeaturespecification,tworequirementsneedtobeaddressed. First,usersmustbeenabledtographicallyinputrelevantdatacharacteristicsas features,andsecond,thegraphicalinputmustmaptoasuitableformalfeature description to enable the later automatic extraction of features [DGH03]. Figure 5.12a shows an interface that supports the interactive specification of features [Eic+14]. The parallel histograms in Figure 5.12a facilitate an informed definition of suitable thresholds. The graphical input performed on the parallel histograms easily translates to a formal definition based on predicatelogic.Thecorrespondingformulasdescribeopenorclosedintervalsof value ranges of interest as combinations of comparison predicates and Boolean logic operators, as illustrated in Figure 5.12b. The formulas can be stored for later reuse and fine tuning. They are also the basis for the automatic feature extraction. FeatureExtraction Once the feature specification is complete, the next step is to extract instances of features (or simply features) from the data. To this end, the automatic feature extraction is carried out for each type of feature and for each time step in the data. This involves spatial and temporal aspects. Spatial Aspects In general, the goal is to determine where features are locatedinthedataspaceandtocharacterizethembytheirposition,size, shape,andorientation.Again,howtheextractionprocessisimplemented depends on the type of features being relevant in the application domain. Dedicatedmethodsexistforthecriticalpoints,vortices,andshockwaves in flows as mentioned earlier [Pos+03].
222 (cid:4) Interactive Visual Data Analysis For our running example of reaction-diffusion systems, the data consist of particles located in a 3D grid of volumetric cells. Each of the cells is testedwhetheritmatchesoneofthespecifiedcriteriaornot.Neighboring cells that do match are merged in order to generate coherent 3D regions representing the features. In a second step, the extracted 3D regions can be abstracted further to 3D ellipsoids [vWal+96]. The ellipsoid axes are oriented according to the eigenvectors of the covariance matrix of the positions of the matching cells, and the length of the axes is determined by the respective eigenvalues. In other words, the ellipsoids appear at locations and stretch out in directions corresponding to the interesting parts of the data. Temporal Aspects Inordertolearnaboutthetemporalevolutionoffeatures, theellipsoidsneedtobetrackedovertime.Thequestioniswhichellipsoid at time t corresponds to which ellipsoid at time t ? In other words, i i−1 which ellipsoid at time t represents an evolved version of an ellipsoid at i t ? i−1 By solving this correspondence problem for all time steps, we obtain paths of features over time. Typically, an ellipsoid simply continues to exist with varying position, volume, and orientation, which results in a linear path. Yet, features may also split and merge, or exit and re-enter the data, which might indicate interesting events in the data’s evolution. The detection of such events is an integral part of the feature extraction step. The result of the event detection is a layered graph in which layers are time steps and nodes represent features. Edges exist when there is a correspondencebetweentwofeaturesofconsecutivetimesteps.Thepaths through the graph describe the evolution of features, where particular connectivity patterns represent different events. For instance, a node with one incoming edge and multiple outgoing edges corresponds to a split event. In summary, the feature extraction produces two results: a set of ellipsoids that characterize features spatially for each time step and an event graph of the temporal evolution of features. FeatureVisualization The visualization of features is at the heart of feature-based visual analysis. Its goal is to communicate the spatial and the temporal aspects of features as captured in the set of ellipsoids and the event graph. The spatial aspects can be visualized by showing the 3D ellipsoids of a particulartimestepascolor-coded,three-bandcontours.Thecontourrendering helps in reducing 3D occlusion. The color-coding allows us to distinguish different types of features. In our case, different colors represent different proteins. The advantage of this encoding becomes clear in Figure 5.13. In
Automatic Analysis Support (cid:4) 223 (a)Volume visualization. (b)Ellipsoid visualization. Figure5.13 Comparison of direct volume visualization of the particle concen- tration of one protein and ellipsoid-based visualization of features representing high concentrations of two different proteins. Courtesy of Christian Eichner. Figure 5.13a, which depicts the concentration of just one protein by direct volume visualization, we can hardly locate and quantify regions with higher proteinconcentrations.Figure5.13bvisualizestheextractedfeaturesinsteadof the raw data. Red and blue ellipsoids mark regions with high concentration for twodifferentproteins.IncomparisontoFigure5.13a,thespatialcharacteristics of regions with high concentrations can be recognized more easily even for two proteins. But how do the features evolve? In order to create an overview of the evolution of features across all time steps, the event graph can be visualized as layerednode-linkdiagram.Figure5.14ashowsanexamplewithfeaturesoftwo different proteins. Time is shown from left to right. Each node represents a featureataparticulartimepoint,andtheedgesshowtheevolutionoffeatures. Symbols indicate interesting events. The size of the nodes corresponds to the extent of the ellipsoids, which makes bigger features easier to spot. Tracing paths of connected nodes allows us to see how features grow, shrink, split, or merge. By comparing sizes and numbers of nodes at a certain time point, it is also possible to estimate whether there are few bigger spots of higher concentration or several smaller ones. For a detailed comparison, one can visualize the ellipsoids of successive time steps, as shown in Figure 5.14b. It is immediately clear how positions and shapes of features change from one time step to the other. However, this type of visual representation only makes sense when the number of time steps and features is small. Otherwise, the display would be too cluttered to gain any insight.
224 (cid:4) Interactive Visual Data Analysis (a)Node-link diagram of the event graph. (b)Twofeaturesattwotimesteps. Figure5.14 Visualizing the temporal evolution of features. Courtesy of Chris- tian Eichner. Tosummarize,thissectionpresentedthebasicstepsoffeature-basedvisual analysis. We learned that specifying, extracting, and visualizing features can help us better understand data in their spatio-temporal frame of reference. In the following section, we will discuss the feature-based approach further in the context of a particularly challenging problem: the analysis of chaotic movement. 5.2.3 AnalyzingFeaturesofChaoticMovement Analyzing chaotic movement is challenging, because interesting patterns are usually covered by loads of irrelevant information. Here, we consider chaotic movementthatisgeneratedbyastochasticsimulationprocess.Suchsimulations are typically controlled by parameters whose influence on the simulation outcome is not clear upfront. More specifically, the data consist of r simulation runs. Each run R = i (P ,M ) : 1 ≤ i ≤ r is characterized by a parameter setting P (the input i i i of the simulation) and by a corresponding movement M (the output). Each i movement M consists of the trajectories T ,...,T of m moving entities. The i 1 m trajectoriesaresampleduniformlyattimestepst ,...,t .Eachtrajectorypoint 1 n stores various pieces of information, such as position, speed, acceleration, or distance to other entities. Inadditiontothespatialandtemporalcontextofthesimulatedmovements, there are also the dependencies on the simulation parameters. There can be thousands of different parameter configurations, each resulting in thousands of
Automatic Analysis Support (cid:4) 225 … R= (P, M) r r r R = (P, M) 2 2 2 R = (P, M) 1 1 1 Figure5.15 Drawing thousands of trajectories of chaotic movements for mul- tiple simulations leads to cluttered and indecipherable visual representations. Courtesy of Martin Röhlig. complex movements with chaotic trajectories. As can be seen in Figure 5.15, indiscriminately drawing all data will certainly lead to extreme visual clutter, which makes it impossible to study individual trajectories or the influence of parameter configurations. Now let’s see how the feature-based approach can help us gain insight into chaotic movement despite its size and complexity. Instead of dealing with individual movement trajectories, we will now work with features of movement. FeatureSpecification The literature on visual analysis of movement data provides various specifi- cations of features that can be interesting to consider [And+13; vLan+14; Lub+15]. Four fundamental categories of features can be distinguished: • Basic features describe aggregated values for all trajectories of the movement such as average speed or average acceleration of the moving entities. • Group features characterize groups of moving entities. Interesting group features are, for example, the number of groups per time step or the ratio of group members and entities not being in any group. • Region features take the spatial distribution of the moving entities into account. For example, it can be interesting to extract regions with high or low density of entities of certain type, and to consider the count, position, and size of these regions. • Advanced features can be derived by further analytical processing of the previous features. As such, they describe features of features.
226 (cid:4) Interactive Visual Data Analysis Figure5.16 From entities (dot marks) to density map (gray-scale image) to regions (colored image). Courtesy of Martin Röhlig. Advanced features may, for example, detect time points with significant changes of the movement behavior. A comprehensive visual analysis typically requires several features from all four categories. Which features concretely will be most helpful in a given case depends on the data and the analysis task. FeatureExtraction The different categories of features listed above require different extraction mechanisms. The extraction of basic features and group features operates on individualtrajectoriesorsetsofthem.Theextractionofregionfeaturesrequires the computation of 2D density maps for each time step of the movement. By quantizing the density maps, regions with different characteristics can be extracted. Figure 5.16 shows a collage for the purpose of illustration. Moving entities and groups are displayed as dot marks and circles, respectively. A density map is visualized as a gray-scale image. Extracted high-density and low-density regions are colored in red and green. Regions that overlap with groups are orange. Blue represents uninteresting parts of the data. Given the variety of movement features and the complexity of chaotic movement data, it is impossible to carry out the feature extraction on the fly. Instead, an extensive pre-processing step computes as many features as
Automatic Analysis Support (cid:4) 227 2D Movement 1D Time series Figure5.17 2D movement reduced to 1D time series of feature values. possible in advance. This enables users to include or exclude certain features on demand during the actual data analysis. As a result of the feature extraction, the data are reduced considerably. In fact, the complex 2D movements with thousands of entities are condensed down to 1D time series of feature values as indicated in Figure 5.17. These time series are to be represented visually for all simulation runs in the final feature visualization step. FeatureVisualization The visualization must depict not only the temporal aspects of features, but alsothespatialcontextofthemovementandthedependenciesontheparameter settings. Because not all information can be squeezed into a single image, an overview+detail design with several linked views makes sense. Overview The overview supports the exploration of movement features and parameter dependencies across all simulation runs. To this end, parameter con- figurations and feature values are shown in a matrix-like fashion as illustrated in the lower left part of Figure 5.18. Parameter values are arranged to the left and the values of a selected feature are shown in the main part of the matrix. A small gap separates parameter values and feature values. The i-th matrix row represents the parameter configuration and the feature time series of the i-th simulation run R . The matrix cells are color-coded, where darker colors i stand for lower values and brighter colors stand for higher values. Note that the visualization shows only one feature at a time, but interactively switching between different features is not a problem thanks to the feature extraction pre-process. Detail The detail views depict temporal and spatial dependencies in more detail, but only for selected parts of the data. As illustrated in the upper part of Figure 5.18, the temporal aspect is communicated by means of a chart view that shows the time series of selected simulation runs. This facilitates comparing the characteristics of the movements under different parametric conditions. The spatial aspect is detailed by showing selected trajectories for a selected time interval in a trajectory view, which is depicted to the right of Figure 5.18.
228 (cid:4) Interactive Visual Data Analysis Chart view R i R j Parameters Selected feature over time R i ... ... R j ... ... Matrix overview Trajectory view Figure5.18 Feature visualization with overview and detail views. The trajectory segments are color-coded, which allows us to see where the movement exhibits certain characteristics. Additionally, the trajectory view can be combined with the computed 2D density map of a selected time step. Until now, we have sketched the basic ingredients of feature-based visual analysis for chaotic movement data. Implemented and applied to real data, the approach creates visual representations as shown in Figure 5.19. The views are linked, and basic interaction mechanisms allow the user to select particular timeseriesordatarangestobeshowninthedetailviews.Next,wetakealook at some of the insights that could be gained with the feature-based approach. c d a b Figure 5.19 Visualization of parameter settings, feature values, and detail information for selected parts of the data. (a) Parameter settings as gray-scale matrix; (b) Feature values over time as color-coded matrix; (c) Chart with selected time series; (d) Trajectory view with selected trajectory segments. Courtesy of Martin Röhlig.
Automatic Analysis Support (cid:4) 229 Figure 5.20 Visualizing the parameter dependency of average group size. Courtesy of Martin Röhlig. ApplicationExample We already mentioned that the data we are dealing with come from stochastic simulations. The concrete application background is in the context of systems biology where researchers study chaotic movements on cell surfaces. In par- ticular, the researchers seek to gain insight into proteins that move on the cell surface, dock to so-called lipid rafts, move along with them for a short period of time, and then leave them to continue moving freely or dock to other lipid rafts. These dynamic interactions between proteins and lipid rafts play an important role in medicine, for example, in cancer-related research. In our case, the simulation of the movement is controlled by 8 parameters. Simulations have been carried out for about 2,000 different combinations of parameter values. In each of these simulations, the chaotic movement of about 1,000 lipid rafts and 5,000 proteins has been calculated stochastically over a period of about 4,000 time steps. As a result, the researchers have obtained 180 GB worth of chaotic movement data to be analyzed. To support different analysis goals for these large data, about 60 features have been computed, including the average speed of all proteins per time step, regions with high and low density of proteins, group characteristics of the lipid rafts, and time periods with relatively constant behavior [Lub+15]. It is beyond the scope of this section to discuss all the features and their visual interpretation in detail. Instead, we want to highlight two findings that could be made by applying the feature-based approach to the described data. Parameter Dependency Based on the visual representation shown in Fig- ure 5.20, the researchers could detect a dependency of the average group size on the parameters that control the size of the lipid rafts, the number of proteins, and the fluidity of the medium. To make the dependency visible, the rows of the visualization have been sorted with respect to the values of these parameters. The feature-part of the visualization then
230 (cid:4) Interactive Visual Data Analysis (a)Density map. (b)Average protein-raft distance. Figure5.21 Visualization of the average distance to free proteins reveals the sweeping effect. Courtesy of Martin Röhlig. clearly shows bands with lower group size (darker greens) and higher group size (lighter greens). Movment Behavior In the visualization shown in Figure 5.21, the researchers could find a behavior called the sweeping effect: Regions surrounding the lipid rafts have particularly low density of proteins. The density map in Figure 5.21a makes this obvious for a selected time point. To confirm that the sweeping effect is permanent throughout the simula- tion, the feature visualization in Figure 5.21b shows the average distance of free proteins to their nearest lipid rafts over time. As can be seen in the chart view and from the mostly bright colors in the overview matrix, the distance is rather large, and for the two selected runs it even steadily increases slightly toward the end of the simulation. These two example findings nicely demonstrate that feature-based visual analysis can be very helpful when studying data that are not only large, but also chaotic. The visualization of selected features allows us to concentrate on the important aspects of the data, while leaving aspects that are irrelevant to the task at hand out of consideration. At this point, we close the section on facilitating the visual analysis by focusingonrelevantdata.Wehavelearnedthatfourcomponentsarenecessary in this context. First, we need a way to formally specify what is relevant. For the degree-of-interest approach introduced at the beginning of this section, relevancy is described with respect to data values. The feature-based approach discussed in this section’s second part targets higher-level data characteristics. Second, an automatic computation step has to determine interesting data elements or extract features from the data. This can involve basic comparison operations, but also more complex tracking and event detection methods.
Automatic Analysis Support (cid:4) 231 The third component is the visualization of only the relevant parts of the data. Thanks to the reduced amount of information to be communicated, the visual representations are clearer and more focused, which makes gaining insight easier or possible at all. Fourth and finally, interaction is required to specify one’s interest in the data or to select features to be visualized. Interaction adds the flexibility that is necessary to adjust the analysis to changing requirements. In fact, it is again the interplay of computation, visualization, and interaction that makes feature-based approaches so powerful. There is one aspect that is critical though: In order to focus on relevant data, users must know the target of their tasks. Unfortunately, this is not always the case, particularly when studying unknown data. Therefore, we need further concepts to support the data analysis via automatic computations as we will discuss in the next section. 5.3 ABSTRACTING DATA We already know that for today’s datasets, the number of data values easily exceeds the number of pixels on the display. This might lead to severe over- plotting where very many data values are mapped onto one and the same pixel. Figure 5.22, for example, shows a time series with more than 1.7 million time points. Given the limited width of the chart, each pixel column must accommodate about 1,000 time steps. From a visual data analysis perspective, the question is how to appropriately determine the values that should be visualized in the available display space. Figure5.22 Visualization of a time series with more than 1.7 million time points, where each black pixel represents about 1,000 data points. Courtesy of Martin Luboschik. 5.3.1 SamplingandAggregation Samplingandaggregationaretwoapproachesthataimatsolvingthisproblem. Thebasicprincipleofsamplingistoselectparticulardatavaluestobedisplayed. Thatis,samplingmethodspickfromtheoriginaldataasubsetofselecteddata values. Aggregation methods, on the other hand, combine and reduce several data values to representative values to be displayed. That is, aggregation methods condense the original data down to a reduced set of aggregated
232 (cid:4) Interactive Visual Data Analysis values. Note that with sampling, original data values are visualized, whereas with aggregation, derived values representing a couple of original values are visualized. Sampling In signal processing, sampling aims at representing a continuous signal by a discrete one. To this end, signals are gathered at certain sample points. The Nyquist Shannon sampling theorem tells us that in order to be able to reconstruct the original signal, the number of sample points must be greater than twice the bandwidth of the continuous signal [II91]. In statistics, sampling is understood differently. Here, sampling aims at defining a subset of individuals that are representative of or characterize an entire statistical population [Loh19]. In visualization, both definitions are applied. Sampling refers to gathering data values of variables at certain sample points as well as to gathering data elements. In general, sampling aims at determining a particular data subset that enables the user to see major characteristics of the original dataset. To this end, different sampling strategies can be applied ranging from simple equidistant sampling of data values to sophisticated methods such as graph sampling [LF06]. The used sampling method determines the quality of the output. With simple equidistant sampling, essential data characteristics might be lost. On the other hand, sophisticated sampling methods can preserve essential data characteristics, but they are computationally expensive and time-consuming. Therefore, a balance must be found between the quality of the produced result and the needed computation time. Aggregation While sampling methods define a subset of data values, aggre- gation methods replace the data values within an interval by an aggregated value describing certain statistical characteristics of that interval: • Minimum: The smallest value in the interval. • Maximum: The largest value in the interval. • Count: The number of values in the interval. • Sum: The sum of all values in the interval. • Average: The arithmetic mean (the sum divided by the count). • Median: The geometric mean (the central value of the sorted interval). • Mode: The most frequent value in the interval. • Count unique: The number of distinct values in the interval. • Standard deviation: The amount of variation in the interval.
Automatic Analysis Support (cid:4) 233 Which aggregation function to use depends on the analysis objective. The demonstrating example in the next section replaces the original data values by maximum and average values. For very large datasets, a simple data abstraction might not be sufficient. In Figure 5.22, the time series was represented by only one sampling value per 1,000timepoints.Suchasmallsubsetofsamplescanhardlyrepresentthewhole datafaithfully.Therefore,itmakessensetoproducemultipledataabstractions at different scales. Next, we will discuss such a multi-scale approach in more depth. 5.3.2 ExploringMulti-scaleDataAbstractions Sampling and aggregation can be done repeatedly to produce multi-scale data abstractions. With each repetition, a new coarser scale is created with a decreased number of data points. The creation of multiple scales comes alongwithtwoimportantbenefits.First,multi-scaledataabstractionsallowus to generate scalable and less cluttered visual representations [EF10]. Second, multi-scale data abstractions facilitate the analysis of different characteristics of the data. In fact, coarser data abstractions communicate global trends, whereas less-abstracted scales support the detection of local trends and details. Hence, switching between the scales enables users to gain different insights. With a larger number of scales, however, it becomes increasingly difficult to inspect all data at all scales. This raises the question of where to drill down from a coarser scale to a finer scale to get to additional information? Because this question is often not easy to answer, it makes sense to guide users to those scales that might yield new findings [Lub+12]. The idea is to exploit data differences between successive scales as an indicator for the availability of additional information at the finer scale. In other words, if two successive scales are quite similar, studying the finer scale might not reveal much new. In contrast, if the two scales exhibit larger differences, it makes sense to drill down and search for local patterns that only take shape at the finer scale. As an example, Figure 5.23 shows two line charts of a time series at two successivescales.Thefinerscaleatthebottomconsistsofelevensamplepoints, whereas the coarser scale at the top has only six points. In the left part, both chartsdifferonlymarginally:Bothscalesshowmonotonicallyincreasingvalues. Thus,wecanconcludethatforthefirstpartofthedata,thefinerscaledoesnot add substantially new behavior and is therefore less worth exploring. However, for the right part of the charts, things are different. The finer scale depicts a considerablelocalminimumthatisnotpresentatthecoarserscale.Identifying and emphasizing such deviating data behavior can in fact guide the user in exploring multi-scale data.
234 (cid:4) Interactive Visual Data Analysis S i S i+1 Figure5.23 One and the same time series at two different scales. GuidingtheUser The simple example of Figure 5.23 illustrates that differences between scales mightbeagoodindicatorwherefurtherinsightscanbefound.Next,wediscuss how such differences can be extracted and visualized along with the abstracted data. Extracting Differences Between Scales The extraction of differences is done for all pairs of successive data scales. For each pair, the following three steps are carried out: map sample points, compute difference measures, and aggregate differences. 1. Map sample points. First,thesamplepointsatthetwosuccessivescales must be unified. To this end, sample points that exist only at one scale are mapped to the respective other scale. This typically involves the computation of new data values, for example, by linear interpolation. The example in Figure 5.24 illustrates how every second sample point of the finer scale is mapped to a new sample point with an interpolated data value at the coarser scale. In the end, both scales have the same sample points, but different data values. 2. Compute difference measures. Computing the differences between the two scales is the second step. Various measures can be used to quantify differences. Figure 5.25 illustrates two examples, a point-wise measure and a segment-wise one. A simple point-wise measure is to compute the absolute value difference (AVD) per sample point, for example, by means of the Euclidean distance. A simple segment-wise measure is the slope sign difference (SSD). It captures whether the signs of the slopes of two segments differ. In Figure 5.25, the signs are indicated per segment as − and +. If the signs differ, the slope sign difference is 1, otherwise it is 0. While being computationally inexpensive, these simple measures capture just enough information to be useful as indicators.
Automatic Analysis Support (cid:4) 235 Original sample point Mapped sample point S i S i+1 Figure5.24 Unifying the sample points of two successive scales by mapping and interpolation. + + ++ – – + + S Si + + i+1 + + + – + + + – + + Absolute Value Difference (AVD) 0 3 0 1 0 1 0 18 0 6 0 Slope Sign Difference (SSD) 0 0 0 0 0 0 0 1 1 0 Figure5.25 Computing the absolute value difference (AVD) and the slope sign difference (SSD) between two successive data scales. 3. Aggregate differences. Because our goal is to guide users to regions worthstudyingindetail,weneedtoaggregatethepoint-wiseorsegment- wisedifferencevaluesoverlargerintervals.Figure5.26showsamaximum aggregation of AVD and an average aggregation of SSD as examples. A maximumaggregationensuresthatlargerdifferencesarenotcompensated by smaller ones within the same interval. If only the existence or absence of differences between scales is captured, the average can indicate how much additional information will appear when zooming into a certain range. In our example, both aggregations suggest drilling down primarily into the interval where the local minimum would be revealed. The outlined basic approach is freely adaptable. Different methods can be appliedformappingthesamplepointsandderivingthemissingdatavalues,for measuring the differences between scales, and for aggregating the differences. What remains to be done is to visualize the aggregated differences along with the actual data.
236 (cid:4) Interactive Visual Data Analysis Absolute Value 0 3 0 1 0 1 0 18 0 6 0 Difference (AVD) Maximum 3 1 18 6 Aggregation Slope Sign 0 0 0 0 0 0 0 1 1 0 Difference (SSD) Average 0 0 0.66 0 Aggregation Figure5.26 Aggregation of data differences with maximum aggregation for the absolute value difference (AVD) and average aggregation for the slope sign difference (SSD) function. VisualizingAggregatedScaleDifferences Toguideusersinexploringmulti- scale data, the aggregated differences are visualized along with the data. A schematic illustration of the approach is given in Figure 5.27. The main time series plot is accompanied by a stack of so-called difference-bands. Each band is further sub-divided into color-coded cells representing the aggregated differences between the adjacent data scales. There are two modes for the color-coding: global and local. For global color-coding, colors are mapped between the global minimum and maximum value of all cells. This facilitates comparisonacrossscales.Forlocalcolor-coding,colorsaremappedwithrespect to the minimum and maximum value per band. This makes it easier to see smaller differences per individual scale. In both modes, brighter cells indicate regions with no or only small differences between scales. Darker cells represent regions where the finer scale is substantially different from the coarser scale, and hence, might be interesting to inspect in more detail. Additionally, an over-plotting indicator shows, depending on the current display resolution, at which scale over-plotting will occur. It is reasonable to start the data analysis with the finest scale that is still free from over-plotting. Zoomable Time Series Plot Multi-scale Difference Over-plotting Bands Indicator Figure5.27 Visualizing aggregated differences along with the actual data.
Automatic Analysis Support (cid:4) 237 From there, the user can zoom and pan the time series plot to reach different scales and different regions as suggested by the color-coded cells. Next, we will illustrate how the described approach can support the visual analysis of a large time series from systems biology. AUseCasefromSystemsBiology Our example is based on time-series data that result from a simulation of the cell division cycle in fission yeast [Lub+12]. For our purpose, it is sufficient to know that the simulated model consists of two different proteins that control the major events of the cell division. The simulation produces 3.6 million data points that describe the dynamic change of molecule quantities over time. By repeated average aggregation, the original data are transformed into a multi-scale data abstraction with 21 levels. Differences between the scales are computed by the measures introduced before: slope sign difference (SSD) with average aggregation and absolute value difference (AVD) with maximum aggregation. Figure5.28 Time series plot of the simulation outcome and corresponding multi-scale difference bands of SSD and AVD with local color mode. Courtesy of Martin Luboschik. The outcome of the simulation is visualized in Figure 5.28. The time series shows considerable oscillations. To get more insights, we check the multi-scale difference bands of SSD (top in blue-yellow) and AVD (bottom in green- yellow). Particularly, at the finer scales that would produce over-plotting, as signaled in red color in the over-plotting indicator, we can observe something interesting. For the SSD measure, we can see for each peak in the time series a corresponding brighter notch in the otherwise rather bluish difference bands. Within each notch, there seems to be a very thin blue upward spike, which wouldsuggestthatsomeadditionalinformationcanbefound.Thisisconfirmed
238 (cid:4) Interactive Visual Data Analysis Zooming on tip of middle peak Fish-eye magnification Figure5.29 Studying the details of the middle peak-notch-spike pattern from Figure 5.28. Courtesy of Martin Luboschik. by the AVD measure, where green downward spikes are clearly visible. This prompts us to look into these parts of the data at a finer scale. We decide to study the middle peak-notch-spike pattern in more detail. In Figure 5.29, we perform two separate drill-down operations. First, we enlarge the multi-scale difference bands where the middle notch is located by applying a horizontal fish-eye magnification (a 1D variant of the fish-eye lens introduced in Section 4.6.3 of Chapter 4). We can see, yes, there are indeed some significant differences in this region as indicated by the darker, more saturated colors. Second, we switch the time series plot to the finest data scale and zoom far in on the tip of the peak. This reveals the source of the differences: There are considerable fluctuations at the tip. These are caused by the two proteins competing with each other in the simulated process. We can conclude that there is no clear tipping point as the coarser scales might have suggested. This is an interesting finding that would likely have gone unnoticed without the guidance offered by the multi-scale difference bands. In summary, data abstraction by means of sampling and aggregation is an important approach to dealing with very large data. However, representing millions of data points by a comparatively small number of samples or aggre- gated values does not adequately communicate the complete data behavior. Therefore, multi-scale data abstractions are required. They facilitate the anal- ysis of data behavior at different scales and help us generate scalable visual representations. Computing and visualizing differences between subsequent scales helps users identify those scales and ranges of the data that should be investigated in detail.
Automatic Analysis Support (cid:4) 239 Whilesamplingandaggregationarenotveryselectivewithrespecttowhich data elements are abstracted, we will next look into methods that particularly focus on data elements that are similar. 5.4 GROUPING SIMILAR DATA ELEMENTS With sampling and aggregation, data values within a certain data interval are replaced by a single representative value. Now, we consider the grouping of similar data elements. Classification and clustering are typical approaches for grouping data elements. While the goal of grouping is the same for both approaches, they achieve the goal in different ways. Classification subdivides the data space, whereas clustering partitions the set of data elements. A class describes a subspace of the data space, whereas a cluster represents a subset of similar data elements. Classes are characterized by value ranges, whereas clusters are characterized by the properties of their affiliated data elements. Representing a dataset by classes or clusters significantly reduces the data complexity and facilitates the generation of overviews. In the following, we will discuss classification and data clustering in more detail. 5.4.1 Classification Classification subdivides the data space. Each subspace defines a particular class. Data elements falling into the same subspace share the same data characteristics and are said to be in the same class. The key to classification is a suitable description of the classes. BasicMeansofSpecifyingClasses There are different strategies to define the classes. A very basic approach is to divide the domain of the data variables into intervals. For example, the age of persons in a social network could be grouped into four classes: kids (age≤12), youths (12 < age ≤ 18), adults (18 < age ≤ 60), and seniors (60 < age). This reduces the range of age values, usually between 0 and 110, to only four class values. Another example is to group the wind direction into eight major directions:N,NE,E,SE,S,SW,W,andNW.Here,thepotentiallycontinuous angle value between 0° and 360° is reduced to eight class values. A common approach to classifying multivariate data elements is to use decision trees. Decision trees partition the data space based on a series of tests. For quantitative or ordinal data variables, tests are usually comparisons against given thresholds. Categorical data can be partitioned by means of yes/no questions. The specified tests are the basic building blocks of decision trees. A test defines a node in a decision tree. For each class distinguished by a test, there is an edge attached to the test’s node. The edges connect a test to follow-up nodes in the decision tree. One test is chosen to be the root node of the decision tree. The leaves of the decision tree are the produced classes. A
240 (cid:4) Interactive Visual Data Analysis Tests Small Sales High Volume Classes Average Decreasing Sales Increasing Decreasing Sales Increasing Decreasing Sales Increasing Evolution Evolution Evolution Stable Stable Stable Small still Small and Small but Average Average Average High but High and High still decreasing stable increasing decreasing stable increasing decreasing stable increasing Figure5.30 A decision tree classifying enterprises according to their sales. data element is assigned to a class, if the data element matches all the tests along the path from the root node of the decision tree to the class leaf node. Visualizing decision trees can help users understand how the test criteria connect to a hierarchy of decisions that result in the partitioning of the data space. For this purpose, common tree visualization methods can be applied, for example, node-link diagrams as introduced in Section 3.5 of Chapter 3. A simple decision tree for a dataset with enterprise sales volumes is depicted in Figure 5.30. The first test in the root node refers to sales volumes. It distinguishesenterpriseswithsmall,average,andhighsales.Thenexthierarchy level considers the evolution of sales to further distinguish enterprises with increasing, decreasing, or stable sales. In the end, we obtain a grouping of the enterprises into nine classes: small still decreasing, small and stable, small but increasing, and so forth. Each class represents enterprises that share the same characteristics. Note that in other scenarios, decision trees are not necessarily leveled and balanced as in our example. Moreover, the structure of how tests lead to a decision for one class or the other can be significantly more complex. In fact, the specification of adequate test criteria for complex domain problems can be quite challenging. There are even scenarios where it is not possible at all to formulate the classification as a decision tree. In such cases, sophisticated algorithms and careful parameter tuning are required in order to arrive at appropriate classification results. Next, we will discuss such a sophisticated classification approach and illustrate how visual analysis can help to tune and evaluate the classification outcome. VisualClassificationSupport:AnExamplefromActivityRecognition Therecognitionofactivitiesisarelevanttaskinvariousdomains.Forexample, assistance systems observe people’s activities in order to automatically offer on-demand help [Tei+17]. To this end, people are instrumented with several sensorsthatproducelargevolumesoftime-seriesdata.Thetaskofclassification in this context is to deduce from the multivariate sensor data the activity that
Automatic Analysis Support (cid:4) 241 Ground Truth Data Activities A B C D Sensor Data Parameters Labeled Segments Activity Recognition ... ... Algorithm Figure5.31 Illustration of activity recognition based on parameter-dependent algorithms that learn from some ground truth. Adapted from [Röh+15]. a person is or was doing. A very simple example would be to label certain intervals in time as walking activities. Yet, an activity can also be to cook a meal, which is certainly not so easy to discriminate. In general, recognizing activities requires sophisticated data classifiers. A typical way how such classifiers work is to learn from human-labeled ground truth data. The ground truth is collected in observational studies where a person’s activities are video recorded in addition to being monitored by the sensors. Based on the video, a human can easily label each characteristic sequencesoftheobservationasacertainactivity,forexample,sitting,standing, orwalking.Theassumptionisthatthesensordataassociatedwiththemanually labeled activities are discriminatory enough so that a classifier can learn from the sensor data how to recognize activities robustly. Yet, as mentioned before, the algorithms involved in activity recognition typically depend on careful parameter tuning. To obtain accurate results, the algorithms need to be tested with different parameter configurations. The goal is to find configurations that produce classifications that match the ground truth best. Figure 5.31 illustrates the described situation. Next, we explain in more detail how visual analysis methods can help users understand the influence of parameters on the classification results. The better understanding makes it easier to adjust and fine-tune the parameters to achieve a better classification quality. Following the previous schematic illustration, Figure 5.32 shows a concrete visualizationoftheclassifiedsensordatainconcertwiththeunderlyingparam- eter settings and the ground truth [Röh+15]. The visual design is similar to the matrix-based visualization for feature visualization as described back in Section 5.2.3. The left part of the visualization (a) shows the parameter con- figurations. Each column corresponds to a parameter, and each row represents the values of a parameter configuration. The main part of the visualization (b) shows the activity-labeled time series as colored pixel rows from left to
242 (cid:4) Interactive Visual Data Analysis d a b c f e Figure5.32 Visualization of parameter-dependent classification outcomes for activityrecognition.(a)Parameterconfigurations;(b)Recognizedactivities;(c) Colorlegend;(d)Groundtruth;(e)and(f)Stackedhistogramswithaggregated information. Courtesy of Martin Röhlig. right. For each parameter configuration, there is a corresponding pixel row in the main part, and each activity is painted with a distinct color (c). This visualization provides a nice overview of the classification outcome for all different parameter configurations. Moreover, we can easily see which activity has been performed at which time step. Figure 5.32 also shows the ground truth as a separate colored band at the top (d). This facilitates the visual comparison of the recognized activities and the actually performed activities. Additionally, stacked histograms (e) and (f) are attached at the bottom and to the right. They show the distribution of detected activities per time step and per parameter configuration, respectively. This gives us a sense of the stability of the activity recognition, both over time (e) and over the different parameter configurations (f). For evaluating the activity recognition in more detail, it makes sense to visually analyze the discrepancy between the detected activities and the groundtruth.Tothisend,thecolorcodingandtheorderingofrowsischanged in Figure 5.33. Instead of showing distinct colors per data class, the visual representation now shows red color where the recognized activity does not correspond to the ground truth. We can now see that the number of incorrect classifications increases over time for the majority of parameter setting and that the overall quality of the activity recognition depends mostly on the parameter sensor. Whilevisualanalysiscannotimprovetheclassificationoutcome,itnonethe- less helps users to understand why the activity recognition behaves the way it does. The insights generated with the visualization can be utilized to estimate the influence of parameters and to fine-tune parameter configurations accord- ingly. Parameters with little impact can be paid less attention. For parameters
Automatic Analysis Support (cid:4) 243 Figure5.33 Highlighting the incorrectly classified time steps in red. Courtesy of Martin Röhlig. with substantial impact, the visualization can indicate parameter ranges that are good candidates for further refinement in search of better classification results. Insummary,thissectiontaughtusthatclassificationisaboutgroupingdata by dividing the data space into subspaces with specific data characteristics. Classificationcanbeaseasyastestingspecifiedcriteria,possiblyorganizedina hierarchicalschema.Yet,classificationcanalsobesocomplexthatsophisticated algorithms need to be involved. In both cases, visualization, either of the classification schema as for the decision trees or of the parameter-dependent classification outcome as for the activity recognition example, can help users understand how the data are grouped into classes. Next, we study clustering as another common approach to group similar data elements. 5.4.2 DataClustering The goal of clustering is to group similar data elements into clusters. Data elements in the same cluster are similar with respect to a specified similarity measure.Theelementsofdifferentclustersshouldbedissimilar.Clusteringcan support the visual analysis in different ways. Data elements can be arranged or colored according to their cluster affiliation. Overview representations can concentrateontheclusters,ratherthanonindividualdataelements.Clustering can also help us study the data at different levels of abstraction. All this can reduce the analysis complexity significantly. ConfiguringtheClustering Dataclusteringcanbedoneinvariousways.Therefore,itisfirstofallnecessary toconfiguretheclustering,thatis,tospecifywhattoclusterandhowtocluster.
244 (cid:4) Interactive Visual Data Analysis Letusnextbrieflydiscussthedecisionsthatneedtobemadebeforetheactual clustering can be performed. What to cluster? First, we must decide which data elements should be takenintoaccount.Redundantdataelementsanddataelementsthatrepresent outliers can distort the grouping, and thus they are usually excluded from clustering. Second,inthecaseofmultivariatedata,wemustdecidewhichdatavariables should be taken into account. Should we consider all variables or only selected ones?Wemustalsodecidewhetherthevariablesshouldbeweightedequallyor whether some of them should be given higher or lower weight. Variables that do not contribute to the main topic of interest could be ignored or assigned a lower weight. Highly relevant variables should get higher weights to give them more influence on the clustering outcome. Forexample,consideradatasetthatcontainsinformationaboutcompanies, their name and field of business, chief executive officer, sales, revenue, balance sheet total, and number of employees. If the analysis objective is to cluster highlyprofitableandbigcompanies,theattributerevenueshouldbeprioritized, the attributes sales, balance sheet total, and number of employees should be included, and the attributes name, field of business, and chief executive officer can be neglected. How to cluster? Third, we have to decide on the similarity measure to be used to determine whether data elements are similar. A widely applied measure for quantitative data is the Euclidean distance, which describes the straight-line distance between two data elements. The Manhattan distance defines the distance between two data elements as the sum of the absolute difference of their data values. Further measures for quantitative data are the cosine distance or correlation coefficients such as the Pearson correlation. For qualitative data, other distance measures must be applied, for example the Hamming distance, the Levenshtein distance, or the Jaccard index. Continuing our previous example, for clustering companies according to profits and size, the Euclidean distance would be an appropriate measure. Yet, things are different when we change our analysis objective. Let’s now assume that the business data are given on a yearly basis. If our analysis objective is to study companies with similar development over time, a correlation-based measurewouldbeappropriate.Thisway,theclusteringwouldgroupcompanies that have similar development histories. Finally, we must decide on the clustering strategy to apply. The clustering strategy defines how the data elements being similar according to the chosen similarity measure are grouped into clusters. There are four basic clustering strategies:partitioningclustering,hierarchicalclustering,grid-basedclustering, anddensity-basedclustering[HKP11].Eachclusteringstrategytakesadifferent approachandcomeswithitsownsetofparameters,aswewillseeinamoment.
Automatic Analysis Support (cid:4) 245 Taken together, excluding data elements from clustering, weighting vari- ables, using different similarity measures, applying different clustering strate- gies, and choosing different parameters, all these decisions must be carefully weighed. It is important to realize that slightly different decisions can produce totally different clustering results. Which configuration to employ depends on the data and the objective of the analysis. ComputingtheClusters In the following, we will explain how clusters of similar data elements can actually be computed. To this end, we will briefly describe the four different clustering strategies just mentioned. Partitioning clustering algorithms generate k < n groups from n data elements by partitioning the data space. The most popular method is the k-means algorithm. It describes each cluster by its centroid. Initially, the k centroids are distributed randomly in the data space. Then, the data elements are assigned to their closest centroid. The positions of the centroids are then updated to the average position of the data elements within a cluster. After updating the centroid positions, it can happen that for some data elements, their assigned cluster centroid is no longer the closest one. This requires a reassignment of the data elements to the cluster that is now the closest. These changes again imply re-calculations of the centroid positions. This iterative clustering process stops when the centroid positions have settled at sufficiently stable positions. The k-means algorithm is popular for its simplicity and general appli- cability. However, a difficulty is that the number of clusters must be specified in advance, which can be problematic for unknown data. Hierarchical clustering algorithms organize nested clusters in a hierarchy. The similarity of data elements within the clusters increases as the hierarchy is descended. The root node of the hierarchy represents the cluster that contains all data elements. The leaves of the hierarchy represent clusters that contain only a single data element. A horizontal cut through the hierarchy describes a particular level of abstraction of the clustered data. Defining an appropriate cut is the difficultyofhierarchicalclustering.Here,visualizingtheclusterhierarchy as a so-called dendrogram can help users find a good level of abstraction. Cutting the hierarchy near the root produces an overview of the data. Cutting closer to the leaves delivers more details. Hierarchical clustering can be implemented in two ways. Agglomerative algorithms recursively group the two most similar data elements in a bottom-up fashion. A popular example is Ward’s method. In contrast, divisive algorithms recursively split the data into two dissimilar subsets in a top-down fashion until each data element forms its own cluster.
246 (cid:4) Interactive Visual Data Analysis Grid-based clustering algorithms subdivide the data domain by means of a regular grid. Each data element falls into a particular cell of the grid, and for each grid cell statistical meta-data, such as minimum, maximum, or average, are computed. Based on a data query, clusters are formed by connecting those grid cells that contain a sufficient number of data elements that match the query. Obviously, the grid resolution is a critical parameter in terms of the clustering quality. STING (statistical information grid)isan example thatusesahierarchical structure ofgrid cells. Such multi-resolution grids make it possible to formulate cluster queries against very large databases. Density-based clustering algorithms distinguish between regions with higher and lower density, and define clusters based on this distinction. DBSCAN (density-based spatial clustering of applications with noise) is a prominent example of density-based clustering. DBSCAN groups data elements based on two criteria. The data elements must be close to each other (distance) and there must be sufficiently many data elements around the same spot (density). Data elements that lie in sparse regions are considered noise or outliers. Being robust against outliers is a partic- ular advantage of this method. On the other hand, DBSCAN requires careful tuning of the distance and density criteria in order to be able to detect clusters. It works less well with evenly distributed data. Figure 5.34 illustrates the four clustering strategies applied to an artificial bivariate dataset. The data (a) contain two well-separated subsets and also two major outliers and two minor outliers. It is interesting to see how the different clustering algorithms cope with these specific data characteristics. For the k-means clustering, setting k =2 seems to be appropriate, since we can see that there are two distinguishable subsets. Similarly, cutting the dendrogram(f)neartheroottoobtaintwoclustersforWard’smethodseemsto makesense.However,thetwoclustersextractedin(b)and(c)donotcorrespond tothetwoobvioussubsetsinthedata.Beingapartitioningapproach,k-means generates convex clusters. Moreover, the major outliers distort the clustering of k-means and Ward’s method. Incontrast,STING(d)andDBSCAN(e)producetwoclustersrepresenting the two obvious subsets quite well. Yet, STING also generates clusters for each of the four outliers. DBSCAN treats the outliers differently. The major outliersarerecognizedassuch.Theminoroutliers,however,areassignedtothe two clusters. In order to keep them as outliers too, the distance parameter of DBSCAN could be decreased. However, only carefully so, because the clusters could fall apart if the distance parameter is too low. In light of these examples, it is clear that there is no silver bullet strategy to clustering. Instead, it is necessary to configure the clustering appropriately to obtain suitable results. However, it is often unclear how the clustering result should look like, especially when analyzing unknown data. Similarly, how different configurations affect the result is often unclear as well. Again,
Automatic Analysis Support (cid:4) 247 (a)Input data. (b)K-means. (c)Ward’s method. (d)STING. (e)DBSCAN. (f)Dendrogram. Figure5.34 Illustration of clustering strategies. Adapted from [Gla14] with permission of Sylvia Saalfeld. interactive visual methods can help us perform the clustering appropriately, check the computed clusters, and, if necessary, to specify a new configuration that delivers more plausible results. StratomeX is a tool that supports the direct visual comparison of cluster- ing results generated with different algorithms and configurations [Lex+12]. Figure 5.35 shows the interface of StratomeX. The columns represent the same microRNA data, but differently clustered by three algorithms. Bands between the columns connect the same data elements. We can nicely see that data elements being together in a group for one clustering are distributed across several groups for another clustering. For example, the data elements of the highlighted first group of the k-means clustering (middle column) disperse into three different groups for the hierarchical clustering (left column). On the other hand, a large portion of the k-means group also forms a large portion of an affinity propagation cluster. Based on these observations, the user can inspect the data elements in the marked clusters in more detail in order to judge whether the produced clusters are in fact adequate groupings. Finally,itisworthmentioningthatinadditiontotheillustratedfundamen- tal clustering strategies, there are further ways of clustering data. For example,
248 (cid:4) Interactive Visual Data Analysis (a)Hierarchical clustering. (b)K-means clustering. (c)Affinitypropagation. Figure 5.35 Comparison of clusters generated with hierarchical clustering, k-means, and affinity propagation. Software courtesy of Alexander Lex. distribution-based clustering groups data elements according to statistical distribution models. Spectral clustering works on the spectrum of eigenvalues of the similarity matrix of the data. Clustering can also be done with neural models, such as self-organizing maps, as we will see next. HybridSOM-basedClustering Self-organizing maps (SOM) are a form of artificial neural networks. SOMs can be applied to all kinds of data analysis problems. Here, we utilize them for grouping similar data elements. SOMs are suited for finding groups even in large and unstructured data. Basic SOM Clustering A SOM basically corresponds to a regular grid of neurons, each being represented as a so-called reference vector. The number of components per reference vector, that is, the dimensionality, corresponds to the number of data values per data element. SOMs operate in two steps: training and mapping. In the training phase, the grid of neurons is trained based on so-called input vectors, which in our case is a subset of the data elements to be clustered. In the mapping phase, the remaining data elements are processed.
Automatic Analysis Support (cid:4) 249 In the training phase, the following four steps are performed to align the reference vectors with the input vectors (the training data): 1. Initialization Thereferencevectorsofallneuronsinthegridareinitialized with random values. 2. Competition For each input vector, all neurons in the grid compete for the assignment. The neuron with the most similar reference vector is the winner neuron. The corresponding reference vector is updated such that it more closely represents the input vector. 3. Cooperation The winner neuron stimulates neurons in its local neighbor- hood. The stimulus impact decreases with the training duration. 4. Adaptation The reference vectors associated with the stimulated neurons are adjusted with regard to the input vector. The degree of adaptation also decreases with the training duration. After the training phase, the grid’s reference vectors reflect a similarity- based arrangement of the input vectors. Adjacent grid cells with similar reference vectors represent groups of similar data elements. Then, in the mapping phase, the remaining data elements, that is, those data elements that were not used in the training phase, can be assigned to the closest matching neuron. With the outlined procedure, SOMs can produce useful groupings of data elements. Yet, it is not easy to determine the appropriate number of neurons (or reference vectors) required to generate results that are balanced in terms of clustering quality and computation time. The proper number of neurons is not known in advance, as it depends heavily on the given data. With too few neurons, the clustering quality will be low. With too many neurons, the quality might no longer improve, but the computation time will be higher, which might impair the interactive visual data analysis. Hybrid Clustering with SOM To keep computational costs and cluster quality balanced, it makes sense to implement a hybrid clustering procedure that combines SOM clustering with a second clustering algorithm [JTS08]. Such a hybrid clustering involves two steps. First, a coarse SOM training generates a moderate number of SOM clusters. Then, a second clustering algorithm, such as hierarchical clustering, further refines each SOM cluster. Figure 5.36 illustrates how such a hybrid clustering can be employed to sort the rows of a table lens visualization, a tabular data representation that we introduced in Section 3.2.1 of Chapter 3. The goal is to order the rows of the table such that similar data tuples are arranged closely together. To this end, a SOM delivers a rough grouping of rows in the first step. In the secondstep,hierarchicalclusteringcreatesaclusterhierarchyperSOMcluster. Figure 5.36b shows the ordered table lens after the hybrid clustering has been
250 (cid:4) Interactive Visual Data Analysis applied. While the original arrangement of rows in Figure 5.36a exhibits many color switches, the hybrid clustering leads to fewer color switches across rows. Additionally, the right-most column in the table in Figure 5.36b shows the cluster hierarchies as icicle plots. This visual representation can be utilized to adjust the visualized level of abstraction by expanding and collapse clusters. (a)Unordered rows before clustering. (b)Ordered rows after clustering. Figure5.36 Applying hybrid SOM-based clustering to sort rows in a table lens visualization. Thanks to the hybrid nature of the procedure, the clustering is computa- tionally efficient. Since we need only a small number of neurons for the initial rough SOM grouping, computation time is reduced. The same applies to the hierarchicalclusteringpart.Becausewedonotcomputeaglobalhierarchy,but refine each SOM cluster separately, the individual runtime is reduced. Over- all, combining clustering methods to a hybrid approach can help us balance clustering quality and computational costs. The clustering strategies described so far are generally applicable to multi- variate data. Yet, when we go beyond multivariate data, it is often necessary to employ advanced clustering strategies that are dedicated to the peculiarities of a specific class of data. The next section will introduce such an advanced strategy for the particular case of clustering multivariate dynamic graphs. 5.4.3 ClusteringMultivariateDynamicGraphs Clustering graphs is a complex problem [Sch07]. The goal is to group the nodes of a graph into clusters based on some notion of similarity. The clusters can then be represented by so-called meta-nodes, which basically subsume all nodesbelongingtoacluster.Visualizingthemeta-nodesfacilitatesanoverview on key characteristics of a graph. By applying the clustering repeatedly on nodes and meta-nodes, a clustering hierarchy can be generated to facilitate multi-scale data analyses, which is particularly useful for large and complex graph structures.
Automatic Analysis Support (cid:4) 251 Inthefollowing,wewillstudywaystoclustermultivariatedynamicgraphs, that is, graphs whose nodes have data attributes and also references to time. A multivariate dynamic graph DG can be denoted as a sequence of graphs DG = (G ,G ,...,G ), where G = (V ,E ) is a graph with multivariate 1 2 n i i i nodes V and edges E at time t ∈T. i i i There are two basic clustering strategies taking into account the time- varying character of multivariate dynamic graphs [Had14]. Attribute-driven methods group nodes according to the temporal evolution of their attribute values. In contrast, structure-driven methods cluster a graph based on its structure as defined by the time-depended existence of nodes and edges. Attribute-basedClustering Attribute-based clustering operates on the super-graph SG = S G , which i subsumes all graphs G ∈DG. The goal is to extract from SG clusters whose i attributes share similar temporal behavior, for example, stable, increasing, decreasing, or recurring values. This can be achieved in three steps: preprocess, configure, and cluster. 1. Preprocess First, the attribute values associated with the graph elements mustbetransformedintotimeseriesthatdescribethetemporalbehavior. To this end, the attribute values are concatenated according to the sequence as defined by DG. However, the resulting time series might contain missing values, because nodes and edges do not necessarily exist at all points in time. These missing values are unambiguously marked with a dedicated null value ω. Additionally, minor fluctuations in the time series should be filtered out in order to reduce the influence of noise on the clustering results. In the end, we obtain for each graph element and for each attribute a complete and smoothed time series with exactly one value (actual data or ω) per time point. 2. Configure The second step is to configure the clustering, which in the first place means deciding which attributes should be taken into account. Moreover, the role of the ω values must decided. Including or excluding ω leads to different clustering outcomes. If the ω values are included, theydominatetheclusteringofthosenodesthatexistonlyforafewtime points. Such rare nodes will then appear in a distinct cluster, which can be intended. On the other hand, excluding the ω values leads to clusters that contain nodes with similar temporal behavior, which is what we typically want. The configuration also involves a decision on the similarity measure and the clustering method to be used. Typically hierarchical clustering is applied. However, other methods can be used as well, in particular methods that are tailored to clustering time series [Lia05].
252 (cid:4) Interactive Visual Data Analysis (a)Initial grouping. (b)Refined clusters. Figure5.37 Two-step procedure of clustering nodes based on their attributes. First, nodes with similar attribute behavior are grouped. Second, groups are refined based on connected components. Adapted from [Had14]. 3. Cluster Finally, the super-graph SG is actually clustered using a two-step procedure. It is illustrated in Figure 5.37 with a node-link visualization whose 13 nodes incorporate miniature charts of their attribute values over time. The first step is to group nodes with similar time series as in Figure 5.37a. Our example shows three preliminary groups (in red, green, and blue) and the resulting abstracted graph at the bottom with three meta-nodes. However, the green and blue groups subsume nodes that are actually not directly connected in the super-graph SG. This is not yet the desired result. Therefore, the structure-preserving second step refines the groups with respect to their intra-group connected components. Figure 5.37b visual- izes the resulting clusters by varying edge colors. The red group has only one connected component. Yet, the green group and the blue group each consist of two connected components, indicated by brighter and darker shades of green and blue, respectively. As the final result, we obtain an abstracted graph that better captures the original connectivity and is still considerably reduced to five meta-nodes. The extracted attribute-based clusters can now be used to visualize an original multivariate dynamic graph in an abstracted fashion. To this end, the visualization displays the meta-nodes and the corresponding meta-edges and only traces of the original data. Figure 5.38 shows such an abstracted representation. The meta-nodes are represented as small boxes depicting the timeseriesthatisrepresentativeofthecorrespondingcluster.Alabelshowsthe
Automatic Analysis Support (cid:4) 253 Figure5.38 Visualizationofanattribute-basedclusteredmultivariatedynamic graph. Reprinted from [Had+13]. number of original nodes subsumed in a meta-node. The box colors encode the intra-clustersimilarityoftheunderlyingoriginaltimeseries,withdarkercolors symbolizing larger similarity. The meta-edges visualize connections between the clusters. The edge width encodes the number of original edges subsumed in a meta-edge. Overall, this abstracted visual representation can help us gain an overview of the key temporal behaviors and structural relationships of multivariate dynamic graphs. Structure-basedClusteringofDynamicGraphs Structure-based clustering aims to extract key characteristics of the structural changes that occur in a dynamic graph over time. In contrast to the previous section, we now group graphs G ∈ DG with similar structure, rather than i nodeswithsimilarattributebehavior.Inotherwords,ourclustersnowcontain anumberofsimilargraphs.Assuch,aclustercanbeunderstoodasaparticular state thatthedynamicgraphassumesatcertainperiodsintime.Consequently, thechangesinthestructureofadynamicgraphcanbeunderstoodastransitions between states. Following this thinking, the idea of structure-based clustering is to construct a state-transition graph that can help us understand how the structural aspect of a dynamic graph varies temporally. Computing the state-transition graph involves two steps: the initial setup of states and transitions, and the subsequent hierarchical grouping of similar states to meta-states and the corresponding definition of meta-transitions. The overall procedure is illustrated in Figure 5.39 and will be explained in more detail next.
254 (cid:4) Interactive Visual Data Analysis a) 1 2 3 4 5 6 7 8 b) 1 6 5 7 2 8 3 4 1, 6 5, 7 2, 8 3, 4 1, 6, 5, 7 2, 8, 3, 4 State Transition State grouping 1-8 Retained structure Changed structure Figure5.39 Structure-basedclustering.(a)Initialsetofstatesandtransitions based on the sequence of graphs G ∈DG; (b) Hierarchical grouping of states i and transitions based on similar structures. Adapted from [Had14]. 1. Initial setup. Initially, each original graph G ∈DG is represented by a i separate state. Any two states accommodating successive graphs G and i G are connected by transitions. The created states and transitions as i+1 shown in Figure 5.39a are the basis for the subsequent grouping. 2. Hierarchical grouping. For the actual grouping of states, their pairwise similarity needs to be calculated. This can be done in different ways, for example, by means of the graph edit distance, which basically counts the number of inserted, deleted, and substituted nodes and edges. States with high similarity can be subsumed to form new meta-states. Figure 5.39b illustrates this with pairs of states. The new meta-states themselves can in turn be grouped into further new meta-states. In our example,thisisdoneinalevel-wisewaytocreateastatehierarchy.Atits coarsest level is a meta-state that subsumes all original graphs G ∈DG. i Foreachlevelofthehierarchy(exceptforthecoarsestone),corresponding meta-transitions are determined. If a meta-state contains a graph G ∈ i DG and another state contains the subsequent graph G ∈DG, then i+1 a meta-transition is inserted between the two states.
Automatic Analysis Support (cid:4) 255 Figure5.40 Example of a state-transition graph characterizing an underlying dynamic graph. Courtesy of Steffen Hadlak. Overall, the outlined procedure generates a hierarchical clustering of the original dynamic graph. Each level of the clustering hierarchy corresponds to a state-transition graph that represents the dynamic graph at a different level of abstraction. To support the visual analysis of large dynamic graphs, a state-transition graph can be visualized as in Figure 5.40. The states are shown as boxes with an embedded miniature node-link diagram that communicates the rep- resentative structure of the subsumed original graphs. The border width of the boxes reflects the number of subsumed graphs. Transitions are visualized as directed links between states. Link width encodes the frequency of state changes subsumed by a transition. This visual design allows us to detect interesting temporal patterns in a dynamic graph: • Dominant graph structures are visible as states whose boxes have wider borders. • Rare graph structures manifest in states with thin borders and only few thin incident transitions. • Typical structural changes are expressed by paths along transitions that have wider links. • Recurring structural changes correspond to cycles in the state-transition graph. • Branching behavior isindicatedbystateshavingmorethanoneoutgoing transition.
256 (cid:4) Interactive Visual Data Analysis DemonstratingExample Let us next demonstrate the benefit of the introduced clustering methods for the visual analysis of multivariate dynamic graphs. Our example concerns the analysis of link quality in a wireless network operated by the OpenNet community. The network consists of 297 WiFi devices and 2,008 links between them monitored at 217,253 time points (five months at minute resolution). Each link has a quality attribute that describes the probability of a successful packet transmission along the link. The goal of OpenNet is to provide an overallsatisfyingnetworkquality.Visuallyanalyzingthedynamicallychanging network allows them to identify, locate, and quickly resolve quality issues. Tosupporttheanalysis,bothattribute-basedandstructure-basedclustering can be utilized. Attribute-based clustering groups the devices for which the averagelinkqualityoftheirincidentlinksevolvessimilarly.Theclusteredsuper- graph enables OpenNet to distinguish clusters in the network with varying, consistently high, or consistently low link quality. Analyzing the derived time series at multiple temporal scales as in Section 5.3.2 can provide more insight into the evolution of the link quality. Coarser temporal scales facilitate the exploration of global trends, whereas finer scales support the inspection of more fine-grained events such as dropouts. Dropouts describe incidents that may affect larger parts of the network. They are critical if they occur frequently. To investigate this aspect, structure- based clustering is applied and a generated state-transition graph is visualized in Figure 5.41a. Wide borders of the states emphasize recurring structures, which have a high impact on the stability of the wireless connections. As we can see, S , S , and S represent such frequent states. 1 2 3 a b c Figure5.41 Analyzing a wireless network supported by structure-based clus- tering. (a) State-transition graph; (b) Average link quality of selected state; (c) Representative graph structure of selected state. Courtesy of Steffen Hadlak.
Automatic Analysis Support (cid:4) 257 For a more in-depth analysis of states, two additional views are available, a temporal view and a structural overview. They depict further information for a selected state. The temporal view in Figure 5.41b visualizes the average quality of all links contained in the selected state. The structural overview in Figure 5.41c shows the state’s representative graph structure. When the structural overview is well connected, and the temporal overview shows a sufficiently high link quality without dropouts, then we can conclude that the state is sufficiently functional. By analyzing all states in this manner, it is possible to detect the states with dropouts. Whether these states are singular events or occur more frequently can be seen from the border width of the state’s box in the view of the state-transition graph. Thissmallexampledemonstrateshowthetheoreticalconceptsforclustering multivariate dynamic graphs can be employed in practice to address a real- world data analysis problem. The clustering not only helps us to reduce the amount of data to be visualized, but also to detect characteristic structures and interesting temporal patterns. At this point, we conclude the section on grouping data elements with the help of classification and clustering. We have learned that classification subdivides the data space in order to define meaningful subspaces, the classes. Each class describes data with particular properties. How to define the classes depends on the application domain and the tasks to be performed. Clustering, on the other hand, creates groups based on the similarity of dataelements.Theclusteringoutcomedependsnotsomuchondomain-specific or task-specific constraints, but primarily on the data and on the configuration of the clustering. The latter includes the selection of appropriate similarity measuresandsuitableclusteringstrategies,theparameterizationoftheselected methods, the weighting of data attributes, and the treatment of outliers. All these decisions lead to different clustering results. Overall, configuring classification and clustering approaches is a non-trivial task. Interactive visual representations can help us obtain suitable setups that reliably extract and convey the key characteristics of the analyzed data. Only then can classification and clustering support the generation of meaningful overviews that show the important as suggested by Keim’s visual analytics mantra [Kei+06]. 5.5 REDUCING DIMENSIONALITY Clustering and classification as discussed before are concerned with data elements. In this section, we will reduce the number of data variables. Data with very many variables are difficult to analyze for the following reasons: • Data issues: With many variables, the data space gets so huge that its dataelementsbecomesparse.Asadirectconsequence,thenumberofdata elements needed to estimate a function describing a data property grows
258 (cid:4) Interactive Visual Data Analysis exponentially. With the sparsity of the data space, the data get more uniform and global structures and features disappear. This phenomenon has been coined as the curse of dimensionality [Bel61]. • Visualization issues: With an increasing number of variables, the visual representations become more and more complex. Over-plotting can become a serious issue. It is more difficult to perceive relevant structures, outliers, or trends. • Performance issues: Visualizing many variables also places higher demands on memory and runtime. It becomes more and more difficult to ensure interactive frame rates. In order to address these issues, dimensionality reduction aims to reduce the number of data variables. The goal is to figure out which data variables carry relevant information and to focus the data analysis on them, neglecting the unimportant data variables. This can substantially simplify the analysis. The idea of dimensionality reduction is to project data elements from the high-dimensional data space onto a lower-dimensional projection space in such a way that the original information is preserved as much as possible. There are different approaches to perform the projection. Their difference lies in how they measure which parts and characteristics of the data are considered relevant and hence should be preserved in the projection space. Two different approaches can be differentiated: linear methods and non- linear methods. Linear methods define the dimensions of the projection space as linear combination of data variables. A prominent linear method is the principal component analysis (PCA). It is based on a rotation of the data space so that the dimensions of the projection space, the principal components, describe the directions of the largest data variations. In contrast, non-linear methods define the dimensions of the projection space with regard to certain proximity constraints. For example, with multi-dimensional scaling (MDS) pairwise data distances are preserved. It can be quite difficult to figure out which approach to apply and how to configure it. As before, we can employ interactive visual methods to help users understand and steer the automatic computations toward meaningful projections. Next, we discuss this in more detail for the example of using PCA to determine relevant data variables. 5.5.1 PrincipalComponentAnalysis InordertogetabetterunderstandingofPCA,letusbrieflyexplainthegeneral idea by the simple example in Figure 5.42. Figure 5.42a depicts a number of data elements in a two-dimensional data space spanned by the variables V 1 and V . The figure also shows the two principal components PC and PC . 2 1 2 We can easily see that PC is aligned with the axis of the greatest variance in 1 the original data. PC is orthogonal to PC . 2 1
Automatic Analysis Support (cid:4) 259 V V 2 2 PC 1 PC PC 2 2 V PC PC 1 1 1 V 1 (a)Original data space. (b)Principal component space. (c)Reduced space. Figure5.42 Reducing dimensionality with principal component analysis. In Figure 5.42b, the data space has been rotated according to PC and 1 PC to define the projection space, more specifically, the principal component 2 space, or short PC space. In PC space, most of the information inherent in the data elements is distributed along the horizontal axis, whereas the vertical axis carries only little information. This allows us to apply dimensionality reduction and to represent the data just by PC . 1 Figure 5.42c illustrates the result of the dimensionality reduction. The data elements are now only given with respect to PC . Although we lose 1 the information with respect to PC , the one-dimensional representation still 2 clearly shows two data groups at the beginning and the end of PC and one 1 outlier in the center. This simple example shows us how PCA can be utilized to decrease the data complexity, while still maintaining the distribution of data elements along the major data trend described by PC . 1 For actually calculating the principal components, we need tabular data modeled as an n×m matrix X, where the columns represent m data vari- ables and the rows represent n data elements. By applying a singular value decomposition, the matrix X is decomposed as follows: X =W ·Σ·CT The rows of the matrix CT are the transposed eigenvectors of the data’s covariance matrix XTX. They define the principal components as linear combinations of the original data variables. The factors involved in the linear combinations are denoted as loadings. The loadings capture how much an original data variable contributes to a principal component. ThematrixΣisadiagonalmatrix.Thediagonalentriescontainsignificance values resulting from the ranked square roots of the eigenvalues of the data’s covariance matrix XTX. A principal component’s significance tells us how much original data variance it covers. The principal components in CT are ordered with regard to their signifi- cance. The first principal component captures most of the data’s variance and thus represents the major data trend. The second principal component cap- tures most of the remaining variance and so on. The least significant principal
260 (cid:4) Interactive Visual Data Analysis components, that is, the last rows of CT represent only very small variances. These components can be neglected in order to reduce dimensionality. The rows of the matrix W contain the transformed coordinates of the data elements in the principal component space. These coordinates are denoted as scores to make them distinguishable from the original positions of the data elements. Next, we will see how principal components, loadings, significance, and scores can be utilized for data analysis purposes. 5.5.2 VisualDataAnalysiswithPrincipalComponents Visualdataanalysiswithprincipalcomponentstypicallyrequirestwomutually dependent analysis perspectives. First, we investigate how visual methods can help us understand and steer the dimensionality reduction with PCA. Second, we describe how the actual data analysis can be carried out based on the principal components. VisualSupportforPrincipalComponentAnalysis Before the PCA can be calculated, it needs to be decided what should be included in the data matrix X. Theoretically, X can cover the whole dataset. However, it can make sense to compute principal components only for a data subset depending on the task at hand. For example, variables describing time and space are often excluded to leave them as a frame of reference. Moreover, highly correlated variables might overemphasize certain data trends, and outliers could distort the analysis results. It is up to the user to exclude these variables and data elements from the PCA. The decision regarding which parts of the data should be excluded can be supported by visual methods. Table-based visualizations as introduced in Section 3.2.1 of Chapter 3 can be used for this purpose. When a table-based visualization is sorted properly, highly correlated variables and outliers to be ignored can be recognized easily. OncethePCAhasbeencarriedoutonthedefineddatasubset,weobtainthe principal components. For each principal component, we know its significance and how each variable contributes to it, that is, the loadings. Based on this information, we can reduce the PC space. Typically, only the two or three most significant principal components are retained, because they capture the major data trends and can easily be plotted in 2D or 3D visual representation. However, less significant principal components might also bear interesting and unexpected information. This raises the question of how to choose principal components in an adequate way. To answer this question, we need to take not only the significance but also the loadings into account. In fact a subset of principal components sufficiently characterizes the original data, only if:
Automatic Analysis Support (cid:4) 261 1. all loadings of these principal components are sufficiently high, that is, all variables are represented well, and 2. the loadings of the remaining principal components weighted by their significance are sufficiently small. Again, a table-based visualization can help us check whether these two conditions are satisfied [MNS06]. In Figure 5.43, we illustrate this for a demo- graphic dataset. The rows of the table show the variables of the original data, including population, literacy, or life expectancy. The columns contain the principal components ordered from left to right according to their significance. The table cells visualize loadings weighted by significance. Each table cell corresponds to a pair of a data variable and a principal component. Per cell, a bar encodes information as follows. The bar extends from the cell center to the right and is colored blue when the loading has a positive sign. The bar extends to the left and is yellow when the sign is negative. The length of the bar represents the value of the variable’s loading weighted by the principal component’s significance. As such, the bars in a column act as an indicator for the relevance of the corresponding principal component. The bars in a row show the influence of the data variables on the different principal components. Figure 5.43 Table-based visualization of loadings weighted by significance. Reprinted from [Aig+11]. In Figure 5.43, we can see many longer bars in the first three columns. This means that the first three principal components actually capture a lot of information, suggesting that all other components could be candidates for reduction. However, the first two variables, population and population density, hardly contribute to the first three principal components. Hence, condition one as stated above is violated. Moreover, there are also a couple of longer bars in columns three to six, which means that also the second condition is not met. If we retain only the first three principal components, we will likely lose information. Instead, we should consider at least four, better five or six principal components. Only then are all variables faithfully represented by the reduced PC space. However, still interesting information might be hidden in the data. In order to reveal them, it may help to look at the unweighted loadings. In Figure 5.44, the plain loadings are visualized in the same way as before. Now, we can
262 (cid:4) Interactive Visual Data Analysis Figure5.44 Visualizing unweighted loadings empasizes the contribution of individual variables to each principal component. Reprinted from [Aig+11]. see the contribution of the individual variables to each principal component. This facilitates the detection of deviating data behavior. For example, in the ninth principal component, we can identify a difference between the loadings of the two variables for life expectancy of females and males. This difference might suggest including the principal component in the analysis as well or investigating the two variables separately in more detail. In summary, the above examples demonstrated how the visualization of loadings and significance can help us make an informed decision when it comes to reducing dimensionality of data. Next, we briefly describe how the actually reduced principal component space can be analyzed. VisualDataAnalysisBasedonPrincipalComponents Visual data analysis based on principal components means showing the data elements in the PC space, more precisely, in the reduced PC space. Generally, thevisualizationisstraightforward.Thedataelementscansimplybevisualized accordingtothescores calculatedduringthePCA.Ingeneral,thevisualization techniques for multivariate data introduced in Section 3.2 in Chapter 3 can be utilized for this purpose. For example, we could use parallel coordinates with as many axes as we have principal components and plot each data element based on its score values. Alternatively, we could adapt the idea of the tabular visualizationusedbefore.Wemaintaintheprincipalcomponentsinthecolumns, but show data elements as rows. Each table cell would then represent a score value.Whilethesearealltechnicallyfeasiblevisualrepresentations,akeyissue needs to be addressed. Because the principal components spanning the PC space correspond to a combination of several original data variables, interpreting the scores and relating them back to the original data space is difficult for a human observer. Therefore, we need visual support helping users gain an understanding of the rather abstract principal components. Two visual approaches can be useful in this regard: • Annotating principal components with data variables: The principal com- ponents are labeled with the names of those variables that mainly con-
Automatic Analysis Support (cid:4) 263 tribute to them. To avoid clutter, labels can be shown on demand when the user hovers over a principal component with the cursor. • Associating scores with data values: A subset of scores in PC space can be associated with their corresponding values in data space. This can be done either by a side-by-side visualization of scores and data values or by a combined visualization. In the side-by-side case, brushing & linking allows users to connect the scores from one view with the data values of the other view. In the case of a combined visualization, a limited number of principal components and data variables are presented simultaneously. An example could be our table-based visualization. As before, the table columns represent the principal components, but the rows now show the original data variables, and the bars in the cells represent scores. In such a visual representation, small bars symbolize data values that follow the main trend of a principal component, whereas larger bars correspond to substantial deviations from the trend. To conclude, again we have seen that automatic computations, PCA in this case, can support the interactive visual data analysis. PCA is a helpful approach to determine which data variables play an important role and which can potentially be neglected. Knowing this, we can simplify the visualization and concentrate the analysis on those parts of the data where insight are likely to be made. Yet, we have also seen again that employing automatic computations requires a careful configuration and a good understanding of the consequences. Visualmethodscanhelpusersinthisregard.Infact,weneedatightinterplayof automaticcalculationandhumansense-makingbasedonvisualrepresentations. 5.6 SUMMARY In this chapter, we discussed automatic computational methods to support interactive visual data analysis. The key idea was to reduce the complexity of the data and their visual representations. A figurative overview of the covered methods is provided in Figure 5.45. First, we briefly described how the complexity of visual representations can be addressed with density-based visualization and bundling. The idea of density-based visualization is to compute and visualize the frequency of data elements. Bundling improves the visual structure in visualization images by summarizing graphical elements. For the most part of this chapter, we studied ways to reduce the size and complexity of the data. A first strategy was to focus the analysis on relevant parts of the data. The degree-of-interest (DoI) approach was introduced as a means to determine the relevancy of data elements based on well-defined interest functions. A related approach is feature-based visual analysis, where characteristic data features are automatically extracted and visualized.
264 (cid:4) Interactive Visual Data Analysis Density-based Bundling Degree of Feature-based visualization interest (DoI) visual analysis Sampling Aggregation Classification Clustering Dimensionality reduction Figure5.45 Overview of automatic computational methods to support inter- active visual data analysis by reducing the complexity of the data and their visual representations. We further discussed how abstracting and grouping data elements can facilitate the data analysis. Sampling and aggregation are two well-established data abstraction methods. Sampling methods aim to generate a reduced, but stillrepresentativesubsetoftheoriginaldata.Aggregationmethodsreplacethe data values within an interval by statistical descriptions such as the average or the mode. Classification and clustering are typical strategies for grouping data elements. With classification, the data space is partitioned into classes with specific properties, whereas with clustering, the data are organized in clusters of similar data elements. Last but not least, we described how dimensionality reduction can help us reduce the complexity of data with many variables. All of the above approaches address the “Analyse First”-step of the visual analytics mantra as stated in the beginning of this chapter [Kei+06]. Although each approach pursues a different strategy, they all have in common that they transform unstructured raw data into a form that emphasizes the data’s basic properties and key characteristics. This way, the data is prepared to be more meaningful for human exploration and insight discovery [Sac+14]. However, applying automatic computations typically requires a careful configuration. What computational methods should be applied, maybe even in combination, and how should their parameters be set? Finding suitable answerstothesequestionsisimportant,becausedifferentmethodsanddifferent parameter settings lead to different results, and not all of them might be appropriate for the task at hand. In this chapter, we have seen several examples where visual methods were employed not only to show the data, but also to help users understand how automatic computations work and how their parameters should be set. In fact, we have seen a mutual relationship between automatic computation and visual methods. On the one hand, automatic computations support the visual
Automatic Analysis Support (cid:4) 265 analysisbyextractingimportantcharacteristicsofthedata.Ontheotherhand, visual methods help us to configure the automatic computations appropriately. Both sides of the relationship are tied together by the human user who has interactive control over all involved methods [End+17]. This control paired with expressive visual representations enhances trust in and interpretability of the generated analysis results. Overall, we have now dealt with the fundamental ingredients of interactive visualdataanalysis.Inthenextchapter,wewillmoveontoadvancedconcepts for visualization, interaction, and automatic computation. FURTHER READING General Literature: [Kei+06] • [Kei+10] • [End+17] Decluttering Visual Representations: [NH06] • [BW08a] • [LHT17] Focusing on Relevant Data: [RPS01] • [DGH03] • [Abe+14] Abstracting Data: [ED06] • [EF10] • [Lub+12] Grouping Data: [HKP11] • [XW05] • [Emm+16] Reducing Dimensionality: [Jol02] • [LV07] • [Sac+17]
6 CHAPTER Advanced Concepts CONTENTS 6.1 Visualization in Multi-display Environments ............... 268 6.1.1 Environment and Requirements .................... 269 6.1.2 Supporting Collaborative Visual Data Analysis .... 270 6.1.3 Multi-display Analysis of Climate Change Impact . 276 6.2 Guiding the User ............................................ 277 6.2.1 Characterization of Guidance ....................... 278 6.2.2 Guiding the Navigation in Hierarchical Graphs .... 283 6.2.3 Guiding the Visual Analysis of Heterogeneous Data 286 6.3 Progressive Visual Data Analysis ........................... 288 6.3.1 Conceptual Considerations .......................... 290 6.3.2 Multi-threading Architecture ....................... 294 6.3.3 Scenarios ............................................ 297 6.4 Summary .................................................... 303 IN THE PREVIOUS Chapters 3 to 5, we described the three fundamental parts of interactive visual data analysis: the visualization, the interaction, and the automatic analysis support. We learned about basic visualization strategies, interaction techniques, and computational methods, which usually work in concert to facilitate data analysis activities. The knowledge from the previous chapters enables the reader to apply existing approaches to analytic problems at hand or to design new ones if necessary. The goal of this chapter is to broaden our view on interactive visual data analysis to advanced topics that are not yet mainstream, but have the potential to become so in the future. Following the structure of the previous three chapters, we will discuss three selected topics: • advanced visualization in multi-display environments, • advanced interaction through user guidance, and • advanced automatic computation via progressive procedures. Section 6.1 will discuss the visualization of data in multi-display envi- ronments. We will see that such environments provide us with interesting 267
268 (cid:4) Interactive Visual Data Analysis opportunities to enhance the visual data analysis. They allow us to show more visual content and also to show it to more people. This enables collaborative data analysis and in-depth discussions involving several experts. Section 6.2 deals with facilitating the human-computer partnership by providing guidance that helps users carry out expedient analysis actions. We will discuss a conceptual framework that characterizes guidance in the context of interactive visual data analysis, and two selected examples will illustrate how guidance can actually be implemented. Finally, in Section 6.3, we examine the challenge of time-consuming auto- matic computations, which may impede the data analysis. One answer to this problem is to progressively compute intermediate results and present them to the user timely, allowing for early evaluation and, if necessary, intervention. Withthesethreetopics,wearemovingintoaterrainofactivedataanalysis research.Thatsaid,thecontenttocomeisnaturallysubjecttofurtheracademic discussions and revisions. Still the following sections provide an outlook on exiting new perspectives on interactive visual data analysis. Let us start with visualization in multi-display environments. 6.1 VISUALIZATION IN MULTI-DISPLAY ENVIRONMENTS Most of the visual analysis solutions described in this book are designed for traditional environments where a single user is working with one or two displays. However, such environments are limited in two regards. First, only a single individual is involved in the data analysis. Critical reflections of results or creative discussions of alternative analysis strategies are hardly possible. Second, the available display space is limited. This can make the analysis of larger volumes of data difficult. A natural step to address these limitations is to bring the visual data analysis to advanced multi-display environments (MDEs). MDEs can facilitate visualdataanalysisbyenablingmoreuserstoobservemoredata.Theincreased overalldisplayspacenotonlymakesitpossibletovisualizemoredataelements, but also to show more aspects of the data simultaneously. The increased physicalsizeofMDEsallowsmultipleuserstostudythevisualizeddata,which promotes collaborative analytic work. Collaborative analysis in MDEs is particularly useful in scenarios where experts from different application domains have to discuss heterogeneous data toarriveatacommonunderstandingofcomplexphenomena.Onesuchscenario isthestudyoftheimpactofclimatechange.Climatechangeaffectsamultitude ofsectors,suchasagriculture,forestry,ecosystems,andeconomy.Expertsfrom manyfieldsneed toworktogetherand sharetheirdomain-specific observations anddataanalyses tobetterunderstandthe crucial questions relatedtoclimate change. Collaborative visual data analysis in MDEs can be a paradigm shift away from the otherwise single-user setting at regular desktop computers. Yet, this shift also comes with new challenges. For example, distributing and
Advanced Concepts (cid:4) 269 Figure 6.1 Smart meeting room at the University of Rostock. Reprinted from [ENS15]. arranging visual representations manually on multiple displays can be a time- consuming task that should not be burdened on the users. Keeping track of analysis steps and intermediate findings is also more difficult in a collaborative setting. Therefore, the visual data analysis in MDEs must be facilitated by dedicated methods that relieve users of laborious manual work and allow them to concentrate on their analytic objectives. This section outlines how advanced visual data analysis can be implemented in MDEs. 6.1.1 EnvironmentandRequirements Tostartwith,wewillbrieflyintroduceaparticularMDEandtherequirements that need to be considered to support collaborative visual data analyses in it. Environment Our environment is a smart meeting room as illustrated in Figure 6.1. A smart meeting room is an instance of smart environments, which Cook and Das define as “a small world where all kinds of smart devices are continuously working to make inhabitants’ lives more comfortable” [CD04]. The smart meeting room combines several heterogeneous displays to form a coherent display space in which content can be distributed even across device boundaries. Moreover, the smart meeting room can dynamically integrate various input, computing, and output devices into the device ensemble, which allows users to bring and use their own personal devices. These characteristics make a smart meeting room a distinguished MDE, namely a smart MDE. While the MDE provides a technical basis, it is the MDE’s smartness that enables the actual visual data analysis.
270 (cid:4) Interactive Visual Data Analysis Requirements for Collaborative Visual Analysis In order to support the collaborative visual data analysis in smart MDEs, we must understand how users work. In the first place, the users compile a corpus of information to be analyzedanddiscussed.Anyusermaycontributecontents,suchasvisualization images, slides, or documents. For simplicity, let us call these different contents justviews.Then,theusersdefinewhichviewsshouldbeshowntogetherandin what sequence. This way, a kind of presentation is created that meaningfully aligns the views with the analysis goals. While setting up the presentation is the task of the users, distributing and arranging the views on the multiple displays should be the task of the machine. This brings us to the first two requirements: • The system should afford easy contribution of views and creation of the presentation. This ensures that the users can control the content and the flow toward the analysis goal. • The system should distribute and lay out the views in the environment automatically according to the state of the presentation. This relieves the user from cumbersome manual layout tasks. While discussing and analyzing the data contained in the views, it is often necessary to go back to previously shown information, to add, adjust, or replace certain visual representations, or to compare performed analysis steps. Furthermore, newly gained insights may raise new questions, which in turn might require inspecting additional data. In this case, the users must be able to generate the needed visualizations without interrupting the ongoing presentation. From this, two further requirements can be derived: • The system should support the seamless switching between the presenta- tion of views and the generation and adjustment of views. This narrows the gap between phases of presentation and phases of more personal exploratory analytical work. • The system should keep track of the analysis history, including changes to the presentation and the findings made during the analysis session. This helps users to return to previously discussed aspects and to recap the analysis and its results. Next, we will illustrate by concrete methods how these requirements can be addressed to support the collaborative visual data analysis in smart MDEs. 6.1.2 SupportingCollaborativeVisualDataAnalysis The solutions to be presented next focus on supporting four key tasks: the creation of the presentation, the layout of views on multiple displays, the adjustment of presentation and views during the analysis, and the recording of the visual analysis process.
Advanced Concepts (cid:4) 271 Figure6.2 Graphicalinterfaceforcreatingandcontrollingmulti-displayvisual analysis presentations, including content pool (top), logical presentation struc- ture (middle), and preview (bottom). Courtesy of Christian Eichner. CreatingaMulti-displayPresentation The creation of presentations for collaborative visual data analysis can be supported by a graphical user interface as shown in Figure 6.2. The interface is available on all devices being connected to the smart MDE, so that everyone can participate in preparing the presentation, even from their personal devices. The top panel of Figure 6.2 represents the content pool and affords the collection of views. Visualization images, slides, or documents can be added to the content pool by simple drag and drop. A special thing about the collected viewsisthattheycanbelinkedtothesoftwarethatwasusedtogeneratethem. The linked software can be started at any time by clicking the corresponding view. This allows users to alter existing views or to generate new ones on the fly during the presentation. The middle panel in Figure 6.2 serves to set up the presentation based on the views in the content pool. For this purpose, views from the top panel are dragged to the vertical layers in the middle panel. From left to right, the layers represent the sequence of views to be shown in the course of the presentation. Views within the same layer are visible at the same time. Links can be added between views to define spatial and temporal constraints for the later automatic layout of the views on multiple displays. Spatial constraints (within layers) link views that are to be shown on the same display, whereas temporal constraints (between subsequent layers) link views to be replaced when advancing the presentation from one step to the next.
272 (cid:4) Interactive Visual Data Analysis The bottom panel in Figure 6.2 provides a preview of the presentation. It tells the users which views are currently being shown in the smart MDE, which views were displayed before, and which views are still to come. This panel is also used to advance the presentation to the next layer, to return to previously shown layers, or to directly go to a certain layer. In summary, the described graphical interface enables users to set up a multi-display visual analysis presentation. To avoid conflicts during the collaborativeworkandtoensureaconsistentpresentation,amoderatorshould chair the process. LayoutofViewsonMultipleDisplays Smart MDEs provide the technological basis for distributing the graphical contents on multiple displays. What is needed to support collaborative visual analysis is a mechanism that distributes and arranges the views according to the prepared presentation. This can be done with a dedicated multi-display layout algorithm. The automatic view layout has to be carried out in two steps as illustrated in Figure 6.3. In the first step, views are allocated to the available displays of the environment. The second step computes the arrangement of views per display.Becausebothstepsmustconsidervariousinfluencingfactors,including the number and the properties of the available displays and the user-defined temporal and spatial constraints, it makes sense to define the automatic layout as an optimization problem [Eic+15]. The goal of the optimization is to find view positions such that the overall layout quality is maximal. Three terms contribute to the overall layout quality: the spatial quality, the temporal quality, and the visibility quality. The spatial quality is high if the views being linked via spatial constraints are near to each other, ideally on the same display. The temporal quality is high if temporally stable layouts are produced. That is, views being linked via temporal constraints are ideally presented at the same position when advancing the presentation from one step to the next. The visibility quality is a bit more complex to define. It rates how well users can see the different views in the display environment, which can Display 1 View View Allocation Display 2 Arrangement Views Display 3 Figure6.3 Basic two-step procedure of the automatic view layout.
Advanced Concepts (cid:4) 273 be approximated by considering the directional visibility of views. A smart MDE can estimate this information based on the display configuration (size, position, and orientation of displays) and by tracking the participating users (position and viewing direction of users) [RLS11]. The outlined optimization problem has to be solved whenever a change happens either to the presentation or to the environment. In order to manage to compute the layout for a dozen of views in less than a second, a heuristic optimization approach should be applied. For example, by employing the branch-and-cut algorithm, overestimating the achievable qualities of possible layouts, and reusing previous calculations, it is possible to obtain expedient results in a short time. InteractiveAdjustmentsinMulti-displayEnvironments Oncethelayoutmechanismhasdelivereditsinitialresults,theviewsareshown in the smart MDE and the data analysis can start. The regular process will be to discuss the depicted information, agree on intermediate findings and results, andadvancethepresentationfurther.Yet,withchangingtopicsofinterest,the actual course of the analysis might divert from the originally planned one. In this case, the users must be able to adjust the presentation on the fly. To this end, two types of interactive adjustments should be supported: adjustment of the layout of views and adjustment of the content of views. Adjusting the Layout of Views Adjusting the view layout involves moving and resizing views. For example, views might need to be moved from one display to another for side-by-side comparison. Fine details spotted during the comparative analysis could make it necessary to enlarge a view. To this end, the moderator (or another authorized user) can select and adjust a certain view to improve the visibility of details. (a)Point at central view. (b)Move view downward. (c)Enlarge view. Figure6.4 AdjustingpositionandsizeofaviewusingaWiiRemotecontroller. Reprinted from [Rad+12]. An example of such a layout adjustment is illustrated in Figure 6.4. We alreadysawthesevisualrepresentationsofgraphsinSection3.5.3ofChapter3, but now they are projected onto a canvas of the smart MDE. A user points at
274 (cid:4) Interactive Visual Data Analysis Different View Contents 3. Store 4. Update 2. Generate 1. Launch Graphical Interface Visualization Software Multi-display Environment Figure6.5 Changing the content of views by launching visualization software. the central Magic Eye View using a Wii Remote controller, moves the view a bit downward, and then enlarges the view to make it stand out. On the user’s side, these interactions are easy to perform. On the system’s side, dedicated mechanisms such as device integration and interaction mapping are employed to facilitate the interaction [Rad+15]. Adjusting the Content of Views We already mentioned that views can be linked with a compatible software. This makes it possible to re-generate the content of views to better align them with varying analysis requirements. Figure 6.5 illustrates an example with the feature-based visualization already described in Section 5.2.2. First, the linked visualization software is launched from the graphical interface with a click on the view’s thumbnail. This can be done by users who have the visualization software installed on their personal device. On the personal device, the data can then be explored until a suitable new view has been found. The new view can be an alteration of the already existing version or a totally new visual representation. Once generated, new views are immediately stored and integrated into the presentation, and the layout of views is updated automatically. The great flexibility offered by this mechanism is a key advantage for the collaborative data analysis. KeepingTrackofCollaborativeDataAnalyses It is generally accepted that keeping an analysis history is beneficial for coor- dinating the insight-generation process and reflecting about it [KNS04]. Com- monly, analysis histories store information about the interactive adjustments performed by users and the findings that have been derived. In a collaborative multi-display setting, it makes sense to log additional information about who contributedandchangedviewsandwhenandwhereviewshavebeendisplayed. Some of this information can be determined automatically. For example, the
Advanced Concepts (cid:4) 275 Figure6.6 Graphical interface for analysis coordination and meta-analysis, including filtering support (top), analysis history graph (middle), and timeline with undo and redo buttons (bottom). Courtesy of Christian Eichner. view layout mechanism can keep track of when and where views were shown. Interactive adjustments can also be recorded automatically, including the actions that were taken, the users who carried them out, and the resulting visual representations. Yet, some information cannot be derived automatically. For example, findings derived from a view need to be annotated manually. The recorded and annotated information is stored in the form of a graph. The graph’s nodes represent views, more precisely, the state changes logged per view. As such, a node captures a piece of analytical progress made during the data analysis. Links between nodes form paths of analytical progress as defined by the sequence of actions taken. Toactuallygainfromtheanalysishistory,itcanbedisplayedinagraphical interface as illustrated with a small example in Figure 6.6. During an ongoing analysis session, the interface can be employed to reset the analysis to a previous state via selective undo and redo. “Selective” means it only affects operations that were triggered by a certain user, affected a specific view, or concerned a particular display. This is helpful when the data analysis stalls in a dead end or if the participating users cannot come to an agreement about findingsandintermediateanalysisresults.Undoandredoallowsthemoderator to keep the analysis going, for example, by collecting further evidence for or againstahypothesisfrompreviousviews.If,afterreturningtoapreviousstate, an alternative course of actions is pursued, a new analysis branch is created, which is also visible in Figure 6.6. A graphical depiction of the analysis history can also support a post-hoc meta-analysis to understand how individual analysis steps contributed to the generation of new insights. For example, in Figure 6.6, we can see that
276 (cid:4) Interactive Visual Data Analysis three alternative analysis routes were tried out. Small icons overlaid on the thumbnails indicate which interactions were performed, and the thumbnails’ colored borders tell us who performed them. When a thumbnail is clicked, an on-demand text box will provide information about when and where a view was displayed and which findings have been derived from it. With the help of additional filter controls, the meta-analysis can even answer questions such as which adjustments led to promising findings or which results required longer discussions. Taken together, the support for creating collaborative analysis sessions, for laying out views on multiple displays, for adjusting views and their content on the fly, and for keeping track of and utilizing an analysis history is essential for facilitating visual data analysis in smart MDEs. Next, we will illustrate how a smart MDE can be put to use for analyzing the impact of climate change. 6.1.3 Multi-displayAnalysisofClimateChangeImpact As already indicated, analyzing the impact of climate change requires the collaboration of multiple experts. In this section, we sketch a scenario with experts from meteorology, forestry, and hydrology. The course of collaborative analysis could be as follows [Eic+15]. First,themeteorologistwantstoexplainextremeprecipitationeventsbased onvisualrepresentationsthatshealreadyhasonherlaptopcomputer.Shelogs into the smart MDE and contributes her contents via the graphical interface introduced earlier. The automatic layout will make sure that all experts can see the visual representations well. Now the joint analysis can start. The explanations of the meteorologist trigger the hydrologist to join the discussion.Tobettermakehispoint,thehydrologistusesthegraphicalinterface on his computer to put a visualization about groundwater recharge into the content pool. Then, he defines a spatial constraint to link his view to a precipitation view of the climatologist. In the blink of an eye, the view layout in the smart MDE is automatically updated to show the linked views side-by- side on the same display. This allows a comparison and in-depth discussion of the views. At some point, the experts realize that they need additional views to make progress. Therefore, the hydrologist connects his view with the climateimpactsonline.com web portal. This allows him to generate the necessary views on the fly. Depending on the discussion of the new views, the experts can decide to either keep them or to perform undo operations to reset the presentation to the original views. Finally, the forestry expert takes part in the discussion. She explains how climate-related risks in Africa and South America are expected to develop in comparison to Europe. To illustrate this, she puts further views into the content pool and defines spatial and temporal constraints to connect them to the existing views. Again, the presentation is updated automatically.
Advanced Concepts (cid:4) 277 In the course of the visual analysis, the knowledge and findings of the different experts are discussed and combined to form a broader understanding of the impacts of climate change. After the analysis session, all experts can inspect the analysis history to reflect about the discussion or reproduce the analysis results. In summary, we see that bringing interactive visual data analysis to smart multi-displays environments is an exciting opportunity for collaborative sense- making. Yet, in order to fully exploit this opportunity, it is necessary to designdedicatedsupporttofacilitatetheanalysis.Here,weillustratedselected solutions based on a mix of automatic methods and interactive graphical interfaces implemented in a smart multi-display environment, which together form an advanced visualization environment. Next up in this chapter on advanced concepts is advanced interaction through user guidance. 6.2 GUIDING THE USER The meta-analysis in the previous section confirmed what we already stated severaltimes:Visualdataanalysisisnotaone-wayroadbutadynamicprocess during which several what-if scenarios are tried out. How should the data be processed with analytical calculations? Should clusters be computed, if yes, how many clusters are expected? Are certain data variables to be excluded from the processing? Which techniques should be employed to visualize the data? Should different data facets be represented in an integrated fashion or in separate dedicated views? Which parts of the data should be explored in what sequence? Only if appropriate answers to these questions can be found is it possible to make progress toward the desired analytic results. Butwhohastoanswerthequestions,theusers?Orcouldthemachinestepin andhelpusoutincertainsituations?Thisiswhatguidance isabout[Sch+13b; Cen+17; Col+18]. As a response to the challenge of ensuring analytic progress, it is the goal to guide users towards choices that present the most interesting aspects of the data with the most suitable combination of visual, interactive, and analysis methods. In Section 5.3.2 of Chapter 5, we already saw how guidance can assist userswhenexploringmulti-scaletimeseries.Wewillnextdiscusstherelatively new topic of guidance in more detail. The first part of this section will be of conceptual nature offering an in-depth characterization of guidance. The second and the third parts will be more practical. Two examples will illustrate how guidance can help users in analyzing complex data other than time series.
278 (cid:4) Interactive Visual Data Analysis 6.2.1 CharacterizationofGuidance So, what is guidance? In order to make more clear what we mean by guidance, we will start with a definition of guidance in the context of interactive visual data analysis, discuss its key aspects, and then introduce a conceptual model. DefinitionofGuidance Guidance is a broad term with much room for interpretation. The Oxford Dictionary and the Merriam-Webster Dictionary define guidance as “advice or information aimed at resolving a problem or difficulty” and “the act or process of guiding someone or something”. These definitions are interesting because they highlight guidance as a process aiming at solving a problem. This is also reflected in the following definition of guidance in the context of interactive visual data analysis [Cen+17]: “Guidance is a computer-assisted process that aims to actively resolve a knowledgegapencounteredbyusersduringaninteractivevisualanalytics session.” Cenedaetal.,2017 The three important aspects of this definition are emphasized in italics. First, guidance is a dynamic process that runs alongside the regular data analysis activities of the user. Second, there is a knowledge gap that causes the data analysis to stall. The user does not know how to proceed. The goal of guidance is to narrow the knowledge gap. Finally, the definition of guidance describes an interactive scenario where human and machine cooperate. The above definition also suggests what guidance is not. Guidance is not just an additional algorithm that computes a unique answer to the knowledge gap. Typically, this is not even possible due to ill-defined or too complex analyticproblems.Ifguidancewereabletocomputeapreciseanswer,wecould neglect the interactive visual approach to data analysis at all, compute the answer, and provide it to the user right away. But this would contradict with the idea of having the human in the loop. Thatsaid,guidancedoesnottakeoverthereasoningpart.Instead,guidance facilitates the data analysis to help users in forming decisions. Making the decisions remains the responsibility of the user. In this sense, guidance is comparable to mentors helping students. While mentors do not necessarily know the solution of the students’ problems, they can provide hints as how to approach the problems, guiding the students towards finding solutions on their own.
Advanced Concepts (cid:4) 279 Current Analysis Situation Path? Target? Figure6.7 A knowledge gap exists when target or path is unknown. AspectsofGuidance Allinall,guidanceisinfactacatalystforhuman-computercooperation.There are four main aspects that are worth further detailed consideration [Cen+17]: • Knowledge Gap: Why is guidance needed? • Input: What information can be utilized for providing guidance? • Output: How is guidance conveyed and how does it look like? • Degree: How much help does guidance provide? Knowledge Gap The knowledge gap captures what a user needs to know to make progress. While one can easily imagine many different knowledge gaps, from a conceptual point of view, there are only two distinct types of knowledge gaps: • Target unknown.Theuserdoesnotknowthedesiredresult.Forexample, the user does not know which data features to look at to falsify a hypothesis. • Path unknown. The user does not know how to reach the desired result. For example, the user knows the relevant features, but has no clue how to configure the visualization to reveal them. Figure 6.7 illustrates the two types of knowledge gaps. Certainly, if both target and path are known to the user, no guidance is needed. In general, capturing the knowledge gap is difficult. Users may or may not beawareoftheirknowledgegap.Ifusersareawareofit,theycanactivelymake it known to the system. If not, the system has to infer the knowledge gap, for example, by detecting deviations from domain conventions or long dwell times during exploration. Ultimately, guidance should narrow down the knowledge gap in a dynamic process that eventually converges to zero knowledge gap. Input The input subsumes the different sources of information based on which guidance can be generated. In the context of interactive visual data analysis systems, the following inputs can be utilized:
280 (cid:4) Interactive Visual Data Analysis Data input relates to information that is readily available or derivable from theoriginaldata,includingtherawdatathemselves,statisticalproperties of the data, extracted topology, or meta-data. Domain knowledge input refers to information that is commonly agreed upon in the application domain, such as domain models and conventions, established workflows, or expert systems. Visualization input captureswhattheuserisactuallyseeingonthedisplay. It includes both the specification of the visualization transformation as well as the actual visualization images. User knowledge input corresponds to information that users input to the system or that the system can infer from the user. Examples include annotations, preferences, or user interests. History input is based on keeping track of the course of analysis sessions by logging interaction steps, employed methods and parameters, intermedi- ate views, or visited parts of the data. Output Concerning the output of guidance, there are two aspects to be considered. In the first place, guidance must be generated, and secondly, it must be conveyed to the user. Generating Guidance Conceptually, generating guidance can be modeled as a function guidance(g,i)→o that takes the knowledge gap g and some input i from the available sourcesandthencomputesasuitableoutputo.Suitablemeanstheoutput contains pieces of information that alleviate the user’s problem. Hence, iterating the function several times should narrow the knowledge gap. Each iteration contributes a variable amount of knowledge, depending on the user’s expertise and perceptual and cognitive abilities. Differentknowledgegapscanbetackledwithdifferentguidancefunctions addressingdiverseaspectsofthedataanalysis.Forexample,iftheuseris unsure about what data should be investigated, guidance could identify interesting data subsets and suggest navigational routes toward them. If the user needs help in structuring the analysis into a series of tasks that match the analysis objective, the system could hint at what to do next. A user who is baffled by the variety of interactive, visual, and analytical methods could be supported by suggesting suitable combinations of techniques and corresponding parametrizations. Thegeneratedoutputcanaddresstheknowledgegapdirectlyorindirectly. For example, if a user has difficulties configuring a clustering algorithm, direct guidance could suggest promising parameter values. For the same
Advanced Concepts (cid:4) 281 problem, indirect guidance could accentuate interesting sub-structures in the data, whose analysis (note the indirection) could help the user better understand the influence of parameter values. Conveying Guidance The next step is to convey the guidance output in a way that actually facilitates the data analysis, yet without interfering too much with it. This can involve enhancing perception or inducing impulses in the user to trigger exploratory actions. It seems natural that guidance is primarily conveyed visually, for exam- ple, by adjusting the visualization, providing visual enhancements, or including additional graphical interface elements. We will later see two examples, where visually conveyed guidance is central. Yet, depending on the application context, guidance can also be conveyed via non-visual channels, including sounds or tactile feedback. Guidance Degree The guidance degree characterizes the extent to which guidance is required and actually provided. The degree is defined on a scale whose two extremes are labeled “no guidance/full freedom” and “fully guid- ed/nofreedom”.Apparently,thedegreeofguidanceisinverselyproportionalto the users’ freedom. In general, an effective guidance solution restricts freedom as little as possible, but as much as necessary. Ideally, the guidance degree is not fixed, but rather resonates with the course of the analysis session. In practice, the actually delivered guidance typically follows one of three characteristic scenarios: Orienting: Merelyorientingtheuserrepresentsalowdegreeofguidance.The objective of orienting is to support the user in building and maintaining a mental map. Providing visual cues hinting at potential targets and suitablepathsisacommonstrategyforimplementingorientation.Visual overview techniques also provide a kind of orientation. Directing: Directing represents a moderate degree of guidance. In contrast to orienting, directing emphasizes a certain preference for a future course of action. The system suggests to the user a set of options that lead to promising results. The suggestions may differ in terms of quality and costs. Visual preview techniques can help users make informed decisions for one or the other option. Prescribing: A rather high degree of guidance is reached when the system prescribes certain analytical steps towards a specified goal. This can be compared to a guided presentation that takes the user through the analysis process. To keep the human in the loop, it is important to visually present the intermediate steps taken by the system and to make the decisions that lead from one step to the next understandable. Of course it must be possible for the user to regain control and continue the analysis on another path or to another target.
282 (cid:4) Interactive Visual Data Analysis DATA ANALYTIC AND VISUAL USER TRANSFORMATION D T I P dK/dt K Data Transf. Image Percept. Knowl. S dS/dt E Spec. Explor. Figure6.8 Adapted variant of van Wijk’s model of visualization. Artifacts as boxes: data [D], specifications [S], visualization images [I], and user knowledge [K]. Functions as circles: analytic and visual transformation (T), perception and cognition (P), and interactive exploration (E). Adapted from [vWij06]. These three scenarios complete our characterization of the different aspects of guidance in the context of interactive visual data analysis. In the following, we will shape the idea of guidance into a conceptual model. ConceptualModelofGuidedInteractiveVisualDataAnalysis The conceptual model will help us understand how guidance interoperates with the interactive visual data analysis. To this end, we will attach guidance- related components to an existing model that describes how visually driven data analysis leads to new knowledge. The basic model we will be using is van Wijk’s model of visualization [vWij06]. A slightly adapted variant of van Wijk’s model is shown in Figure 6.8. It combines the data transformation and knowledge generation models from Sections 2.3.2 and 2.3.3 in Chapter 2 and the human action cycle from Section 4.1.2 in Chapter 4 in a fairly simple and elegant way. Boxes represent artifacts, such as data or images, while circles represent functions that process some input and generate some output. According to the depicted model, visual data analysis works as follows. Analytic and visual methods transform (T) data [D] into images [I] based on somespecifications[S].Humansperceive(P)theimagesandcognitivelyextract the visually encoded information to accumulate more and more knowledge [K]. Based on their accumulated knowledge, users can interactively explore (E) the data by adjusting the specifications. As indicated by dK/dt, knowledge change occurs as a consequence of the interpretation of the visual representations and the exploratory adjustment of the specification dS/dt. Now let us attach the guidance-related components and see how they can help us to keep the knowledge generation loop going. The central component intheextendedmodelinFigure6.9istheguidancegenerationprocess(G*).It draws from different sources of input and computes different forms of guidance as output. Before any measures of guidance can be taken, the particular knowledge gap of the user must be known, which is indicated by the link
Advanced Concepts (cid:4) 283 DATA ANALYTIC AND VISUAL USER TRANSFORMATION D T I P dK/dt K Data Transf. Image Percept. Knowl. S dS/dt E D* Spec. Explor. Domain C* O* H* Prescribing Cues Options History G* Orienting Directing Guidance Input Output GUIDANCE Figure 6.9 Conceptual model of guided interactive visual data analysis. *Added artifacts and functions: domain conventions and models [D*], his- tory and provenance [H*], visual cues [C*], options and alternatives [O*], and guidance generation (G*). Adapted from [Cen+17]. between [K] and (G*). Further sources of input connect to (G*), including the original data [D], the visualization images [I] and the underlying specification [S], the interaction history or provenance [H*], and domain conventions or models [D*]. On the output side, guidance can be delivered in various ways and to different degrees. Basic guidance can show visual cues [C*] alongside the visualization to help users orient themselves. Directing users to promising analysis paths works by determining and offering options [O*] that, if chosen during the interactive exploration, lead to improved visualizations. Finally, the guidance mechanism can take over control and circumvent progress-hindering obstacles automatically by prescribing certain specifications [S] directly. Thesketchedconceptualmodelprovidesuswithablueprintofhowguidance methods are coupled with interactive visual data analysis. Next, we will introduce two examples that illustrate how guidance can be implemented based on the introduced conceptual model. 6.2.2 GuidingtheNavigationinHierarchicalGraphs The first example is about navigational guidance, which we already briefly covered in the introduction in Section 1.2.3. Now, we focus on an approach for guiding the navigation in hierarchical graphs [GST13]. Data, Analytic and Visual Transformation, Exploration A hierarchical graph is a regular graph on top of which a hierarchy defines a nested structure
284 (cid:4) Interactive Visual Data Analysis of clusters. Cuts through the hierarchy define views of the underlying graph at different levels of abstraction. The views correspond to regular plain graphs and as such can be visualized using standard node-link diagrams. Hierarchical graphs can be navigated vertically and horizontally. Vertical navigation changes the level of abstraction and with it the degree of detail shown in the visualization. Different abstractions can be created by expanding or collapsing clusters to include or exclude their individual nodes from the node-link diagram. Horizontal navigation relates to changing which part of the node-link diagram is visible on the screen. Zooming and panning are the typical operations to support horizontal navigation. KnowledgeGap Acomprehensiveexplorationofahierarchicalgraphusually requires numerous horizontal and vertical navigation steps. However, it is not always easy for users to decide on where they should continue the data exploration, which corresponds to a target unknown knowledge gap. Even if users have an idea of the data they want to inspect, it can be difficult to define an appropriate sequence of horizontal and vertical navigational steps to get to the desired target, which is obviously a path unknown knowledge gap. Guidance To assist users during the visual analysis of hierarchical graphs, it makes sense to offer guidance at two degrees. First, orienting guidance can indicate to the user where interesting nodes are located. Second, directing guidance can recommend and provide direct access to nodes that are most worth visiting next based on the current exploration situation. The guidance generation (G*) starts with a search for recommendation candidates. The search takes place in the neighborhood of the data currently being visible to ensure that the recommendations are indeed related to what theuserisseeinginthevisualization.Infact,thesearchinvolvesthreedifferent neighborhoods. The graph neighborhood relating to distances in the graph, the attribute neighborhood concerning similarities among the attribute values associated with nodes, and the visualization neighborhood as defined by node positions in the graph layout. While the graph neighborhood and the attribute neighborhood are given in [D], the visualization neighborhood takes into account [S] and [I]. Once a set of candidates has been collected, the next step is to select a few of them to be suggested to the user. This can be done by means of a degree of interest (DoI) function, as discussed in Section 5.2.1 in the previous chapter. TheDoIfunctionincludesseveralcomponents,suchasaprioridomaininterest given in [D*], user interest from [K], and the distance to the current view as specified in [S]. It further makes sense to model interest degradation for data that have already been visited as stored in [H*]. Finally, the candidates with the highest DoI are presented as navigation recommendations to the user. The visual design should follow a defensive strategy in order to only minimally interfere with regular data exploration.
Advanced Concepts (cid:4) 285 Potential navigation target Recommended navigation target On-screen Enriched Off-screen wedge Figure6.10 Navigation recommendations for graph visualization. Only when users have difficulties in determining a good next navigation target on their own should their attention shift to the navigation recommendations. Figure 6.10 utilizes the enriched wedges introduced in Section 4.5.3 in Chapter 4 to guide users. The wedges themselves serve as visual cues [C*] that indicate direction and distance of recommended targets for horizontal navigation.ThebarsinthewedgesvisualizethecomponentsoftheDoIfunction to make clear to the user why a target is recommended. Recommendedtargetsforverticalnavigationarebydefinitionnotcontained in the currently visualized graph cut, and hence they cannot be pointed at with an enriched wedge. Therefore, visual cues for vertical navigation recommendations are attached to anchor nodes whose expansion (or collapse) would make the recommended target visible. In Figure 6.10, mildly pulsing rings around anchors suggest that an expand (outward pulsing) or a collapse (inward pulsing) operation will uncover a target of interest. Both the enriched wedges and the pulsing rings serve a second purpose. Eachofthemisassociatedwithanavigationshortcuttobeusedasoption[O*] for the data exploration. If the user decides to follow a shortcut by clicking or taping on it, the cut through the hierarchical graph and the view on the graph layout are automatically set such that the associated target becomes visible. This enables the user to get to the target without traversing the path to it manually. Meanwhile, new navigation recommendations are prepared in the background based on the new analysis situation. Should further assistance be necessary, the system can suggest new targets at once. In summary, this first example of guided visual analysis illustrated how users can be assisted in making informed navigation decisions when exploring hierarchical graphs. Next, we will look at an approach for guiding the visual analysis of large heterogeneous data.
286 (cid:4) Interactive Visual Data Analysis 6.2.3 GuidingtheVisualAnalysisofHeterogeneousData The second example investigates how guidance can help physicians exploring heterogeneous biomedical data [Str+12]. The visual analysis is conducted to inform the treatment of newly diagnosed cancer patients. Data,AnalyticandVisualTransformation,Exploration Asillustratedin Figure6.11,thedatainvolvedinthetreatmentplanningarediverse.Physicians havetoconsultpatient-relateddatafromdifferentsources,includinganamnesis information, MR, CT, and X-ray images, tissue samples, and lab results. Moreover,generalbiomedicaldatahavetobeconsidered,includingproteinand gene expression data, pathways and published articles from medical databases. The visual analysis of such heterogeneous data naturally involves diverse analytic computations and different visual representations. For example, multi- variate patient data usually need to be filtered before they can be represented inatabularorparallelcoordinatesvisualization.Geneexpressionsaretypically clustered before being represented in dendrogram heatmaps. During the data analysis, physicians carry out different exploratory tasks inastep-by-stepfashion.Forexample,theybrowsepatientdatatofindsimilar cases,studyanamnesistranscriptstounderstanddiagnoses,andinspectrelated publications, gene expressions, and pathways to learn about latest research results. In the end, the objective is to determine the treatment for a patient. Knowledge Gap In the first place, it must be decided which particular part of the heterogeneous data is to be investigated to accomplish the task at hand. Second, suitable analytic and visual transformations must be employed to make the relevant information visible. Third, individual analysis steps must be sequenced properly to gain comprehensive insight into a case. Physicians who are not visualization experts might find these questions difficult to deal with, resulting in target unknown and path unknown knowledge gaps. Guidance The guidance generation (G*) relies on a tailored domain model [D*]. It is constructed by data analysis experts in an extensive modeling phase prior to the visual data analysis. The modeling starts with the individual subsets of the heterogeneous biomedical data. They are shown as larger boxes in Figure 6.11. Each part of the data is then annotated. The annotations indicate which analytical and visual tools can process certain data, and which tasks can be performed by inspecting them. Finally, links are established between the individual parts of the data to model workflows as sequences of analysis tasks. Each workflow forms the basis for a concrete analysis session that pursues a specific goal. The result of these efforts is a tailored domain model [D*], which will be the basis for guiding the visual analysis.
Advanced Concepts (cid:4) 287 Tissue Data Samples View tissue Lab Inspect lab results T CV Results View anamnesis X-ray DV TL DB Record treatment decision Images Anamnesis Determine patients I V CV Browse patients DV GV TL DB Patient Select patients MR Images Information Discard patients I V CV PC TL GV DB Inspect images Segment tumor Diseases CT Images (ICD-10) Search disease View disease description I V CV Protein Gene TL B DB Cluster expression data Expression Expression Inspect expression data Tools Filter expression data HHMM PPCC RR WW HM PC R W IImage View Find gene VVolume View Protein DB Gene DB View gene information TTissue View TLTable Lens (NCBI) (NCBI) Explore related pathways PCParallel Coordinates Find pathway GVGlyph View TL B DB TL B DB HMHeatmap Investigate related articles NLNode-link Diagram Pathways BBrowser Publications (KEGG, BioCarta) DVDocument View Tasks DBSQL Query (PubMed) NL B DB CVOpen CV RR Toolkit B DB WWEKA Toolkit Figure6.11 Tailored domain model as the basis for user guidance. Adapted from [Str+12]. Twotypesofguidancesupportthephysicians.Orienting guidance isoffered to help physicians keep track of the data that have already been explored and those data that are still to be examined. To this end, the current state of the analysis workflow and the data that have already been analyzed are indicated through visual cues [C*]. In addition, directing guidance recommends tasks to be accomplished next. The corresponding options [O*] are based on the current analysis path through the domain model. Altogether, the guidance provides physicians with information about where they are, what they have already done, and what they can do now. Figure 6.12 illustrates the described approach. A series of larger symbols at the bottom of the figure depict the analysis path taken. Possible options for continuing the analysis path are shown as smaller symbols in the bottom-right corner. The highlighted red option hints at the recommended next step. The actual visual analysis is carried out with the visual representations shown in the central part of the figure. The view for the current analysis step faces the user, whereas other views are tilted. Visual links assist the physicians in relating the information being displayed in the different views. Aswehaveseen,guidancecanbeassimpleassuggestingnavigationalsteps, but also as comprehensive as coordinating many different aspects, including views, methods, and tasks based on domain-specific workflow models.
288 (cid:4) Interactive Visual Data Analysis Figure6.12 Depictingtheanalysispath(bottom)andprovidingrecommended next steps (bottom-right) can guide the visual analysis of heterogeneous biomedical data. Reprinted from [Str+12]. We can conclude that guidance has the potential to improve the human- computer cooperation in interactive visual data analyses. Guidance has to be unobtrusive to the user, and adaptive to the particular context, as the type of assistance a user requires varies and depends on many factors. Good guidance provided in difficult analysis situations allows users to focus more on a deeper understandingoftheinterestingphenomenainthedata,ratherthanthemeans employed for acquiring the understanding. 6.3 PROGRESSIVE VISUAL DATA ANALYSIS In general, interactive visual data analysis depends on a smoothly running loopofhumananalyticalthinkingandsystem-generatedvisualrepresentations. Guidance, as described in the previous section, helps us keep the loop alive on the human side. Let us briefly return to van Wijk’s original model, but now depicted with a slightly different focus in Figure 6.13. We can see how knowledge [K] is generated step-by-step through perception (P), as indicated by dK/dt. Similarly, interactive exploration (E) changes the transformation specification [S] incrementally, which is denoted as dS/dt. Guidance aims to keep these human-oriented processes running. But we can also see that there is one more process participating in the loop. It is the system-oriented transformation (T) of data [D] into images
Advanced Concepts (cid:4) 289 DATA ANALYTIC AND VISUAL USER TRANSFORMATION ? dK/dt D T I P K Data Transf. Image Percept. Knowl. dS/dt S E Spec. Explor. Figure6.13 Incremental processes highlighted in van Wijk’s model of visual- ization. Adapted from [vWij06]. [I] based on the specification [S]. In van Wijk’s model, in our book so far, and in the vast majority of the literature in general, this transformation is considered to be a single, non-incremental step. A corresponding dI/dt is therefore missing. However, transforming large and complex data can require considerableprocessingtime,whichcanleadtoastutteringanalysisloop.This computational challenge of interactive visual data analysis can be addressed by introducing progressive means, which will be discussed in this section. So, in addition to human aspects, the system architecture is critical for a smooth interactive visual data analysis. Heer and Shneiderman make the following statement on the engineering of efficient visualization infrastruc- tures [HS12]: “Especiallyforlargedatasets,supportingreal-timeinteractivityrequirescare- ful attention to system design and poses important research challenges rangingfromlow-latencyarchitecturestointelligentsamplingandaggregation methods.” HeerandShneiderman,2012 Bringing in line the cost involved in generating visual representations with the need to present visual feedback immediately is a major technical chal- lenge. A straightforward monolithic implementation of the classic visualization pipeline might not be sufficient in this regard. Any analytical computation, mapping transformation, or graphical operation along the pipeline that fails to deliver results within interactive response time will disrupt fluid interactive analytic work [LH14]. What is needed is an architecture that can cope with complex, time- consuming computations, while still being able to react to interactive user requests and to provide rich visual feedback. One option to address this need is to implement what is called progressive visual data analysis. That is, the
290 (cid:4) Interactive Visual Data Analysis datatransformation(T)isrealizedinaprogressivefashion,whichconceptually replaces the question mark in Figure 6.13 with the missing dI/dt. Next, we introduce the basics of progressive visual data analysis. First, we consider fundamental conceptual aspects. Then we outline a multi-threading architecture for implementing progressive solutions. Finally, we discuss key scenarios where progressive means can enhance the visual data analysis. 6.3.1 ConceptualConsiderations The key idea behind progressive visual analysis is to generate partial visu- alization results of increasing completeness and correctness [SPG14]. There are two options for breaking down time-consuming computations on large data [Sch+16]: • Subdivide computations into smaller steps. • Subdivide data into smaller chunks. Performing calculations on smaller chunks of data or in smaller computa- tional steps has three key advantages with respect to: • responsiveness of the system, • transparency of the involved calculations, and • control of the visual analysis. Taking smaller steps on smaller data chunks allows a system to be respon- sive to interaction requests of the user. Long-running tasks, which block the system, are avoided and the latency between user request and system response isreduced.Showingpartialresultsinaprogressivefashionalsofacilitatestrans- parency.Particularlyanalyticalmethodsoftencomeasblack boxes whoseinner workingsremainundisclosedtotheuser.Throughprogressiveapproaches,users can observe and thus better understand how analytical calculations converge to a final result. Responsiveness and transparency together improve control of the overall analytic process. Already before the final result is produced, users cansteertheanalyticprocess,forexample,byprioritizingregionsofinterestor stopping calculations whose partial results did not yield any fruitful insights. ModelingtheSubdivisionofComputationsandData In the following, we describe how subdivisions of computations and data can be modeled conceptually [Sch+16]. To this end, we build upon the data transformation as introduced in Section 2.3 of Chapter 2. Originally, we described the data transformation as a pipeline of monolithic operators that carry out computations on the entire data and pass on a single result to subsequent operators. Now, for progressive visual data analysis, the notion of operators and transitions between them needs to be refined to accommodate smaller computational steps on chunks of data.
Advanced Concepts (cid:4) 291 Monolithic Monolithic Data Data operator transition chunking buffering Progressive Progressive Increasing Reducing operator transition granularity granularity Figure6.14 Extended notation for operators and transitions for progressive visual data analysis. Adapted from [Sch+16]. Progressive Operators Progressive operators realize a sub-division of the computational process. To make this clear, the operator notation is extended to include small marks that represent the generated results. As illustrated in Figure 6.14, a monolithic operator generates only a single, complete result. On the other hand, a progressive operator can generate multiple partial results of increasing quality, indicated by increasingly darker shades of gray, until the complete result is produced. How many results can theoretically be produced is operator-dependent. A prerequisite for more than a single complete result is that the underlying algorithms are able to produce results incrementally. Often, classic implemen- tations of known algorithms do not have this property. For example, a sorting algorithm usually does not provide intermediate results. Therefore, it can be necessary to utilize or develop adapted variants that fulfill said requirement. How many partial results should sensibly be produced is user-dependent, because different analytic tasks require different update rates. For example, it may be more helpful to show only those partial results that differ at least by a given ∆ from the previously shown one, rather than showing each and every possible partial result. In any case, each partial result must be a valid input to the subsequent operatorthatfollowsdownthetransformationpipeline.Thepassingonofdata and results is modeled via progressive transitions. Progressive Transitions For the classic pipeline, we have not talked about transitions between operators per se, because they trivially transfer the full data. Yet, for progressive visual data analysis, transitions deserve special attention, because they model the subdivision of the data. In Figure 6.14, a progressive transition can be recognized by a series of marks along its arrow line. These marks denote the data flow to be a stream of data chunks. The distance between the marks indicates the size of the chunks. The smaller the space between consecutive marks, the smaller is the chunk size. To create data chunks in the first place, a transition for data chunking is required. There are different strategies for chunking the data: • Incremental Chunking. The data are sub-divided into disjoint subsets. Eachsubsetcontainsasampledversionoftheoriginaldata,andtheunion
292 (cid:4) Interactive Visual Data Analysis of all subsets represents the entire dataset. Ideally, incremental chunking leads to the key features of the data being visible in the progressive visualization early. The challenge is to find a suitable sampling strategy and an appropriate order for processing the chunks. • Semantic Chunking. The subdivision is based on the semantics of the data and the visualization. For example, for a geographic visualization of movement data, it makes sense to deal separately with geographic boundaries, streets, map tiles, and movement trajectories. Small and relevant semantic chunks should be processed early, whereas larger and less-relevant chunks can be queued in the data stream later. • Level-of-Detail Chunking. The data are sub-divided at different levels of granularity. The result is a hierarchy of chunks, where the root node representstheentiredatasetatahighlevelofabstraction,andtheleaves represent smaller subsets with details at finer granularity. Level-of-detail chunking can be done with respect to the data space or the view space, forexample,byhierarchicaldataabstractionormulti-resolutionmethods, respectively. The outlined chunking strategies can also be applied in combination. For example,fortheprogressivevisualizationofalargegraph,itcanmakesenseto create a graph hierarchy where each chunk represents the graph at a different level of abstraction. Per chunk in the hierarchy, nodes and edges can be organized in separate semantic chunks. These can further be sampled into subsets for incremental chunking. The conceptual counterpart of data chunking is data buffering. It accu- mulates data chunks or partial results to produce the full data or complete results. For example, visualization images can work with buffering in that they accumulate several partial results of an upstream operator before showing a meaningful or complete result. To make progressive operators compatible in terms of the granularity of data chunks, it also makes sense to introduce transitions that increase or decrease data granularity along the transformation pipeline. Such granularity changes can be necessary, for example, when one operator requires larger data chunks to create partial results, while a subsequent operator is only able to process smaller pieces of data. DesigningProgressiveVisualDataAnalysis With the introduced notations, we can now design a progressive analytical and visual transformation of data into images. Figure 6.15 shows a simple example pipeline [Sch+16]. First, the data are sequenced into chunks, each of them is analytically processed by a monolithic similarity search. The partial results of thesearcharethenmappedtoascatterplot.Inordertoproduceadensitymap, the partial scatter plots are further sub-divided into smaller chunks. Then,
Advanced Concepts (cid:4) 293 D I Data Image Similarity search Scatter plot Density map Figure6.15 Simpleexampleofaprogressivetransformationpipeline.Adapted from [Sch+16]. the density map operator generates partial results with increasing quality. Finally,thevisualizationimagebuffersallpartialresultsintoacompletevisual representation of the original data. The design process of such progressive visualization pipelines follows the generaldesignstepsforregularvisualizationsolutionsasdescribedinSection2.3 of Chapter 2. Still, there are some peculiarities that deserve special attention. Meaningful Partial Results First and foremost, a key requirement of pro- gressive visual data analysis must be kept in mind: The employed progressive means should generate meaningful partial results [Sch+16]. That is, the par- tial results should be interpretable and lead to valid partial insight. What meaningful means concretely depends on the application at hand. Oneoptiontoobjectivelyquantifymeaningfulnessaremetricsthatmeasure the quality of partial results. Quality metrics can be calculated with respect to the data or their visual representation. A very simple quality metric is the ratio of the already processed data. The more data have been processed, the higher is the quality. For another example, if a progressive computation involves some statistical error, for example due to the employed data chunking, the error could be used as a quality indicator. Also the differences between successive partial visual representations can indicate result quality. If the difference is rather low, not much new insight can be expected. On the other hand, a significant change between two visual representations could mark the beginning of a progressively unfolding visual pattern. Progress, error, and differences are valuable indicators that can support users in judging the partial results and steering the analysis. Therefore, design- ers of progressive visual data analysis solutions should incorporate suitable quality metrics and consider visualizing them along with the actual data. Progress bars should be a mandatory element in the visualization interface. DesignPerspectives Anotheraspectofthedesignistodecidewhereandhow operators and transitions should employ progressive means. These decisions have to take two perspectives into account: the input perspective and the output perspective. Input Perspective From an input perspective, the data and the involved transformation calculations are relevant. If the data do not fit into the memory, progressive data chunking needs to be considered. If, on the
294 (cid:4) Interactive Visual Data Analysis other hand, the data are highly structured, it might be rather difficult to come up with a reasonable sub-division strategy for the data. In terms of computations,iftheruntimeofanoperatorisratherlong,itmakessense to replace it with a progressive alternative. Yet, if a time-consuming operatorproducesmerelyoptionaldecorations,itmaybemoresensibleto leave the operator as is, because it is acceptable to wait for the operator to finish in the few cases this is indeed needed. Output Perspective From the output perspective, the tasks and interests of the analyst are relevant. The designer needs to know whether the analysis follows an overview-first strategy or a detail-first strategy. With an overview-first strategy, the goal is to show the complete data as quickly as possible, where a lower degree of detail is initially acceptable. Following this strategy means progressively presenting data abstractions with increasing granularity. First, a coarse overview is displayed based on which the user can start interpreting the data. Looking into regions of interest and studying specific details becomes possible as more and finer-grained information is being transmitted progressively. In contrast to that, the detail-first strategy prioritizes highly detailed representations, where it is acceptable to see only subsets of the data in the beginning. The detail-first strategy progressively visualizes chunks of theoriginaldata,ratherthandataabstractions.Oncethefirstchunkhas been transmitted, a detailed view of a specific piece of data is presented to the user. During the progression, more and more pieces will be added until the visualization shows the complete data. Note, however, that the visibility of details is gradually diminished the more data chunks are being presented. In summary, this section described the basic building blocks of progres- sive visual data analysis and the design requirements and perspectives to be considered. Yet, these conceptual aspects are only one part of progressive visualdataanalysis.Anotherimportantpracticalquestionishowtoimplement the underlying progressive software. This question will be addressed in the following. 6.3.2 Multi-threadingArchitecture Progressive visual data analysis software must be able to generate and show partial results while still being responsive to user requests. Therefore, it is practical to follow a multi-threading architecture [Pir+09]. A schematic outline of such an architecture is given in Figure 6.16. It consists of the data, the specification, and the image artifacts, which we already know from earlier figures. Additionally, there are two central computing components: the control thread and the processing threads.
Advanced Concepts (cid:4) 295 D Access Processing Update III Data Threads ImImI amagagegee HUMAN Start Invalidate Access Terminate S Modify Control Spec. Thread Figure6.16 Amulti-threadingarchitectureforprogressivevisualdataanalysis. Control Thread The control thread is in charge of receiving user input and coordinating the other components. As these tasks are usually easy to accomplish, the overall system stays responsive at all times. The typical workflowofthecontrolthreadisasfollows.Whenaninteractionrequestarrives, the control thread modifies the specification and invalidates the visualization. Additionally, any processing threads that operate with the old specification are terminated early because they would no longer generate valid, but obsolete results.Finally,newprocessingthreadsarestartedtogeneratenewvalidvisual output. Start Process Thread #1 Thread #2 Thread #3 Refine Transmit Figure6.17 Illustrationofasynchronousprocessingthreadsoperatingondata chunks stored in priority queues. Processing Threads Theprocessingthreadsimplementtheprogressiveoper- ators introduced before. As illustrated in Figure 6.17, these threads have a priority queue of data chunks to be processed. When incoming data chunks are appended to the queue, the processing thread starts working. It removes a data chunk from the queue and processes it in an internal loop until a given break condition is reached. This condition can be based on the processing time spent or on the quality of the partial result. If the partial result is of sufficient quality, it can be transmitted to the subsequent thread’s priority queue. If the result is not yet good enough, it is re-appended to the thread’s own queue for further refinement. The different shades of gray in Figure 6.17 indicate how close the queued chunks are to the final result. These shades correspond directly to the marks in the progressive operators, where white stands for unprocessed chunks, gray for intermediate results, and black for the final result.
296 (cid:4) Interactive Visual Data Analysis Human Computer Human Control Processing Request Request Partial result Partial result Partial result Interaction latency Result Complete result Feedback latency (a)Regular single-thread solution. (b)Progressive multi-thread solution. Figure6.18 Comparison of single-thread and multi-thread solutions. Assoonasmeaningfulresultsreachtheendofthepipeline,thevisualization gets updated. Here it makes sense to organize the visual output into multiple layers according to the different data chunking strategies (semantic layers, incremental layers, or level-of-detail layers). Using multiple layers enables the architecture to provide rich and scalable visual feedback, and to avoid redundant computations by reusing cached results that remain valid after a user interaction. If a processing thread’s queue is empty, the thread stops working. A processing thread can also be terminated early at any time by the control thread, either by emptying its queue or by dismissing thread and queue altogether. A key advantage of the described multi-threading architecture is the reduc- tion of latency by generating partial results. This is made clear in Figure 6.18. The classic monolithic approach is depicted in Figure 6.18a. As indicated by the colored vertical bars, the human has to wait for a while until a single complete response is generated. While the system is busy producing the com- plete response, the system is unresponsive and no feedback about the ongoing computations is delivered to the user. InFigure6.18b,wecanseehowtheprogressivemulti-threadingarchitecture reduces the latency. The interaction latency is much shorter and the system remains responsive at all times, because the control thread handles only brief coordinationtasks.Theprocessingthreadworksonthedatainthebackground andprovidesmeaningfulpartialresultsassoonastheyareready,whichreduces the feedback latency. Up to this point, we have discussed how progressive visual data analysis can be modeled, designed, and implemented. The next section will illustrate how progressive means can be put to use to support a smooth interactive visual data analysis.
Advanced Concepts (cid:4) 297 Preprocessing Mapping Rendering Operators Operators Operators Data Analytical Visual Image Values Abstractions Abstractions Data Value Analytical Visual Image Operators Operators Operators Operators Progressive Progressive Progressive data processing visualization display Figure6.19 The three typical scenarios of progressive visual data analysis: progressive data processing, progressive visualization, and progressive display. 6.3.3 Scenarios Operating progressively is a general approach and any phase of the analytical andvisualdatatransformationcanbenefitfromit.Dependingonwhichphases primarily utilize progressive means, three typical scenarios can be identified: progressive data processing, progressive visualization, and progressive display. Figure 6.19 illustrates the three scenarios and their relation to the phases of the visualization pipeline. Progressive data processing subsumes all data chunkingandcomputationaloperatorsthatworkondatavaluesandanalytical abstractions. Progressive visualization covers chunking and operators that are used to create and manipulate visual abstractions. Finally, progressive display deals with chunking and operators at the level of image data. Next, we discuss the three scenarios of progressive visual data analysis in more detail. ProgressiveDataProcessing Given the fact that progressive data processing takes place in the beginning of the analytical and visual transformation, it is primarily concerned with the input perspective, that is, the properties of the data and the calculations. Progressive data processing is typically applied to cope with two challenging situations. First, if the data are too large to be processed in a timely manner all at once, progressive data processing can provide early feedback that allows users to see in which direction the calculations are going, and if necessary, to steer the calculations toward more desirable results. Second, if the data processing involves algorithmic black boxes or the data are unknown, looking at progressively generated partial results can help users better understand both the data and the operations performed on them. Progressive value operators, preprocessing operators, and analytical opera- tors aim at an efficient and effective handling of the data by sub-dividing the involved computations. An example of a progressive value operator is adaptive
298 (cid:4) Interactive Visual Data Analysis sampling. Adaptive sampling delivers data samples successively and allows userstoadaptthesamplingstrategyonthefly.Thisway,therepresentativeness of data being analyzed can be improved based on insight gained from early partial results. Progressive preprocessing operators transform data values into analytical abstractions in a step-by-step fashion. Chapter 5 discussed several fundamen- tal methods for creating analytical abstractions. Many of them can also be employed in a progressive setting. For example, if the data to be analyzed do not fit into memory, a principal component analysis can be done progressively. In particular, the involved singular value decomposition can be computed in incremental steps [Sar+02]. For another example, if a k-means clustering is part of the analysis pipeline, but users are unfamiliar with the data and hence do not know what a suitable k is, it makes sense to employ iterative clustering [Kim+17]. It allows users to adjust the number of clusters or the positionofclusterrepresentativesontheflywhilealreadyexaminingthepartial clusters being produced. Progressive analytical operators are concerned with computations on the derived analytical abstractions. This can involve structuring, organizing, and prioritizing the abstractions. For example, data abstractions can be organized in hierarchical data structures. When traversing such data structures, it is sensible to prioritize data chunks representing larger data deviations early. As we know from Section 5.3.2 of Chapter 5, larger deviations tend to add more information to the visualization than chunks with only smaller changes. The progressive operators described so far sub-divide the computations on data. Yet, with very large data, it is hardly possible to deliver results in interactive frame rates. Therefore, a second key concern of progressive data processing is to sub-divide the data into chunks using a suitable strategy (incremental chunking, semantic chunking, or level-of-detail chunking). The following example will briefly illustrate the positive effect that chunking can have on the visual data analysis. The example concentrates on the first part of the progressive pipeline presented earlier in Figure 6.15. The dataset to be processed comes from the Fatality Analysis Reporting System of the US National Highway Traffic Safety Administration and contains information about more than 370,000 car crashes between 2001 and 2009. When analyzing the crashes with respect to certain criteria, a similarity search must be performed on the data. Yet, searching the whole dataset is computationally expensive. Therefore, the search operates on chunks of 5,000 crashes instead. This ensures reasonably quick feedback for interactive adjustments [Sch+16]. Figure6.20aillustrateshowthechunkedsearchprogressivelyfillsthedisplay withcrashesmatchingthesearchcriteria.Duringtheprocess,userscanupdate the criteria as needed. Moreover, they can define a region of interest in order to prioritize the data chunks associated with that region. Such a prioritized progression is illustrated in Figure 6.20b. The effectiveness of the visual data analysis is increased because interesting data can be inspected early.
Advanced Concepts (cid:4) 299 First First reliable Reliable Mature Final partial result partial result partial result partial result result 20 Iterations 120 Iterations 180 Iterations 220 Iterations (a)Regular progression of chunks. First User Reliable Mature Final partial result selection partial result partial result result 20 Iterations 34 Iterations 57 Iterations 88 Iterations (b)Prioritized progression of chunks. Figure6.20 Visualizationofprogressivelyprocesseddatachunksofcarcrashes from a database with more than 370,000 entries. Courtesy of Marco Angelini. This section provided just a glimpse on what is possible in terms of pro- gressive data processing. Next, we will discuss the scenario of progressive visualization. ProgressiveVisualization Progressive visualization is primarily concerned with the graphically oriented transformation steps that map data to visual abstractions and render them to visualization images. Progressive visualization is typically applied when the amount of data or visual abstractions is excessively large, or if the involved mapping and rendering operations are computationally expensive. In such cases, it is usually not feasible to process everything in a single step. Even if it was, the visualization could be too dense for the user to interpret it with ease. Progressive visualizations utilize chunking and progressive computations to buildupvisual representationsstep-by-step.Theindividual steps arecomputa- tionallyeasiertodealwithandtheuserhasachancetoobservetheconstruction process and hence can gain a better understanding of the visualization design and the data. Picking up on the output strategies overview-first and detail-first discussed earlier, we will next present two examples that illustrate progressive visualiza- tion. Progressive Force-directed Graph Layout As we know from Section 3.5 of Chapter 3, node-link diagrams are a common representation for graphs. Nodes are basically represented as dots and edges are shown as links between
300 (cid:4) Interactive Visual Data Analysis ^ 1 Iteration 5 Iterations 10 Iterations 50 Iterations 100 Iterations Figure 6.21 Progressive force-directed layout of a social network with 747 nodes and 60,050 edges. dots. These visual abstractions are easy to create. Yet, a key challenge is to determine a suitable layout of the dots. This requires sophisticated graph layout algorithms, which are usually computationally expensive. For our example, we utilize the force-directed layout approach, which is known to generate visually pleasing results and also to have a high runtime complexity due to the involved simulation of the forces. Yet, as the simulation is iterative by nature, it perfectly fits the scenario of progressive visualization. Followingtheoverview-firststrategy,theprogressionstartswitharoughlayout thatcontainstheentiregraph.Witheachiterationofthesimulation,thelayout is refined until a high-quality output is obtained. High quality means that the simulated forces are in equilibrium. Figure 6.21 shows a series of snapshots of a progressively unfolding node- link diagram of a social network with 747 nodes and 60,050 edges. One can nicely observe how the layout algorithm separates the key structures in the data between the 5th and 50th iteration. ProgressiveNetworkMapping Oursecondexamplewillillustratethedetail- first strategy applied to a climate network with 6,816 nodes and 232,940 edges. Climate researchers use such networks to study climate change on Earth. The nodes of the network span a grid across the globe, so there is no need to compute a graph layout. Yet, the number of visual abstractions is relatively high. For our example, we visualize the climate network on a 3D globe. Each node is represented as a 3D sphere and each edge is shown as a 3D curved link. Creating these 3D visual abstractions poses significant stress on the graphics hardware. Therefore, it makes sense to process the data in chunks. First, semantic chunking is applied: The nodes form one chunk and the edges form
Advanced Concepts (cid:4) 301 Nodes being added Edges being added Figure6.22 Progressive visualization of a climate network with about 6,816 nodes and 232,940 edges. another chunk. Because there are still very many nodes and edges per chunk, an additional sub-division by incremental chunking takes place. Eventually, we obtainsevenchunksofnodesand233chunksofedges,eachholdingathousand dataelements.Theprogressivevisualizationwillprocessthesechunksoneafter the other. The result is depicted in Figure 6.22. According to the detail-first strategy, the progression starts with full details, but only for a part of the data. The node chunks come first followed by the chunks of edges. The first pieces of data are presented to the user after about five seconds. Without progressive chunking, the user would see a blank screen for about 40 seconds. The above examples illustrate quite well how progressive visualizations graduallyimprovethevisualoutputbeforetheyultimatelyendinarefinedand completefinalresult.Thegradualbuild-upofvisualrepresentationscontributes to a better understanding of the visualization and the data. ProgressiveDisplay Lastbutnotleast,theprogressivedisplayofvisualrepresentationsisascenario where progressive techniques are employed to support the visual data analysis. Thisscenariobringsusclosertomethodsforprogressiveencoding,transmission, and decoding of image data, which have a long history in computer graphics research. Progressive display techniques are particularly interesting in environments with limited connection bandwidth and in environments with heterogeneous outputdevices,suchasthesmartmeetingroomdescribedinSection6.1.Based on a well-prepared stream of image data, progressive techniques can ensure that important image characteristics are transmitted first, whereas additional
302 (cid:4) Interactive Visual Data Analysis details are progressively fetched to complete the view. Moreover, individual output devices can acquire exactly the amount of image data they need for their particular size and resolution. Smaller low-resolution devices can stop the transmission early, whereas larger high-resolution displays extract the full pixel information from the data stream [Ros+11]. Figure6.23illustratessuchadevice-dependentprogressivetransmissionofa treemapvisualization.Thetreemapistobedisplayedonheterogeneousdisplays inasmartmeetingroom.Notethatthereisonlyonestaticvisualizationimage. Theprogressiveimageviewerrunningoneachdisplaywillextractandtransmit exactly the pixels that match the device resolution. Progressive display is not limited to device-dependent adaptations. It is also possible to display visualization images progressively based on user interests [Rad+15]. For example, when a discussion in a smart meeting room shifts from a global perspective to local areas, the progressive display can provide further image data allowing users to see the areas of interest at higher resolution. Figure 6.24 shows an example with a color-coded map of the world. Once theuserhasmarkedaregionofinterest,additionalimagedataareprogressively transmitted to enhance the visualization. It will then be shown either with an overview+detail approach or as a focus+context display. Both overview+detail andfocus+contexthavealreadybeenintroducedasfundamentalvisualarrange- ments in Section 3.1.2 of Chapter 3. Note again that the progressive display operates only on the visualization image. There is no need to reprocess the entire dataset in order to obtain interest-dependent views of it. Whenever user interests change, new regions of interest can be marked and additional image data is progressively transferred accordingly. Technically, a key requirement is that the visualization images be encoded in such a way that progressive image transmission and refinement are possible. ProgressiveJPEG,forexample,canpresentanoverviewoftheimageandthen refine the image successively. The JPEG2000 standard adds support for the Image Progressive Heterogeneous File Transm ission Display Devices Figure6.23 Device-dependent progressive transmission of a treemap visual- ization image. Adapted from [Ros+11].
Advanced Concepts (cid:4) 303 (a)Global view. (b)Overview+detail view. (c)Focus+context view. Figure6.24 Progressive display for a dynamically defined region of interest. Courtesy of Axel Radloff-Delosea. definition and prioritized transmission of regions of interest. The two examples of the device-dependent treemap visualization and interest-dependent map visualization are based on JPEG2000-encoded image data [RS09]. With the three scenarios of progressive data processing, progressive visual- ization, and finally progressive display, this section on progressive techniques for interactive visual data analysis is complete. Overall, allowing fluid inter- action and providing timely feedback are important aspects for visual data analysis. Studies provide first evidence of the positive impact of progressive techniques [Zgr+17]. Users can generate more insights per minute and are more agile in their interactions when working with progressive visualizations. Meaningful partial results can be employed to pre-process information, extract insights early, or decide quickly that an alternative visualization could be more useful for the task at hand. All these benefits suggest that progressive visual data analysis is an important approach when exploring large datasets. 6.4 SUMMARY Research on interactive visual data analysis is advancing on various fronts. In this chapter, we discussed a selection of three advanced topics: advanced visualization in multi-display environments, guidance as advanced support for interactive analysis, and progressive techniques for advanced processing of analytical and visual data abstractions. It is in the nature of such advanced topics that they have typically not yet been fully investigated in research. There are still open issues to be discussed and remaining challenges to be addressed. Recent works on multi-device visualization further study the seamless integration of multiple devices to form coherent visual analysis environ- ments [Kis+17; LKD19; Hor+19]. Interesting questions are how to best utilize displays of different form factor and their different interaction modalities, how to support collaborative exploration in dynamic multi-devices ensem- bles, and how to technically cope with a heterogeneous zoo of platforms and infrastructures.
304 (cid:4) Interactive Visual Data Analysis Withrespecttoguidanceforinteractivevisualdataanalysis,moreresearch is needed on designing and implementing actual guidance solutions [Cen+17]. This involves questions such as: how can knowledge gaps and the required degree of guidance be inferred, when is the right moment to provide guidance, what are suitable methods to convey guidance to the user, and how can the success or failure of guidance be evaluated? As indicated in the previous section, progressive techniques bear much potential to enhance interactive visual data analysis. Yet, research on this topic is still young and there are many questions to be investigated [Fek+19]. Already the conceptual definition and fundamental theories behind progressive techniques are subject to discussions. Moreover, the benefits and also threats of progressive techniques need to be studied in more depth from a human perspective. As a conclusion, we can state that the advanced methods discussed in this chapter illustrate quite nicely how data analysis can go beyond plain visualization, interaction, and computation. Yet, it is also clear that these advanced methods are not yet mature and that it will certainly need some time before they appear in regular data analysis solutions for day-to-day use. FURTHER READING Multi-display Visualization: [CD04] • [Chu+15] • [Rad+15] • [EST19] Guidance: [Hor99] • [Cen+17] • [Col+18] ProgressiveAnalysis: [Fis+12]•[Müh+14]•[SPG14]•[BEF17]•[Ang+18]
7 CHAPTER Summary CONTENTS 7.1 What’s been Discussed ..................................... 305 7.2 How to Continue ............................................ 307 T HIS BOOK on interactive visual data analysis comes to an end. We workedthroughsixchaptersonvisual,interactive,andanalyticalmethods for making sense of data. In this final chapter, we will briefly recapitulate the key points of the book and provide you with ideas on how to continue once you put this book down. 7.1 WHAT’S BEEN DISCUSSED In Chapter 2, we introduced fundamental aspects. In the first place, there are the three key criteria of interactive visual data analysis: expressiveness, effectiveness, and efficiency. Expressiveness tells us that visual representations should show us exactly what is in the data and that interaction should allow us to do exactly what is needed to understand the data. Effectiveness calls for the consideration of human factors. Efficiency is satisfied when the costs and the benefits of the interactive visual analysis are balanced. Further, we discussed the data, the tasks, and the context as three key influencingfactorsofinteractivevisualdataanalysis.Whilethedataproperties characterizewhat istobeanalyzed,thetaskaspectisconcernedwithwhy data are analyzed. The context, describes by whom and where the data analysis is carried out. Finally, we studied models of the processes that take place at different stages of interactive visual data analysis. We considered the design process as a cascade of individual design steps, the process of transforming the data into visual representations as a pipeline of operators, and the knowledge generation process as analysis loops that produce new findings and insight. Chapter 3 was dedicated to visualization. We started with basic methods forencodingdatavisuallyandpresentingthemtotheuserinameaningfulway. Visual variables such as position, length, or color hue were introduced as the fundamental means to convey information graphically. In fact, visual variables 305
306 (cid:4) Interactive Visual Data Analysis modify the appearance of graphical marks, which allows us to perceive visual differences and interpret the depicted information. Visual variables and marks are the basic building blocks of visualiza- tion techniques. We introduced many different techniques for different data classes. For multivariate data, we described table-based, combinded bivariate, polyline-based, glyph-based, pixel-based, and nested visualizations. They can be considered the basic tools of the trade. Then we moved on to visualization techniques for temporal data and geo-spatial data, and combinations thereof. We characterized time and geographic space as special dimensions and pre- sentedvariousdedicatedvisualizationstakingintoaccountthespecificsoftime and space. Last but not least, our interest concerned the visualization of graph data. Graphs describe not only data elements per se, but also relations between data elements. Basic graph visualization techniques show these relations as linksbetweenelementsor viacleverarrangementsof elements. Wealsolearned how multi-faceted graphs can be visualized by combining ideas from different visualization techniques. Following the chapter on visualization was Chapter 4 on interacting with visualizations. We begun the chapter with a description of various scenarios where it is useful or even necessary to let the user interact. What “interacting” actually means was abstractly modeled as a cycle of actions. In order to allow this cycle to run smoothly, several requirements and guidelines need to be considered. In terms of interaction for visual data analysis, we built upon basic oper- ations and discussed fundamental selection and accentuation techniques. To address the analysis of large data, a whole section was dedicated to navigating zoomable visualizations. Zooming is a fundamental technique to satisfy the need for an overview of the data and also for finer details of selected parts of the data. Interactive lenses were introduced as a versatile lightweight means for locally adapting visual representations of data. The potential of naturally inspiredinteractionwasillustratedfortheimportanttaskofvisualcomparison. Leaving classic desktop interaction behind, we moved on to modern inter- action technologies and learned how they can enhance visual data analysis. Touch interaction makes working with visual representations of data truly direct. Tangible views provide an extended interaction vocabulary making it possible to analyze data in new and interesting ways. Finally, with proxemic interaction it is possible to explore more and larger data on high-resolution display walls. Chapter 5 taught us that large and complex data also require analytical support through automatic computation. The primary goal was to reduce the complexity of the data and their visual representations. Density-based representationsandbundlingwereintroducedasmethodstoreducecomplexity on the visual side. Reducing the complexity of the data can be done in various ways. We considereddegree-of-interestapproachesandfeature-basedmethodsasameans
Summary (cid:4) 307 to focus on relevant data. Another option is to abstract the data via sampling and aggregation. Data abstraction can be done repeatedly to generate multi- scale data abstractions. Automatic computations can also serve to group data elements. We intro- duced two fundamental approaches: classification and clustering. Classification is a means to sub-divide the data space, and clustering groups data elements according to a certain similarity measure. How complex multivariate dynamic graphs can be clustered was discussed in a separate section. Finally, we described principal component analysis as a tool to address the challengeofanalyzingdatawithverymanyvariables.Thekeyideaherewasto project the original high-dimensional data space to a lower-dimensional space. In the end, Chapter 6 provided an outlook on advanced topics in inter- active visual data analysis research. Following the three-fold structure of the previous chapters, we discussed advanced visualization, advanced interaction, and advanced automatic computation. Presenting data in smart multi-display environments can be considered an advanced form of visualization. This makes it possible for multiple users to analyze data collaboratively. We also saw that several mechanisms must be integrated into the environment to support the data analysis. Userguidance was introduced asanadvancedmeansto supportinteraction when users have difficulties in making analytical progress. We elaborated on a characterization of guidance and a conceptual model. Our last advanced concept considered the use of progressive methods to enhance automatic computation. We made a step from otherwise monolithic operators that generate a single complete result to progressive operators that generate several partial results with increasing quality. 7.2 HOW TO CONTINUE Now that this book is over, we want to point you to ideas of how you can continue with the topic of interactive visual data analysis. We think of four key avenues for your next activities: learn, apply, create, and advance. Learn A first activity could be to go on and study interactive visual data analysisfurther.Certainly,thisbookofferedabroadoverview,butitcouldnot discuss all aspects in full detail. Therefore, each chapter ended with a list of references recommended for further reading. The lists contain excellent books that cover topics we could only touch here briefly. We also included research articles with in-depth investigations of selected topics of interest. Weparticularlyrecommendlearningmoreabouthumanfactors.Perception and cognition are key to interactive visual data analysis. Learning more about them will definitely pay off. Another direction for further studies are the fields of volume visualization and flow visualization. They are concerned with
308 (cid:4) Interactive Visual Data Analysis potentially time-varying 3D volumetric data and vector fields. Such data are particularly relevant in medicine and engineering. Apply If you are a practitioner and have not yet used interactive visual methods,yournextstepmightbetoapplysomeofthemethodsandtechniques discussed in this book to your analytic domain problems. Interactive visual data analysis can be beneficial in all application domains where data are available, which means in virtually any domain. In the financial sector, visual representations can help us understand transactionsordetectfraud.Inurbanplanning,theycanhelpusbuildmodelsof humanmobility.Climateresearchersrelyonvisualrepresentationtoinvestigate the impact of climate change and to communicate their findings to the general public. In humanities, interactive visual data analysis can be applied to reveal hidden relationships in large text corpora. Sociologists can be supported in predicting election outcomes based on visual representations of stance in social media. Even the classic spreadsheet applications already offer basic visualization capabilities. Tableau, QlikView, and Plotly are flagships among the software packages for interactive visual data analysis in general. Many other dedicated tools exist for specific application problems. For example, Gephi, Cytoscape, and Tulip are dedicated to the visual analysis of graphs. KNIME is a software that focuses on the analytics part. As you can see, there are many options to apply the knowledge gained in this book. It is up to you to introduce or strengthen interactive visual data analysis in your particular application domain. Create If you are intrigued by the examples presented in this book or if the existing tools do not meet the requirements of your application problem, go ahead and create your own interactive visual analysis techniques. Depending on your level of expertise, you can extend the software mentioned before or implement a tailored visualization solution from scratch. Readerswithprogrammingskillscanresorttoopensourcesoftwarelibraries. Forexample,D3.jsisthequasi-standardforweb-basedvisualization.Thefollow- up project Observable even allows multiple people to develop visualization in a team effort. Many other visualization toolkits and libraries exist, for example, the Visualization Toolkit (VTK) and Processing for general-purpose applications, GraphStream and sigma.js specifically for visualizing graphs, or mapbox and CesiumJS for geo-visualization. Writing software for visualizing larger data typically requires advanced programming skills. Analytic computations should be distributed on multiple computing threads and the visualization should utilize the enormous power of moderngraphicscards.WorkingonsuchalowlevelwithAPIssuchasOpenMP, OpenCL, OpenGL, WebGL, DirectX, or Vulkan might be more difficult, but it will allow you to visualize millions of data elements at interactive frame rates.
Summary (cid:4) 309 Advance Interactivevisualdataanalysisisarelativelyyoungfieldofresearch. So your next step could be to advance the field. There are still many unsolved problems to be tackled and new discoveries to be made. Visual data analysis has found its way into smart multi-display environments. And it will be used in many different environments, including large display walls, industrial settings,ormobilesmartwatches.Howvisualization,interaction,andautomatic computation can be designed to scale with these heterogeneous environments is an open question. Dealing with ever-increasing data remains a major challenge. Progressive data analysis has been discussed as a suitable means to address this challenge. But we are still lacking the concepts and models to broadly apply progression beyond the simple examples illustrated in this book. Increasing data size and increasing data complexity also lead to more complex tools. How can we support users in handling these tools? Providing user guidance is one option. However, when is the right moment to provide guidance and what is the appropriate amount of guidance with respect to the situation at hand? When we think about guidance for users, we could also consider guidance for designers and developers. How should a visualization be designed given certaindatacharacteristicsandanalysistasks?Usually,comingupwithagood answer requires a good amount of experience. Wouldn’t it be great if we had actionable guidelines to come to our aid? Yes, it would, but we are not there yet. Wecouldgoonlistingresearchquestionsforpages.Asamatteroffact,there is still much to be investigated. The major scientific conferences on interactive visual data analysis testify to this fact. VIS, EuroVis, and PacificVis publish innovative research results annually. Journals such as IEEE Transactions on Visualization and Computer Graphics, the Computer Graphics Forum, and Information Visualization are great for submitting original articles on visual analytics research. In related fields, such as human-computer interaction, cognitive psychology, data mining, or machine learing, it is also possible to publish—and to learn. This concludes our broad overview of concepts, models, methods, and techniques,basicandadvancedones,forinteractivevisualdataanalysis.Given what you have learned from reading this book, you have everything now to make an informed choice on where to venture next in this exciting field. Amongallthepossibledirectionsoflearning,applying,creating,andadvancing interactive visual data analysis, with one being as exciting as the other, you cannot choose wrongly.
Bibliography [AA06] Andrienko, N. and Andrienko, G. Exploratory Analysis of Spatial and Temporal Data – A Systematic Approach.Springer,2006.doi: 10.1007/3-540-31190-4 (cited on pages 31, 127). [Abe+14] Abello, J., Hadlak, S., Schumann, H., and Schulz, H.-J. “A Mod- ular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks”. In: IEEE Transactions on Visual- ization and Computer Graphics 20.3 (2014), pp. 337–350. doi: 10.1109/TVCG.2013.109(citedonpages215,217–219,265,346). [Aig+05] Aigner, W., Miksch, S., Thurnher, B., and Biffl, S. “Planning- Lines:NovelGlyphsforRepresentingTemporalUncertaintiesand theirEvaluation”.In:Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2005, pp. 457–463. doi: 10.1109/IV.2005.97 (cited on pages 93, 94). [Aig+11] Aigner, W., Miksch, S., Schumann, H., and Tominski, C. Visual- izationofTime-OrientedData.Springer,2011.doi:10.1007/978- 0-85729-079-3 (cited on pages 87, 127, 168, 261, 262, 346). [All83] Allen, J. F. “Maintaining Knowledge about Temporal Intervals”. In: Communications of the ACM 26.11 (1983), pp. 832–843. doi: 10.1145/182.358434 (cited on page 83). [AMA07] Archambault, D. W., Munzner, T., and Auber, D. “TopoLayout: Multilevel Graph Layout by Topological Features”. In: IEEE TransactionsonVisualizationandComputerGraphics13.2(2007), pp. 305–317. doi: 10.1109/TVCG.2007.46 (cited on page 118). [AN13] Andrews, C. and North, C. “The Impact of Physical Navigation onSpatialOrganizationforSensemaking”.In:IEEE Transactions on Visualization and Computer Graphics 19.12 (2013), pp. 2207– 2216. doi: 10.1109/TVCG.2013.205 (cited on page 204). [And+10] Andrienko, G., Andrienko, N., Demšar, U., Dransch, D., Dykes, J., Fabrikant, S. I., Jern, M., Kraak, M.-J., Schumann, H., and Tominski, C. “Space, Time and Visual Analytics”. In: Interna- tional Journal of Geographical Information Science 24.10 (2010), pp. 1577–1600. doi: 10.1080/13658816.2010.508043 (cited on page 96). 311
312 (cid:4) Bibliography [And+13] Andrienko, G., Andrienko, N., Bak, P., Keim, D., and Wrobel, S. Visual Analytics of Movement.Springer,2013.doi:10.1007/978- 3-642-37583-5 (cited on pages 127, 225). [Ang+18] Angelini, M., Santucci, G., Schumann, H., and Schulz, H.-J. “A Review and Characterization of Progressive Visual Ana- lytics”. In: Informatics 5.3 (2018), p. 31. doi: 10.3390/ informatics5030031 (cited on page 304). [AS05] Amar,R.A.andStasko,J.T.“KnowledgePreceptsforDesignand EvaluationofInformationVisualizations”.In:IEEE Transactions onVisualizationandComputerGraphics 11.4(2005),pp.432–442. doi: 10.1109/TVCG.2005.63 (cited on page 18). [AS94] Ahlberg, C. and Shneiderman, B. “Visual Information Seeking: TightCouplingofDynamicQueryFilterswithStarfieldDisplays”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1994, pp. 313–317. doi: 10.1145/191666.191775 (cited on page 149). [Asi85] Asimov, D. “The Grand Tour: A Tool for Viewing Multidimen- sionalData”.In:SIAM Journal on Scientific and Statistical Com- puting 6.1 (1985), pp. 128–143. doi: 10.1137/0906011 (cited on page 69). [AWP97] Andrews, K., Wolte, J., and Pichler, M. “Information Pyramids: A New Approach to Visualising Large Hierarchies”. In: Proceed- ings of the IEEE Visualization Conference (Vis). Late Breaking Hot Topics. IEEE Computer Society, 1997, pp. 49–52 (cited on page 117). [Bac+17] Bach, B., Dragicevic, P., Archambault, D. W., Hurter, C., and Carpendale, S. “A Descriptive Framework for Temporal Data VisualizationsBasedonGeneralizedSpace-TimeCubes”.In:Com- puter Graphics Forum 36.6 (2017), pp. 36–61. doi: 10.1111/cgf. 12804 (cited on page 127). [Bat+99] Battista, G. D., Eades, P., Tamassia, R., and Tollis, I. G. Graph Drawing: Algorithms for the Visualization of Graphs. 1st edition. Prentice Hall, 1999 (cited on page 113). [BC87] Becker, R. A. and Cleveland, W. S. “Brushing Scatterplots”. In: Technometrics 29.2 (1987), pp. 127–142. doi: 10.2307/1269768 (cited on pages 149, 157). [BCK08] Boriah, S., Chandola, V., and Kumar, V. “Similarity Measures forCategoricalData:AComparativeEvaluation”.In:Proceedings of the SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2008, pp. 243– 254. doi: 10.1137/1.9781611972788.22 (cited on page 191).
Bibliography (cid:4) 313 [Bed11] Bederson, B. B. “The Promise of Zoomable User Interfaces”. In: Behaviour & Information Technology 30.6 (2011), pp. 853–866. doi: 10.1080/0144929X.2011.586724 (cited on pages 159, 206). [BEF17] Badam,S.K.,Elmqvist,N.,andFekete,J.-D.“SteeringtheCraft: UIElementsandVisualizationsforSupportingProgressiveVisual Analytics”. In: Computer Graphics Forum 36.3 (2017), pp. 491– 502. doi: 10.1111/cgf.13205 (cited on page 304). [Beh+16] Behrisch, M., Bach, B., Riche, N. H., Schreck, T., and Fekete, J. “Matrix Reordering Methods for Table and Network Visualiza- tion”. In: Computer Graphics Forum 35.3 (2016), pp. 693–716. doi: 10.1111/cgf.12935 (cited on page 115). [Bel61] Bellman, R. E. Adaptive Control Processes: A Guided Tour. PrincetonUniversityPress,1961.doi:10.1002/nav.3800080314 (cited on page 258). [Ber67] Bertin, J. Sémiologie Graphique. Gauthier-Villars, 1967 (cited on page 54). [Ber81] Bertin, J. Graphics and Graphic Information-Processing. de Gruyter, 1981 (cited on pages 3, 131). [Ber83] Bertin, J. Semiology of Graphics (W. J. Berg, trans). University of Wisconsin Press, 1983 (cited on page 54). [BH94] Bederson,B.B.andHollan,J.D.“Pad++:AZoomingGraphical Interface for Exploring Alternate Interface Physics”. In: Proceed- ings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 1994, pp. 17–26. doi: 10.1145/ 192426.192435 (cited on page 206). [BHvW00] Bruls,M.,Huizing,K.,andvanWijk,J.J.“SquarifiedTreemaps”. In: Proceedings of the Joint Eurographics - IEEE TCVG Sympo- sium on Visualization (VisSym). Springer, 2000, pp. 33–42. doi: 10.1007/978-3-7091-6783-0_4 (cited on page 117). [Bie+93] Bier, E. A., Stone, M. C., Pier, K., Buxton, W., and DeRose, T. D. “Toolglass and Magic Lenses: the See-Through Interface”. In: Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). ACM Press, 1993, pp. 73–80. doi: 10.1145/166117.166126 (cited on page 206). [BLS99] Brandstädt, A., Le, V. B., and Spinrad, J. P. Graph Classes: A Survey. SIAM, 1999. doi: 10.1137/1.9780898719796 (cited on page 112). [BMG10] Ballendat,T.,Marquardt,N.,andGreenberg,S.“ProxemicInter- action:DesigningforaProximityandOrientation-AwareEnviron- ment”.In:ProceedingsoftheInternationalConferenceonInterac- tive Tabletops and Surfaces (ITS).ACMPress,2010,pp.121–130. doi: 10.1145/1936652.1936676 (cited on page 203).
314 (cid:4) Bibliography [Bor+13] Borgo, R., Kehrer, J., Chung, D. H. S., Maguire, E., Laramee, R. S., Hauser, H., Ward, M., and Chen, M. “Glyph-based Visual- ization: Foundations, Design Guidelines, Techniques and Appli- cations”. In: Eurographics 2013 - State of the Art Reports. Euro- graphics Association, 2013, pp. 39–63. doi: 10.2312/conf/ EG2013/stars/039-063 (cited on page 73). [BR03] Baudisch, P. and Rosenholtz, R. “Halo: A Technique for Visualiz- ing Off-Screen Objects”. In: Proceedings of the SIGCHI Confer- ence Human Factors in Computing Systems (CHI). ACM Press, 2003, pp. 481–488. doi: 10.1145/642611.642695 (cited on page 165). [Bre16] Brehmer, M. M. “Why Visualization? Task Abstraction for Anal- ysis and Design”. PhD thesis. University of British Columbia, 2016 (cited on page 50). [BRL09] Bertini, E., Rigamonti, M., and Lalanne, D. “Extended Excentric Labeling”. In: Computer Graphics Forum 28.3 (2009), pp. 927– 934. doi: 10.1111/j.1467-8659.2009.01456.x (cited on pages 175, 177). [BRT95] Bergman, L. D., Rogowitz, B. E., and Treinish, L. “A Rule-Based Tool for Assisting Colormap Selection”. In: Proceedings of the IEEE Visualization Conference (Vis). IEEE Computer Society, 1995,pp.118–125.doi:10.1109/VISUAL.1995.480803(citedon pages 56, 127). [Buj+91] Buja,A.,McDonald,J.A.,Michalak,J.,andStuetzle,W.“Interac- tiveDataVisualizationUsingFocusingandLinking”.In:Proceed- ings of the IEEE Visualization Conference (Vis).IEEEComputer Society, 1991, pp. 156–163, 419. doi: 10.1109/VISUAL.1991. 175794 (cited on page 157). [But+08] Butkiewicz, T., Dou, W., Wartell, Z., Ribarsky, W., and Chang, R. “Multi-Focused Geospatial Analysis Using Probes”. In: IEEE TransactionsonVisualizationandComputerGraphics14.6(2008), pp. 1165–1172. doi: 10.1109/TVCG.2008.149 (cited on page 102). [Bux90] Buxton, W. “A Three-state Model of Graphical Input”. In: Proceedings of the IFIP International Conference on Human- Computer Interaction (INTERACT). North-Holland, 1990, pp. 449–456 (cited on page 145). [BW08a] Bachthaler, S. and Weiskopf, D. “Continuous Scatterplots”. In: IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1428–1435. doi: 10.1109/TVCG.2008.119 (cited on pages 209, 265, 341).
Bibliography (cid:4) 315 [BW08b] Byron, L. and Wattenberg, M. “Stacked Graphs – Geometry & Aesthetics”. In: IEEE Transactions on Visualization and Com- puter Graphics 14.6 (2008), pp. 1245–1252. doi: 10.1109/TVCG. 2008.166 (cited on page 89). [BW18] Belmonte,N.andWang,Y.RefoldingtheEarth:InteractiveMyri- ahedralProjectionandFabrication.PosterattheIEEEConference on Information Visualization. Berlin, Germany, 2018 (cited on page 100). [CD04] Cook, D. and Das, S. K. Smart Environments: Technology, Pro- tocols and Applications. Wiley-Interscience, 2004. doi: 10.1002/ 047168659X (cited on pages 269, 304). [Cen+17] Ceneda,D.,Gschwandtner,T.,May,T.,Miksch,S.,Schulz,H.-J., Streit, M., and Tominski, C. “Characterizing Guidance in Visual Analytics”.In:IEEETransactionsonVisualizationandComputer Graphics 23.1 (2017), pp. 111–120. doi: 10.1109/TVCG.2016. 2598468 (cited on pages 277–279, 283, 304). [Che04] Chen,H.“CompoundBrushingExplained”.In:Information Visu- alization 3.2 (2004), pp. 96–108. doi: 10.1057/palgrave.ivs. 9500068 (cited on page 158). [Chi00] Chi, E. H.-H. “A Taxonomy of Visualization Techniques Using the Data State Reference Model”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 2000, pp. 69–75. doi: 10.1109/INFVIS.2000.885092 (cited on page 45). [Chu+15] Chung, H., North, C., Joshi, S., and Chen, J. “Four Considera- tions for Supporting Visual Analysis in Display Ecologies”. In: Proceedings of the IEEE Conference on Visual Analytics Science andTechnology(VAST).IEEEComputerSociety,2015,pp.33–40. doi: 10.1109/VAST.2015.7347628 (cited on page 304). [CKB08] Cockburn, A., Karlson, A., and Bederson, B. B. “A Review of Overview+Detail, Zooming, and Focus+Context Interfaces”. In: ACM Computing Surveys 41.1 (2008), 2:1–2:31. doi: 10.1145/ 1456650.1456652 (cited on page 64). [Cle93] Cleveland, W. S. Visualizing Data. Hobart Press, 1993 (cited on pages 69, 91). [CM84] Cleveland, W. S. and McGill, R. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graph- ical Methods”. In: Journal of the American Statistical Associa- tion 79.387 (1984), pp. 531–554. doi: 10.1080/01621459.1984. 10478080 (cited on page 55).
316 (cid:4) Bibliography [Col+18] Collins, C., Andrienko, N., Schreck, T., Yang, J., Choo, J., Engelke, U., Jena, A., and Dwyer, T. “Guidance in the Human- Machine Analytics Process”. In: Visual Informatics 3.1 (2018). doi: 10.1016/j.visinf.2018.09.003 (cited on pages 277, 304). [Con+08] Conversy,S.,Barboni,E.,Navarre,D.,andPalanque,P.“Improv- ing Modularity of Interactive Software with the MDPC Archi- tecture”. In: Engineering Interactive Systems: EIS 2007 Joint Working Conferences, EHCI 2007, DSV-IS 2007, HCSE 2007, Salamanca, Spain, March 22-24, 2007. Selected Papers.Editedby Gulliksen, J., Harning, M. B., Palanque, P., van der Veer, G. C., and Wesson, J. Springer, 2008, pp. 321–338. doi: 10.1007/978- 3-540-92698-6_20 (cited on page 196). [CR98] Chi, E. H.-H. and Riedl, J. T. “An Operator Interaction Frame- work for Visualization Systems”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1998, pp. 63–70. doi: 10.1109/INFVIS.1998.729560 (cited on page 44). [CRC07] Cooper, A., Reimann, R., and Cronin, D. About Face 3: The Essentials of Interaction Design. Wiley, 2007 (cited on pages 139, 140). [CvW11] Claessen, J. H. and van Wijk, J. J. “Flexible Linked Axes for MultivariateDataVisualization”.In:IEEE Transactions on Visu- alization and Computer Graphics 17.12 (2011), pp. 2310–2316. doi: 10.1109/TVCG.2011.201 (cited on page 72). [DGH03] Doleisch,H.,Gasser,M.,andHauser,H.“InteractiveFeatureSpec- ification for Focus+Context Visualization of Complex Simulation Data”. In: Proceedings of the Joint Eurographics - IEEE TCVG Symposium on Visualization (VisSym). Eurographics Association, 2003, pp. 239–248. doi: 10.2312/VisSym/VisSym03/239-248 (cited on pages 221, 265). [DH02a] Doleisch,H.andHauser,H.“SmoothBrushingforFocus+Context Visualization of Simulation Data in 3D”. In: Journal of WSCG 10.1–3 (2002), pp. 147–154. url: http://wscg.zcu.cz/ wscg2002/Papers_2002/E71.pdf (cited on page 156). [DH02b] Dragicevic, P. and Huot, S. “SpiraClock: A Continuous and Non- Intrusive Display for Upcoming Events”. In: Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). Extended Abstracts. ACM Press, 2002, pp. 604–605. doi: 10. 1145/506443.506505 (cited on page 195). [Dix+04] Dix,A.,Finlay,J.,Abowd,G.D.,andBeale,R.Human-Computer Interaction. 3rd edition. Pearson Education, 2004 (cited on page 206).
Bibliography (cid:4) 317 [DP20] Dimara, E. and Perin, C. “What is Interaction for Data Visual- ization?” In: IEEE Transactions on Visualization and Computer Graphics 26.1 (2020), pp. 119–129. doi: 10.1109/TVCG.2019. 2934283 (cited on page 206). [dSai39] DeSaint-Exupéry,A.Wind, Sand, and Stars.translatedbyLewis Galantière. Harcourt, Inc., 1939 (cited on page 62). [Düb+14] Dübel, S., Röhlig, M., Schumann, H., and Trapp, M. “2D and 3D Presentation of Spatial Data: A Systematic Review”. In: Info- Vis Workshop: Does 3D Really Make Sense for Data Visualiza- tion? IEEE Computer Society, 2014. doi: 10.1109/3DVis.2014. 7160094 (cited on pages 103, 104). [Düb+17] Dübel, S., Röhlig, M., Tominski, C., and Schumann, H. “Visual- izing 3D Terrain, Geo-Spatial Data, and Uncertainty”. In: Infor- matics 4.1 (2017), pp. 1–18. doi: 10.3390/informatics4010006 (cited on page 126). [ED06] Ellis, G. and Dix, A. J. “The Plot, the Clutter, the Sampling and its Lens: Occlusion Measures for Automatic Clutter Reduction”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2006, pp. 266–269. doi: 10.1145/1133265. 1133318 (cited on pages 178, 265). [EF10] Elmqvist,N.andFekete,J.-D.“HierarchicalAggregationforInfor- mation Visualization: Overview, Techniques, and Design Guide- lines”. In: IEEE Transactions on Visualization and Computer Graphics 16.3 (2010), pp. 439–454. doi: 10.1109/TVCG.2009.84 (cited on pages 233, 265). [Eic+14] Eichner, C., Bittig, A., Schumann, H., and Tominski, C. “Ana- lyzing Simulations of Biochemical Systems with Feature-Based VisualAnalytics.”In: Computers & Graphics 38.1(2014), pp. 18– 26. doi: 10.1016/j.cag.2013.09.001 (cited on page 221). [Eic+15] Eichner, C., Nocke, T., Schulz, H. J., and Schumann, H. “Inter- active Presentation of Geo-Spatial Climate Data in Multi- Display Environments”. In: ISPRS International Journal of Geo- Information 4.2(2015),pp.493–514.doi:10.3390/ijgi4020493 (cited on pages 272, 276). [Eic94] Eick, S. G. “Data Visualization Sliders”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 1994, pp. 119–120. doi: 10.1145/192426. 192472 (cited on page 152). [Elm+11] Elmqvist, N., Moere, A. V., Jetter, H.-C., Cernea, D., Reiterer, H., and Jankun-Kelly, T. “Fluid Interaction for Information Visu- alization”.In:Information Visualization 10.4(2011),pp.327–340. doi: 10.1177/1473871611413180 (cited on page 143).
318 (cid:4) Bibliography [Emm+16] Emmons,S.,Kobourov,S.,Gallant,M.,andBörner,K.“Analysis of Network Clustering Algorithms and Cluster Quality Metrics at Scale”. In: PLOS ONE 11.7 (2016), pp. 1–18. doi: 10.1371/ journal.pone.0159161 (cited on page 265). [End+17] Endert, A., Ribarsky, W., Turkay, C., Wong, W., Nabney, I. T., Blanco, I. D., and Rossi, F. “The State of the Art in Integrating Machine Learning into Visual Analytics”. In: Computer Graphics Forum 36.8 (2017), pp. 458–486. doi: 10.1111/cgf.13092 (cited on page 265). [ENS15] Eichner,C.,Nyolt,M.,andSchumann,H.“ANovelInfrastructure forSupportingDisplayEcologies”.In:AdvancesinVisualComput- ing: Proceedings of the International Symposium on Visual Com- puting (ISVC). Springer, 2015, pp. 722–732. doi: 10.1007/978- 3-319-27863-6\_68 (cited on pages 269, 342). [EST19] Eichner,C.,Schumann,H.,andTominski,C.Multi-display Visual Analysis: Model, Interface, and Layout Computation. Tech. rep. arXiv:1912.08558 [cs.GR]. CoRR, 2019. url: https://arxiv. org/abs/1912.08558 (cited on page 304). [FB90] Feiner, S. K. and Beshers, C. “Worlds within Worlds: Metaphors for Exploring n-dimensional Virtual Worlds”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).ACMPress,1990,pp.76–83.doi:10.1145/97924.97933 (cited on page 79). [FB95] Furnas,G.W.andBederson,B.B.“Space-ScaleDiagrams:Under- standing Multiscale Interfaces”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1995, pp. 234–241. doi: 10.1145/223904.223934 (cited on page 206). [FD13] Frisch, M. and Dachselt, R. “Visualizing Offscreen Elements of Node-Link Diagrams”. In: Information Visualization 12.2 (2013), pp. 133–162. doi: 10.1177/1473871612473589 (cited on page 165). [Fek+19] Fekete,J.-D.,Fisher,D.,Nandi,A.,andSedlmair,M.“Progressive Data Analysis and Visualization (Dagstuhl Seminar 18411)”. In: Dagstuhl Reports 8.10 (2019), pp. 1–40. doi: 10.4230/DagRep.8. 10.1 (cited on page 304). [Fis+12] Fisher,D.,Popov,I.O.,Drucker,S.M.,andm.c.schraefel.“Trust Me, I’m Partially Right: Incremental Visualization Lets Analysts Explore Large Datasets Faster”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2012, pp. 1673–1682. doi: 10.1145/2207676.2208294 (cited on page 304).
Bibliography (cid:4) 319 [FR91] Fruchterman, T. M. J. and Reingold, E. M. “Graph Drawing by Force-DirectedPlacement”.In:Software: Practice and Experience 21.11 (1991), pp. 1129–1164. doi: 10.1002/spe.4380211102 (cited on page 113). [Fra98] Frank,A.U.“DifferentTypesof“Times”inGIS”.In:Spatial and Temporal Reasoning in Geographic Information Systems. Edited by Egenhofer, M. J. and Golledge, R. G. Oxford University Press, 1998, pp. 40–62 (cited on page 84). [FS04] Fuchs, G. and Schumann, H. “Intelligent Icon Positioning for Interactive Map-Based Information Systems”. In: Innovations Through Information Technology. Edited by Khosrow-Pour, M. Hershey, PA, USA: Idea Group Inc., 2004, pp. 261–264. doi: 10.4018/978-1-59140-261-9.ch067. url: http://www.irma- international.org/viewtitle/32349/ (cited on page 102). [Fur86] Furnas, G. W. “Generalized Fisheye Views”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1986, pp. 16–23. doi: 10.1145/22339.22342 (cited on page 214). [Fur97] Furnas,G.W.“EffectiveViewNavigation”.In:Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). ACM Press, 1997, pp. 367–374. doi: 10.1145/258549.258800 (cited on page 206). [Gla+14] Gladisch, S., Schumann, H., Ernst, M., Füllen, G., and Tominski, C.“Semi-AutomaticEditingofGraphswithCustomizedLayouts”. In: Computer Graphics Forum 33.3 (2014), pp. 381–390. doi: 10.1111/cgf.12394 (cited on pages 182, 183). [Gla14] Glaßer, S. “Visual Analysis, Clustering, and Classification of Contrast-Enhanced Tumor Perfusion MRI Data”. PhD the- sis. Otto von Guericke University Magdeburg, 2014 (cited on page 247). [Gle+11] Gleicher,M.,Albers,D.,Walker,R.,Jusufi,I.,Hansen,C.D.,and Roberts,J.C.“VisualComparisonforInformationVisualization”. In: Information Visualization 10.4 (2011), pp. 289–309. doi: 10. 1177/1473871611416549 (cited on pages 184, 206). [Gle18] Gleicher, M. “Considerations for Visualizing Comparison”. In: IEEE Transactions on Visualization and Computer Graphics 24.1 (2018), pp. 413–423. doi: 10.1109/TVCG.2017.2744199 (cited on page 206). [GS06] Griethe, H. and Schumann, H. “The Visualization of Uncertain Data: Methods and Problems”. In: Proceedings of the Simulation and Visualization (SimVis). SCS Publishing House e.V., 2006, pp. 143–156 (cited on page 125).
320 (cid:4) Bibliography [GST13] Gladisch, S., Schumann, H., and Tominski, C. “Navigation Rec- ommendations for Exploring Hierarchical Graphs”. In: Advances in Visual Computing: Proceedings of the International Symposium on Visual Computing (ISVC). Springer, 2013, pp. 36–47. doi: 10.1007/978-3-642-41939-3_4 (cited on pages 165, 283). [Gua13] Guastello, S. J. Human Factors Engineering and Ergonomics: A Systems Approach. 2nd edition. CRC Press, 2013 (cited on page 50). [Gus+08] Gustafson, S., Baudisch, P., Gutwin, C., and Irani, P. “Wedge: Clutter-Free Visualization of Off-Screen Locations”. In: Proceed- ings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2008, pp. 787–796. doi: 10.1145/ 1357054.1357179 (cited on page 165). [GYZ14] Gross, J. L., Yellen, J., and Zhang, P., eds. Handbook of Graph Theory. CRC Press, 2014 (cited on page 112). [Had+10] Hadlak,S.,Tominski,C.,Schulz,H.-J.,andSchumann,H.“Visual- izationofAttributedHierarchicalStructuresinaSpatio-Temporal Context”. In: International Journal of Geographical Information Science 24.10 (2010), pp. 1497–1513. doi: 10.1080/13658816. 2010.510840 (cited on page 121). [Had+13] Hadlak, S., Schumann, H., Cap, C. H., and Wollenberg, T. “Sup- porting the Visual Analysis of Dynamic Networks by Clustering AssociatedTemporalAttributes”.In:IEEETransactionsonVisu- alization and Computer Graphics 19.12 (2013), pp. 2267–2276. doi: 10.1109/TVCG.2013.198 (cited on page 253). [Had14] Hadlak, S. “Graph Visualization in Space and Time”. PhD thesis. University of Rostock, 2014 (cited on pages 251, 252, 254). [Häg70] Hägerstrand, T. “What About People in Regional Science?” In: Papers of the Regional Science Association 24 (1970), pp. 7–21 (cited on page 108). [Hal+16] Hall,K.W.,Perin,C.,Kusalik,P.G.,Gutwin,C.,andCarpendale, M. S. T. “Formalizing Emphasis in Information Visualization”. In: Computer Graphics Forum 35.3 (2016), pp. 717–737. doi: 10.1111/cgf.12936 (cited on page 155). [Han09] Hanrahan, P. Systems of Thought. Keynote presentation at the Eurographics/IEEESymposiumonVisualization(EuroVis).2009 (cited on page 52). [Har96] Harris, R. L. Information Graphics: A Comprehensive Illustrated Reference. Managment Graphics, 1996 (cited on page 72).
Bibliography (cid:4) 321 [Hau06] Hauser, H. “Generalizing Focus+Context Visualization”. In: Sci- entific Visualization: The Visual Extraction of Knowledge from Data. Springer, 2006, pp. 305–327. doi: 10.1007/3-540-30790- 7_18 (cited on page 64). [HAW08] Heer, J., Agrawala, M., and Willett, W. “Generalized Selection viaInteractiveQueryRelaxation”.In:Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2008, pp. 959–968. doi: 10.1145/1357054.1357203 (cited on page 158). [HB03] Harrower, M. A. and Brewer, C. A. “ColorBrewer.org: An Online Tool for Selecting Color Schemes for Maps”. In: The Cartographic Journal 40.1 (2003), pp. 27–37. doi: 10.1179/ 000870403235002042 (cited on page 127). [HB10] Heer, J. and Bostock, M. “Crowdsourcing Graphical Percep- tion: Using Mechanical Turk to Assess Visualization Design”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2010, pp. 203–212. doi: 10.1145/1753326.1753357 (cited on page 55). [HC04] Heer, J. and Card, S. K. “DOITrees Revisited: Scalable, Space- Constrained Visualization of Hierarchical Data”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2004, pp. 421–424. doi: 10.1145/989863.989941 (cited on page 215). [HE12] Healey, C. G. and Enns, J. T. “Attention and Visual Memory in VisualizationandComputerGraphics”.In:IEEETransactionson Visualization and Computer Graphics 18.7 (2012), pp. 1170–1188. doi: 10.1109/TVCG.2011.127 (cited on pages 56, 156). [Hei+12] Heinrich, J., Luo, Y., Kirkpatrick, A. E., and Weiskopf, D. “Eval- uationofaBundlingTechniqueforParallelCoordinates”.In:Pro- ceedings of the International Conference on Computer Graphics Theory and Applications and International Conference on Infor- mation Visualization Theory and Applications (VISIGRAPP). SciTePress,2012,pp.594–602.doi:10.5220/0003821205940602 (cited on page 213). [HFM07] Henry, N., Fekete, J.-D., and McGuffin, M. J. “NodeTrix: a Hybrid Visualization of Social Networks”. In: IEEE Transactions on Visualization and Computer Graphics 13.6 (2007), pp. 1302– 1309. doi: 10.1109/TVCG.2007.70582 (cited on page 118). [HHN85] Hutchins,E.L.,Hollan,J.D.,andNorman,D.A.“DirectManip- ulation Interfaces”. In: Human-Computer Interaction 1.4 (1985), pp. 311–338. doi: 10.1207/s15327051hci0104_2 (cited on pages 138, 194, 205).
322 (cid:4) Bibliography [HJ05] Hansen, C. D. and Johnson, C. R., eds. The Visualization Hand- book. Elsevier, 2005 (cited on pages 21, 22). [HKP11] Han, J., Kamber, M., and Pei, J. Data Mining: Concepts and Techniques. Morgan Kaufmann, 2011 (cited on pages 244, 265). [HM90] Haber,R.B.andMcNabb,D.A.“VisualizationIdioms:AConcep- tualModelforScientificVisualizationSystems”.In:Visualization in Scientific Computing. Edited by Nielson, G. M., Shriver, B. D., and Rosenblum, L. J. IEEE Computer Society, 1990, pp. 74–93 (cited on page 44). [Hol06] Holten, D. “Hierarchical Edge Bundles: Visualization of Adja- cency Relations in Hierarchical Data”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 741–748. doi: 10.1109/TVCG.2006.147 (cited on page 213). [Hor+19] Horak, T., Mathisen, A., Klokmose, C. N., Dachselt, R., and Elmqvist, N. “Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press,2019,616:1–616:13.doi:10.1145/3290605.3300846(cited on page 303). [Hor99] Horvitz, E. “Principles of Mixed-Initiative User Interfaces”. In: Proceedings of the SIGCHI Conference Human Factors in Com- puting Systems (CHI). ACM Press, 1999, pp. 159–166. doi: 10. 1145/302979.303030 (cited on page 304). [HS12] Heer, J. and Shneiderman, B. “Interactive Dynamics for Visual Analysis”. In: Communications of the ACM 55.4 (2012), pp. 45– 54. doi: 10.1145/2133806.2133821 (cited on pages 144, 289). [HSS15] Hadlak, S., Schumann, H., and Schulz, H.-J. “A Survey of Multi-faceted Graph Visualization”. In: EuroVis State of the Art Reports. Eurographics Association, 2015, pp. 1–20. doi: 10. 2312/eurovisstar.20151109 (cited on pages 112, 119, 120). [HvW09] Holten,D.andvanWijk,J.J.“Force-DirectedEdgeBundlingfor Graph Visualization”. In: Computer Graphics Forum 28.3 (2009), pp. 983–990. doi: 10.1111/j.1467-8659.2009.01450.x (cited on page 213). [II91] II, R. J. M. Introduction to Shannon Sampling and Interpolation Theory. Springer, 1991 (cited on page 232). [Ins09] Inselberg, A. Parallel Coordinates – Visual Multidimensional GeometryandItsApplications.Springer,2009.doi:10.1007/978- 0-387-68628-8 (cited on page 71).
Bibliography (cid:4) 323 [Ise+13] Isenberg, P., Isenberg, T., Hesselmann, T., Lee, B., von Zadow, U., and Tang, A. “Data Visualization on Interactive Surfaces: A ResearchAgenda”.In:IEEEComputerGraphicsandApplications 33.2 (2013), pp. 16–24. doi: 10.1109/MCG.2013.24 (cited on page 206). [Jak+13] Jakobsen, M. R., Haile, Y. S., Knudsen, S., and Hornbæk, K. “Information Visualization and Proxemics: Design Opportunities and Empirical Findings”. In: IEEE Transactions on Visualization and Computer Graphics 19.12 (2013), pp. 2386–2395. doi: 10. 1109/TVCG.2013.166 (cited on page 204). [JD13] Jansen, Y. and Dragicevic, P. “An Interaction Model for Visual- izations Beyond The Desktop”. In: IEEE Transactions on Visual- ization and Computer Graphics 19.12 (2013), pp. 2396–2405. doi: 10.1109/TVCG.2013.134 (cited on page 206). [JMG07] Jankun-Kelly, T. J., Ma, K.-L., and Gertz, M. “A Model and FrameworkforVisualizationExploration”.In:IEEETransactions onVisualizationandComputerGraphics 13.2(2007),pp.357–369. doi: 10.1109/TVCG.2007.28 (cited on page 44). [Jol02] Jolliffe,I.T.PrincipalComponentAnalysis.2ndedition.Springer, New York, USA, 2002 (cited on page 265). [JTS08] John,M.,Tominski,C.,andSchumann,H.“VisualandAnalytical Extensions for the Table Lens”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2008, pp. 680907-1–680907-12. doi: 10.1117/12.766440 (cited on pages 57, 60, 249). [Kei+06] Keim, D. A., Mansmann, F., Schneidewind, J., and Ziegler, H. “Challenges in Visual Data Analysis”. In: Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2006, pp. 9–16. doi: 10.1109/IV.2006.31 (cited on pages 3, 207, 208, 257, 264, 265). [Kei+10] Keim, D., Kohlhammer, J., Ellis, G., and Mannsmann, F., eds. Mastering the Information Age – Solving Problems with Visual Analytics. Eurographics Association, 2010 (cited on page 265). [Kim+17] Kim, H., Choo, J., Lee, C., Lee, H., Reddy, C. K., and Park, H. “PIVE: Per-Iteration Visualization Environment for Real- Time Interactions with Dimension Reduction and Clustering”. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI Press, 2017, pp. 1001–1009. url: http:// aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14381 (cited on page 298).
324 (cid:4) Bibliography [Kis+17] Kister, U., Klamka, K., Tominski, C., and Dachselt, R. “GraSp: Combining Spatially-aware Mobile Devices and a Display Wall for Graph Visualization and Interaction”. In: Computer Graphics Forum 36.3 (2017), pp. 503–514. doi: 10.1111/cgf.13206 (cited on page 303). [KK17] Kerracher, N. and Kennedy, J. “Constructing and Evaluating Visualisation Task Classifications: Process and Considerations”. In: Computer Graphics Forum 36.3 (2017), pp. 47–59. doi: 10. 1111/cgf.13167 (cited on pages 31, 50). [KK93] Keller, P. R. and Keller, M. M. Visual Cues: Practical Data Visualization. IEEE Computer Society, 1993 (cited on page 34). [KK94] Keim, D. A. and Kriegel, H.-P. “VisDB: Database Exploration Using Multidimensional Visualization”. In: IEEE Computer Graphics and Applications 14.5 (1994), pp. 40–49. doi: 10.1109/ 38.310723 (cited on page 75). [KM94] Kirsh, D. and Maglio, P. “On Distinguishing Epistemic from Pragmatic Action”. In: Cognitive Science 18.4 (1994), pp. 513– 549. doi: 10.1207/s15516709cog1804_1 (cited on page 131). [KNS04] Kreuseler, M., Nocke, T., and Schumann, H. “A History Mech- anism for Visual Data Mining”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 2004, pp. 49–56. doi: 10.1109/INFVIS.2004.2 (cited on page 274). [KP88] Krasner,G.E.andPope,S.T.“ACookbookforUsingtheModel- View-Controller User Interface Paradigm in Smalltalk-80”. In: Journal of Object-Oriented Programming 1.3 (1988), pp. 26–49 (cited on page 146). [KPW14] Kerren, A., Purchase, H. C., and Ward, M. O., eds. Multivariate Network Visualization.Springer,2014.doi:10.1007/978-3-319- 06793-3 (cited on page 127). [Kra03] Kraak, M.-J. “The Space-Time Cube Revisited from a Geovisu- alization Perspective”. In: Proceedings of the 21st International CartographicConference(ICC).Newcastle,UK:TheInternational Cartographic Association (ICA), 2003, pp. 1988–1995 (cited on page 108). [KRD14] Kister,U.,Reipschläger,P.,andDachselt,R.“Multi-TouchManip- ulation of Magic Lenses for Information Visualization”. In: Pro- ceedings of the International Conference on Interactive Table- tops and Surfaces (ITS). ACM Press, 2014, pp. 431–434. doi: 10.1145/2669485.2669528 (cited on page 178).
Bibliography (cid:4) 325 [KS99] Kreuseler,M.andSchumann,H.“InformationVisualizationUsing a New Focus+Context Technique in Combination with Dynamic Clustering of Information Space”. In: Proceedings of the ACM Workshop on New Paradigms in Information Visualization and Manipulation (NPIVM). ACM Press, 1999, pp. 1–5 (cited on page 123). [Kul06] Kulpa, Z. “A Diagrammatic Approach to Investigate Interval Relations”. In: Journal of Visual Languages & Computing 17.5 (2006), pp. 466–502. doi: 10.1016/j.jvlc.2005.10.004. url: http://dx.doi.org/10.1016/j.jvlc.2005.10.004 (cited on page 88). [LA94] Leung, Y. K. and Apperley, M. D. “A Review and Taxonomy of Distortion-Oriented Presentation Techniques”. In: ACM Transac- tions on Computer-Human Interaction 1.2 (1994), pp. 126–160. doi: 10.1145/180171.180173 (cited on page 64). [Lam08] Lam, H. “A Framework of Interaction Costs in Information Visu- alization”.In:IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1149–1156. doi: 10.1109/TVCG.2008. 109 (cited on page 136). [Lee+12] Lee, B., Isenberg, P., Riche, N. H., and Carpendale, S. “Beyond MouseandKeyboard:ExpandingDesignConsiderationsforInfor- mation Visualization Interactions”. In: IEEE Transactions on VisualizationandComputerGraphics 18.12(2012),pp.2689–2698. doi: 10.1109/TVCG.2012.204 (cited on page 206). [Leh+11] Lehmann,A.,Schumann,H.,Staadt,O.,andTominski,C.“Phys- ical Navigation to Support Graph Exploration on a Large High- Resolution Display”. In: Advances in Visual Computing: Pro- ceedings of the International Symposium on Visual Computing (ISVC). Springer, 2011, pp. 496–507. doi: 10.1007/978-3-642- 24028-7_46 (cited on pages 202–204). [Lex+12] Lex, A., Streit, M., Schulz, H.-J., Partl, C., Schmalstieg, D., Park, P. J., and Gehlenborg, N. “StratomeX: Visual Analysis of Large-Scale Heterogeneous Genomics Data for Cancer Subtype Characterization”. In: Computer Graphics Forum 31.3 (2012), pp. 1175–1184. doi: 10.1111/j.1467-8659.2012.03110.x (cited on page 246). [LF06] Leskovec, J. and Faloutsos, C. “Sampling from Large Graphs”. In: Proceedings of the ACM Conference on Knowledge discovery and data mining (SIGKDD). ACM Press, 2006, pp. 631–636. doi: 10.1145/1150402.1150479 (cited on page 232).
326 (cid:4) Bibliography [LH14] Liu, Z. and Heer, J. “The Effects of Interactive Latency on Exploratory Visual Analysis”. In: IEEE Transactions on Visual- ization and Computer Graphics 20.12 (2014), pp. 2122–2131. doi: 10.1109/TVCG.2014.2346452 (cited on pages 140, 289). [LHT17] Lhuillier, A., Hurter, C., and Telea, A. “State of the Art in Edge and Trail Bundling Techniques”. In: Computer Graphics Forum 36.3 (2017), pp. 619–645. doi: 10.1111/cgf.13213 (cited on pages 212, 213, 265). [Lia05] Liao,W.“ClusteringofTimeSeriesData—ASurvey”.In:Pattern Recognition38.11(2005),pp.1857–1874.doi:10.1016/j.patcog. 2005.01.025 (cited on page 251). [LKD19] Langner, R., Kister, U., and Dachselt, R. “Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances”. In: IEEE Trans- actions on Visualization and Computer Graphics 25.1 (2019), pp. 608–618. doi: 10.1109/TVCG.2018.2865235 (cited on page 303). [Loh19] Lohr, S. L. Sampling: Design and Analysis. 2nd edition. CRC Press, 2019 (cited on page 232). [LS07] Luboschik, M. and Schumann, H. “Explode to Explain – Illus- trative Information Visualization”. In: Proceedings of the Inter- national Conference Information Visualisation (IV). IEEE Com- puterSociety,2007.doi:10.1109/IV.2007.50(citedonpages77, 341). [LTM18] Lam, H., Tory, M., and Munzner, T. “Bridging from Goals to Tasks with Design Study Analysis Reports”. In: IEEE Trans- actions on Visualization and Computer Graphics 24.1 (2018), pp. 435–445. doi: 10.1109/TVCG.2017.2744319 (cited on pages 29, 50). [Lub+12] Luboschik,M.,Maus,C.,Schulz,H.-J.,Schumann,H.,andUhrma- cher, A. “Heterogeneity-Based Guidance for Exploring Multiscale Data in Systems Biology”. In: Proceedings of the IEEE Sympo- sium on Biological Data Visualization (BioVis). IEEE Computer Society, 2012, pp. 33–40. doi: 10.1109/BioVis.2012.6378590 (cited on pages 233, 236, 265). [Lub+15] Luboschik, M., Röhlig, M., Bittig, A. T., Andrienko, N., Schu- mann, H., and Tominski, C. “Feature-Driven Visual Analytics of Chaotic Parameter-Dependent Movement”. In: Computer Graph- ics Forum 34.3 (2015), pp. 421–430. doi: 10.1111/cgf.12654 (cited on pages 225, 229). [LV07] Lee,J.A.andVerleysen,M.Nonlinear Dimensionality Reduction. Springer, 2007. doi: 10.1007/978-0-387-39351-3 (cited on page 265).
Bibliography (cid:4) 327 [LWW90] LeBlanc, J., Ward, M. O., and Wittels, N. “Exploring n- Dimensional Databases”. In: Proceedings of the IEEE Visual- ization Conference (Vis). IEEE Computer Society, 1990, pp. 230– 237. doi: 10.1109/VISUAL.1990.146386 (cited on page 78). [Mac86] Mackinlay,J.“AutomatingtheDesignofGraphicalPresentations of Relational Information”. In: ACM Transactions on Graphics 5.2 (1986), pp. 110–141. doi: 10.1145/22949.22950 (cited on pages 50, 54). [Mac95] MacEachren, A. M. How Maps Work: Representation, Visualiza- tion, and Design. Guilford Press, 1995 (cited on pages 54, 99, 127). [Mar+18] Marriott, K., Schreiber, F., Dwyer, T., Klein, K., Riche, N. H., Itoh, T., Stuerzlinger, W., and Thomas, B. H., eds. Immersive Analytics. Springer, 2018. doi: 10.1007/978-3-030-01388-2 (cited on page 206). [MDB87] McCormick, B. H., DeFanti, T. A., and Brown, M. D. “Visualiza- tion in Scientific Computing”. In: ACM SIGGRAPH Computer Graphics 21.6 (1987), p. 3. doi: 10.1145/41997.41998 (cited on page 2). [Mey+15] Meyer, M., Sedlmair, M., Quinan, P. S., and Munzner, T. “The Nested Blocks and Guidelines Model”. In: Information Visualiza- tion 14.3 (2015), pp. 234–249. doi: 10.1177/1473871613510429 (cited on page 41). [ML17] McNabb, L. and Laramee, R. S. “Survey of Surveys (SoS) - Mapping The Landscape of Survey Papers in Information Visual- ization”. In: Computer Graphics Forum 36.3 (2017), pp. 589–617. doi: 10.1111/cgf.13212 (cited on page 126). [MNS06] Müller, W., Nocke, T., and Schumann, H. “Enhancing the Visu- alization Process with Principal Component Analysis to Support the Exploration of Trends”. In: Asia-Pacific Symposium on Infor- mation Visualisation (APVIS). Australian Computer Society, 2006, pp. 121–130. url: https://dl.acm.org/citation.cfm? id=1151922 (cited on page 261). [Mos+09] Moscovich, T., Chevalier, F., Henry, N., Pietriga, E., and Fekete, J.-D. “Topology-Aware Navigation in Large Networks”. In: Pro- ceedings of the SIGCHI Conference Human Factors in Com- puting Systems (CHI). ACM Press, 2009, pp. 2319–2328. doi: 10.1145/1518701.1519056 (cited on page 166).
328 (cid:4) Bibliography [Müh+14] Mühlbacher,T.,Piringer,H.,Gratzl,S.,Sedlmair,M.,andStreit, M.“OpeningtheBlackBox:StrategiesforIncreasedUserInvolve- ment in Existing Algorithm Implementations”. In: IEEE Trans- actions on Visualization and Computer Graphics 20.12 (2014), pp. 1643–1652. doi: 10.1109/TVCG.2014.2346578 (cited on page 304). [Mun09] Munzner, T. “A Nested Model for Visualization Design and Vali- dation”. In: IEEE Transactions on Visualization and Computer Graphics 15.6(2009),pp.921–928.doi:10.1109/TVCG.2009.111 (cited on pages 41, 42). [Mun14] Munzner,T.Visualization Analysis and Design.AKPeters/CRC Press, 2014 (cited on page 50). [MW14] Mackinlay, J. D. and Winslow, K. Designing Great Visualiza- tions. White Paper. Tableau Software Inc., 2014. url: https:// www.tableau.com/sites/default/files/media/designing- great-visualizations.pdf (cited on page 55). [MW95] Martin, A. R. and Ward, M. O. “High Dimensional Brushing for Interactive Exploration of Multivariate Data”. In: Proceedings of the IEEE Visualization Conference (Vis). IEEE Computer Society, 1995, pp. 271–278. doi: 10.1109/VISUAL.1995.485139 (cited on pages 149, 156). [Nac+16] Nachmanson, L., Nocaj, A., Bereg, S., Zhang, L., and Holroyd, A. “Node Overlap Removal by Growing a Tree”. In: Proceedings of the International Symposium on Graph Drawing (GD). Springer, 2016, pp. 33–43. doi: 10.1007/978-3-319-50106-2_3 (cited on page 114). [NH06] Novotny, M. and Hauser, H. “Outlier-Preserving Focus+Context Visualization in Parallel Coordinates”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 893–900. doi: 10.1109/TVCG.2006.170 (cited on pages 210, 211, 265, 342). [Nob+19] Nobre, C., Streit, M., Meyer, M., and Lex, A. “The State of the ArtinVisualizingMultivariateNetworks”.In:ComputerGraphics Forum 38.3 (2019), pp. 807–832. doi: 10.1111/cgf.13728 (cited on page 127). [Nor13] Norman, D. A. The Design of Everyday Things. Revised and expanded edition. Basic Books, 2013 (cited on pages 135, 136). [Nor88] Norman, D. A. The Psychology of Everyday Things. Basic Books, 1988 (cited on pages 135, 136).
Bibliography (cid:4) 329 [NSS05] Nocke, T., Schlechtweg, S., and Schumann, H. “Icon-Based Visu- alization Using Mosaic Metaphors”. In: Proceedings of the Inter- national Conference Information Visualisation (IV). IEEE Com- puter Society, 2005, pp. 103–109. doi: 10.1109/IV.2005.58 (cited on pages 75, 341). [PB07] Preim, B. and Bartz, D. Visualization in Medicine: Theory, Algo- rithms, and Applications. Morgan Kaufmann, 2007 (cited on page 22). [Pin+12] Pindat, C., Pietriga, E., Chapuis, O., and Puech, C. “JellyLens: Content-aware Adaptive Lenses”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 2012, pp. 261–270. doi: 10.1145/2380116.2380150 (cited on page 177). [Pir+09] Piringer, H., Tominski, C., Muigg, P., and Berger, W. “A Multi- Threading Architecture to Support Interactive Visual Explo- ration”. In: IEEE Transactions on Visualization and Computer Graphics 15.6 (2009), pp. 1113–1120. doi: 10.1109/TVCG.2009. 110 (cited on page 294). [Pos+03] Post, F. H., Vrolijk, B., Hauser, H., Laramee, R. S., and Doleisch, H. “The State of the Art in Flow Visualisation: Feature Extrac- tion and Tracking”. In: Computer Graphics Forum 22.4 (2003), pp. 775–792. doi: 10.1111/j.1467-8659.2003.00723.x (cited on pages 220, 221). [PW06] Plumlee, M. and Ware, C. “Zooming versus Multiple Win- dow Interfaces: Cognitive Costs of Visual Comparisons”. In: ACM Transactions on Computer-Human Interaction 13.2 (2006), pp. 179–209. doi: 10 . 1145 / 1165734 . 1165736 (cited on page 185). [Qia+12] Qiang, Y., Delafontaine, M., Versichele, M., Maeyer, P. D., and de Weghe, N. V. “Interactive Analysis of Time Intervals in a Two-dimensional Space”. In: Information Visualization 11.4 (2012), pp. 255–272. doi: 10.1177/1473871612436775 (cited on page 89). [Rad+12] Radloff, A., Lehmann, A., Staadt, O. G., and Schumann, H. “Smart Interaction Management: An Interaction Approach for Smart Meeting Rooms”. In: Proceedings of the Eighth Interna- tional Conference on Intelligent Environments (IE). IEEE Com- puter Society, 2012, pp. 228–235. doi: 10.1109/IE.2012.34 (cited on pages 273, 342).
330 (cid:4) Bibliography [Rad+15] Radloff, A., Tominski, C., Nocke, T., and Schumann, H. “Sup- porting Presentation and Discussion of Visualization Results in Smart Meeting Rooms”. In: The Visual Computer 31.9 (2015), pp. 1271–1286. doi: 10.1007/s00371-014-1010-x (cited on pages 274, 302, 304). [Rag+16] Ragan, E. D., Endert, A., Sanyal, J., and Chen, J. “Character- izing Provenance in Visualization and Data Analysis: An Orga- nizational Framework of Provenance Types and Purposes”. In: IEEE Transactions on Visualization and Computer Graphics 22.1 (2016), pp. 31–40. doi: 10.1109/TVCG.2015.2467551 (cited on page 125). [RC94] Rao, R. and Card, S. K. “The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus + Context Visualization for Tabular Information”. In: Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). ACM Press, 1994, pp. 318–322. doi: 10.1145/191666.191776 (cited on page 68). [RLP10] Riche,N.H.,Lee,B.,andPlaisant,C.“UnderstandingInteractive Legends: a Comparative Evaluation with Standard Widgets”. In: Computer Graphics Forum 29.3 (2010), pp. 1193–1202. doi: 10.1111/j.1467-8659.2009.01678.x (cited on page 151). [RLS11] Radloff, A., Luboschik, M., and Schumann, H. “Smart Views in Smart Environments”. In: Proceedings of the Smart Graphics. Springer, 2011, pp. 1–12. doi: 10.1007/978-3-642-22571-0_1 (cited on pages 12, 273, 341). [Rob07] Roberts,J.C.“StateoftheArt:Coordinated&MultipleViewsin Exploratory Visualization”. In: Proceedings of the International Conference on Coordinated and Multiple Views in Exploratory Visualization (CMV). IEEE Computer Society, 2007, pp. 98–102. doi: 10.1109/CMV.2007.20 (cited on page 65). [Röh+15] Röhlig, M., Luboschik, M., Krüger, F., Kirste, T., Schumann, H., Bögl, M., Bilal, A., and Miksch, S. “Supporting Activity Recognition by Visual Analytics”. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE Computer Society, 2015, pp. 41–48. doi: 10.1109/VAST. 2015.7347629 (cited on page 241). [Ros+04] Rosario,G.E.,Rundensteiner,E.A.,Brown,D.C.,Ward,M.O., andHuang,S.“MappingNominalValuestoNumbersforEffective Visualization”. In: Information Visualization 3.2 (2004), pp. 80– 95. doi: 10.1057/palgrave.ivs.9500072 (cited on page 57).
Bibliography (cid:4) 331 [Ros+11] Rosenbaum,R.,Giménez,A.,Schumann,H.,andHamann,B.“A Flexible Low-complexity Device Adaptation Approach for Data Presentation”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2011, 78680F-1–78680F- 12. doi: 10.1117/12.871975 (cited on page 302). [RPS01] Reinders, F., Post, F. H., and Spoelder, H. J. “Visualization of Time-Dependent Data with Feature Tracking and Event Detec- tion.” In: The Visual Computer 17.1 (2001), pp. 55–71. doi: 10.1007/PL00013399 (cited on pages 220, 265). [RS09] Rosenbaum,R.andSchumann,H.“ProgressiveRefinement:More Than a Means to Overcome Limited Bandwidth”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2009, pp. 72430-1–72430-13. doi: 10.1117/12. 810501 (cited on page 303). [RS17] Röhlig, M. and Schumann, H. “Visibility Widgets for Unveiling OccludedDatain3DTerrainVisualization”.In:JournalofVisual Languages & Computing 42 (2017), pp. 86–98. doi: 10.1016/j. jvlc.2017.08.008 (cited on page 105). [Ruz+12] Ruzinoor,C.M.,Shariff,A.R.M.,Pradhan,B.,RodziAhmad,M., and Rahim, M. S. M. “A Review on 3D Terrain Visualization of GIS Data: Techniques and Software”. In: Geo-spatial Information Science 15.2 (2012), pp. 105–115. doi: 10.1080/10095020.2012. 714101 (cited on page 101). [Sac+14] Sacha, D., Stoffel, A., Kwon, B. C., Ellis, G., and Keim, D. A. “Knowledge Generation Model for Visual Analytics”. In: IEEE Transactions on Visualization and Computer Graphics 20.12 (2014), pp. 1604–1613. doi: 10.1109/TVCG.2014.2346481 (cited on pages 47, 264). [Sac+17] Sacha,D.,Zhang,L.,Sedlmair,M.,Lee,J.,Peltonen,J.,Weiskopf, D., North, S. C., and Keim, D. A. “Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis”. In: IEEE Transactions on Visualization and Computer Graphics 23.1 (2017), pp. 241–250. doi: 10.1109/TVCG.2016.2598495 (cited on page 265). [Sai+05] Saito, T., Miyamura, H. N., Yamamoto, M., Saito, H., Hoshiya, Y.,andKaseda,T.“Two-TonePseudoColoring:CompactVisual- ization for One-Dimensional Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society,2005,pp.173–180.doi:10.1109/INFVIS.2005.1532144 (cited on page 59).
332 (cid:4) Bibliography [Sar+02] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. “Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems”. In: Proceedings of the 5th International Conference on Computer and Information Technology (ICCIT). East West University, Dhaka, Bangladesh, 2002, pp. 399–404 (cited on page 298). [SB94] Sarkar, M. and Brown, M. H. “Graphical Fisheye Views”. In: Communications of the ACM 37.12 (1994), pp. 73–83. doi: 10. 1145/198366.198384 (cited on page 178). [Sch+13a] Schulz, H.-J., Nocke, T., Heitzler, M., and Schumann, H. “A Design Space of Visualization Tasks”. In: IEEE Transactions on VisualizationandComputerGraphics 19.12(2013),pp.2366–2375. doi: 10.1109/TVCG.2013.120 (cited on pages 29, 34, 50). [Sch+13b] Schulz, H.-J., Streit, M., May, T., and Tominski, C. Towards a Characterization of Guidance in Visualization. Poster at IEEE ConferenceonInformationVisualization(InfoVis).Atlanta,USA, 2013 (cited on page 277). [Sch+16] Schulz, H.-J., Angelini, M., Santucci, G., and Schumann, H. “An Enhanced Visualization Process Model for Incremental Visual- ization”. In: IEEE Transactions on Visualization and Computer Graphics 22.7 (2016), pp. 1830–1842. doi: 10.1109/TVCG.2015. 2462356 (cited on pages 290–293, 298). [Sch+17] Schulz, H.-J., Nocke, T., Heitzler, M., and Schumann, H. “A Systematic View on Data Descriptors for the Visual Analysis of TabularData”.In:InformationVisualization16.3(2017),pp.232– 256. doi: 10.1177/1473871616667767 (cited on page 26). [Sch07] Schaeffer,S.E.“GraphClustering”.In:Computer Science Review 1.1 (2007), pp. 27–64. doi: 10.1016/j.cosrev.2007.05.001 (cited on page 250). [SH10] Shaer, O. and Hornecker, E. “Tangible User Interfaces: Past, Present and Future Directions”. In: Foundations and Trends in Human-Computer Interaction 3.1–2 (2010), pp. 4–137. doi: 10.1561/1100000026 (cited on page 197). [She68] Shepard, D. “A Two-dimensional Interpolation Function for Irregularly-spaced Data”. In: Proceedings of the 23rd ACM National Conference. ACM Press, 1968, pp. 517–524. doi: 10. 1145/800186.810616 (cited on page 25). [Shn94] Shneiderman, B. “Dynamic Queries for Visual Information Seek- ing”. In: IEEE Software 11.6 (1994), pp. 70–77. doi: 10.1109/ 52.329404 (cited on pages 140, 149).
Bibliography (cid:4) 333 [Shn96] Shneiderman, B. “The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations”. In: Proceedings of the IEEE Symposium on Visual Languages (VL). IEEE Computer Society,1996,pp.336–343.doi:10.1109/VL.1996.545307(cited on pages 34, 159). [SHS11] Schulz, H.-J., Hadlak, S., and Schumann, H. “The Design Space of Implicit Hierarchy Visualization: A Survey”. In: IEEE Trans- actions on Visualization and Computer Graphics 17.4 (2011), pp. 393–411. doi: 10.1109/TVCG.2010.79 (cited on page 117). [SM00] Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642- 57193-0 (cited on pages 13, 75, 80, 341). [SM07] Shen, Z. and Ma, K.-L. “Path Visualization for Adjacency Matri- ces”. In: Proceedings of the Joint Eurographics - IEEE TCVG Symposium on Visualization (VisSym). Eurographics Association, 2007, pp. 83–90. doi: 10.2312/VisSym/EuroVis07/083-090 (cited on page 115). [SMD12] Spindler, M., Martsch, M., and Dachselt, R. “Going Beyond the Surface: Studying Multi-layer Interaction Above the Tabletop”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI).ACMPress,2012,pp.1277–1286.doi: 10.1145/2207676.2208583 (cited on page 201). [SP09] Shneiderman, B. and Plaisant, C. Designing the User Interface: Strategies for Effective Human-Computer Interaction. 5th edition. Addison-Wesley, 2009 (cited on pages 138, 143, 206). [SP13] Sedig, K. and Parsons, P. “Interaction Design for Complex Cog- nitive Activities with Visual Representations: A Pattern-Based Approach”. In: AIS Transactions on Human-Computer Interac- tion 5.2 (2013), pp. 84–133 (cited on pages 133, 134). [SP16] Sedig, K. and Parsons, P. Design of Visualizations for Human- Information Interaction: A Pattern-Based Framework. Vol. 4. Synthesis Lectures on Visualization. Morgan and Claypool Pub- lishers,2016. doi: 10.2200/S00685ED1V01Y201512VIS005(cited on page 206). [Spe07] Spence, R. Information Visualization: Design for Interaction. 2nd edition. Prentice Hall, 2007 (cited on pages 13, 127, 131, 140, 162). [SPG14] Stolper, C. D., Perer, A., and Gotz, D. “Progressive Visual Ana- lytics: User-Driven Visual Exploration of In-Progress Analytics”. In: IEEE Transactions on Visualization and Computer Graphics 20.12 (2014), pp. 1653–1662. doi: 10.1109/TVCG.2014.2346574 (cited on pages 290, 304).
334 (cid:4) Bibliography [Spi+10] Spindler,M.,Tominski,C.,Schumann,H.,andDachselt,R.“Tan- gible Views for Information Visualization”. In: Proceedings of the International Conference on Interactive Tabletops and Surfaces (ITS). ACM Press, 2010, pp. 157–166. doi: 10.1145/1936652. 1936684 (cited on pages 198, 199). [Spi+14] Spindler, M., Schuessler, M., Martsch, M., and Dachselt, R. “Pinch-Drag-Flick vs. Spatial Input: Rethinking Zoom & Pan on Mobile Displays”. In: Proceedings of the SIGCHI Confer- ence Human Factors in Computing Systems (CHI). ACM Press, 2014, pp. 1113–1122. doi: 10.1145/2556288.2557028 (cited on page 201). [SS06] Schulz, H.-J. and Schumann, H. “Visualizing Graphs – A Gen- eralized View”. In: Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2006, pp. 166–173. doi: 10.1109/IV.2006.130 (cited on page 113). [Ste98] Steiner,A.“AGeneralisationApproachtoTemporalDataModels and their Implementations”. PhD thesis. Swiss Federal Institute of Technology, Zürich, Switzerland, 1998 (cited on pages 85, 86). [Str+12] Streit,M.,Schulz,H.-J.,Lex,A.,Schmalstieg,D.,andSchumann, H.“Model-DrivenDesignfortheVisualAnalysisofHeterogeneous Data”. In: IEEE Transactions on Visualization and Computer Graphics 18.6 (2012), pp. 998–1010. doi: 10.1109/TVCG.2011. 108 (cited on pages 286–288, 342). [Tam13] Tamassia, R., ed. Handbook of Graph Drawing and Visualization. CRC Press, 2013 (cited on page 113). [TAS04] Tominski, C., Abello, J., and Schumann, H. “Axes-Based Visu- alizations with Radial Layouts”. In: Proceedings of the ACM Symposium on Applied Computing (SAC). ACM Press, 2004, pp. 1242–1247. doi: 10.1145/967900.968153 (cited on pages 87, 171). [TAS09] Tominski, C., Abello, J., and Schumann, H. “CGV – An Inter- active Graph Visualization System”. In: Computers & Graphics 33.6 (2009), pp. 660–678. doi: 10.1016/j.cag.2009.06.002 (cited on pages 123, 166). [Tat+12] Tatu, A., Maass, F., Färber, I., Bertini, E., Schreck, T., Seidl, T., and Keim, D. A. “Subspace Search and Visualization to Make Sense of Alternative Clusterings in High-dimensional Data”. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE Computer Society, 2012, pp. 63– 72. doi: 10.1109/VAST.2012.6400488 (cited on page 191). [TC05] Thomas, J. J. and Cook, K. A. Illuminating the Path: The Research and Development Agenda for Visual Analytics. IEEE Computer Society, 2005 (cited on page 29).
Bibliography (cid:4) 335 [Tei+17] Teipel, S., Heine, C., Hein, A., Krüger, F., Kutschke, A., Kernebeck, S., Halek, M., Bader, S., and Kirste, T. “Multidimen- sional Assessment of Challenging Behaviors in Advanced Stages ofDementiainNursingHomes—TheinsideDEMFramework”.In: Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Mon- itoring 8 (2017), pp. 36–44. doi: 10.1016/j.dadm.2017.03.006 (cited on page 240). [Tel14] Telea, A. C. Data Visualization: Principles and Practice. 2nd edi- tion. A K Peters/CRC Press, 2014 (cited on pages 21, 22). [TFJ12] Tominski, C., Forsell, C., and Johansson, J. “Interaction Sup- port for Visual Comparison Inspired by Natural Behavior”. In: IEEETransactionsonVisualizationandComputerGraphics18.12 (2012), pp. 2719–2728. doi: 10.1109/TVCG.2012.237 (cited on pages 186, 188–190). [TFS08a] Thiede, C., Fuchs, G., and Schumann, H. “Smart Lenses”. In: Proceedings of the Smart Graphics (SG). Springer, 2008, pp. 178– 189. doi: 10.1007/978-3-540-85412-8_16 (cited on page 206). [TFS08b] Tominski, C., Fuchs, G., and Schumann, H. “Task-Driven Color Coding”.In:ProceedingsoftheInternationalConferenceInforma- tion Visualisation (IV). IEEE Computer Society, 2008, pp. 373– 380. doi: 10.1109/IV.2008.24 (cited on page 58). [TLH10] Talbot, J., Lin, S., and Hanrahan, P. “An Extension of Wilkin- son’s Algorithm for Positioning Tick Labels on Axes”. In: IEEE TransactionsonVisualizationandComputerGraphics16.6(2010), pp. 1036–1043. doi: 10.1109/TVCG.2010.130 (cited on page 57). [Tob70] Tobler, W. R. “A Computer Movie Simulating Urban Growth in the Detroit Region”. In: Economic Geography 46.6 (1970), pp. 234–240. doi: 10.2307/143141 (cited on pages 25, 98). [Tom+06] Tominski,C.,Abello,J.,vanHam,F.,andSchumann,H.“Fisheye Tree Views and Lenses for Graph Visualization”. In: Proceedings of the International Conference Information Visualisation (IV). IEEEComputerSociety,2006,pp.17–24.doi:10.1109/IV.2006. 54 (cited on page 179). [Tom+12] Tominski, C., Schumann, H., Andrienko, G., and Andrienko, N. “Stacking-Based Visualization of Trajectory Attribute Data”. In: IEEETransactionsonVisualizationandComputerGraphics18.12 (2012), pp. 2565–2574. doi: 10.1109/TVCG.2012.265 (cited on pages 107, 181). [Tom+17] Tominski, C., Gladisch, S., Kister, U., Dachselt, R., and Schu- mann, H. “Interactive Lenses for Visualization: An Extended Survey”. In: Computer Graphics Forum 36.6 (2017), pp. 173–200. doi: 10.1111/cgf.12871 (cited on pages 173, 174, 206).
336 (cid:4) Bibliography [Tom15] Tominski, C. Interaction for Visualization. Synthesis Lectures on Visualization 3. Morgan & Claypool, 2015. doi: 10.2200/ S00651ED1V01Y201506VIS003 (cited on pages 203, 206, 341). [Tom16] Tominski, C. “CompaRing: Reducing Costs of Visual Compar- ison”. In: Short Paper Proceedings of the Eurographics Confer- ence on Visualization (EuroVis). Eurographics Association, 2016, pp. 137–141. doi: 10.2312/eurovisshort.20161175 (cited on page 192). [TS08] Tominski, C. and Schumann, H. “Enhanced Interactive Spiral Display”. In: Proceedings of the Annual SIGRAD Conference, Special Theme: Interactivity. Linköping University Electronic Press, 2008, pp. 53–56. url: https://www.ep.liu.se/ecp_ article/index.en.aspx?issue=034;article=013 (cited on page 89). [TS12] Tominski, C. and Schulz, H.-J. “The Great Wall of Space-Time”. In:Proceedings of the Workshop on Vision, Modeling & Visualiza- tion (VMV). Eurographics Association, 2012, pp. 199–206. doi: 10.2312/PE/VMV/VMV12/199-206 (cited on page 109). [TSS05] Tominski, C., Schulze-Wollgast, P., and Schumann, H. “3D Infor- mation Visualization for Time Dependent Data on Maps”. In: Proceedings of the International Conference Information Visual- isation (IV). IEEE Computer Society, 2005, pp. 175–181. doi: 10.1109/IV.2005.3 (cited on page 108). [Tuf83] Tufte, E. R. The Visual Display of Quantitative Information. Graphics Press, 1983 (cited on pages 18, 86). [Tuk77] Tukey, J. W. Exploratory Data Analysis. Addison-Wesley, 1977 (cited on page 58). [vHP09] Van Ham, F. and Perer, A. “Search, Show Context, Expand on Demand: Supporting Large Graph Exploration with Degree-of- Interest ”. In: IEEE Transactions on Visualization and Computer Graphics 15.6(2009),pp.953–960.doi:10.1109/TVCG.2009.108 (cited on page 215). [vHSD09] Van Ham, F., Schulz, H.-J., and Dimicco, J. M. “Honeycomb: Visual Analysis of Large Scale Social Networks”. In: Proceedings of the 12th IFIP Conference on Human-Computer Interaction (INTERACT). Springer, 2009, pp. 429–442. doi: 10.1007/978- 3-642-03658-3_47 (cited on page 115). [Vic99] Vicente, K. J. Cognitive Work Analysis: Toward Safe, Productive, and Healthy Computer-Based Work. CRC Press, 1999 (cited on page 50).
Bibliography (cid:4) 337 [vLan+11] Von Landesberger, T., Kuijper, A., Schreck, T., Kohlhammer, J., van Wijk, J. J., Fekete, J.-D., and Fellner, D. W. “Visual Analysis of Large Graphs: State-of-the-Art and Future Research Challenges”.In:Computer Graphics Forum 30.6(2011),pp.1719– 1749. doi: 10.1111/j.1467-8659.2011.01898.x (cited on page 127). [vLan+14] VonLandesberger,T.,Bremm,S.,Schreck,T.,andFellner,D.W. “Feature-Based Automatic Identification of Interesting Data Seg- ments in Group Movement Data”. In: Information Visualization 13.3 (2014), pp. 190–212. doi: 10.1177/1473871613477851.2 (cited on page 225). [vLan18] Von Landesberger, T. “Insights by Visual Comparison: The State and Challenges”. In: IEEE Computer Graphics and Applications 38.3 (2018), pp. 140–148. doi: 10.1109/MCG.2018.032421661 (cited on page 206). [VW93] Visvalingam, M. and Whyatt, J. D. “Line Generalisation by Repeated Elimination of Points”. In: The Cartographic Jour- nal 30.1 (1993), pp. 46–51. doi: 10.1179/000870493786962263 (cited on page 100). [vWal+96] Van Walsum, T., Post, F. H., Silver, D., and Post, F. J. “Feature Extraction and Iconic Visualization”. In: IEEE Transactions on Visualization and Computer Graphics 2.2 (1996), pp. 111–119. doi: 10.1109/2945.506223 (cited on page 222). [vWij06] Van Wijk, J. J. “Views on Visualization”. In: IEEE Transactions onVisualizationandComputerGraphics 12.4(2006),pp.421–433. doi: 10.1109/TVCG.2006.80 (cited on pages 47, 50, 282, 289). [vWij08] Van Wijk, J. J. “Unfolding the Earth: Myriahedral Projections”. In: The Cartographic Journal 45.1 (2008), pp. 32–42. doi: 10. 1179/000870408X276594 (cited on page 100). [vWN04] Van Wijk, J. J. and Nuij, W. A. A. “A Model for Smooth View- ing and Navigation of Large 2D Information Spaces”. In: IEEE TransactionsonVisualizationandComputerGraphics10.4(2004), pp. 447–458. doi: 10.1109/TVCG.2004.1 (cited on page 167). [vWvS99] VanWijk,J.J.andvanSelow,E.R.“ClusterandCalendarBased Visualization of Time Series Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1999, pp. 4–9. doi: 10.1109/INFVIS.1999.801851 (cited on pages 92, 93, 341). [War02] Ward,M.O.“ATaxonomyofGlyphPlacementStrategiesforMul- tidimensional Data Visualization”. In: Information Visualization 1.2 (2002), pp. 194–210. doi: 10.1057/palgrave.ivs.9500025 (cited on page 75).
338 (cid:4) Bibliography [War12] Ware, C. Information Visualization: Perception for Design. 3rd edition. Morgan Kaufmann, 2012 (cited on pages 50, 127). [WGK15] Ward, M. O., Grinstein, G., and Keim, D. Interactive Data Visu- alization: Foundations, Techniques, and Applications. 2nd edition. A K Peters/CRC Press, 2015 (cited on pages 13, 127). [WH04] Wolfe, J. M. and Horowitz, T. S. “What Attributes Guide the Deployment of Visual Attention and How do They do it?” In: Nature Reviews Neuroscience 05.6 (2004), pp. 495–501. doi: 10. 1038/nrn1411 (cited on page 155). [Wil11] Wills, G. Visualizing Time: Designing Graphical Representations for Statistical Data. Springer, 2011. doi: 10.1007/978-0-387- 77907-2 (cited on page 127). [Wil96] Wills,G.J.“Selection:524,288WaystoSay“ThisisInteresting””. In:ProceedingsoftheIEEESymposiumInformationVisualization (InfoVis). IEEE Computer Society, 1996, pp. 54–60. doi: 10. 1109/INFVIS.1996.559216 (cited on page 150). [WWK00] WangBaldonado,M.Q.,Woodruff,A.,andKuchinsky,A.“Guide- lines for Using Multiple Views in Information Visualization”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2000, pp. 110–119. doi: 10.1145/345513. 345271 (cited on page 65). [XW05] Xu, R. and Wunsch, D. C. “Survey of Clustering Algorithms”. In: IEEE Transactions on Neural Networks 16.3 (2005), pp. 645–678. doi: 10.1109/TNN.2005.845141 (cited on page 265). [Yi+07] Yi, J. S., ah Kang, Y., Stasko, J. T., and Jacko, J. A. “Toward a Deeper Understanding of the Role of Interaction in Informa- tion Visualization”. In: IEEE Transactions on Visualization and Computer Graphics 13.6 (2007), pp. 1224–1231. doi: 10.1109/ TVCG.2007.70515 (cited on page 132). [Yu+12] Yu, L., Efstathiou, K., Isenberg, P., and Isenberg, T. “Efficient Structure-Aware Selection Techniques for 3D Point Cloud Visual- izations with 2DOF Input”. In: IEEE Transactions on Visualiza- tion and Computer Graphics 18.12 (2012), pp. 2245–2254. doi: 10.1109/TVCG.2012.217 (cited on page 158). [Zgr+17] Zgraggen,E.,Galakatos,A.,Crotty,A.,Fekete,J.-D.,andKraska, T. “How Progressive Visualizations Affect Exploratory Analysis”. In: IEEE Transactions on Visualization and Computer Graphics 23.8 (2017), pp. 1977–1987. doi: 10.1109/TVCG.2016.2607714 (cited on page 303). [ZH16] Zhou, L. and Hansen, C. D. “A Survey of Colormaps in Visual- ization”. In: IEEE Transactions on Visualization and Computer Graphics 22.8 (2016), pp. 2051–2069. doi: 10.1109/TVCG.2015. 2489649 (cited on page 127).
Index 2D, 63, 103 density-based representations 3D, 63, 103, 107 data density, 209 visual density, 210 action cycle, 135 dimensional stacking, 78 action patterns, 133 dimensionality reduction, 257 activity recognition, 240 dimming, 154 aggregation, 232 direct manipulation, 138 analysis history, 133, 274 dynamic filtering, 8, 149 analytic questions, 31 dynamic graphs, 250 animation, 66, 86, 147, 167 dynamic querying, 149 attenuation, 153 attribute space, 22, 27 effectiveness, 17, 55 efficiency, 18 box-whisker, 58 elementary question, 31 bring & go, 166 emphasis, 153 brushing & linking, 70, 157 exploration, 29, 47 bundling, 212 expressiveness, 17 cartographic generalization, 100 factors, 19 cartography, 99 context, 35 classification, 239 data, 19 clustering, 92, 243, 250 tasks, 28 color coding, 6, 57 feature-based analysis, 220 computational analysis, 208 features confirmation, 30 definition, 220 context, 35 extraction, 221 cyclic visualization, 89 visualization, 222 filtering, 154 data abstraction, 231 first law of geography, 98 data chunking, 291 fish-eye distortion, 64, 178 data classes, 27 fluid interaction, 143, 289 data domain, 19 focus+context, 64, 302 data element, 21 data scale, 19 geo-spatial data, 98 data scope, 24 geographic space, 96 data space, 22 globe, 300 data table, 21 glyphs, 73, 108 decision trees, 239 goals, 29 degree of interest, 11, 214, 284 graph data, 5, 28, 111 339
340 (cid:4) Index graph layout, 113, 299 multi-scale data, 233 graph lenses, 178 multi-scale slider, 169 graph visualization, 6, 113, 250, 283 multi-threading, 294 guidance, 10, 233, 277 multiple views, 9, 65, 123, 227 definition, 278 multivariate data, 23, 27, 67 degree, 281 guidelines, 143 navigation, 159, 283 nested model, 41 highlighting, 154 networks, 111 human factors, 35 node-link diagram, 6, 113, 299 human in the loop, 131 occlusion, 105 icicle plot, 249 off-screen visualization, 165 incremental visualization, see overview+detail, 64, 162, 227, 302 progressive visual analysis interaction parallel coordinates, 71 continuous, 146, 149, 151, 169 partial results, 293 discrete, 146 PCA, 258 interaction costs, 136 presentation, 30 interaction intents, 132 principal component analysis, 258 interactive lenses, 173 progressive visual analysis, 290 interpolation, 25 quality criteria, 17 knowledge gap, 279 reference space, 22, 27 knowledge generation, 48 region of interest, 298 lasso, 149 representation effect, 52 latency, 289, 296 rubberband, 149 magic lenses, see interactive lenses sampling, 232 mantra scalar data, 21 visual analytics, 3, 208 scatter plot, 61 visual information seeking, 159 scatter plot matrix, 69 map projection, 99 selection, 148 maps, 99 self-organizing maps, 248 marks, 54, 62 singular value decomposition, 259 matrix, 115, 227, 241 sliders, 151 matrix ordering, 116 small multiples, 86 MDS, 258 smart environments, 269 meta-data, 26 smooth brushing, 156 mosaic plot, 78 space-time cube, 108, 121 movement data, 106, 224 spatial data, 27 multi-dimensional data, 23 spatio-temporal data, 27, 106 multi-dimensional scaling, 258 spiral visualization, 89 multi-display environments, 268 stream graph, 89 multi-faceted graphs, 28, 112, 118 synoptic question, 31
Index (cid:4) 341 table lens, 68, 249 verification, 48 tasks, 28 visibility widgets, 105 temporal data, 27, 85 visual comparison, 58 temporal relations, 83 visual cues, 11, 165 tensor data, 21 visual encoding, 54 terrain rendering, 100 visual variables, 54 time, 82 visualization trajectory visualization, 107 axes-based, 72, 87, 171 treemap visualization, 117, 302 glyph-based, 73, 101 trees, 111 nested, 77, 91 tuple, 21 pixel-based, 75 two-tone coloring, 59, 68 table-based, 67, 261 visualization definition, 2 uncertainty, 93, 125 visualization design, 41 undo, 133, 275 visualization pipeline, 44 univariate data, 23 visualization reference model, 44 usability, 143 zoomable interfaces, 159 value range, 21 animated transitions, 167 variable, 22 zooming & panning, 163 vector data, 21
Figure Credits CopyrightedFigures The following figures have been taken from previously published works. The copyright of these figures remains with the respective publishers. Figure 1.5 Reprinted by permission from Springer Nature Customer Service Centre GmbH: Radloff, A. et al. “Smart Views in Smart Environments”. In: Proceedings of the Smart Graphics. Springer, 2011, pp. 1–12. doi: 10.1007/978-3-642-22571-0_1, © 2011. Figure 3.24a and Figure 3.28 (central part) Reprinted by permission from Springer Nature Customer Service Centre GmbH: Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642-57193-0, © 2000. Figure 3.24b © 2005 IEEE. Reprinted, with permission, from Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642-57193-0. Figure 3.26 (main part) © 2007 IEEE. Reprinted, with permission, from Luboschik, M. and Schumann, H. “Explode to Explain – Illustrative Information Visualization”. In: Proceedings of the International Confer- ence Information Visualisation (IV). IEEE Computer Society, 2007. doi: 10.1109/IV.2007.50. Figure 3.38 © 1999 IEEE. Reprinted, with permission, from van Wijk, J. J. and van Selow, E. R. “Cluster and Calendar Based Visualization of Time Series Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1999, pp. 4–9. doi: 10.1109/INFVIS.1999.801851. Figure 4.49 © 2015 Morgan & Claypool. Reprinted with kind permis- sion from Tominski, C. Interaction for Visualization. Synthesis Lec- tures on Visualization 3. Morgan & Claypool, 2015. doi: 10.2200/ S00651ED1V01Y201506VIS003. Figure 5.1 © 2008 IEEE. Reprinted, with permission, from Bachthaler, S. and Weiskopf, D. “Continuous Scatterplots”. In: IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1428–1435. doi: 10.1109/TVCG.2008.119. 343
344 (cid:4) Figure Credits Figure 5.3 © 2006 IEEE. Reprinted, with permission, from Novotny, M. and Hauser, H. “Outlier-Preserving Focus+Context Visualization in Parallel Coordinates”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 893–900. doi: 10.1109/TVCG.2006.170. Figures 5.9 and 5.10 © 2014 IEEE. Reprinted, with permission, from Abello, J. et al. “A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks”. In: IEEE Transactions on Visualization and Computer Graphics 20.3 (2014), pp. 337–350. doi: 10.1109/TVCG.2013.109. Figures 5.43 and 5.44 Reprinted by permission from Springer Nature Cus- tomer Service Centre GmbH: Aigner, W. et al. Visualization of Time- Oriented Data. Springer, 2011. doi: 10.1007/978-0-85729-079-3, © 2011. Figure 6.1 Reprinted by permission from Springer Nature Customer Service Centre GmbH: Eichner, C. et al. “A Novel Infrastructure for Supporting Display Ecologies”. In: Advances in Visual Computing: Proceedings of the International Symposium on Visual Computing (ISVC). Springer, 2015, pp. 722–732. doi: 10.1007/978-3-319-27863-6\_68, © 2015. Figure 6.4 © 2012 IEEE. Reprinted, with permission, from Radloff, A. et al. “Smart Interaction Management: An Interaction Approach for Smart Meeting Rooms”. In: Proceedings of the Eighth International Conference onIntelligentEnvironments(IE).IEEEComputerSociety,2012,pp.228– 235. doi: 10.1109/IE.2012.34. Figure 6.12 © 2012 IEEE. Reprinted, with permission, from Streit, M. et al. “Model-Driven Design for the Visual Analysis of Heterogeneous Data”. In: IEEE Transactions on Visualization and Computer Graphics 18.6 (2012), pp. 998–1010. doi: 10.1109/TVCG.2011.108. FiguresLicensedUnderCreativeCommonsLicense The figures listed in the following are licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit creativecommons.org/licenses/by/4.0/. Figures 3.12, 3.51, 5.15, 5.16, 5.19 to 5.21, 5.32 and 5.33 by Martin Röhlig. Figure 3.25 by Thomas Nocke. Figures 3.46 and 3.50 by Steve Dübel. Figure 5.4 by Helwig Hauser. Figures 5.8, 5.40 and 5.41 by Steffen Hadlak.
Figure Credits (cid:4) 345 Figure 5.12a and Figures 5.13, 5.14, 6.2 and 6.6 by Christian Eichner. Figures 5.22, 5.28 and 5.29 by Martin Luboschik. Figure 6.20 by Marco Angelini. Figure 6.24 by Axel Radloff-Delosea. AvailabilityofOriginalFigures All original figures from this book are released under the Creative Commons Attribution 4.0 International License (CC BY 4.0). The figures are available on https://ivda-book.de.
--- End of content from file: (AK Peters Visualization) Christian Tominski_ Heidrun Schumann - Interactive Visual Data Analysis-CRC Press (2020).pdf ---


--- Start of content from file: 2021 Chan, Stanley ~ Introduction to Probability for Data Science [Michigan Publishing] _.pdf ---
Introduction to Probability for Data Science Stanley H. Chan Purdue University
Copyright (cid:13)c2021 Stanley H. Chan This book is published by Michigan Publishing under an agreement with the author. It is made available free of charge in electronic form to any student or instructor interested in the subject matter. Published in the United States of America by Michigan Publishing Manufactured in the United States of America ISBN 978-1-60785-746-4 (hardcover) ISBN 978-1-60785-747-1 (electronic) ii
To Vivian, Joanna, and Cynthia Chan And ye shall know the truth, and the truth shall make you free. John 8:32 iii
iv
Preface Thisbookisanintroductorytextbookinundergraduateprobability.Ithasamission:tospell out the motivation, intuition, and implication of the probabilistic tools we use in science and engineering. From over half a decade of teaching the course, I have distilled what I believetobethecoreofprobabilisticmethods.Iputthebookinthecontextofdatascience to emphasize the inseparability between data (computing) and probability (theory) in our time. Probability is one of the most interesting subjects in electrical engineering and com- puter science. It bridges our favorite engineering principles to the practical reality, a world thatisfullofuncertainty.However,becauseprobabilityissuchamaturesubject,theunder- graduate textbooks alone might fill several rows of shelves in a library. When the literature issorich,thechallengebecomeshowonecanpiercethroughtotheinsightwhiledivinginto thedetails.Forexample,manyofyouhaveusedanormalrandomvariablebefore,buthave you ever wondered where the “bell shape” comes from? Every probability class will teach you about flipping a coin, but how can “flipping a coin” ever be useful in machine learning today? Data scientists use the Poisson random variables to model the internet traffic, but where does the gorgeous Poisson equation come from? This book is designed to fill these gaps with knowledge that is essential to all data science students. Thisleadstothethreegoalsofthebook.(i)Motivation:Intheoceanofmathematical definitions,theorems,andequations,whyshouldwespendourtimeonthisparticulartopic but not another? (ii) Intuition: When going through the derivations, is there a geometric interpretation or physics beyond those equations? (iii) Implication: After we have learned a topic, what new problems can we solve? The book’s intended audience is undergraduate juniors/seniors and first-year gradu- ate students majoring in electrical engineering and computer science. The prerequisites are standard undergraduate linear algebra and calculus, except for the section about charac- teristic functions, where Fourier transforms are needed. An undergraduate course in signals and systems would suffice, even taken concurrently while studying this book. Thelengthofthebookissuitableforatwo-semestercourse.Instructorsareencouraged tousethesetofchaptersthatbestfitstheirclasses.Forexample,abasicprobabilitycourse canuseChapters1-5asitsbackbone.Chapter6onsamplestatisticsissuitableforstudents whowishtogaintheoreticalinsightsintoprobabilisticconvergence.Chapter7onregression and Chapter 8 on estimation best suit students who want to pursue machine learning and signalprocessing.Chapter9discussesconfidenceintervalsandhypothesistesting,whichare criticaltomoderndataanalysis.Chapter10introducesrandomprocesses.Myapproachfor random processes is more tailored to information processing and communication systems, which are usually more relevant to electrical engineering students. Additional teaching resources can be found on the book’s website, where you can v
findlecturevideosandhomeworkvideos.Throughoutthebookyouwillsee many“practice exercises”,whichareeasyproblemswithworked-outsolutions.Theycanbeskippedwithout loss to the flow of the book. Acknowledgements: If I could thank only one person, it must be Professor Fawwaz Ulaby of the University of Michigan. Professor Ulaby has been the source of support in all aspects, from the book’s layout to technical content, proofreading, and marketing. The book would not have been published without the help of Professor Ulaby. I am deeply movedbyProfessorUlaby’svisionthateducationshouldbemadeaccessibletoallstudents. Withtextbookpricesrocketingup,theEECSfreetextbookinitiativelaunchedbyProfessor Ulaby is the most direct response to the publishers, teachers, parents, and students. Thank you, Fawwaz, for your unbounded support — technically, mentally, and financially. Thank you also for recommending Richard Carnes. The meticulous details Richard offered have significantly improved the fluency of the book. Thank you, Richard. I thank my colleagues at Purdue who had shared many thoughts with me when I taughtthecourse(inalphabeticalorder):ProfessorsMarkBell,MaryComer,SaulGelfand, Amy Reibman, and Chih-Chun Wang. My teaching assistant I-Fan Lin was instrumental in the early development of this book. To the graduate students of my lab (Yiheng Chi, Nick Chimitt, Kent Gauen, Abhiram Gnanasambandam, Guanzhe Hong, Chengxi Li, Zhiyuan Mao, Xiangyu Qu, and Yash Sanghvi): Thank you! It would have been impossible to finish the book without your participation. A few students I taught volunteered to help edit the book: Benjamin Gottfried, Harrison Hsueh, Dawoon Jung, Antonio Kincaid, Deepak Ravikumar, Krister Ulvog, Peace Umoru, Zhijing Yao. I would like to thank my Ph.D. advisor Professor Truong Nguyen for encouraging me to write the book. Finally, I would like to thank my wife Vivian and my daughters, Joanna and Cynthia, for their love, patience, and support. Stanley H. Chan, West Lafayette, Indiana May, 2021 Companion website: https://probability4datascience.com/ vi
Contents 1 Mathematical Background 1 1.1 Infinite Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Geometric Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.2 Binomial Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2 Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.2.1 Taylor approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.2.2 Exponential series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2.3 Logarithmic approximation . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3.1 Odd and even functions . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3.2 Fundamental Theorem of Calculus . . . . . . . . . . . . . . . . . . . . 17 1.4 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 1.4.1 Why do we need linear algebra in data science? . . . . . . . . . . . . . 20 1.4.2 Everything you need to know about linear algebra . . . . . . . . . . . 21 1.4.3 Inner products and norms . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.4.4 Matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.5 Basic Combinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.5.1 Birthday paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.5.2 Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.5.3 Combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2 Probability 43 2.1 Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.1.1 Why study set theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.1.2 Basic concepts of a set . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.1.3 Subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.1.4 Empty set and universal set . . . . . . . . . . . . . . . . . . . . . . . . 48 2.1.5 Union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.1.6 Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 2.1.7 Complement and difference . . . . . . . . . . . . . . . . . . . . . . . . 52 2.1.8 Disjoint and partition . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 2.1.9 Set operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 2.1.10 Closing remarks about set theory . . . . . . . . . . . . . . . . . . . . . 57 vii
CONTENTS 2.2 Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 2.2.1 Sample space Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 2.2.2 Event space F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 2.2.3 Probability law P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 2.2.4 Measure zero sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 2.2.5 Summary of the probability space . . . . . . . . . . . . . . . . . . . . 74 2.3 Axioms of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 2.3.1 Why these three probability axioms? . . . . . . . . . . . . . . . . . . . 75 2.3.2 Axioms through the lens of measure . . . . . . . . . . . . . . . . . . . 76 2.3.3 Corollaries derived from the axioms . . . . . . . . . . . . . . . . . . . 77 2.4 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 2.4.1 Definition of conditional probability . . . . . . . . . . . . . . . . . . . 81 2.4.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 2.4.3 Bayes’ theorem and the law of total probability . . . . . . . . . . . . . 89 2.4.4 The Three Prisoners problem . . . . . . . . . . . . . . . . . . . . . . . 92 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 2.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 2.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 3 Discrete Random Variables 103 3.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 3.1.1 A motivating example . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 3.1.2 Definition of a random variable . . . . . . . . . . . . . . . . . . . . . . 105 3.1.3 Probability measure on random variables . . . . . . . . . . . . . . . . 107 3.2 Probability Mass Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 3.2.1 Definition of probability mass function . . . . . . . . . . . . . . . . . . 110 3.2.2 PMF and probability measure. . . . . . . . . . . . . . . . . . . . . . . 110 3.2.3 Normalization property . . . . . . . . . . . . . . . . . . . . . . . . . . 112 3.2.4 PMF versus histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 3.2.5 Estimating histograms from real data . . . . . . . . . . . . . . . . . . 117 3.3 Cumulative Distribution Functions (Discrete) . . . . . . . . . . . . . . . . . . 121 3.3.1 Definition of the cumulative distribution function . . . . . . . . . . . . 121 3.3.2 Properties of the CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 3.3.3 Converting between PMF and CDF . . . . . . . . . . . . . . . . . . . 124 3.4 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 3.4.1 Definition of expectation. . . . . . . . . . . . . . . . . . . . . . . . . . 125 3.4.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 130 3.4.3 Properties of expectation . . . . . . . . . . . . . . . . . . . . . . . . . 130 3.4.4 Moments and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 3.5 Common Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . 136 3.5.1 Bernoulli random variable . . . . . . . . . . . . . . . . . . . . . . . . . 137 3.5.2 Binomial random variable . . . . . . . . . . . . . . . . . . . . . . . . . 143 3.5.3 Geometric random variable . . . . . . . . . . . . . . . . . . . . . . . . 149 3.5.4 Poisson random variable . . . . . . . . . . . . . . . . . . . . . . . . . . 152 3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 3.7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 3.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 viii
CONTENTS 4 Continuous Random Variables 171 4.1 Probability Density Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 4.1.1 Some intuitions about probability density functions. . . . . . . . . . . 172 4.1.2 More in-depth discussion about PDFs . . . . . . . . . . . . . . . . . . 174 4.1.3 Connecting with the PMF . . . . . . . . . . . . . . . . . . . . . . . . . 178 4.2 Expectation, Moment, and Variance . . . . . . . . . . . . . . . . . . . . . . . 180 4.2.1 Definition and properties . . . . . . . . . . . . . . . . . . . . . . . . . 180 4.2.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 183 4.2.3 Moment and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 4.3 Cumulative Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . 185 4.3.1 CDF for continuous random variables . . . . . . . . . . . . . . . . . . 186 4.3.2 Properties of CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 4.3.3 Retrieving PDF from CDF . . . . . . . . . . . . . . . . . . . . . . . . 193 4.3.4 CDF: Unifying discrete and continuous random variables . . . . . . . 194 4.4 Median, Mode, and Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.4.1 Median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.4.2 Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.4.3 Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 4.5 Uniform and Exponential Random Variables. . . . . . . . . . . . . . . . . . . 201 4.5.1 Uniform random variables . . . . . . . . . . . . . . . . . . . . . . . . . 202 4.5.2 Exponential random variables . . . . . . . . . . . . . . . . . . . . . . . 205 4.5.3 Origin of exponential random variables. . . . . . . . . . . . . . . . . . 207 4.5.4 Applications of exponential random variables . . . . . . . . . . . . . . 209 4.6 Gaussian Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 4.6.1 Definition of a Gaussian random variable . . . . . . . . . . . . . . . . 211 4.6.2 Standard Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 4.6.3 Skewness and kurtosis . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 4.6.4 Origin of Gaussian random variables . . . . . . . . . . . . . . . . . . 220 4.7 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 223 4.7.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 4.7.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 4.8 Generating Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 4.8.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 4.8.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 4.10 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 4.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 5 Joint Distributions 241 5.1 Joint PMF and Joint PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 5.1.1 Probability measure in 2D . . . . . . . . . . . . . . . . . . . . . . . . . 244 5.1.2 Discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . 245 5.1.3 Continuous random variables . . . . . . . . . . . . . . . . . . . . . . . 247 5.1.4 Normalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 5.1.5 Marginal PMF and marginal PDF . . . . . . . . . . . . . . . . . . . . 250 5.1.6 Independent random variables . . . . . . . . . . . . . . . . . . . . . . 252 5.1.7 Joint CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 5.2 Joint Expectation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 ix
CONTENTS 5.2.1 Definition and interpretation . . . . . . . . . . . . . . . . . . . . . . . 257 5.2.2 Covariance and correlation coefficient . . . . . . . . . . . . . . . . . . 262 5.2.3 Independence and correlation . . . . . . . . . . . . . . . . . . . . . . . 264 5.2.4 Computing correlation from data . . . . . . . . . . . . . . . . . . . . . 265 5.3 Conditional PMF and PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 5.3.1 Conditional PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 5.3.2 Conditional PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 5.4 Conditional Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 5.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 5.4.2 The law of total expectation . . . . . . . . . . . . . . . . . . . . . . . 276 5.5 Sum of Two Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 280 5.5.1 Intuition through convolution . . . . . . . . . . . . . . . . . . . . . . . 280 5.5.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 5.5.3 Sum of common distributions . . . . . . . . . . . . . . . . . . . . . . . 282 5.6 Random Vectors and Covariance Matrices . . . . . . . . . . . . . . . . . . . . 286 5.6.1 PDF of random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 286 5.6.2 Expectation of random vectors . . . . . . . . . . . . . . . . . . . . . . 288 5.6.3 Covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 5.6.4 Multidimensional Gaussian . . . . . . . . . . . . . . . . . . . . . . . . 290 5.7 Transformation of Multidimensional Gaussians . . . . . . . . . . . . . . . . . 293 5.7.1 Linear transformation of mean and covariance. . . . . . . . . . . . . . 293 5.7.2 Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 295 5.7.3 Covariance matrices are always positive semi-definite . . . . . . . . . . 297 5.7.4 Gaussian whitening . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 5.8 Principal-Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 303 5.8.1 The main idea: Eigendecomposition . . . . . . . . . . . . . . . . . . . 303 5.8.2 The eigenface problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 5.8.3 What cannot be analyzed by PCA? . . . . . . . . . . . . . . . . . . . 311 5.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 5.10 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 5.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315 6 Sample Statistics 319 6.1 Moment-Generating and Characteristic Functions . . . . . . . . . . . . . . . . 324 6.1.1 Moment-generating function. . . . . . . . . . . . . . . . . . . . . . . . 324 6.1.2 Sum of independent variables via MGF . . . . . . . . . . . . . . . . . 327 6.1.3 Characteristic functions . . . . . . . . . . . . . . . . . . . . . . . . . . 329 6.2 Probability Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 6.2.1 Union bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 6.2.2 The Cauchy-Schwarz inequality . . . . . . . . . . . . . . . . . . . . . . 335 6.2.3 Jensen’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 6.2.4 Markov’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339 6.2.5 Chebyshev’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 6.2.6 Chernoff’s bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 6.2.7 Comparing Chernoff and Chebyshev . . . . . . . . . . . . . . . . . . . 344 6.2.8 Hoeffding’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 6.3 Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 6.3.1 Sample average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 x
CONTENTS 6.3.2 Weak law of large numbers (WLLN) . . . . . . . . . . . . . . . . . . . 354 6.3.3 Convergence in probability . . . . . . . . . . . . . . . . . . . . . . . . 356 6.3.4 Can we prove WLLN using Chernoff’s bound? . . . . . . . . . . . . . 359 6.3.5 Does the weak law of large numbers always hold? . . . . . . . . . . . . 360 6.3.6 Strong law of large numbers . . . . . . . . . . . . . . . . . . . . . . . . 361 6.3.7 Almost sure convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 362 6.3.8 Proof of the strong law of large numbers . . . . . . . . . . . . . . . . . 364 6.4 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 6.4.1 Convergence in distribution . . . . . . . . . . . . . . . . . . . . . . . . 368 6.4.2 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 372 6.4.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 6.4.4 Limitation of the Central Limit Theorem . . . . . . . . . . . . . . . . 379 6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 6.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 6.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 7 Regression 389 7.1 Principles of Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394 7.1.1 Intuition: How to fit a straight line? . . . . . . . . . . . . . . . . . . . 395 7.1.2 Solving the linear regression problem . . . . . . . . . . . . . . . . . . . 397 7.1.3 Extension: Beyond a straight line . . . . . . . . . . . . . . . . . . . . . 401 7.1.4 Overdetermined and underdetermined systems . . . . . . . . . . . . . 409 7.1.5 Robust linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . 412 7.2 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 7.2.1 Overview of overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 7.2.2 Analysis of the linear case . . . . . . . . . . . . . . . . . . . . . . . . . 420 7.2.3 Interpreting the linear analysis results . . . . . . . . . . . . . . . . . . 425 7.3 Bias and Variance Trade-Off. . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 7.3.1 Decomposing the testing error . . . . . . . . . . . . . . . . . . . . . . 430 7.3.2 Analysis of the bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 7.3.3 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436 7.3.4 Bias and variance on the learning curve . . . . . . . . . . . . . . . . . 438 7.4 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 7.4.1 Ridge regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 7.4.2 LASSO regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 7.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 7.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458 7.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459 8 Estimation 465 8.1 Maximum-Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 468 8.1.1 Likelihood function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468 8.1.2 Maximum-likelihood estimate . . . . . . . . . . . . . . . . . . . . . . . 472 8.1.3 Application 1: Social network analysis . . . . . . . . . . . . . . . . . . 478 8.1.4 Application 2: Reconstructing images . . . . . . . . . . . . . . . . . . 481 8.1.5 More examples of ML estimation . . . . . . . . . . . . . . . . . . . . . 484 8.1.6 Regression versus ML estimation . . . . . . . . . . . . . . . . . . . . . 488 8.2 Properties of ML Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 xi
CONTENTS 8.2.1 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 8.2.2 Unbiased estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 8.2.3 Consistent estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 494 8.2.4 Invariance principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500 8.3 Maximum A Posteriori Estimation . . . . . . . . . . . . . . . . . . . . . . . . 503 8.3.1 The trio of likelihood, prior, and posterior . . . . . . . . . . . . . . . . 503 8.3.2 Understanding the priors . . . . . . . . . . . . . . . . . . . . . . . . . 504 8.3.3 MAP formulation and solution . . . . . . . . . . . . . . . . . . . . . . 506 8.3.4 Analyzing the MAP solution . . . . . . . . . . . . . . . . . . . . . . . 508 8.3.5 Analysis of the posterior distribution . . . . . . . . . . . . . . . . . . . 512 8.3.6 Conjugate prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 8.3.7 Linking MAP with regression . . . . . . . . . . . . . . . . . . . . . . . 517 8.4 Minimum Mean-Square Estimation . . . . . . . . . . . . . . . . . . . . . . . . 521 8.4.1 Positioning the minimum mean-square estimation . . . . . . . . . . . 521 8.4.2 Mean squared error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 8.4.3 MMSE estimate = conditional expectation . . . . . . . . . . . . . . . 524 8.4.4 MMSE estimator for multidimensional Gaussian . . . . . . . . . . . . 530 8.4.5 Linking MMSE and neural networks . . . . . . . . . . . . . . . . . . . 533 8.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534 8.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535 8.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536 9 Confidence and Hypothesis 543 9.1 Confidence Interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545 9.1.1 The randomness of an estimator . . . . . . . . . . . . . . . . . . . . . 545 9.1.2 Understanding confidence intervals . . . . . . . . . . . . . . . . . . . . 547 9.1.3 Constructing a confidence interval . . . . . . . . . . . . . . . . . . . . 550 9.1.4 Properties of the confidence interval . . . . . . . . . . . . . . . . . . . 553 9.1.5 Student’s t-distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 556 9.1.6 Comparing Student’s t-distribution and Gaussian . . . . . . . . . . . . 560 9.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 9.2.1 A brute force approach . . . . . . . . . . . . . . . . . . . . . . . . . . 562 9.2.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 9.3 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 9.3.1 What is a hypothesis? . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 9.3.2 Critical-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569 9.3.3 p-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573 9.3.4 Z-test and T-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576 9.4 Neyman-Pearson Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 9.4.1 Null and alternative distributions . . . . . . . . . . . . . . . . . . . . . 579 9.4.2 Type 1 and type 2 errors . . . . . . . . . . . . . . . . . . . . . . . . . 581 9.4.3 Neyman-Pearson decision . . . . . . . . . . . . . . . . . . . . . . . . . 584 9.5 ROC and Precision-Recall Curve . . . . . . . . . . . . . . . . . . . . . . . . . 591 9.5.1 Receiver Operating Characteristic (ROC) . . . . . . . . . . . . . . . . 591 9.5.2 Comparing ROC curves . . . . . . . . . . . . . . . . . . . . . . . . . . 594 9.5.3 The ROC curve in practice . . . . . . . . . . . . . . . . . . . . . . . . 600 9.5.4 The Precision-Recall (PR) curve . . . . . . . . . . . . . . . . . . . . . 603 9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607 xii
CONTENTS 9.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608 9.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 10 Random Processes 613 10.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614 10.1.1 Everything you need to know about a random process . . . . . . . . . 614 10.1.2 Statistical and temporal perspectives . . . . . . . . . . . . . . . . . . . 616 10.2 Mean and Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . . . 620 10.2.1 Mean function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620 10.2.2 Autocorrelation function . . . . . . . . . . . . . . . . . . . . . . . . . . 624 10.2.3 Independent processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 631 10.3 Wide-Sense Stationary Processes . . . . . . . . . . . . . . . . . . . . . . . . . 632 10.3.1 Definition of a WSS process . . . . . . . . . . . . . . . . . . . . . . . . 633 10.3.2 Properties of R (τ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634 X 10.3.3 Physical interpretation of R (τ) . . . . . . . . . . . . . . . . . . . . . 635 X 10.4 Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 10.4.1 Basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 10.4.2 Origin of the power spectral density . . . . . . . . . . . . . . . . . . . 643 10.5 WSS Process through LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . 646 10.5.1 Review of linear time-invariant systems . . . . . . . . . . . . . . . . . 646 10.5.2 Mean and autocorrelation through LTI Systems. . . . . . . . . . . . . 647 10.5.3 Power spectral density through LTI systems . . . . . . . . . . . . . . . 649 10.5.4 Cross-correlation through LTI Systems . . . . . . . . . . . . . . . . . . 652 10.6 Optimal Linear Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656 10.6.1 Discrete-time random processes . . . . . . . . . . . . . . . . . . . . . . 656 10.6.2 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657 10.6.3 Yule-Walker equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 659 10.6.4 Linear prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 10.6.5 Wiener filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665 10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672 10.8 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673 10.8.1 The Mean-Square Ergodic Theorem . . . . . . . . . . . . . . . . . . . 677 10.9 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678 10.10Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679 A Appendix 683 xiii
CONTENTS xiv
Chapter 1 Mathematical Background “Datascience”hasdifferentmeaningstodifferentpeople.Ifyouaskabiologist,datascience could mean analyzing DNA sequences. If you ask a banker, data science could mean pre- dicting the stock market. If you ask a software engineer, data science could mean programs anddatastructures;ifyouaskamachinelearningscientist,datasciencecouldmeanmodels and algorithms. However, one thing that is common in all these disciplines is the concept of uncertainty. We choose to learn from data because we believe that the latent information is embedded in the data — unprocessed, contains noise, and could have missing entries. If there is no randomness, all data scientists can close their business because there is simply no problem to solve. However, the moment we see randomness, our business comes back. Therefore, data science is the subject of making decisions in uncertainty. Themathematicsofanalyzinguncertaintyisprobability.Itisthe tooltohelpusmodel, analyze,andpredictrandomevents.Probabilitycanbestudiedinasmanywaysasyoucan thinkof.Youcantakearigorouscourseinprobabilitytheory,ora“probabilityfordummies” on the internet, or a typical undergraduate probability course offered by your school. This bookisdifferentfromallthese.Ourgoalistotellyouhowthingswork inthecontextofdata science. For example, why do we need those three axioms of probabilities and not others? Where does the “bell shape” Gaussian random variable come from? How many samples do we need to construct a reliable histogram? These questions are at the core of data science, and they deserve close attention rather than sweeping them under the rug. Tohelpyougetusedtothepaceandstyleofthisbook,inthischapter,wereviewsome of the very familiar topics in undergraduate algebra and calculus. These topics are meant to warm up your mathematics background so that you can follow the subsequent chapters. Specifically, in this chapter, we cover several topics. First, in Section 1.1 we discuss infinite series,somethingthatwillbeusedfrequentlywhenweevaluatetheexpectationandvariance of random variables in Chapter 3. In Section 1.2 we review the Taylor approximation, which will be helpful when we discuss continuous random variables. Section 1.3 discusses integration and reviews several tricks we can use to make integration easy. Section 1.4 deals with linear algebra, aka matrices and vectors, which are fundamental to modern data analysis. Finally, Section 1.5 discusses permutation and combination, two basic techniques to count events. 1
CHAPTER 1. MATHEMATICAL BACKGROUND 1.1 Infinite Series Imagine that you have a fair coin. If you get a tail, you flip it again. You do this repeatedly until you finally get a head. What is the probability that you need to flip the coin three times to get one head? This is a warm-up exercise. Since the coin is fair, the probability of obtaining a head is 1. The probability of getting a tail followed by a head is 1 × 1 = 1. Similarly, the 2 2 2 4 probabilityofgettingtwotailsandthenaheadis 1×1×1 = 1.Ifyoufollowthislogic,you 2 2 2 8 canwritedowntheprobabilitiesforallothercases.Foryourconvenience,wehavedrawnthe first few in Figure 1.1. As you have probably noticed, the probabilities follow the pattern {1,1,1,...}. 2 4 8 Figure 1.1: Supposeyouflipacoinuntilyouseeahead.ThisrequiresyoutohaveN−1tailsfollowed byahead.Theprobabilityofthissequenceofeventsare 1, 1, 1,...,whichformsaninfinitesequence. 2 4 8 We can also summarize these probabilities using a familiar plot called the histogram as shown in Figure 1.2. The histogram for this problem has a special pattern, that every value is one order higher than the preceding one, and the sequence is infinitely long. 0.5 0.4 0.3 0.2 0.1 0 1 2 3 4 5 6 7 8 9 10 Figure1.2:Thehistogramofflippingacoinuntilweseeahead.Thex-axisisthenumberofcoinflips, and the y-axis is the probability. Let us ask something harder: On average, if you want to be 90% sure that you will get a head, what is the minimum number of attempts you need to try? Five attempts? Ten attempts? Indeed, if you try ten attempts, you will very likely accomplish your goal. However, this would seem to be overkill. If you try five attempts, then it becomes unclear whether you will be 90% sure. 2
1.1. INFINITE SERIES This problem can be answered by analyzing the sequence of probabilities. If we make two attempts, then the probability of getting a head is the sum of the probabilities for one attempt and that of two attempts: 1 P[success after 1 attempt]= =0.5 2 1 1 P[success after 2 attempts]= + =0.75 2 4 Therefore, if you make 3 attempts or 4 attempts, you get the following probabilities: 1 1 1 P[success after 3 attempts]= + + =0.875 2 4 8 1 1 1 1 P[success after 4 attempts]= + + + =0.9375. 2 4 8 16 So if we try four attempts, we will have a 93.75% probability of getting a head. Thus, four attempts is the answer. The MATLAB / Python codes we used to generate Figure 1.2 are shown below. % MATLAB code to generate a geometric sequence p = 1/2; n = 1:10; X = p.^n; bar(n,X,’FaceColor’,[0.8, 0.2,0.2]); # Python code to generate a geometric sequence import numpy as np import matplotlib.pyplot as plt p = 1/2 n = np.arange(0,10) X = np.power(p,n) plt.bar(n,X) Thiswarm-upexercisehasperhapsraisedsomeofyourinterestinthesubject.However, we will not tell you everything now. We will come back to the probability in Chapter 3 when we discuss geometric random variables. In the present section, we want to make sure you have the basic mathematical tools to calculate quantities, such as a sum of fractional numbers. For example, what if we want to calculate P[success after 107 attempts]? Is there a systematic way of performing the calculation? Remark. You should be aware that the 93.75% only says that the probability of achieving thegoalishigh.Ifyouhaveabadday,youmaystillneedmorethanfourattempts.Therefore, when we stated the question, we asked for 90% “on average”. Sometimes you may need more attempts and sometimes fewer attempts, but on average, you have a 93.75% chance of succeeding. 1.1.1 Geometric Series A geometric series is the sum of a finite or an infinite sequence of numbers with a constant ratio between successive terms. As we have seen in the previous example, a geometric series 3
CHAPTER 1. MATHEMATICAL BACKGROUND appears naturally in the context of discrete events. In Chapter 3 of this book, we will use geometric series when calculating the expectation and moments of a random variable. Definition 1.1. Let 0<r <1, a finite geometric sequence of power n is a sequence of numbers (cid:26) (cid:27) 1,r,r2,...,rn . An infinite geometric sequence is a sequence of numbers (cid:26) (cid:27) 1,r,r2,r3,... . Theorem 1.1. The sum of a finite geometric series of power n is (cid:88)n 1−rn+1 rk =1+r+r2+···+rn = . (1.1) 1−r k=0 Proof. We multiply both sides by 1−r. The left hand side becomes (cid:32) n (cid:33) (cid:88) rk (1−r)=(cid:0) 1+r+r2+···+rn(cid:1) (1−r) k=0 =(cid:0) 1+r+r2+···+rn(cid:1) −(cid:0) r+r2+r3+···+rn+1(cid:1) ( =a) 1−rn+1, where (a) holds because terms are canceled due to subtractions. (cid:3) A corollary of Equation (1.1) is the sum of an infinite geometric sequence. Corollary 1.1. Let 0<r <1. The sum of an infinite geometric series is ∞ (cid:88) 1 rk =1+r+r2+···= . (1.2) 1−r k=0 Proof. We take the limit in Equation (1.1). This yields (cid:88)∞ (cid:88)n 1−rn+1 1 rk = lim rk = lim = . n→∞ n→∞ 1−r 1−r k=0 k=0 (cid:3) Remark. Note that the condition 0 < r < 1 is important. If r > 1, then the limit lim rn+1 in Equation (1.2) will diverge. The constant r cannot equal to 1, for oth- n→∞ erwise the fraction (1−rn+1)/(1−r) is undefined. We are not interested in the case when r =0, because the sum is trivially 1: (cid:80)∞ 0k =1+01+02+···=1. k=0 4
1.1. INFINITE SERIES ∞ Practice Exercise 1.1. Compute the infinite series (cid:80) 1 . 2k k=2 Solution. ∞ (cid:88) 1 1 1 = + +···+ 2k 4 8 k=2 (cid:18) (cid:19) 1 1 1 = 1+ + +··· 4 2 4 1 1 1 = · = . 4 1− 1 2 2 Remark. You should not be confused about a geometric series and a harmonic series. A harmonic series concerns with the sum of {1,1,1,1,...}. It turns out that1 2 3 4 ∞ (cid:88) 1 1 1 1 =1+ + + +···=∞. n 2 3 4 n=1 On the other hand, a squared harmonic series {1, 1 , 1 , 1 ,...} converges: 22 32 42 (cid:88)∞ 1 1 1 1 π2 =1+ + + +···= . n2 22 32 42 6 n=1 The latter result is known as the Basel problem. We can extend the main theorem by considering more complicated series, for example the following one. Corollary 1.2. Let 0<r <1. It holds that ∞ (cid:88) 1 krk−1 =1+2r+3r2+···= . (1.3) (1−r)2 k=1 Proof. Take the derivative on both sides of Equation (1.2). The left hand side becomes ∞ d (cid:88) rk = d (cid:0) 1+r+r2+···(cid:1) dr dr k=0 ∞ (cid:88) =1+2r+3r2+···= krk−1 k=1 (cid:18) (cid:19) d 1 1 The right hand side becomes = . dr 1−r (1−r)2 (cid:3) 1ThisresultcanbefoundinTomApostol,Mathematical Analysis,2ndEdition,Theorem8.11. 5
CHAPTER 1. MATHEMATICAL BACKGROUND Practice Exercise 1.2. Compute the infinite sum (cid:80)∞ k· 1 . k=1 3k Solution. We can use the derivative result: ∞ (cid:88) 1 1 1 1 k· =1· +2· +3· +··· 3k 3 9 27 k=1 (cid:18) (cid:19) 1 1 1 1 1 1 1 3 = · 1+2· +3· +··· = · = · = . 3 3 9 3 (1− 1)2 3 4 4 3 9 1.1.2 Binomial Series A geometric series is useful when handling situations such as N −1 failures followed by a success. However, we can easily twist the problem by asking: What is the probability of getting one head out of 3 independent coin tosses? In this case, the probability can be determined by enumerating all possible cases: P[1 head in 3 coins]=P[H,T,T]+P[T,H,T]+P[T,T,H] (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 1 1 1 1 1 1 = × × + × × + × × 2 2 2 2 2 2 2 2 2 3 = . 8 Figure 1.3 illustrates the situation. Figure 1.3: When flipping three coins independently, the probability of getting exactly one head can come from three different possibilities. What lessons have we learned in this example? Notice that you need to enumerate all possible combinations of one head and two tails to solve this problem. The number is 3 in our example. In general, the number of combinations can be systematically studied using combinatorics, which we will discuss later in the chapter. However, the number of combinations motivates us to discuss another background technique known as the binomial series. The binomial series is instrumental in algebra when handling polynomials such as (a+b)2 or (1+x)3. It provides a valuable formula when computing these powers. Theorem1.2 (Binomialtheorem). Foranyrealnumbersaandb,thebinomialseries of power n is n (cid:18) (cid:19) (cid:88) n (a+b)n = an−kbk, (1.4) k k=0 where (cid:0)n(cid:1) = n! . k k!(n−k)! 6
1.1. INFINITE SERIES The binomial theorem is valid for any real numbers a and b. The quantity (cid:0)n(cid:1) reads k as “n choose k”. Its definition is (cid:18) (cid:19) n n! def = , k k!(n−k)! where n! = n(n−1)(n−2)···3·2·1. We shall discuss the physical meaning of (cid:0)n(cid:1) in k Section 1.5. But we can quickly plug in the “n choose k” into the coin flipping example by letting n=3 and k =1: (cid:18) (cid:19) 3 3! Number of combinations for 1 head and 2 tails= = =3. 1 1!2! So you can see why we want you to spend your precious time learning about the binomial theorem. In MATLAB and Python, (cid:0)n(cid:1) can be computed using the commands as follows. k % MATLAB code to compute (N choose K) and K! n = 10; k = 2; nchoosek(n,k) factorial(k) # Python code to compute (N choose K) and K! from scipy.special import comb, factorial n = 10 k = 2 comb(n, k) factorial(k) The binomial theorem makes the most sense when we also learn about the Pascal’s identity. Theorem 1.3 (Pascal’s identity). Let n and k be positive integers such that k ≤ n. Then, (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) n n n+1 + = . (1.5) k k−1 k Proof. We start by recalling the definition of (cid:0)n(cid:1) . This gives us k (cid:18) (cid:19) (cid:18) (cid:19) n n n! n! + = + k k−1 k!(n−k)! (k−1)!(n−(k−1))! (cid:18) (cid:19) 1 1 =n! + , k!(n−k)! (k−1)!(n−k+1)! where we factor out n! to obtain the second equation. Next, we observe that 1 (n−k+1) n−k+1 × = , k!(n−k)! (n−k+1) k!(n−k+1)! 1 k k × = . (k−1)!(n−k+1)! k k!(n−k+1)! 7
CHAPTER 1. MATHEMATICAL BACKGROUND Substituting into the previous equation we obtain (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) n n n−k+1 k + =n! + k k−1 k!(n−k+1)! k!(n−k+1)! (cid:18) (cid:19) n+1 =n! k!(n−k+1)! (cid:18) (cid:19) (n+1)! n+1 = = . k!(n+1−k)! k (cid:3) The Pascal triangle is a visualization of the coefficients of (a+b)n as shown in Fig- ure 1.4.Forexample,whenn=5,weknowthat(cid:0)5(cid:1) =10.However,byPascal’sidentity,we 3 know that (cid:0)5(cid:1) =(cid:0)4(cid:1) +(cid:0)4(cid:1) . So the number 10 is actually obtained by summing the numbers 3 2 3 4 and 6 of the previous row. Figure 1.4: Pascal triangle for n = 0,...,5. Note that a number in one row is obtained by summing two numbers directly above it. Practice Exercise 1.3. Find (1+x)3. Solution. Using the binomial theorem, we can show that n (cid:18) (cid:19) (cid:88) 3 (1+x)3 = 13−kxk =1+3x+3x2+x3. k k=0 Practice Exercise 1.4. Let 0<p<1. Find n (cid:18) (cid:19) (cid:88) n pn−k(1−p)k. k k=0 Solution. By using the binomial theorem, we have n (cid:18) (cid:19) (cid:88) n pn−k(1−p)k =(p+(1−p))n =1. k k=0 This result will be helpful when evaluating binomial random variables in Chapter 3. 8
1.1. INFINITE SERIES We now prove the binomial theorem. Please feel free to skip the proof if this is your first time reading the book. Proof of the binomial theorem. We prove by induction. When n=1, 1 (cid:88) (a+b)1 =a+b= a1−kbk. k=0 Therefore, the base case is verified. Assume up to case n. We need to verify case n+1. n (cid:18) (cid:19) (cid:88) n (a+b)n+1 =(a+b)(a+b)n =(a+b) an−kbk k k=0 n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n = an−k+1bk+ an−kbk+1. k k k=0 k=0 WewanttoapplythePascal’sidentitytocombinethetwoterms.Inordertodoso,wenote that the second term in this sum can be rewritten as n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n an−kbk+1 = an+1−k−1bk+1 k k k=0 k=0 n+1(cid:18) (cid:19) (cid:88) n = an+1−(cid:96)b(cid:96), where (cid:96)=k+1 (cid:96)−1 (cid:96)=1 n (cid:18) (cid:19) (cid:88) n = an+1−(cid:96)b(cid:96)+bn+1. (cid:96)−1 (cid:96)=1 The first term in the sum can be written as n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n an−k+1bk = an+1−(cid:96)b(cid:96)+an+1, where (cid:96)=k. k (cid:96) k=0 (cid:96)=1 Therefore, the two terms can be combined using Pascal’s identity to yield n (cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21) (cid:88) n n (a+b)n+1 = + an+1−(cid:96)b(cid:96)+an+1+bn+1 (cid:96) (cid:96)−1 (cid:96)=1 n (cid:18) (cid:19) n+1(cid:18) (cid:19) (cid:88) n+1 (cid:88) n+1 = an+1−(cid:96)b(cid:96)+an+1+bn+1 = an+1−(cid:96)b(cid:96). (cid:96) (cid:96) (cid:96)=1 (cid:96)=0 Hence, the (n+1)th case is also verified. By the principle of mathematical induction, we have completed the proof. (cid:3) The end of the proof. Please join us again. 9
CHAPTER 1. MATHEMATICAL BACKGROUND 1.2 Approximation Consider a function f(x)=log(1+x), for x>0 as shown in Figure 1.5. This is a nonlinear function, and we all know that nonlinear functions are not fun to deal with. For example, (cid:82)b if you want to integrate the function xlog(1+x) dx, then the logarithm will force you a to do integration by parts. However, in many practical problems, you may not need the full range of x > 0. Suppose that you are only interested in values x (cid:28) 1. Then the logarithm can be approximated, and thus the integral can also be approximated. 2 0.2 1.5 0.15 1 0.1 0.5 0.05 0 0 0 1 2 3 4 5 0 0.05 0.1 0.15 0.2 Figure 1.5: The function f(x)=log(1+x) and the approximation f(cid:98)(x)=x. To see how this is even possible, we show in Figure 1.5 the nonlinear function f(x)= log(1+x)andanapproximationf(cid:98)(x)=x.Theapproximationiscarefullychosensuchthat for x (cid:28) 1, the approximation f(cid:98)(x) is close to the true function f(x). Therefore, we can argue that for x(cid:28)1, log(1+x)≈x, (1.6) thereby simplifying the calculation. For example, if you want to integrate xlog(1+x) for 0 < x < 0.1, then the integral can be approximated by (cid:82)0.1 xlog(1+x) dx ≈ (cid:82)0.1 x2 dx = 0 0 x3 = 3.33×10−4. (The actual integral is 3.21×10−4.) In this section we will learn about 3 the basic approximation techniques. We will use them when we discuss limit theorems in Chapter 6, as well as various distributions, such as from binomial to Poisson. 1.2.1 Taylor approximation Given a function f : R → R, it is often useful to analyze its behavior by approximating f using its local information. Taylor approximation (or Taylor series) is one of the tools for such a task. We will use the Taylor approximation on many occasions. Definition 1.2 (TaylorApproximation). Letf :R→Rbeacontinuousfunctionwith infinite derivatives. Let a ∈ R be a fixed constant. The Taylor approximation of f at 10
1.2. APPROXIMATION x=a is f(cid:48)(cid:48)(a) f(x)=f(a)+f(cid:48)(a)(x−a)+ (x−a)2+··· 2! (cid:88)∞ f(n)(a) = (x−a)n, (1.7) n! n=0 where f(n) denotes the nth-order derivative of f. Taylor approximation is a geometry-based approximation. It approximates the function according to the offset, slope, curvature, and so on. According to Definition 1.2, the Taylor series has an infinite number of terms. If we use a finite number of terms, we obtain the nth-order Taylor approximation: First-Order: f(x)=f(a)+f(cid:48)(a)(x−a)+O((x−a)2) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) offset slope f(cid:48)(cid:48)(a) Second-Order: f(x)=f(a)+f(cid:48)(a)(x−a)+ (x−a)2+O((x−a)3). 2! (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) offset slope curvature Here, the big-O notation O(εk) means any term that has an order at least power k. For small ε, i.e., ε(cid:28)1, a high-order term O(εk)≈0 for large k. Example 1.1. Let f(x)=sinx. Then the Taylor approximation at x=0 is f(cid:48)(cid:48)(0) f(cid:48)(cid:48)(cid:48)(0) f(x)≈f(0)+f(cid:48)(0)(x−0)+ (x−0)2+ (x−0)3 2! 3! sin(0) cos(0) =sin(0)+(cos0)(x−0)− (x−0)2− (x−0)3 2! 3! x3 x3 =0+x−0− =x− . 6 6 We can expand further to higher orders, which yields x3 x5 x7 f(x)=x− + − +··· 3! 5! 7! We show the first few approximations in Figure 1.6. OneshouldberemindedthatTaylorapproximationapproximatesafunctionf(x) at a particular point x = a. Therefore, the approximation of f near x = 0 and the approximationoff nearx=π/2aredifferent.Forexample,theTaylorapproximation at x=π/2 for f(x)=sinx is π π (cid:16) π(cid:17) sinπ (cid:16) π(cid:17)2 cosπ (cid:16) π(cid:17)3 f(x)=sin +cos x− − 2 x− − 2 x− 2 2 2 2! 2 3! 2 1(cid:16) π(cid:17)2 1(cid:16) π(cid:17)2 =1+0− x− −0=1− x− . 4 2 4 2 11
CHAPTER 1. MATHEMATICAL BACKGROUND 4 4 sin x sin x 3rd order 3rd order 2 5th order 2 5th order 7th order 7th order 0 0 -2 -2 -4 -4 -10 -5 0 5 10 -10 -5 0 5 10 x x (a) Approximate at x=0 (b) Approximate at x=π/2 Figure 1.6: Taylor approximation of the function f(x)=sinx. 1.2.2 Exponential series An immediate application of the Taylor approximation is to derive the exponential series. Theorem 1.4. Let x be any real number. Then, x2 x3 (cid:88)∞ xk ex =1+x+ + +···= . (1.8) 2 3! k! k=0 Proof. Let f(x)=ex for any x. Then, the Taylor approximation around x=0 is f(cid:48)(cid:48)(0) f(x)=f(0)+f(cid:48)(0)(x−0)+ (x−0)2+··· 2! e0 =e0+e0(x−0)+ (x−0)2+··· 2! x2 (cid:88)∞ xk =1+x+ +···= . 2 k! k=0 (cid:3) (cid:88)∞ λke−λ Practice Exercise 1.5. Evaluate . k! k=0 Solution. (cid:88)∞ λke−λ (cid:88)∞ λk =e−λ =e−λeλ =1. k! k! k=0 k=0 This result will be useful for Poisson random variables in Chapter 3. 12
1.2. APPROXIMATION √ If we substitute x=jθ where j = −1, then we can show that (jθ)2 ejθ =1+jθ+ +··· (cid:124)(cid:123)(cid:122)(cid:125) 2! =cosθ+jsinθ (cid:18) θ2 θ4 (cid:19) (cid:18) θ3 (cid:19) = 1− + +··· +j θ− +··· 2! 4! 3! (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) real imaginary Matching the real and the imaginary terms, we can show that θ2 θ4 cosθ =1− + +··· 2! 4! θ3 θ5 sinθ =θ− + +··· 3! 5! This gives the infinite series representations of the two trigonometric functions. 1.2.3 Logarithmic approximation Taylor approximation also allows us to find approximations to logarithmic functions. We start by presenting a lemma. Lemma 1.1. Let 0<x<1 be a constant. Then, x2 log(1+x)=x− +O(x3). (1.9) 2 Proof. Let f(x)=log(1+x). Then, the derivatives of f are 1 1 f(cid:48)(x)= , and f(cid:48)(cid:48)(x)=− . (1+x) (1+x)2 Taylor approximation at x=0 gives f(cid:48)(cid:48)(0) f(x)=f(0)+f(cid:48)(0)(x−0)+ (x−0)2+O(x3) 2 (cid:18) 1 (cid:19) (cid:18) 1 (cid:19) x2 =log1+ x− +O(x3) (1+0) (1+0)2 2 x2 =x− +O(x3). 2 (cid:3) The difference between this result and the result we showed in the beginning of this section is the order of polynomials we used to approximate the logarithm: • First-order: log(1+x)=x • Second-order: log(1+x)=x−x2/2. What order of approximation is good? It depends on where you want the approximation to be good, and how far you want the approximation to go. The difference between first-order and second-order approximations is shown in Figure 1.7. 13
CHAPTER 1. MATHEMATICAL BACKGROUND 2 2 1.5 1.5 1 1 0.5 0.5 0 0 0 1 2 3 4 5 0 1 2 3 4 5 First-order approximation Second-order approximation Figure 1.7: The function f(x)=log(1+x), the first-order approximation f(cid:98)(x)=x, and the second- order approximation f(cid:98)(x)=x−x2/2. Example 1.2. When we prove the Central Limit Theorem in Chapter 6, we need to use the following result. (cid:18) s2 (cid:19)N lim 1+ =es2/2. N→∞ 2N The proof of this equation can be done using the Taylor approximation. Consider (cid:16) (cid:17) Nlog 1+ s2 . By the logarithmic lemma, we can obtain the second-order approxi- N mation: (cid:18) s2 (cid:19) s2 s4 log 1+ = − . 2N 2N 4N2 Therefore, multiplying both sides by N yields (cid:18) s2 (cid:19) s2 s4 Nlog 1+ = − . 2N 2 4N Putting the limit N →∞ we can show that (cid:26) (cid:18) s2 (cid:19)(cid:27) s2 lim Nlog 1+ = . N→∞ 2N 2 Taking exponential on both sides yields (cid:26) (cid:18) s2 (cid:19)(cid:27) (cid:26) s2(cid:27) exp lim Nlog 1+ =exp . N→∞ 2N 2 Moving the limit outside the exponential yields the result. Figure 1.8 provides a pic- torial illustration. 14
1.3. INTEGRATION 1.8 1.6 1.4 1.2 1 0 0.2 0.4 0.6 0.8 1 Figure 1.8: We plot a sequence of function f (x)=(cid:16) 1+ s2 (cid:17)N and its limit f(x)=es2/2. N 2N 1.3 Integration When you learned calculus, your teacher probably told you that there are two ways to compute an integral: • Substitution: (cid:90) 1(cid:90) f(ax)dx= f(u)du. a • By parts: (cid:90) (cid:90) udv =uv− v du. Besides these two, we want to teach you two more. The first technique is even and odd functions when integrating a function symmetrically about the y-axis. If a function is even, you just need to integrate half of the function. If a function is odd, you will get a zero. The second technique is to leverage the fact that a probability density function integrates to 1. We will discuss the first technique here and defer the second technique to Chapter 4. Besides the two integration techniques, we will review the fundamental theorem of calculus. We will need it when we study cumulative distribution functions in Chapter 4. 1.3.1 Odd and even functions Definition 1.3. A function f :R→R is even if for any x∈R, f(x)=f(−x), (1.10) and f is odd if f(x)=−f(−x). (1.11) 15
CHAPTER 1. MATHEMATICAL BACKGROUND Essentially, an even function flips over about the y-axis, whereas an odd function flips over both the x- and y-axes. Example 1.3. The function f(x)=x2−0.4x4 is even, because f(−x)=(−x)2−0.4(−x)4 =x2−0.4x4 =f(x). See Figure 1.9(a) for illustration. When integrating the function, we have (cid:90) 1 (cid:90) 1 (cid:90) 1 (cid:20) x3 0.4 (cid:21)x=1 38 f(x)dx=2 f(x)dx=2 x2−0.44 dx=2 − x5 = . 3 5 75 −1 0 0 x=0 Example 1.4. The function f(x)=xexp(−x2/2) is odd, because (cid:26) (−x)2(cid:27) (cid:26) x2(cid:27) f(−x)=(−x)exp − =−xexp − =−f(x). 2 2 See Figure 1.9(b) for illustration. When integrating the function, we can let u=−x. Then, the integral becomes (cid:90) 1 (cid:90) 0 (cid:90) 1 f(x)dx= f(x)dx+ f(x)dx −1 −1 0 (cid:90) 1 (cid:90) 1 = f(−u)du+ f(x)dx 0 0 (cid:90) 1 (cid:90) 1 =− f(u)du+ f(x)dx=0. 0 0 1 1 0.5 0.5 0 0 -0.5 -0.5 -1 -1 -1.5 -1 -0.5 0 0.5 1 1.5 -1.5 -1 -0.5 0 0.5 1 1.5 x x (a) Even function (b) Odd function Figure 1.9: An even function is symmetric about the y-axis, and so the integration (cid:82)a f(x) dx = −a 2(cid:82)af(x)dx. An odd function is anti-symmetric about the y-axis. Thus, (cid:82)a f(x)dx=0. 0 −a 16
1.3. INTEGRATION 1.3.2 Fundamental Theorem of Calculus OurfollowingresultistheFundamental Theorem of Calculus.Itisahandytoolthatlinks integration and differentiation. Theorem 1.5 (Fundamental Theorem of Calculus). Let f :[a,b]→R be a continu- ous function defined on a closed interval [a,b]. Then, for any x∈(a,b), d (cid:90) x f(x)= f(t)dt, (1.12) dx a Beforeweprovetheresult,letusunderstandthetheoremifyouhaveforgottenitsmeaning. Example 1.5. Consider a function f(t) = t2. If we integrate the function from 0 to x, we will obtain another function (cid:90) x (cid:90) x x3 F(x)d =ef f(t)dt= t2 dt= . 3 0 0 On the other hand, we can differentiate F(x) to obtain f(x): d d x3 f(x)= F(x)= =x2. dx dx 3 The fundamental theorem of calculus basically puts the two together: d (cid:90) x f(x)= f(t)dt. dx 0 That’s it. Nothing more and nothing less. How can the fundamental theorem of calculus ever be useful when studying probabil- ity? Very soon you will learn two concepts: probability density function and cumulative distribution function. These two functions are related to each other by the fundamental theorem of calculus. To give you a concrete example, we write down the probability density function of an exponential random variable. (Please do not panic about the exponential random variable. Just think of it as a “rapidly decaying” function.) f(x)=e−x, x≥0. It turns out that the cumulative distribution function is (cid:90) x (cid:90) x F(x)= f(t)dt= e−t dt=1−e−x. 0 0 You can also check that f(x) = d F(x). The fundamental theorem of calculus says that if dx youtellmeF(x)=(cid:82)x e−t dt(forwhateverreason),Iwillbeabletotellyouthatf(x)=e−x 0 merely by visually inspecting the integrand without doing the differentiation. Figure 1.10illustratesthepairoffunctionsf(x)=e−x andF(x)=1−e−x.Onething youshouldnoticeisthattheheight ofF(x)istheareaunderthecurveoff(t)from−∞tox. Forexample,inFigure 1.10weshowtheareaunderthecurvefrom0to2.Correspondingly in F(x), the height is F(2). 17
CHAPTER 1. MATHEMATICAL BACKGROUND 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 0 1 2 3 4 5 0 1 2 3 4 5 f(x) F(x) Figure 1.10: The pair of functions f(x)=e−x and F(x)=1−e−x The following proof of the Fundamental Theorem of Calculus can be skipped if it is your first time reading the book. Proof. Our proof is based on Stewart (6th Edition), Section 5.3. Define the integral as a function F: (cid:90) x F(x)= f(t)dt. a The derivative of F with respect to x is d F(x+h)−F(x) F(x)= lim dx h→0 h (cid:32) (cid:33) 1 (cid:90) x+h (cid:90) x = lim f(t)dt− f(t)dt h→0h a a 1 (cid:90) x+h = lim f(t)dt h→0h x (a) 1 (cid:90) x+h(cid:26) (cid:27) ≤ lim max f(τ) dt h→0h x x≤τ≤x+h (cid:26) (cid:27) = lim max f(τ) . h→0 x≤τ≤x+h Here, the inequality in (a) holds because f(t)≤ max f(τ) x≤τ≤x+h for all x≤t≤x+h. The maximum exists because f is continuous in a closed interval. 18
1.3. INTEGRATION Using the parallel argument, we can show that d F(x+h)−F(x) F(x)= lim dx h→0 h (cid:32) (cid:33) 1 (cid:90) x+h (cid:90) x = lim f(t)dt− f(t)dt h→0h a a 1 (cid:90) x+h = lim f(t)dt h→0h x 1 (cid:90) x+h(cid:26) (cid:27) ≥ lim min f(τ) dt h→0h x x≤τ≤x+h (cid:26) (cid:27) = lim min f(τ) . h→0 x≤τ≤x+h Combining the two results, we have that (cid:26) (cid:27) (cid:26) (cid:27) d lim min f(τ) ≤ F(x)≤ lim max f(τ) . h→0 x≤τ≤x+h dx h→0 x≤τ≤x+h However, since the two limits are both converging to f(x) as h → 0, we conclude that d F(x)=f(x). dx (cid:3) Remark.AnalternativeproofistouseMeanValueTheoremintermsofRiemann-Stieltjes integrals (see, e.g., Tom Apostol, Mathematical Analysis, 2nd edition, Theorem 7.34). To handlemoregeneralfunctionssuchasdeltafunctions,onecanusetechniquesinLebesgue’s integration. However, this is beyond the scope of this book. This is the end of the proof. Please join us again. In many practical problems, the fundamental theorem of calculus needs to be used in conjunction with the chain rule. Corollary 1.3. Letf :[a,b]→Rbeacontinuousfunctiondefinedonaclosedinterval [a,b]. Let g : R → [a,b] be a continuously differentiable function. Then, for any x ∈ (a,b), d (cid:90) g(x) f(t)dt=g(cid:48)(x)·f(g(x)). (1.13) dx a Proof. We can prove this with the chain rule: Let y =g(x). Then we have d (cid:90) g(x) dy d (cid:90) y f(t)dt= · f(t)dt=g(cid:48)(x)f(y), dx dx dy a a which completes the proof. (cid:3) 19
CHAPTER 1. MATHEMATICAL BACKGROUND Practice Exercise 1.6. Evaluate the integral d (cid:90) x−µ 1 (cid:26) t2 (cid:27) √ exp − dt. dx 2πσ2 2σ2 0 Solution. Let y =x−µ. Then by using the fundamental theorem of calculus, we can show that d (cid:90) x−µ 1 (cid:26) t2 (cid:27) dy d (cid:90) y 1 (cid:26) t2 (cid:27) √ exp − dt= · √ exp − dt dx 2πσ2 2σ2 dx dy 2πσ2 2σ2 0 0 d(x−µ) 1 (cid:26) y2 (cid:27) = · √ exp − dx 2πσ2 2σ2 1 (cid:26) (x−µ)2(cid:27) = √ exp − . 2πσ2 2σ2 This result will be useful when we do linear transformations of a Gaussian random variable in Chapter 4. 1.4 Linear Algebra Thetwomostimportantsubjectsfordatascienceareprobability,whichisthesubjectofthe book you are reading, and linear algebra, which concerns matrices and vectors. We cannot cover linear algebra in detail because this would require another book. However, we need to highlight some ideas that are important for doing data analysis. 1.4.1 Why do we need linear algebra in data science? Consider a dataset of the crime rate of several cities as shown below, downloaded from https://web.stanford.edu/~hastie/StatLearnSparsity/data.html. The table shows that the crime rate depends on several factors such as funding for the police department, the percentage of high school graduates, etc. city crime rate funding hs no-hs college college4 1 478 40 74 11 31 20 2 494 32 72 11 43 18 3 643 57 71 18 16 16 4 341 31 71 11 25 19 . . . . . . . . . . . . . . . . . . . . . 50 940 66 67 26 18 16 20
1.4. LINEAR ALGEBRA Whatquestionscanweaskaboutthistable?Wecanask:Whatisthemostinfluential cause of the crime rate? What are the leading contributions to the crime rate? To answer these questions, we need to describe these numbers. One way to do it is to put the numbers in matrices and vectors. For example,       478 40 74 494 32 72 y crime =  . .  , x fund =  . .  , x hs =  . .  ,...  .   .   .  940 66 67 Withthisvectorexpressionofthedata,theanalysisquestionscanroughlybetranslated to finding β’s in the following equation: y =β x +β x +···+β x . crime fund fund hs hs college4 college4 This equation offers a lot of useful insights. First, it is a linear model of y . We call crime it a linear model because the observable y is written as a linear combination of the crime variables x ,x , etc. The linear model assumes that the variables are scaled and added fund hs togeneratetheobservedphenomena.Thisassumptionisnotalwaysrealistic,butitisoften a fair assumption that greatly simplifies the problem. For example, if we can show that all β’s are zero except β , then we can conclude that the crime rate is solely dependent on fund the police funding. If two variables are correlated, e.g., high school graduate and college graduate, we would expect the β’s to change simultaneously. The linear model can further be simplified to a matrix-vector equation:  |   | | |  β  fund     y cr| |ime    =    x fu || nd x || hs ··· x col || lege4        β . . .hs     | | | | β college4 Here, the lines “|” emphasize that the vectors are column vectors. If we denote the matrix in the middle as A and the vector as β, then the equation is equivalent to y =Aβ. So we can find β by appropriately inverting the matrix A. If two columns of A are dependent, we will not be able to resolve the corresponding β’s uniquely. Asyoucanseefromtheabovedataanalysisproblem,matricesandvectorsofferaway to describe the data. We will discuss the calculations in Chapter 7. However, to understand howtointerprettheresultsfromthematrix-vectorequations,weneedtoreviewsomebasic ideas about matrices and vectors. 1.4.2 Everything you need to know about linear algebra Throughout this book, you will see different sets of notations. For linear algebra, we also haveasetofnotations.Wedenotex∈Rd ad-dimensionalvectortakingrealnumbersasits entries.AnM-by-N matrixisdenotedasX ∈RM×N.Thetransposeofamatrixisdenoted as XT. A matrix X can be viewed according to its columns and its rows:  — x1 —   | | | — x2 — X =x 1 x 2 ··· x N, and X =  . .  . | | |  .  — xM — 21
CHAPTER 1. MATHEMATICAL BACKGROUND Here,x denotesthejthcolumnofX,andxi denotestheithrowofX.The(i,j)thelement j of X is denoted as x or [X] . The identity matrix is denoted as I. The ith column of I ij ij is denoted as e =[0,...,1,...,0]T, and is called the ith standard basis vector. An all-zero i vector is denoted as 0=[0,...,0]T. Whatisthemostimportantthingtoknowaboutlinearalgebra?Fromadataanalysis pointofview,Figure1.11givesustheanswer.Thepictureisstraightforward,butitcaptures alltheessence.Inalmostallthedataanalysisproblems,ultimately,therearethreethingswe careabout:(i)Theobservablevectory,(ii)thevariablevectorsx ,and(iii)thecoefficients n β . The set of variable vectors {x }N spans a vector space in which all vectors are living. n n n=1 Some of these variable vectors are correlated, and some are not. However, for the sake of this discussion, let us assume they are independent of each other. Then for any observable vector y, we can always project y in the directions determined by {x }N . The projection n n=1 of y onto x is the coefficient β . A larger value of β means that the variable x has more n n n n contributions. Figure 1.11: Representing an observable vector y by a linear combination of variable vectors x , x 1 2 and x . The combination weights are β ,β ,β . 3 1 2 3 Why is this picture so important? Because most of the data analysis problems can be expressed, or approximately expressed, by the picture: N (cid:88) y = β x . n n n=1 If you recall the crime rate example, this equation is precisely the linear model we used to describe the crime rate. This equation can also describe many other problems. Example 1.6.Polynomial fitting.Consideradatasetofpairsofnumbers(t ,y )for m m m = 1,...,M, as shown in Figure 1.12. After a visual inspection of the dataset, we propose to use a line to fit the data. A line is specified by the equation y =at +b, m=1,...,M, m m where a ∈ R is the slope and b ∈ R is the y-intercept. The goal of this problem is to find one line (which is fully characterized by (a,b)) such that it has the best fit to all the data pairs (t ,y ) for m=1,...,M. This problem can be described in matrices m m 22
1.4. LINEAR ALGEBRA and vectors by noting that       y t 1 1 1 . . .  . = a  . + b .,  .   .  . (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) y M β1 t M β2 1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) y x1 x2 or more compactly, y =β x +β x . 1 1 2 2 Here,x =[t ,...,t ]T containsallthevariablevalues,andx =[1,...,1]T contains 1 1 M 2 a constant offset. 5 t y m m 0.1622 2.1227 4 0.7943 3.3354 . . . . . . 3 0.7379 3.4054 0.2691 2.5672 2 data best fit 0.4228 2.3796 candidate 0.6020 3.2942 1 0 0.2 0.4 0.6 0.8 1 Figure 1.12: Example of fitting a set of data points. The problem can be described by y = β x +β x . 1 1 2 2 Example 1.7. Image compression. The JPEG compression for images is based on the concept of discrete cosine transform (DCT). The DCT consists of a set of basis vectors,or{x }N usingournotation.Inthemoststandardsetting,eachbasisvector n n=1 x consistsof8×8pixels,andthereareN =64ofthesex ’s.Givenanimage,wecan n n partition the image into M small blocks of 8×8 pixels. Let us call one of these blocks y. Then, DCT represents the observation y as a linear combination of the DCT basis vectors: N (cid:88) y = β x . n n n=1 The coefficients {β }N are called the DCT coefficients. They provide a representa- n n=1 tion of y, because once we know {β }N , we can completely describe y because the n n=1 basis vectors {x }N are known and fixed. The situation is depicted in Figure 1.13. n n=1 HowcanwecompressimagesusingDCT?Inthe1970s,scientistsfoundthatmost images have strong leading DCT coefficients but weak tail DCT coefficients. In other words, among the N = 64 β ’s, only the first few are important. If we truncate the n number of DCT coefficients, we can effectively compress the number of bits required to represent the image. 23
CHAPTER 1. MATHEMATICAL BACKGROUND Figure1.13:JPEGimagecompressionisbasedontheconceptofdiscretecosinetransform,which can be formulated as a matrix-vector problem. We hope by now you are convinced of the importance of matrices and vectors in the context of data science. They are not “yet another” subject but an essential tool you must know how to use. So, what are the technical materials you must master? Here we go. 1.4.3 Inner products and norms We assume that you know the basic operations such as matrix-vector multiplication, taking the transpose, etc. If you have forgotten these, please consult any undergraduate linear algebra textbook such as Gilbert Strang’s Linear Algebra and its Applications. We will highlight a few of the most important operations for our purposes. Definition 1.4 (Inner product). Let x = [x ,...,x ]T, and y = [y ,...,y ]T. The 1 N 1 N inner product xTy is N (cid:88) xTy = x y . (1.14) i i i=1 Practice Exercise 1.7. Let x=[1, 0, −1]T, and y =[3, 2, 0]T. Find xTy. Solution. The inner product is xTy =(1)(3)+(0)(2)+(−1)(0)=3. Inner products are important because they tell us how two vectors are correlated. Figure1.14depictsthegeometricmeaningofaninnerproduct.Iftwovectorsarecorrelated (i.e., nearly parallel), then the inner product will give us a large value. Conversely, if the two vectors are close to perpendicular, then the inner product will be small. Therefore, the inner product provides a measure of the closeness/similarity between two vectors. Figure 1.14: Geometric interpretation of inner product: We project one vector onto the other vector. The projected distance is the inner product. 24
1.4. LINEAR ALGEBRA Creating vectors and computing the inner products are straightforward in MATLAB. We simply need to define the column vectors x and y by using the command [] with ; to denotethenextrow.Theinnerproductisdoneusingthetransposeoperationx’andvector multiplication *. % MATLAB code to perform an inner product x = [1 0 -1]; y = [3 2 0]; z = x’*y; In Python, constructing a vector is done using the command np.array. Inside this command,oneneedstoenterthearray.Foracolumnvector,wewrite[[1],[2],[3]],with anouter[],andthreeinner[]foreachentry.Ifthevectorisarowvector,theonecanomit the inner []’s by just calling np.array([1, 2, 3]). Given two column vectors x and y, the inner product is computed via np.dot(x.T,y), where np.dot is the command for inner product, and x.T returns the transpose of x. One can also call np.transpose(x), which is the same as x.T. # Python code to perform an inner product import numpy as np x = np.array([[1],[0],[-1]]) y = np.array([[3],[2],[0]]) z = np.dot(np.transpose(x),y) print(z) In data analytics, the inner product of two vectors can be useful. Consider the vectors in Table 1.1. Just from looking at the numbers, you probably will not see anything wrong. However, let’s compute the inner products. It turns out that xTx = −0.0031, whereas 1 2 xTx =2.0020. There is almost no correlation between x and x , but there is a substan- 1 3 1 2 tial correlation between x and x . What happened? The vectors x and x are random 1 3 1 2 vectors constructed independently and uncorrelated to each other. The last vector x was 3 constructed by x =2x −π/1000. Since x is completely constructed from x , they have 3 1 3 1 to be correlated. x x x 1 2 3 0.0006 −0.0011 −0.0020 −0.0014 −0.0024 −0.0059 −0.0034 0.0073 −0.0099 . . . . . . . . . 0.0001 −0.0066 −0.0030 0.0074 0.0046 0.0116 0.0007 −0.0061 −0.0017 Table 1.1: Three example vectors. One caveat for this example is that the naive inner product xTx is scale-dependent. i j For example, the vectors x = x and x = 1000x have the same amount of correlation, 3 1 3 1 25
CHAPTER 1. MATHEMATICAL BACKGROUND butthesimpleinnerproductwillgivealargervalueforthelattercase.Tosolvethisproblem we first define the norm of the vectors: Definition 1.5 (Norm). Let x=[x ,...,x ]T be a vector. The (cid:96) -norm of x is 1 N p (cid:32) N (cid:33)1/p (cid:88) (cid:107)x(cid:107) = xp , (1.15) p i i=1 for any p≥1. The norm essentially tells us the length of the vector. This is most obvious if we consider the (cid:96) -norm: 2 (cid:32) N (cid:33)1/2 (cid:88) (cid:107)x(cid:107) = x2 . 2 i i=1 By taking the square on both sides, one can show that (cid:107)x(cid:107)2 = xTx. This is called the 2 squared (cid:96) -norm, and is the sum of the squares. 2 On MATLAB, computing the norm is done using the command norm. Here, we can indicatethetypesofnorms,e.g.,norm(x,1)returnsthe(cid:96) -normwhereasnorm(x,2)returns 1 the (cid:96) -norm (which is also the default). 2 % MATLAB code to compute the norm x = [1 0 -1]; x_norm = norm(x); On Python, the norm command is listed in the np.linalg. To call the (cid:96) -norm, we use 1 np.linalg.norm(x,1), and by default the (cid:96) -norm is np.linalg.norm(x). 2 # Python code to compute the norm import numpy as np x = np.array([[1],[0],[-1]]) x_norm = np.linalg.norm(x) Using the norm, one can define an angle called the cosine angle between two vectors. Definition 1.6. The cosine angle between two vectors x and y is xTy cosθ = . (1.16) (cid:107)x(cid:107) (cid:107)y(cid:107) 2 2 The difference between the cosine angle and the basic inner product is the normaliza- tion in the denominator, which is the product (cid:107)x(cid:107) (cid:107)y(cid:107) . This normalization factor scales 2 2 the vector x to x/(cid:107)x(cid:107) and y to y/(cid:107)y(cid:107) . The scaling makes the length of the new vector 2 2 equal to unity, but it does not change the vector’s orientation. Therefore, the cosine angle is not affected by a very long vector or a very short vector. Only the angle matters. See Figure 1.15. 26
1.4. LINEAR ALGEBRA Figure 1.15: The cosine angle is the inner product divided by the norms of the vectors. Going back to the previous example, after normalization we can show that the cosine angle between x and x is cosθ = −0.0031, whereas the cosine angle between x and 1 2 1,2 1 x is cosθ =0.8958. There is still a strong correlation between x and x , but now using 3 1,3 1 3 the cosine angle the value is between −1 and +1. Remark1:Thereareothernormsonecanuse.The(cid:96) -normisusefulforsparsemodels 1 where we want to have the fewest possible non-zeros. The (cid:96) -norm of x is 1 N (cid:88) (cid:107)x(cid:107) = |x |, 1 i i=1 which is the sum of absolute values. The (cid:96) -norm picks the maximum of {x ,...,x }: ∞ 1 N (cid:32) N (cid:33)1/p (cid:88) (cid:107)x(cid:107) = lim xp ∞ i p→∞ i=1 =max{x ,...,x }, 1 N because as p→∞, only the largest element will be amplified. Remark 2: The standard (cid:96) -norm is a circle: Just consider x = [x ,x ]T. The norm 2 1 2 (cid:112) is (cid:107)x(cid:107) = x2+x2. We can convert the circle to ellipses by considering a weighted norm. 2 1 2 Definition 1.7 (Weighted (cid:96) -norm square). Let x = [x ,...,x ]T and let W = 2 1 N diag(w ,...,w ) be a non-negative diagonal matrix. The weighted (cid:96) -norm square of 1 N 2 x is (cid:107)x(cid:107)2 =xTWx W    w ... 0 x 1 1 N =(cid:2) x 1 ... x N(cid:3)  . . . ... . . .    . . .  =(cid:88) w ix2 i. (1.17) 0 ... w x i=1 N N The geometry of the weighted (cid:96) -norm is determined by the matrix W. For example, 2 if W = I (the identity operator), then (cid:107)x(cid:107)2 = (cid:107)x(cid:107)2, which defines a circle. If W is any W 2 “non-negative” matrix2, then (cid:107)x(cid:107)2 defines an ellipse. W 2Thetechnicaltermforthesematricesispositive semi-definite matrices. 27
CHAPTER 1. MATHEMATICAL BACKGROUND In MATLAB, the weighted inner product is just a sequence of two matrix-vector mul- tiplications. This can be done using the command x’*W*x as shown below. % MATLAB code to compute the weighted norm W = [1 2 3; 4 5 6; 7 8 9]; x = [2; -1; 1]; z = x’*W*x In Python, constructing the matrix W and the column vector x is done using np.array. Thematrix-vectormultiplicationisdoneusingtwonp.dotcommands:onefornp.dot(W,x) and the other one for np.dot(x.T, np.dot(W,x)). # Python code to compute the weighted norm import numpy as np W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x = np.array([[2],[-1],[1]]) z = np.dot(x.T, np.dot(W,x)) print(z) 1.4.4 Matrix calculus The last linear algebra topic we need to review is matrix calculus. As its name indicates, matrix calculus is about the differentiation of matrices and vectors. Why do we need differ- entiation for matrices and vectors? Because we want to find the minimum or maximum of a scalar function with a vector input. Let us go back to the crime rate problem we discussed earlier. Given the data, we want to find the model coefficients β ,...,β such that the variables can best explain the 1 N observation.Inotherwords,wewanttominimizethedeviationbetweenyandtheprediction offered by our model: (cid:13) (cid:13)2 (cid:13) (cid:88)N (cid:13) minimize (cid:13)y− β x (cid:13) . (cid:13) n n(cid:13) β1,...,βN (cid:13) n=1 (cid:13) This equation is self-explanatory. The norm (cid:107)♣−♥(cid:107)2 measures the deviation. If y can be perfectly explained by {x }N , then the norm can eventually go to zero by finding a n n=1 good set of {β ,...,β }. The symbol minimize means to minimize the function by finding 1 N β1,...,βN {β ,...,β }. Note that the norm is taking a vector as the input and generating a scalar as 1 N the output. It can be expressed as (cid:13) (cid:13)2 ε(β)d =ef(cid:13) (cid:13)y−(cid:88)N β x (cid:13) (cid:13) , (cid:13) n n(cid:13) (cid:13) (cid:13) n=1 to emphasize this relationship. Here we define β = [β ,...,β ]T as the collection of all 1 N coefficients. Given this setup, how would you determine β such that the deviation is minimized? Our calculus teachers told us that we could take the function’s derivative and set it to zero 28
1.4. LINEAR ALGEBRA for scalar problems. It is the same story for vectors. What we do is to take the derivative of the error and set it equal to zero: d ε(β)=0. dβ Now the question arises, how do we take the derivatives of ε(β) when it takes a vector as input?Ifwecananswerthisquestion,wewillfindthebestβ.Theanswerisstraightforward. Since the function has one output and many inputs, take the derivative for each element independently. This is called the scalar differentiation of vectors. Definition1.8(Scalardifferentiationofvectors). Letf :RN →Rbeadifferentiable scalar function, and let y =f(x) for some input x∈RN. Then,   dy/dx 1 dy . = . . dx  .  dy/dx N Asyoucanseefromthisdefinition,thereisnothingconceptuallychallenginghere.Theonly difficultyisthatthingscangettediousbecausetherewillbemanyterms.However,thegood news is that mathematicians have already compiled a list of identities for common matrix differentiation. So instead of deriving every equation from scratch, we can enjoy the fruit of their hard work by referring to those formulae. The best place to find these equations is the Matrix Cookbook by Petersen and Pedersen.3 Here, we will mention two of the most useful results. Example 1.8. Let y =xTAx for any matrix A∈RN×N. Find dy. dx Solution. d (cid:0) xTAx(cid:1) =Ax+ATx. dx Now, if A is symmetric, i.e., A=AT, then d (cid:0) xTAx(cid:1) =2Ax. dx Example 1.9. Let ε=(cid:107)Ax−y(cid:107)2, where A∈RN×N is symmetric. Find dε. 2 dx Solution. First, we note that ε=(cid:107)Ax−y(cid:107)2 2 =xTATAx−2yTAx+yTy. 3https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf 29
CHAPTER 1. MATHEMATICAL BACKGROUND Taking the derivative with respect to x yields dε =2ATAx−2ATy dx =2AT(Ax−y). Going back to the crime rate problem, we can now show that dε 0= (cid:107)y−Xβ(cid:107)2 =2XT(Xβ−y). dβ Therefore, the solution is β(cid:98) =(XTX)−1Xy. Asyoucansee,ifwedonothaveaccesstothematrixcalculus,wewillnotbeabletosolvethe minimizationproblem.(Therearealternativepathsthatdonotrequirematrixcalculus,but theyrequireanunderstandingoflinearsubspacesandpropertiesoftheprojectionoperators. So in some sense, matrix calculus is the easiest way to solve the problem.) When we discuss the linear regression methods in Chapter 7, we will cover the interpretation of the inverses and related topics. In MATLAB and Python, matrix inversion is done using the command inv in MAT- LAB and np.linalg.inv in Python. Below is an example in Python. # Python code to compute a matrix inverse import numpy as np X = np.array([[1, 3], [-2, 7], [0, 1]]) XtX = np.dot(X.T, X) XtXinv = np.linalg.inv(XtX) print(XtXinv) Sometimes,insteadofcomputingthematrixinversewearemoreinterestedinsolvinga linearequationXβ =y(thesolutionofwhichisβ(cid:98) =(XTX)−1Xy).InbothMATLABand Python,therearebuilt-incommandstodothis.InMATLAB,thecommandis\(backslash). % MATLAB code to solve X beta = y X = [1 3; -2 7; 0 1]; y = [2; 1; 0]; beta = X\y; In Python, the built-in command is np.linalg.lstsq. # Python code to solve X beta = y import numpy as np X = np.array([[1, 3], [-2, 7], [0, 1]]) y = np.array([[2],[1],[0]]) beta = np.linalg.lstsq(X, y, rcond=None)[0] print(beta) 30
1.5. BASIC COMBINATORICS Closing remark: In this section, we have given a brief introduction to a few of the most relevant concepts in linear algebra. We will introduce further concepts in linear algebra in later chapters, such as eigenvalues, principal component analysis, linear transformations, and regularization, as they become useful for our discussion. 1.5 Basic Combinatorics The last topic we review in this chapter is combinatorics. Combinatorics concerns the numberofconfigurationsthatcanbeobtainedfromcertaindiscreteexperiments.Itisuseful because it provides a systematic way of enumerating cases. Combinatorics often becomes very challenging as the complexity of the event grows. However, you may rest assured that inthisbook,wewillnottacklethemoredifficultproblemsofcombinatorics;wewillconfine our discussion to two of the most basic principles: permutation and combination. 1.5.1 Birthday paradox Tomotivatethediscussionofcombinatorics,letusstartwiththefollowingproblem.Suppose there are 50 people in a room. What is the probability that at least one pair of people have the same birthday (month and day)? (We exclude Feb. 29 in this problem.) Thefirstthingyoumightbethinkingisthatsincethereare365days,weneedatleast 366 people to ensure that one pair has the same birthday. Therefore, the chance that 2 of 50 people have the same birthday is low. This seems reasonable, but let’s do a simulated experiment. In Figure 1.16 we plot the probability as a function of the number of people. For a room containing 50 people, the probability is 97%. To get a 50% probability, we just need 23 people! How is this possible? 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 10 20 30 40 50 60 70 80 90 100 Number of people ytilibaborP Figure 1.16: The probability for two people in a group to have the same birthday as a function of the number of people in the group. Ifyouthinkaboutthisproblemmoredeeply,youwillprobablyrealizethattosolvethe problem, we must carefully enumerate all the possible configurations. How can we do this? 31
CHAPTER 1. MATHEMATICAL BACKGROUND Well, suppose you walk into the room and sequentially pick two people. The probability that they have different birthdays is 365 364 P[The first 2 people have different birthdays]= × . 365 365 When you ask the first person to tell you their birthday, he or she can occupy any of the 365 slots. This gives us 365. The second person has one slot short because the first person 365 has taken it, and so the probability that he or she has a different birthday from the first personis 364.Notethatthiscalculationisindependentofhowmanypeopleyouhaveinthe 365 room because you are picking them sequentially. If you now choose a third person, the probability that they have different birthdays is 365 364 363 P[The first 3 people have different birthdays]= × × . 365 365 365 This process can be visualized in Figure 1.17. Figure 1.17: The probability for two people to have the same birthday as a function of the number of peopleinthegroup.Whenthereisonlyoneperson,thispersoncanlandonanyofthe365days.When therearetwopeople,thefirstpersonhasalreadytakenoneday(outof365days),sothesecondperson can only choose 364 days. When there are three people, the first two people have occupied two days, so there are only 363 days left. If we generalize this process, we see that the number of configurations is 365×364×···×(365−k+1), where k is the number of people in the room. So imagine that you keep going down the list to the 50th person. The probability that none of these 50 people will have the same birthday is P[The first 50 people have different birthdays] 365 364 363 316 = × × ×···× ≈0.03. 365 365 365 365 Thatmeansthattheprobabilityfor50peopletohavedifferentbirthdays,theprobabilityis as little as 3%. If you take the complement, you can show that with 97% probability, there is at least one pair of people having the same birthday. The general equation for this problem is now easy to see: 365×364×···×(365−k+1) P[The first k people have different birthdays]= 365×365×···×365 365! 1 = × . (365−k)! 365k 32
1.5. BASIC COMBINATORICS The first term in our equation, 365! , is called the permutation of picking k days from (365−k)! 365 options. We shall discuss this operation shortly. Why is the probability so high with only 50 people while it seems that we need 366 people to ensure two identical birthdays? The difference is the notion of probabilistic and deterministic. The 366-people argument is deterministic. If you have 366 people, you are certain that two people will have the same birthday. This has no conflict with the proba- bilistic argument because the probabilistic argument says that with 50 people, we have a 97% chance of getting two identical birthdays. With a 97% success rate, you still have a 3% chance of failing. It is unlikely to happen, but it can still happen. The more people you put into the room, the stronger guarantee you will have. However, even if you have 364 peopleandtheprobabilityisalmost100%,thereisstillnoguarantee.Sothereisnoconflict between the two arguments since they are answering two different questions. Now, let’s discuss the two combinatorics questions. 1.5.2 Permutation Permutation concerns the following question: Consider a set of n distinct balls. Suppose we want to pick k balls from the set without replacement. How many ordered configurations can we obtain? Note that in the above question, the word “ordered” is crucial. For example, the set A={a,b,c} can lead to 6 different ordered configurations (a,b,c), (a,c,b), (b,a,c), (b,c,a), (c,a,b), (c,b,a). As a simple illustration of how to compute the permutation, we can consider a set of 5 colored balls as shown in Figure 1.18. Figure1.18:Permutation.Thenumberofchoicesisreducedineverystage.Therefore,thetotalnumber is n×(n−1)×···×(n−k+1) if there are k stages. If you start with the base, which contains five balls, you will have five choices. At one level up, since one ball has already been taken, you have only four choices. You continue the process until you reached the number of balls you want to collect. The number of configurations you have generated is the permutation. Here is the formula: 33
CHAPTER 1. MATHEMATICAL BACKGROUND Theorem 1.6. The number of permutations of choosing k out of n is n! (n−k)! where n!=n(n−1)(n−2)···3·2·1. Proof. Let’s list all possible ways: Which ball to pick Number of choices Why? The 1st ball n No has been picked, so we have n choices The 2nd ball n−1 The first ball has been picked The 3rd ball n−2 The first two balls have been picked . . . . . . . . . The kth ball n−k+1 The first k−1 balls have been picked Total: n(n−1)···(n−k+1) The total number of ordered configurations is n(n−1)···(n−k+1). This simplifies to n(n−1)(n−2)···(n−k+1) (n−k)(n−k−1)···3·2·1 =n(n−1)(n−2)···(n−k+1)· (n−k)(n−k−1)···3·2·1 n! = . (n−k)! (cid:3) Practice Exercise 1.8. Consider a set of 4 balls {1,2,3,4}. We want to pick two balls at random without replacement. The ordering matters. How many permutations can we obtain? Solution. The possible configurations are (1,2), (2,1), (1,3), (3,1), (1,4), (4,1), (2,3), (3,2), (2,4), (4,2), (3,4), (4,3). So totally there are 12 configurations. We can also verify this number by noting that there are 4 balls altogether and so the number of choices for picking the first ball is 4 and the number of choices for picking the second ball is (4−1)=3. Thus, the total is 4·3=12. Referring to the formula, this result coincides with the theorem, which states that the number of permutations is 4! = 4·3·2·1 =12. (4−2)! 2·1 34
1.5. BASIC COMBINATORICS 1.5.3 Combination Another operation in combinatorics is combination. Combination concerns the following question: Consider a set of n distinct balls. Suppose we want to pick k balls from the set without replacement. How many unordered configurations can we obtain? Unlike permutation, combination treats a subset of balls with whatever ordering as one single configuration. For example, the subset (a,b,c) is considered the same as (a,c,b) or (b,c,a), etc. Let’s go back to the 5-ball exercise. Suppose you have picked orange, green, and light blue. This is the same combination as if you have picked {green, orange, and light blue}, or {green, light blue, and orange}. Figure 1.19 lists all the six possible configurations for these three balls. So what is combination? Combination needs to take these repeated cases into account. Figure 1.19: Combination. In this problem, we are interested in picking 3 colored balls out of 5. This willgiveus5×4×3=60permutations.However,sincewearenotinterestedintheordering,someof thepermutationsarerepeated.Forexample,thereare6combosof(green,lightblue,orange),whichis computed from 3×2×1. Dividing 60 permutations by these 6 choices of the orderings will give us 10 distinct combinations of the colors. Theorem 1.7. The number of combinations of choosing k out of n is n! k!(n−k)! where n!=n(n−1)(n−2)···3·2·1. Proof.Westartwiththepermutationresult,whichgivesus n! permutations.Notethat (n−k)! every permutation has exactly k balls. However, while these k balls can be arranged in any order, in combination, we treat them as one single configuration. Therefore, the task is to count the number of possible orderings for these k balls. To this end, we note that for a set of k balls, there are in total k! possible ways of ordering them. The number k! comes from the following table. 35
CHAPTER 1. MATHEMATICAL BACKGROUND Which ball to pick Number of choices The 1st ball k The 2nd ball k−1 . . . . . . The kth ball 1 Total: k(k−1)···3·2·1 Therefore, the total number of orderings for a set of k balls is k!. Since permutation gives us n! and every permutation has k! repetitions due to ordering, we divide the (n−k)! number by k!. Thus the number of combinations is n! . k!(n−k)! (cid:3) Practice Exercise 1.9. Consider a set of 4 balls {1,2,3,4}. We want to pick two balls at random without replacement. The ordering does not matter. How many com- binations can we obtain? Solution.Thepermutationresultgivesus12permutations.However,amongallthese 12 permutations, there are only 6 distinct pairs of numbers. We can confirm this by noting that since we picked 2 balls, there are exactly 2 possible orderings for these 2 balls. Therefore, we have 12 = 6 number of combinations. Using the formula of the 2 theorem, we check that the number of combinations is 4! 4·3·2·1 = =6. 2!(4−2)! (2·1)(2·1) Example 1.10. (Ross, 8th edition, Section 1.6) Consider the equation x +x +···+x =N, 1 2 K where{x }arepositiveintegers.Howmanycombinationsofsolutionsofthisequation k are there? Solution. We can determine the number of combinations by considering the figure below.TheintegerN canbemodeledasN ballsinanurn.ThenumberofvariablesK is equivalent to the number ofcolors of these balls.Since all variables are positive,the problem can be translated to partitioning the N balls into K buckets. This, in turn, is the same as inserting K−1 dividers among N −1 holes. Therefore, the number of combinations is (cid:18) (cid:19) N −1 (N −1)! = . K−1 (K−1)!(N −K)! 36
1.6. SUMMARY For example, if N =16 and K =4, then the number of solutions is (cid:18) (cid:19) 16−1 15! = =455. 4−1 3!12! Figure 1.20: One possible solution for N =16 and K =4. In general, the problem is equivalent to inserting K−1 dividers among N −1 balls. Closing remark. Permutations and combinations are two ways to enumerate all the pos- sible cases. While the conclusions are probabilistic, as the birthday paradox shows, permu- tation and combination are deterministic. We do not need to worry about the distribution of the samples, and we are not taking averages of anything. Thus, modern data analysis seldomuses theconceptsofpermutation and combination.Accordingly,combinatoricsdoes not play a large role in this book. Does it mean that combinatorics is not useful? Not quite, because it still provides us with powerful tools for theoretical analysis. For example, in binomial random variables, we need the concept of combination to calculate the repeated cases. The Poisson random vari- ablecanberegardedasalimitingcaseofthebinomialrandomvariable,andsocombination isalsoused.Therefore,whilewedonotusetheconceptsofpermutationperse,weusethem to define random variables. 1.6 Summary In this chapter, we have reviewed several background mathematical concepts that will be- come useful later in the book. You will find that these concepts are important for under- standing the rest of this book. When studying these materials, we recommend not just remembering the “recipes” of the steps but focusing on the motivations and intuitions behind the techniques. We would like to highlight the significance of the birthday paradox. Many of us come from an engineering background in which we were told to ensure reliability and guarantee success. We want to ensure that the product we deliver to our customers can survive even in the worst-case scenario. We tend to apply deterministic arguments such as requiring 366 peopletoensurecompletecoverageofthe365days.Inmoderndataanalysis,theworst-case scenario may not always be relevant because of the complexity of the problem and the cost ofsuchawarranty.Theprobabilisticargument,ortheaverageargument,ismorereasonable and cost-effective, as you can see from our analysis of the birthday problem. The heart of the problem is the trade-off between how much confidence you need versus how much effort you need to expend. Suppose an event is unlikely to happen, but if it happens, it will be 37
CHAPTER 1. MATHEMATICAL BACKGROUND a disaster. In that case, you might prefer to be very conservative to ensure that such a disaster event has a low chance of happening. Industries related to risk management such as insurance and investment banking are all operating under this principle. 1.7 Reference Introductory materials 1-1 Erwin Kreyszig, Advanced Engineering Mathematics, Wiley, 10th Edition, 2011. 1-2 HenryStarkandJohnW.Woods,ProbabilityandRandomProcesseswithApplications to Signal Processing, Prentice Hall, 3rd Edition, 2002. Appendix. 1-3 Michael J. Evans and Jeffrey S. Rosenthal, Probability and Statistics: The Science of Uncertainty, W. H. Freeman, 2nd Edition, 2009. Appendix. 1-4 James Stewart, Single Variable Calculus, Early Transcendentals, Thomson Brooks/- Cole, 6th Edition, 2008. Chapter 5. Combinatorics 1-5 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Section 1.6. 1-6 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Section 2.6. 1-7 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 3. Analysis Insomesectionsofthischapter,weuseresultsfromcalculusandinfiniteseries.Manyformal proofs can be found in the standard undergraduate real analysis textbooks. 1-8 Tom M. Apostol, Mathematical Analysis, Pearson, 1974. 1-9 Walter Rudin, Principles of Mathematical Analysis, McGraw Hill, 1976. 38
1.8. PROBLEMS 1.8 Problems Exercise 1. (Video Solution) (a) Show that (cid:88)n 1−rn+1 rk = . 1−r k=0 for any 0<r <1. Evaluate (cid:80)∞ rk. k=0 (b) Using the result of (a), evaluate 1+2r+3r2+··· . (c) Evaluate the sums (cid:88)∞ (cid:18) 1(cid:19)k+1 (cid:88)∞ (cid:18) 1(cid:19)k−1 k , and k . 3 4 k=0 k=2 Exercise 2. (Video Solution) Recall that (cid:88)∞ λk =eλ. k! k=0 Evaluate (cid:88)∞ λke−λ (cid:88)∞ λke−λ k , and k2 . k! k! k=0 k=0 Exercise 3. (Video Solution) Evaluate the integrals (a) (cid:90) b 1 (cid:18) a+b(cid:19)2 x− dx. b−a 2 a (b) (cid:90) ∞ λxe−λx dx. 0 (c) (cid:90) ∞ λx e−λ|x| dx. 2 −∞ 39
CHAPTER 1. MATHEMATICAL BACKGROUND Exercise 4. (a) ComputetheresultofthefollowingmatrixvectormultiplicationusingNumpy.Submit your result and codes.     1 2 3 1 4 5 6×2. 7 8 9 3 (b) Plot a sine function on the interval [−π,π] with 1000 data points. (c) Generate 10,000 uniformly distributed random numbers on interval [0, 1). Use matplotlib.pyplot.hist to generate a histogram of all the random numbers. Exercise 5. Calculate (cid:88)∞ (cid:18) 2(cid:19)k+1 k . 3 k=0 Exercise 6. Let (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) x 1 4 1 x= , µ= , Σ= . y 0 1 1 (a) Find Σ−1, the inverse of Σ. (b) Find |Σ|, the determinant of Σ. (c) Simplify the two-dimensional function (cid:26) (cid:27) 1 1 f(x)= exp − (x−µ)TΣ−1(x−µ) . 2π|Σ|1/2 2 (d) Use matplotlib.pyplot.contour, plot the function f(x) for the range [−3,3] × [−3,3]. Exercise 7. Out of seven electrical engineering (EE) students and five mechanical engineering (ME) students, a committee consisting of three EEs and two MEs is to be formed. In how many ways can this be done if (a) any of the EEs and any of the MEs can be included? (b) one particular EE must be on the committee? (c) two particular MEs cannot be on the committee? 40
1.8. PROBLEMS Exercise 8. Five blue balls, three red balls, and three white balls are placed in an urn. Three balls are drawn at random without regard to the order in which they are drawn. Using the counting approach to probability, find the probability that (a) one blue ball, one red ball, and one white ball are drawn. (b) all three balls drawn are red. (c) exactly two of the balls drawn are blue. Exercise 9. A collection of 26 English letters, a-z, is mixed in a jar. Two letters are drawn at random, one after the other. (a) Whatistheprobabilityofdrawingavowel(a,e,i,o,u)andaconsonantineitherorder? (b) Write a MATLAB / Python program to verify your answer in part (a). Randomly draw two letters without replacement and check whether one is a vowel and the other is a consonant. Compute the probability by repeating the experiment 10000 times. Exercise 10. There are 50 students in a classroom. (a) What is the probability that there is at least one pair of students having the same birthday? Show your steps. (b) Write a MATLAB / Python program to simulate the event and verify your answer in (a). Hint: You probably need to repeat the simulation many times to obtain a probability. Submit your code and result. You may assume that a year only has 365 days. You may also assume that all days have an equal likelihood of being taken. 41
CHAPTER 1. MATHEMATICAL BACKGROUND 42
Chapter 2 Probability Data and probability are inseparable. Data is the computational side of the story, whereas probability is the theoretical side of the story. Any data science practice must be built on thefoundationofprobability,andprobabilityneedstoaddresspracticalproblems.However, what exactly is “probability”? Mathematicians have been debating this for centuries. The frequentists argue that probability is the relative frequency of an outcome. For example, flipping a fair coin has a 1/2 probability of getting a head because if you flip the coin infinitely many times, you will have half of the time getting a head. The Bayesians argue that probability is a subjective belief. For example, the probability of getting an A in a classissubjectivebecausenoonewouldwanttotakeaclassinfinitelymanytimestoobtain the relative frequency. Both the frequentists and Bayesians have valid points. However, the differentiation is often non-essential because the context of your problem will force you to align with one or the other. For example, when you have a shortage of data, then the subjectivity of the Bayesians allows you to use prior knowledge, whereas the frequentists tell us how to compute the confidence interval of an estimate. No matter whether you prefer the frequentist’s view or the Bayesian’s view, there is somethingmorefundamentalthankstoAndrey Kolmogorov(1903-1987).Thedevelopment ofthis fundamentaldefinition willtake someeffortonour part,but ifwe distillthe essence, we can summarize it as follows: Probability is a measure of the size of a set. This sentence is not a formal definition; instead, it summarizes what we believe to be the essence of probability. We need to clarify some puzzles later in this chapter, but if you can understandwhatthissentencemeans,youarehalfwaydonewiththisbook.Tospelloutthe details, we will describe an elementary problem that everyone knows how to solve. As we discuss this problem, we will highlight a few key concepts that will give you some intuitive insights into our definition of probability, after which we will explain the sequence of topics to be covered in this chapter. Prelude: Probability of throwing a die Supposethatyouhaveafairdie.Ithas6faces:{1,2,3,4,5,6}.Whatistheprobabilitythat you get a number that is “less than 5” and is “an even number”? This is a straightforward 43
CHAPTER 2. PROBABILITY problem.Youprobablyhavealreadyfoundtheanswer,whichis 2 because“lessthan5”and 6 “an even number” means {2,4}. However, let’s go through the thinking process slowly by explicitly writing down the steps. First of all, how do we know that the denominator in 2 is 6? Well, because there are 6 six faces. These six faces form a set called the sample space. A sample space is the set containing all possible outcomes, which in our case is Ω={1,2,3,4,5,6}. The denominator 6 is the size of the sample space. How do we know that the numerator is 2? Again, implicitly in our minds, we have constructed two events: E = “less than 5” = {1,2,3,4}, and E = “an even number” 1 2 = {2,4,6}. Then we take the intersection between these two events to conclude the event E ={2,4}. The numerical value “2” is the size of this event E. So, when we say that “the probability is 2,” we are saying that the size of the event 6 E relative to the sample space Ω is the ratio 2. This process involves measuring the size 6 of E and Ω. In this particular example, the measure we use is a “counter” that counts the number of elements. This example shows us all the necessary components of probability: (i) There is a samplespace,whichisthesetthatcontainsallthepossibleoutcomes.(ii)Thereisanevent, which is a subset inside the sample space. (iii) Two events E and E can be combined to 1 2 construct another event E that is still a subset inside the sample space. (iv) Probability is a number assigned by certain rules such that it describes the relative size of the event E compared with the sample space Ω. So, when we say that probability is a measure of the size of a set, we create a mapping that takes in a set and outputs the size of that set. Organization of this chapter Asyoucanseefromthisexample,sinceprobabilityisameasureofthesizeofaset,weneed to understand the operations of sets to understand probability. Accordingly, in Section 2.1 wefirstdefinesetsanddiscusstheiroperations.Afterlearningthesebasicconcepts,wemove ontodefinethesamplespaceandeventspaceinSection2.2.There,wediscusssamplespaces that are not necessarily countable and how probabilities are assigned to events. Of course, assigning a probability value to an event cannot be arbitrary; otherwise, the probabilities may be inconsistent. Consequently, in Section 2.3 we introduce the probability axioms and formalize the notion of measure. Section 2.4 consists of a trio of topics that concern the relationshipbetweeneventsusingconditioning.WediscussconditionalprobabilityinSection 2.4.1, independence in Section 2.4.2, and Bayes’ theorem in Section 2.4.3. 2.1 Set Theory 2.1.1 Why study set theory? Inmathematics,weareofteninterestedindescribingacollectionofnumbers,forexample,a positiveinterval[a,b]onthereallineortheorderedpairsofnumbersthatdefineacircleon a graph with two axes. These collections of numbers can be abstractly defined as sets. In a nutshell,asetissimplyacollectionofthings.Thesethingscanbenumbers,buttheycanalso bealphabets,objects,oranything.Settheoryisamathematicaltoolthatdefinesoperations on sets. It provides the basic arithmetic for us to combine, separate, and decompose sets. 44
2.1. SET THEORY Whydowestartthechapterbydescribingsettheory?Becauseprobabilityisameasure of the size of a set. Yes, probability is not just a number telling us the relative frequency of events; it is an operator that takes a set and tells us how large the set is. Using the example we showed in the prelude, the event “even number” of a die is a set containing numbers{2,4,6}.Whenweapplyprobabilitytothisset,weobtainthenumber 3,asshown 6 in Figure 2.1. Thus sets are the foundation of the study of probability. Figure 2.1: Probability is a measure of the size of a set. Whenever we talk about probability, it has to be the probability of a set. 2.1.2 Basic concepts of a set Definition 2.1 (Set). A set is a collection of elements. We denote A={ξ ,ξ ,...,ξ } (2.1) 1 2 n as a set, where ξ is the ith element in the set. i Inthisdefinition,Aiscalledaset.Itisnothingbutacollectionofelementsξ ,...,ξ .What 1 n are these ξ ’s? They can be anything. Let’s see a few examples below. i Example 2.1(a). A={apple,orange,pear} is a finite set. Example 2.1(b). A={1,2,3,4,5,6} is a finite set. Example 2.1(c). A={2,4,6,8,...} is a countable but infinite set. Example 2.1(d). A={x|0<x<1} is a uncountable set. To say that an element ξ is drawn from A, we write ξ ∈A. For example, the number 1 is an element in the set {1,2,3}. We write 1∈{1,2,3}. There are a few common sets that we will encounter. For example, Example 2.2(a). R is the set of all real numbers including ±∞. Example 2.2(b). R2 is the set of ordered pairs of real numbers. Example 2.2(c). [a,b]={x|a≤x≤b} is a closed interval on R. Example 2.2(d). (a,b)={x|a<x<b} is an open interval on R. Example 2.2(e). (a,b]={x|a<x≤b} is a semi-closed interval on R. 45
CHAPTER 2. PROBABILITY Figure 2.2: From left to right: a closed interval, a semi-closed (or semi-open) interval, and an open interval. Setsarenotlimitedtonumbers.Asetcanbeusedtodescribeacollectionof functions. Example2.3.A={f :R→R|f(x)=ax+b, a,b∈R}.Thisisthesetofallstraight lines in 2D. The notation f : R → R means that the function f takes an argument from R and sends it to another real number in R. The definition f(x) = ax+b says that f is taking the specific form of ax+b. Since the constants a and b can be any real number, the equation f(x)=ax+b enumerates all possible straight lines in 2D. See Figure 2.3(a). Example 2.4. A = {f : R → [−1,1] | f(t) = cos(ω t+θ), θ ∈ [0,2π]}. This is 0 the set of all cosine functions of a fixed carrier frequency ω . The phase θ, however, 0 is changing. Therefore, the equation f(t) = cos(ω t+θ) says that the set A is the 0 collection of all possible cosines with different phases. See Figure 2.3(b). 1 0.5 0 -0.5 -1 -2 -1 0 1 2 t )t(f 2 1 0 -1 -2 -1 -0.5 0 0.5 1 t )t(f Figure 2.3: (a) The set of straight lines A={f :R→R|f(x)=ax+b, a,b∈R}. (b) The set of phase-shifted cosines A={f :R→[−1,1]|f(t)=cos(ω t+θ), θ∈[0,2π]}. 0 A set can also be used to describe a collection of sets. Let A and B be two sets. Then C ={A,B} is a set of sets. Example 2.5. Let A={1,2} and B ={apple,orange}. Then C ={A,B}={{1,2},{apple,orange}} is a collection of sets. Note that here we are not saying C is the union of two sets. We are only saying that C is a collection of two sets. See the next example. 46
2.1. SET THEORY Example 2.6. Let A={1,2} and B ={3}, then C ={A,B} means that C ={{1,2},{3}}. Therefore C contains only two elements. One is the set {1,2} and the other is the set {3}. Note that {{1,2},{3}}=(cid:54) {1,2,3}. The former is a set of two sets. The latter is a set of three elements. 2.1.3 Subsets Given a set, we often want to specify a portion of the set, which is called a subset. Definition 2.2 (Subset). B is a subset of A if for any ξ ∈ B, ξ is also in A. We write B ⊆A (2.2) to denote that B is a subset of A. B iscalledaproper subsetofAifB isasubsetofAandB (cid:54)=A.Wedenoteapropersubset as B ⊂A. Two sets A and B are equal if and only if A⊆B and B ⊆A. Example 2.7. • If A={1,2,3,4,5,6}, then B ={1,3,5} is a proper subset of A. • If A={1,2}, then B ={1,2} is an improper subset of A. • If A={t|t≥0}, then B ={t|t>0} is a proper subset of A. Practice Exercise 2.1. Let A={1,2,3}. List all the subsets of A. Solution. The subsets of A are: A={∅,{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}}. Practice Exercise 2.2. Prove that two sets A and B are equal if and only if A⊆B and B ⊆A. Solution. Suppose A ⊆ B and B ⊆ A. Assume by contradiction that A (cid:54)= B. Then necessarily there must exist an x such that x ∈ A but x (cid:54)∈ B (or vice versa). But A ⊆ B means that x ∈ A will necessarily be in B. So it is impossible to have x (cid:54)∈ B. Conversely, suppose that A=B. Then any x∈A will necessarily be in B. Therefore, we have A⊆B. Similarly, if A=B then any x∈B will be in A, and so B ⊆A. 47
CHAPTER 2. PROBABILITY 2.1.4 Empty set and universal set Definition 2.3 (Empty Set). A set is empty if it contains no element. We denote an empty set as A=∅. (2.3) A set containing an element 0 is not an empty set. It is a set of one element, {0}. The number of elements of the empty set is 0. The empty set is a subset of any set, i.e., ∅ ⊆ A for any A. We use ⊆ because A could also be an empty set. Example 2.8(a). The set A = {x| sinx > 1} is empty because no x ∈ R can make sinx>1. Example 2.8(b). The set A = {x|x > 5 and x < 1} is empty because the two conditions x>5 and x<1 are contradictory. Definition 2.4 (Universal Set). The universal set is the set containing all elements under consideration. We denote a universal set as A=Ω. (2.4) The universal set Ω contains itself, i.e., Ω ⊆ Ω. The universal set is a relative concept. Usually, we first define a universal set Ω before referring to subsets of Ω. For example, we can define Ω = R and refer to intervals in R. We can also define Ω = [0,1] and refer to subintervals inside [0,1]. 2.1.5 Union We now discuss basic set operations. By operations, we mean functions of two or more sets whose output value is a set. We use these operations to combine and separate sets. Let us first consdier the union of two sets. See Figure 2.4 for a graphical depiction. Definition 2.5 (Finite Union). The union of two sets A and B contains all elements in A or in B. That is, A∪B ={ξ |ξ ∈Aorξ ∈B}. (2.5) As the definition suggests, the union of two sets connects the sets using the logical operator ”or”. Therefore, the union of two sets is always larger than or equal to the individual sets. Example 2.9(a). If A = {1,2}, B = {1,5}, then A∪B = {1,2,5}. The overlapping element 1 is absorbed. Also, note that A∪B (cid:54)= {{1,2},{1,5}}. The latter is a set of sets. Example 2.9(b). If A=(3,4], B =(3.5,∞), then A∪B =(3,∞). Example 2.9(c). If A = {f : R → R|f(x) = ax} and B = {f : R → R|f(x) = b}, then A∪B = a set of sloped lines with a slope a plus a set of constant lines with 48
2.1. SET THEORY height b. Note that A∪B (cid:54)={f :R→R|f(x)=ax+b} because the latter is a set of sloped lines with arbitrary y-intercept. Example 2.9(d). If A={1,2} and B =∅, then A∪B ={1,2}. Example. If A={1,2} and B =Ω, then A∪B =Ω. Figure 2.4: The union of two sets contains elements that are either in A or B or both. Thepreviousexamplecanbegeneralizedinthefollowingexercise.Whatitsaysisthat ifAisasubsetofanothersetB,thentheunionofAandB isjustB.Intuitively,thisshould be straightforward because whatever you have in A is already in B, so the union will just be B. Below is a formal proof that illustrates how to state the arguments clearly. You may like to draw a picture to convince yourself that the proof is correct. Practice Exercise 2.3: Prove that if A⊆B, then A∪B =B. Solution: We will show that A∪B ⊆B and B ⊆A∪B. Let ξ ∈A∪B. Then ξ must be inside either A or B (or both). In any case, since we know that A ⊆ B, it holds that if ξ ∈ A then ξ must also be in B. Therefore, for any ξ ∈ A∪B we have ξ ∈ B. This shows A∪B ⊆ B. Conversely, if ξ ∈ B, then ξ must be inside A∪B because A∪B is a larger set than B. So if ξ ∈B then ξ ∈A∪B and hence B ⊆A∪B. Since A∪B is a subset of B or equal to B, and B is a subset of A∪B or equal to A∪B, it follows that A∪B =B. What should we do if we want to take the union of an infinite number of sets? First, we need to define the concept of an infinite union. Definition 2.6 (Infinite Union). For an infinite sequence of sets A ,A ,..., the in- 1 2 finite union is defined as ∞ (cid:91) A ={ξ | ξ ∈A for at least one n that is finite.}. (2.6) n n n=1 An infinite union is a natural extension of a finite union. It is not difficult to see that ξ ∈A or ξ ∈B ⇐⇒ ξ is in at least one of AandB. 49
CHAPTER 2. PROBABILITY Similarly, an infinite union means that ξ ∈A or ξ ∈A or ξ ∈A ... ⇐⇒ ξ is in at least one of A , A , A , .... 1 2 3 1 2 3 The finite n requirement says that we only evaluate the sets for a finite number of n’s. This n can be arbitrarily large, but it is finite. Why are we able to do this? Because the concept of an infinite union is to determine A , which is the limit of a sequence. Like any sequence ∞ ofrealnumbers,thelimitofasequenceofsetshastobedefinedbyevaluatingtheinstances of all possible finite cases. Consider a sequence of sets A = (cid:2) −1,1− 1(cid:3) , for n = 1,2,.... For example, A = n n 1 [−1,0], A =(cid:2) −1,1(cid:3) , A =(cid:2) −1,2(cid:3) , A =(cid:2) −1,3(cid:3) , etc. 2 2 3 3 4 4 Figure 2.5: The infinite union of (cid:83)∞ (cid:2) −1,1− 1(cid:3) . No matter how large n gets, the point 1 is never n=1 n included. So the infinite union is [−1,1) Totaketheinfiniteunion,weknowthattheset[−1,1)isalwaysincluded,becausethe right-hand limit 1− 1 approaches 1 as n approaches ∞. So the only question concerns the n number 1. Should 1 be included? According to the definition above, we ask: Is 1 an element of at least one of the sets A , A , ..., A ? Clearly it is not: 1 (cid:54)∈ A , 1 (cid:54)∈ A , .... In fact, 1 2 n 1 2 1(cid:54)∈A for any finite n. Therefore 1 is not an element of the infinite union, and we conclude n that ∞ ∞ (cid:20) (cid:21) (cid:91) (cid:91) 1 A = −1,1− =[−1,1). n n n=1 n=1 Practice Exercise 2.4. Find the infinite union of the sequences where (a) A = n (cid:2) −1,1− 1(cid:1) , (b) A =(cid:0) −1,1− 1(cid:3) . n n n Solution. (a) (cid:83)∞ A =[−1,1). (b) (cid:83)∞ A =(−1,1). n=1 n n=1 n 2.1.6 Intersection Theunionoftwosetsisbasedonthelogicaloperatoror.Ifweusethelogicaloperatorand, then the result is the intersection of two sets. Definition 2.7 (Finite Intersection). The intersection of two sets A and B contains all elements in A and in B. That is, A∩B ={ξ |ξ ∈A and ξ ∈B}. (2.7) Figure 2.6 portrays intersection graphically. Intersection finds the common elements of the two sets. It is not difficult to show that A∩B ⊆A and A∩B ⊆B. 50
2.1. SET THEORY Figure 2.6: The intersection of two sets contains elements in both A and B. Example 2.10(a). If A={1,2,3,4}, B ={1,5,6}, then A∩B ={1}. Example 2.10(b). If A={1,2}, B ={5,6}, then A∩B =∅. Example 2.10(c). If A=(3,4], B =[3.5,∞), then A∩B =[3.5,4]. Example 2.10(d). If A=(3,4], B =∅, then A∩B =∅. Example 2.10(e). If A=(3,4], B =Ω, then A∩B =(3,4]. Example 2.11.IfA={f :R→R|f(x)=ax}andB ={f :R→R|f(x)=b},then A∩B =theintersectionofasetofslopedlineswithaslopeaandasetofconstantlines with height b. The only line that can satisfy both sets is the line f(x)=0. Therefore, A∩B ={f|f(x)=0}. Example 2.12. If A = {{1},{2}} and B = {{2,3},{4}}, then A∩B = ∅. This is becauseAisasetcontainingtwosets,andB isasetcontainingtwosets.Thetwosets {2} and {2,3} are not the same. Thus, A and B have no elements in common, and so A∩B =∅. Similarly to the infinite union, we can define the concept of infinite intersection. Definition 2.8 (Infinite Intersection). For an infinite sequence of sets A ,A ,..., 1 2 the infinite intersection is defined as ∞ (cid:92) A ={ξ | ξ ∈A for every finite n.} (2.8) n n n=1 To understand this definition, we note that ξ ∈A and ξ ∈B ⇐⇒ ξ is in every one of AandB. As a result, it follows that ξ ∈A and ξ ∈A and ξ ∈A ... ⇐⇒ ξ is in every one of A , A , A , .... 1 2 3 1 2 3 51
CHAPTER 2. PROBABILITY Since the infinite intersection requires that ξ is in every one of A , A , ..., A , if there is a 1 2 n set A that does not contain ξ, the infinite intersection is an empty set. i Consider the problem of finding the infinite intersection of (cid:84)∞ A , where n=1 n (cid:20) (cid:19) 1 A = 0,1+ . n n We note that the sequence of sets is [0,2], [0,1.5], [0,1.33], .... As n → ∞, we note that the limit is either [0,1) or [0,1]. Should the right-hand limit 1 be included in the infinite intersection? According to the definition above, we know that 1 ∈ A , 1 ∈ A , ..., 1 ∈ A 1 2 n for any finite n. Therefore, 1 is included and so ∞ ∞ (cid:20) (cid:19) (cid:92) (cid:92) 1 A = 0,1+ =[0,1]. n n n=1 n=1 Figure 2.7: The infinite intersection of (cid:84)∞ (cid:2) 0,1+ 1(cid:1) . No matter how large n gets, the point 1 is n=1 n never included. So the infinite intersection is [0,1] Practice Exercise 2.5. Find the infinite intersection of the sequences where (a) A =(cid:2) 0,1+ 1(cid:3) , (b) A =(cid:0) 0,1+ 1(cid:1) , (c) A =(cid:2) 0,1− 1(cid:1) , (d) A =(cid:2) 0,1− 1(cid:3) . n n n n n n n n Solution. (a) (cid:84)∞ A =[0,1]. n=1 n (b) (cid:84)∞ A =(−1,1]. n=1 n (c) (cid:84)∞ A =[0,0)=∅. n=1 n (d) (cid:84)∞ A =[0,0]={0}. n=1 n 2.1.7 Complement and difference Besides union and intersection, there is a third basic operation on sets known as the com- plement. Definition 2.9 (Complement). The complement of a set A is the set containing all elements that are in Ω but not in A. That is, Ac ={ξ |ξ ∈Ωandξ (cid:54)∈A}. (2.9) Figure 2.8 graphically portrays the idea of a complement. The complement is a set that contains everything in the universal set that is not in A. Thus the complement of a set is always relative to a specified universal set. 52
2.1. SET THEORY Figure 2.8: [Left] The complement of a set A contains all elements that are not in A. [Right] The difference A\B contains elements that are in A but not in B. Example 2.13(a). Let A={1,2,3} and Ω={1,2,3,4,5,6}. Then Ac ={4,5,6}. Example 2.13(b). Let A = {even integers} and Ω = {integers}. Then Ac = {odd integers}. Example 2.13(c). Let A={integers} and Ω=R. Then Ac ={any real number that is not an integer}. Example 2.13(d). Let A=[0,5) and Ω=R. Then Ac =(−∞,0)∪[5,∞). Example 2.13(e). Let A=R and Ω=R. Then Ac =∅. The concept of the complement will help us understand the concept of difference. Definition 2.10 (Difference). The difference A\B is the set containing all elements in A but not in B. A\B ={ξ |ξ ∈Aandξ (cid:54)∈B}. (2.10) Figure2.8portraystheconceptofdifferencegraphically.NotethatA\B (cid:54)=B\A.Theformer removes the elements in B whereas the latter removes the elements in A. Example 2.14(a). Let A = {1,3,5,6} and B = {2,3,4}. Then A\B = {1,5,6} and B\A={2,4}. Example 2.14(b). Let A = [0,1], B = [2,3], then A\B = [0,1], and B\A = [2,3]. This example shows that if the two sets do not overlap, there is nothing to subtract. Example2.14(c).LetA=[0,1],B =R,thenA\B =∅,andB\A=(−∞,0)∪(1,∞). This example shows that if one of the sets is the universal set, then the difference will either return the empty set or the complement. 53
CHAPTER 2. PROBABILITY Figure 2.9: [Left] A and B are overlapping. [Right] A and B are disjoint. Practice Exercise 2.6. Show that for any two sets A and B, the differences A\B and B\A never overlap, i.e., (A\B)∩(B\A)=∅. Solution. Suppose, by contradiction, that the intersection is not empty so that there exists an ξ ∈ (A\B)∩(B\A). Then, by the definition of intersection, ξ is an element of (A\B) and (B\A). But if ξ is an element of (A\B), it cannot be an element of B. This implies that ξ cannot be an element of (B\A) since it is a subset of B. This is a contradiction because we just assumed that the ξ can live in both (A\B) and (B\A). Difference can be defined in terms of intersection and complement: Theorem 2.1. Let A and B be two sets. Then A\B =A∩Bc (2.11) Proof. Let x ∈ A\B. Then x ∈ A and x (cid:54)∈ B. Since x (cid:54)∈ B, we have x ∈ Bc. Therefore, x ∈ A and x ∈ Bc. By the definition of intersection, we have x ∈ A∩Bc. This shows that A\B ⊆ A∩Bc. Conversely, let x ∈ A∩Bc. Then, x ∈ A and x ∈ Bc, which implies that x ∈ A and x (cid:54)∈ B. By the definition of A\B, we have that x ∈ A\B. This shows that A∩Bc ⊆A\B. (cid:3) 2.1.8 Disjoint and partition It is important to be able to quantify situations in which two sets are not overlapping. In this situation, we say that the sets are disjoint. Definition 2.11 (Disjoint). Two sets A and B are disjoint if A∩B =∅. (2.12) For a collection of sets {A ,A ,...,A }, we say that the collection is disjoint if, for 1 2 n any pair i(cid:54)=j, A ∩A =∅. (2.13) i j A pictorial interpretation can be found in Figure 2.9. 54
2.1. SET THEORY Example 2.15(a). Let A={x>1} and B ={x<0}. Then A and B are disjoint. Example 2.15(b). Let A={1,2,3} and B =∅. Then A and B are disjoint. Example 2.15(c). Let A=(0,1) and B =[1,2). Then A and B are disjoint. With the definition of disjoint, we can now define the powerful concept of partition. Definition 2.12 (Partition). A collection of sets {A ,...,A } is a partition of the 1 n universal set Ω if it satisfies the following conditions: • (non-overlap) {A ,...,A } is disjoint: 1 n A ∩A =∅. (2.14) i j • (decompose) Union of {A ,...,A } gives the universal set: 1 n n (cid:91) A =Ω. (2.15) i i=1 In plain language, a partition is a collection of non-overlapping subsets whose union is the universal set. Partition is important because it is a decomposition of Ω into a smaller subset, and since these subsets do not overlap, they can be analyzed separately. Partition is a handy tool for studying probability because it allows us to decouple complex events by treating them as isolated sub-events. Figure 2.10: A partition of Ω contains disjoint subsets of which the union gives us Ω. Example 2.16. Let Ω={1,2,3,4,5,6}. The following sets form a partition: A ={1,2,3}, A ={4,5}, A ={6} 1 2 3 Example 2.17. Let Ω={1,2,3,4,5,6}. The collection A ={1,2,3}, A ={4,5}, A ={5,6} 1 2 3 does not form a partition, because A ∩A ={5}. 2 3 55
CHAPTER 2. PROBABILITY If {A ,A ,...,A } forms a partition of the universal set Ω, then for any B ⊆ Ω, we 1 2 n can decompose B into n disjoint subsets: B∩A , B∩A , ...B∩A . Two properties hold: 1 2 n • B∩A and B∩A are disjoint if i(cid:54)=j. i j • The union of B∩A , B∩A , ...B∩A is B. 1 2 n Practice Exercise 2.7. Prove the above two statements. Solution. To prove the first statement, we can pick ξ ∈ (B ∩A ). This means that i ξ ∈ B and ξ ∈ A . Since ξ ∈ A , it cannot be in A because A and A are disjoint. i i j i j Therefore ξ cannot live in B∩A . This completes the proof, because we just showed j that any ξ ∈B∩A cannot simultaneously live in B∩A . i j To prove the second statement, we pick ξ ∈ (cid:83)n (B ∩A ). Since ξ lives in the i=1 i union,ithastoliveinatleastoneofthe(B∩A )forsomei.Nowsupposeξ ∈B∩A . i i Thismeansthatξ isinbothB andA ,soitmustliveinB.Therefore,(cid:83)n (B∩A )⊆ i i=1 i B.Now,supposewepickξ ∈B.ThensinceitisanelementinB,itmustbeanelement in all of the (B∩A )’s for any i. Therefore, ξ ∈(cid:83)n (B∩A ), and so we showed that i i=1 i B ⊆(cid:83)n (B∩A ).Combiningthetwodirections,weconcludethat(cid:83)n (B∩A )=B. i=1 i i=1 i Example 2.18. Let Ω = {1,2,3,4,5,6} and let a partition of Ω be A = {1,2,3}, 1 A = {4,5}, A = {6}. Let B = {1,3,4}. Then, by the result we just proved, B can 2 3 be decomposed into three subsets: B∩A ={1,3}, B∩A ={4}, B∩A =∅. 1 2 3 Thus we can see that B∩A , B∩A and B∩A are disjoint. Furthermore, the union 1 2 3 of these three sets gives B. 2.1.9 Set operations When handling multiple sets, it would be useful to have some basic set operations. There are four basic theorems concerning set operations that you need to know for our purposes in this book: Theorem 2.2 (Commutative). (Order does not matter) A∩B =B∩A, and A∪B =B∪A. (2.16) Theorem 2.3 (Associative). (How to do multiple union and intersection) A∪(B∪C)=(A∪B)∪C, A∩(B∩C)=(A∩B)∩C. (2.17) 56
2.1. SET THEORY Theorem 2.4 (Distributive). (How to mix union and intersection) A∩(B∪C)=(A∩B)∪(A∩C), A∪(B∩C)=(A∪B)∩(A∪C). (2.18) Theorem2.5(DeMorgan’sLaw). (Howtocomplementoverintersectionandunion) (A∩B)c =Ac∪Bc, (A∪B)c =Ac∩Bc. (2.19) Example 2.19. Consider [1,4]∩([0,2]∪[3,5]). By the distributive property we can simplify the set as [1,4]∩([0,2]∪[3,5])=([1,4]∩[0,2])∪([1,4]∩[3,5]) =[1,2]∪[3,4]. Example 2.20. Consider ([0,1]∪[2,3])c. By De Morgan’s Law we can rewrite the set as ([0,2]∪[1,3])c =[0,2]c∩[1,3]c. 2.1.10 Closing remarks about set theory It should be apparent why set theory is useful: it shows us how to combine, split, and remove sets. In Figure 2.11 we depict the intersection of two sets A={even number} and B ={less than or equal to 3}. Set theory tells us how to define the intersection so that the probability can be applied to the resulting set. Figure2.11:WhentherearetwoeventsAandB,theprobabilityofA∩B isdeterminedbyfirsttaking the intersection of the two sets and then evaluating its probability. Universal sets and empty sets are useful too. Universal sets cover all the possible outcomes of an experiment, so we should expect P[Ω] = 1. Empty sets contain nothing, and so we should expect P[∅]=0. These two properties are essential to define a probability because no probability can be greater than 1, and no probability can be less than 0. 57
CHAPTER 2. PROBABILITY 2.2 Probability Space We now formally define probability. Our discussion will be based on the slogan probability is a measure of the size of a set. Three elements constitute a probability space: • Sample Space Ω: The set of all possible outcomes from an experiment. • Event SpaceF:Thecollectionofallpossibleevents.AneventE isasubsetinΩthat defines an outcome or a combination of outcomes. • Probability Law P: A mapping from an event E to a number P[E] which, ideally, measures the size of the event. Therefore, whenever you talk about “probability,” you need to specify the triplet (Ω,F,P) to define the probability space. The necessity of the three elements is illustrated in Figure 2.12. The sample space is the interface with the physical world. It is the collection of all possible states that can result from an experiment. Some outcomes are more likely to happen, and some are less likely, but this does not matter because the sample space contains every possible outcome. The probability law is the interface with the data analysis. It is this law that defines the likelihood of each of the outcomes. However, since the probability law measures the size of a set, the probability law itself must be a function, a function whose argument is a set and whose value is a number. An outcome in the sample space is not a set. Instead, a subset in the sample space is a set. Therefore, the probability should input a subset and map it to a number. The collection of all possible subsets is the event space. Figure 2.12: Given an experiment, we define the collection of all outcomes as the sample space. A subset in the sample space is called an event. The probability law is a mapping that maps an event to a number that denotes the size of the event. A perceptive reader like you may be wondering why we want to complicate things to thisdegreewhencalculatingprobabilityistrivial,e.g.,throwingadiegivesusaprobability 1 per face. In a simple world where problems are that easy, you can surely ignore all these 6 complications and proceed to the answer 1. However, modern data analysis is not so easy. 6 If we are given an image of size 64×64 pixels, how do we tell whether this image is of a cat or a dog? We need to construct a probability model that tells us the likelihood of having a 58
2.2. PROBABILITY SPACE particular 64×64 image. What should be included in this probability model? We need to know all the possible cases (the sample space), all the possible events (the event space), andtheprobabilityofeachoftheevents(theprobabilitylaw).Ifweknowallthese,thenour decision will be theoretically optimal. Of course, for high-dimensional data like images, we need approximations to such a probability model. However, we first need to understand the theoretical foundation of the probability space to know what approximations would make sense. 2.2.1 Sample space Ω We start by defining the sample space Ω. Given an experiment, the sample space Ω is the set containing all possible outcomes of the experiment. Definition 2.13. A sample space Ω is the set of all possible outcomes from an ex- periment. We denote ξ as an element in Ω. A sample space can contain discrete outcomes or continuous outcomes, as shown in the examples below and Figure 2.13. Example 2.21: (Discrete Outcomes) • Coin flip: Ω={H, T}. • Throw a die: Ω={1,2,3,4,5,6}. • Paper / scissor / stone: Ω={paper,scissor,stone}. • Draw an even integer: Ω={2,4,6,8,...}. Example 2.22: (Continuous Outcomes) • Waiting time for a bus in West Lafayette: Ω={t | 0≤t≤30minutes}. • Phase angle of a voltage: Ω={θ | 0≤θ ≤2π}. • Frequency of a pitch: Ω={f | 0≤f ≤f }. max Figure 2.13 also shows a functional example of the sample space. In this case, the sample space contains functions. For example, • Set of all straight lines in 2D: Ω={f |f(x)=ax+b, a,b∈R}. • Set of all cosine functions with a phase offset: Ω={f |f(t)=cos(2πω t+Θ), 0≤Θ≤2π}. 0 As we see from the above examples, the sample space is nothing but a universal set. The elements inside the sample space are the outcomes of the experiment. If you change 59
CHAPTER 2. PROBABILITY Figure 2.13: The sample space can take various forms: it can contain discrete numbers, or continuous intervals, or even functions. the experiment, the possible outcomes will be different so that the sample space will be different. For example, flipping a coin has different possible outcomes from throwing a die. What if we want to describe a composite experiment where we flip a coin and throw a die? Here is the sample space: Example 2.23: If the experiment contains flipping a coin and throwing a die, then the sample space is (cid:26) (H,1),(H,2),(H,3),(H,4),(H,5),(H,6), (cid:27) (T,1),(T,2),(T,3),(T,4),(T,5),(T,6) . In this sample space, each element is a pair of outcomes. Practice Exercise 2.8.Thereare8processorsonacomputer.Acomputerjobsched- uler chooses one processor randomly. What is the sample space? If the computer job scheduler can choose two processors at once, what is the sample space then? Solution. The sample space of the first case is Ω = {1,2,3,4,5,6,7,8}. The sample space of the second case is Ω={(1,2),(1,3),(1,4),...,(7,8)}. Practice Exercise 2.9. A cell phone tower has a circular average coverage area of radius of 10 km. We observe the source locations of calls received by the tower. What is the sample space of all possible source locations? Solution.Assumethatthecenterofthetowerislocatedat(x ,y ).Thesamplespace 0 0 is the set (cid:112) Ω={(x,y)| (x−x )2+(y−y )2 ≤10}. 0 0 60
2.2. PROBABILITY SPACE Noteverysetcanbeasamplespace.Asamplespacemustbeexhaustiveandexclusive. The term “exhaustive” means that the sample space has to cover all possible outcomes. If there is one possible outcome that is left out, then the set is no longer a sample space. The term “exclusive” means that the sample space contains unique elements so that there is no repetition of elements. Example 2.24. (Counterexamples) The following two examples are NOT sample spaces. • Throw a die: Ω={1,2,3} is not a sample space because it is not exhaustive. • Throwadie:Ω={1,1,2,3,4,5,6}isnotasamplespacebecauseitisnotexclu- sive. Therefore, a valid sample space must contain all possible outcomes, and each element must be unique. We summarize the concept of a sample space as follows. What is a sample space Ω? • A sample space Ω is the collection of all possible outcomes. • The outcomes can be numbers, alphabets, vectors, or functions. The outcomes can also be images, videos, EEG signals, audio speeches, etc. • Ω must be exhaustive and exclusive. 2.2.2 Event space F Thesamplespacecontainsallthepossibleoutcomes.However,inmanypracticalsituations, we are not interested in each of the individual outcomes; we are interested in the com- binations of the outcomes. For example, when throwing a die, we may ask “What is the probabilityofrollinganoddnumber?”or“Whatistheprobabilityofrollinganumberthat is less than 3?” Clearly, “odd number” is not an outcome of the experiment because the possible outcomes are {1,2,3,4,5,6}. We call “odd number” an event. An event must be a subset in the sample space. Definition2.14. An eventE isasubsetinthesamplespaceΩ.Thesetofallpossible events is denoted as F. While this definition is extremely simple, we need to keep in mind a few facts about events. First, an outcome ξ is an element in Ω but an event E is a subset contained in Ω, i.e., E ⊆ Ω. Thus, an event can contain one outcome but it can also contain many outcomes. The following example shows a few cases of events: 61
CHAPTER 2. PROBABILITY Example 2.25. Throw a die. Let Ω={1,2,3,4,5,6}. The following are two possible events, as illustrated in Figure 2.14. • E ={even numbers}={2,4,6}. 1 • E ={less than 3}={1,2}. 2 Figure 2.14: Twoexamplesofevents:Thefirsteventcontainsnumbers{2,4,6},andthesecond event contains numbers {1,2}. Practice Exercise 2.10. The “ping” command is used to measure round-trip times for Internet packets. What is the sample space of all possible round-trip times? What is the event that a round-trip time is between 10 ms and 20 ms? Solution. The sample space is Ω=[0,∞). The event is E =[10,20]. Practice Exercise 2.11. A cell phone tower has a circular average coverage area of radius 10 km. We observe the source locations of calls received by the tower. What is theeventwhenthesourcelocationofacallisbetween2kmand5kmfromthetower? Solution. Assume that the center of the tower is located at (x ,y ). The event is 0 0 (cid:112) E ={(x,y)|2≤ (x−x )2+(y−y )2 ≤5}. 0 0 The second point we should remember is the cardinality of Ω and that of F. A sample space containing n elements has a cardinality n. However, the event space constructed from Ω will contain 2n events. To see why this is so, let’s consider the following example. Example 2.26. Consider an experiment with 3 outcomes Ω={♣,♥,(cid:122)}. We can list out all the possible events: ∅, {♣}, {♥}, {(cid:122)}, {♣,♥}, {♣,(cid:122)}, {♥,♣}, {♣,♥,(cid:122)}. So in total there are 23 = 8 possible events. Figure 2.15 depicts the situation. What is the difference between ♣ and {♣}? The former is an element, whereas the latter is a set. Thus, {♣} is an event but ♣ is not an event. Why is ∅ an event? Because we can ask “What is the probability that we get an odd number and an even number?” The probabilityisobviouslyzero,butthereasonitiszeroisthattheeventisanemptyset. 62
2.2. PROBABILITY SPACE Figure 2.15: The event space contains all the possible subsets inside the sample space. In general, if there are n elements in the sample space, then the number of events is 2n. To see why this is true, we can assign to each element a binary value: either 0 or 1. For example, in Table 2.1 we consider throwing a die. For each of the six faces, we assign a binary code. This will give us a binary string for each event. For example, the event {1,5} is encoded as the binary string 100010 because only 1 and 5 are activated. We can count the total number of unique strings, which is the number of strings that can be constructed from n bits. It is easily seen that this number is 2n. Event 1 2 3 4 5 6 Binary Code ∅ × × × × × × 000000 {1,5} (cid:13) × × × (cid:13) × 100010 {3,4,5} × × (cid:13) (cid:13) (cid:13) × 001110 . . . . . . . . . . . . {2,3,4,5,6} × (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 011111 {1,2,3,4,5,6} (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 111111 Table 2.1: Aneventspacecontains2n events,wherenisthenumberofelementsinthesamplespace. Toseethis,weencodeeachoutcomewithabinarycode.Theresultingbinarystringthenformsaunique index of the event. Counting the total number of events gives us the cardinality of the event space. The box below summarizes what you need to know about event spaces. What is an event space F? • An event space F is the set of all possible subsets. It is a set of sets. • We need F because the probability law P is mapping a set to a number. P does not take an outcome from Ω but a subset inside Ω. 63
CHAPTER 2. PROBABILITY Event spaces: Some advanced topics The following discussions can be skipped if it is your first time reading the book. What else do we need to take care of in order to ensure that an event is well defined? A few set operations seem to be necessary. For example, if E = {1} and E = {2} are 1 2 events, it is necessary that E = E ∪E = {1,2} is an event too. Another example: if 1 2 E ={5,6} and E ={1,5} are events, then it is necessary that E =E ∩E ={5} is also 1 2 1 2 an event. The third example: if E = {3,4,5,6} is an event, then E = Ec = {1,2} should 1 1 be an event. As you can see, there is nothing sophisticated in these examples. They are just some basic set operations. We want to ensure that the event space is closed under these set operations. That is, we do not want to be surprised by finding that a set constructed from two events is not an event. However, since all set operations can be constructed from union, intersection and complement, ensuring that the event space is closed under these three operations effectively ensures that it is closed to all set operations. The formal way to guarantee these is the notion of a field. This term may seem to be abstract, but it is indeed quite useful: Definition 2.15. For an event space F to be valid, F must be a field F. It is a field if it satisfies the following conditions • ∅∈F and Ω∈F. • (Closed under complement) If F ∈F, then also Fc ∈F. • (Closed under union and intersection) If F ∈F and F ∈F, then F ∩F ∈F 1 2 1 2 and F ∪F ∈F. 1 2 Forafiniteset,i.e.,asetthatcontainsnelements,thecollectionofallpossiblesubsets is indeed a field. This is not difficult to see if you consider rolling a die. For example, if E ={3,4,5,6}isinsideF,thenEc ={1,2}isalsoinsideF.ThisisbecauseF consistsof2n subsets each being encoded by a unique binary string. So if E =001111, then Ec =110000, which is also in F. Similar reasoning applies to intersection and union. At this point, you may ask: • Why bother constructing a field? The answer is that probability is a measure of the size of a set, so we must input a set to a probability measure P to get a number. The set being input to P must be a subset inside the sample space; otherwise, it will be undefined. If we regard P as a mapping, we need to specify the collection of all its inputs, which is the set of all subsets, i.e., the event space. So if we do not define the field, there is no way to define the measure P. • What if the event space is not a field? If the event space is not a field, then we can easilyconstructpathologicalcaseswherewecannotassignaprobability.Forexample, if the event space is not a field, then it would be possible that the complement of E ={3,4,5,6} (which is Ec ={1,2}) is not an event. This just does not make sense. The concept of a field is sufficient for finite sample spaces. However, there are two other types of sample spaces where the concept of a field is inadequate. The first type of 64
2.2. PROBABILITY SPACE sets consists of the countably infinite sets, and the second type consists of the sets defined on the real line. There are other types of sets, but these two have important practical applications. Therefore, we need to have a basic understanding of these two types. Sigma-field The difficultyof a countablyinfinite set isthat there areinfinitely many subsetsin the field of a countably infinite set. Having a finite union and a finite intersection is insufficient to ensure the closedness of all intersections and unions. In particular, having F ∪F ∈F does 1 2 not automatically give us (cid:83)∞ F ∈ F because the latter is an infinite union. Therefore, n=1 n for countably infinite sets, their requirements to be a field are more restrictive as we need to ensure infinite intersection and union. The resulting field is called the σ-field. Definition 2.16. A sigma-field (σ-field) F is a field such that • F is a field, and • if F ,F ,... ∈ F, then the union (cid:83)∞ F and the intersection (cid:84)∞ F are both 1 2 i=1 i i=1 i in F. When do we need a σ-field? When the sample space is countable and has infinitely many elements. For example, if the sample space contains all integers, then the collection of all possible subsets is a σ-field. For another, if E ={2}, E ={4}, E ={6}, ..., then 1 2 3 (cid:83)∞ E ={2,4,6,8, ...}={positive even numbers}. Clearly, we want (cid:83)∞ E to live in n=1 n n=1 n the sample space. Borel sigma-field While a sigma-field allows us to consider countable sets of events, it is still insufficient for considering events defined on the real line, e.g., time, as these events are not countable. So how do we define an event on the real line? It turns out that we need a different way to define the smallest unit. For finite sets and countable sets, the smallest units are the elements themselves because we can count them. For the real line, we cannot count the elements because any non-empty interval is uncountably infinite. The smallest unit we use to construct a field for the real line is a semi-closed interval def (−∞,b] = {x| −∞<x≤b}. The Borel σ-field is defined as the sigma-field generated by the semi-closed inter- vals. Definition2.17. The Borelσ-fieldBisaσ-fieldgeneratedfromsemi-closedintervals: def (−∞,b] = {x| −∞<x≤b}. The difference between the Borel σ-field B and a regular σ-field is how we measure the subsets. In a σ-field, we count the elements in the subsets, whereas, in a Borel σ-field, we use the semi-closed intervals to measure the subsets. 65
CHAPTER 2. PROBABILITY Being a field, the Borel σ-field is closed under complement, union, and intersection. In particular, subsets of the following forms are also in the Borel σ-field B: (a,b), [a,b], (a,b], [a,b), [a,∞), (a,∞), (−∞,b], {b}. For example, (a,∞) can be constructed from (−∞,a]c, and (a,b] can be constructed by taking the intersection of (−∞,b] and (a,∞). Example 2.27: Waiting for a bus. Let Ω= {0≤t≤30}. The Borel σ-field contains all semi-closed intervals (a,b], where 0≤a≤b≤30. Here are two possible events: • F ={less than 10 minutes}={0≤t<10}={0}∪({0<t≤10}∩{10}c). 1 • F ={more than 20 minutes}={20<t≤30}. 2 Further discussion of the Borel σ-field can be found in Leon-Garcia (3rd Edition,) Chapter 2.9. This is the end of the discussion. Please join us again. 2.2.3 Probability law P The third component of a probability space is the probability law P. Its job is to assign a number to an event. Definition 2.18. A probability law is a function P : F → [0,1] of an event E to a real number in [0, 1]. The probability law is thus a function, and therefore we must specify the input and the output. The input to P is an event E, which is a subset in Ω and an element in F. The output of P is a number between 0 and 1, which we call the probability. The definition above does not specify how an event is being mapped to a number. However, since probability is a measure of the size of a set, a meaningful P should be consistentforalleventsinF.Thisrequiressomerules,knownastheaxioms of probability, when we define the P. Any probability law P must satisfy these axioms; otherwise, we will see contradictions. We will discuss the axioms in the next section. For now, let us look at two examples to make sure we understand the functional nature of P. Example 2.28. Consider flipping a coin. The event space is F = {∅,{H},{T},Ω}. We can define the probability law as 1 1 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1, 2 2 as shown in Figure 2.16. This P is clearly consistent for all the events in F. Is it possible to construct an invalid P? Certainly. Consider the following proba- 66
2.2. PROBABILITY SPACE bility law: 1 1 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1. 3 3 This law is invalid because the individual events are P[{H}] = 1 and P[{T}] = 1 3 3 but the union is P[Ω] = 1. To fix this problem, one possible solution is to define the probability law as 1 2 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1. 3 3 Then, the probabilities for all the events are well defined and consistent. Figure 2.16: A probability law is a mapping from an event to a number. A probability law cannot be arbitrarily assigned; it must satisfy the axioms of probability. Example 2.29. Consider a sample space containing three elements Ω = {♣,♥,(cid:122)}. (cid:26) (cid:27) The event space is then F = ∅,{♣},{♥},{(cid:122)},{♣,♥},{♥,(cid:122)},{♣,(cid:122)},{♣,♥,(cid:122)} . One possible P we could define would be 1 P[∅]=0, P[{♣}]=P[{♥}]=P[{(cid:122)}]= , 3 2 P[{♣,♥}]=P[{♣,(cid:122)}]=P[{♥,(cid:122)}]= , P[{♣,♥,(cid:122)}]=1. 3 What is a probability law P? • A probability law P is a function. • It takes a subset (an element in F) and maps it to a number between 0 and 1. • P is a measure of the size of a set. • For P to be valid, it must satisfy the axioms of probability. 67
CHAPTER 2. PROBABILITY Figure 2.17: Probabilityisameasureofthesizeofaset.Theprobabilitycanbeacounterthatcounts thenumberofelements,arulerthatmeasuresthelengthofaninterval,oranintegrationthatmeasures the area of a region. A probability law P is a measure Consider the word “measure” in our slogan: probability is a measure of the size of a set. Depending on the nature of the set, the measure can be a counter, ruler, scale, or even a stopwatch. So far, all the examples we have seen are based on sets with a finite number of elements.Forthesesets,thenaturalchoiceoftheprobabilitymeasureisacounter.However, if the sets are intervals on the real line or regions in a plane, we need a different probability law to measure their size. Let’s look at the examples shown in Figure 2.17. Example 2.30 (Finite Set). Consider throwing a die, so that Ω={1,2,3,4,5,6}. Then the probability measure is a counter that reports the number of elements. If the die is fair, i.e., all the 6 faces have equal probability of happening, then an event E ={1,3} will have a probability P[E]= 2. 6 Example2.31(Intervals).SupposethatthesamplespaceisaunitintervalΩ=[0,1]. Let E be an event such that E = [a,b] where a,b are numbers in [0,1]. Then the probability measure is a ruler that measures the length of the intervals. If all the numbers on the real line have equal probability of appearing, then P[E]=b−a. Example 2.32 (Regions). Suppose that the sample space is the square Ω=[−1,1]× [−1,1]. Let E be a circle such that E ={(x,y)|x2+y2 <r2}, where r <1. Then the probability measure is an area measure that returns us the area of E. If we assume that all coordinates in Ω are equally probable, then P[E]=πr2, for r <1. Becauseprobabilityisameasureofthesizeofaset,twosetscanbecomparedaccording to their probability measures. For example, if Ω = {♣,♥,(cid:122)}, and if E = {♣} and E = 1 2 {♣,♥}, then one possible P is to assign P[E ]=P[{♣}]= 1 and P[E ]=P[{♣,♥}]=2/3. 1 3 2 68
2.2. PROBABILITY SPACE In this particular case, we see that E ⊆E and thus 1 2 P[E ]≤P[E ]. 1 2 Let’s now consider the term “size.” Notice that the concept of the size of a set is not limited to the number of elements. A better way to think about size is to imagine that it is theweightoftheset.Thismightmayseemfancifulatfirst,butitisquitenatural.Consider the following example. Example 2.33. (Discrete events with different weights) Suppose we have a sample space Ω={♣,♥,(cid:122)}. Let us assign a different probability to each outcome: 2 1 3 P[{♣}]= , P[{♥}]= , P[{(cid:122)}]= . 6 6 6 As illustrated in Figure 2.18, since each outcome has a different weight, when de- termining the probability of a set of outcomes we can add these weights (instead of counting the number of outcomes). For example, when reporting P[{♣}] we find its weightP[{♣}]= 2,whereaswhenreportingP[{♥,(cid:122)}]wefindthesumoftheirweights 6 P[{♥,(cid:122)}] = 1 + 3 = 4. Therefore, the notion of size does not refer to the number of 6 6 6 elements but to the total weight of these elements. Figure 2.18: This example shows the “weights” of three elements in a set. The weights are numbers between 0 and 1 such that the sum is 1. When applying a probability measure to this set, we sum the weights for the elements in the events being considered. For example, P[♥,(cid:122)] = yellow + green, and P[♣]= purple. Example 2.34.(Continuouseventswithdifferentweights)Supposethatthesample space is an interval, say Ω = [−1,1]. On this interval we define a weighting function f(x) where f(x ) specifies the weight for x . Because Ω is an interval, events defined 0 0 on this Ω must also be intervals. For example, we can consider two events E =[a,b] 1 and E =[c,d]. The probabilities of these events are P[E ]=(cid:82)b f(x)dx and P[E ]= 2 1 a 2 (cid:82)d f(x)dx, as shown in Figure 2.19. c Viewing probability as a measure is not just a game for mathematicians; rather, it has fundamental significance for several reasons. First, it eliminates any dependency on probability as relative frequency from the frequentist point of view. Relative frequency is a 69
CHAPTER 2. PROBABILITY Figure 2.19: If the sample space is an interval on the real line, then the probability of an event is the area under the curve of the weighting function. narrowlydefinedconceptthatislargelylimitedtodiscreteevents,e.g.,flippingacoin.While we can assign weights to coin-toss events to deal with those biased coins, the extension to continuous events becomes problematic. By thinking of probability as a measure, we can generalize the notion to apply to intervals, areas, volumes, and so on. Second, viewing probability as a measure forces us to disentangle an event from mea- sures. An event is a subset in the sample space. It has nothing to do with the measure (e.g., a ruler) you use to measure the event. The measure, on the other hand, specifies the weighting function you apply to measure the event when computing the probability. For example, let Ω = [−1,1] be an interval, and let E = [a,b] be an event. We can define two weighting functions f(x) and g(x). Correspondingly, we will have two different probability measures F and G such that (cid:90) (cid:90) b F([a,b])= dF= f(x)dx, E a (cid:90) (cid:90) b G([a,b])= dG= g(x)dx. (2.20) E a Tomakesenseofthesenotations,consideronlyP[[a,b]]andnotF([a,b])andG([a,b]).Asyou can see, the event for both measures is E =[a,b] but the measures are different. Therefore, the values of the probability are different. Example 2.35. (Two probability laws are different if their weighting functions are different.) Consider two different weighting functions for throwing a die. The first one assigns probability as the following: 1 2 3 P[{1}]= , P[{2}]= , P[{3}]= , 12 12 12 4 1 1 P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 whereas the second function assigns the probability like this: 2 2 2 P[{1}]= , P[{2}]= , P[{3}]= , 12 12 12 2 2 2 P[{4}]= , P[{5}]= , P[{6}]= . 12 12 12 70
2.2. PROBABILITY SPACE Let an event E ={1,2}. Let F be the measure using the first set of probabilities, and let G be the measure of the second set of probabilities. Then, 1 2 3 F(E)=F({1,2})= + = , 12 12 12 2 2 4 G(E)=G({1,2})= + = . 12 12 12 Therefore, although the events are the same, the two different measures will give us two different probability values. Remark. The notation (cid:82) dF in Equation (2.20) is known as the Lebesgue integral. You E should be aware of this notation, but the theory of Lebesgue measure is beyond the scope of this book. 2.2.4 Measure zero sets Understanding the measure perspective on probability allows us to understand another important concept of probability, namely measure zero sets. To introduce this concept, we pose the question: What is the probability of obtaining a single point, say {0.5}, when the sample space is Ω=[0,1]? The answer to this question is rooted in the compatibility between the measure and the sample space. In other words, the measure has to be meaningful for the events in the sample space. Using Ω=[0,1], since Ω is an interval, an appropriate measure would be the length of this interval. You may add different weighting functions to define your measure, but ultimately, the measure must be an integral. If you use a “counter” as a measure, then the counter and the interval are not compatible because you cannot count on the real line. Now, suppose that we define a measure for Ω=[0,1] using a weighting function f(x). This measure is determined by an integration. Then, for E ={0.5}, the measure is (cid:90) 0.5 P[E]=P[{0.5}]= f(x)dx=0. 0.5 In fact, for any weighting function the integral will be zero because the length of the set E is zero.1 An event that gives us zero probability is known as an event with measure 0. Figure 2.20 shows an example. Figure 2.20: The probability of obtaining a single point in a continuous interval is zero. 1We assume that f is continuous throughout [0,1]. If f is discontinuous at x = 0.5, some additional considerationswillapply. 71
CHAPTER 2. PROBABILITY What are measure zero sets? • A set E (non-empty) is called a measure zero set when P[E]=0. • For example, {0} is a measure zero set when we use a continuous measure F. • But {0} can have a positive measure when we use a discrete measure G. Example 2.36(a). Consider a fair die with Ω={1,2,3,4,5,6}. Then the set {1} has a probability of 1. The sample space does not have a measure zero event because the 6 measure we use is a counter. Example 2.36(b).ConsideranintervalwithΩ=[1,6].Thentheset{1}hasmeasure 0 because it is an isolated point with respect to the sample space. Example 2.36(c). For any intervals, P[[a,b]] = P[(a,b)] because the two end points have measure zero: P[{a}]=P[{b}]=0. Formal definitions of measure zero sets The following discussion of the formal definitions of measure zero sets is optional for the first reading of this book. We can formally define measure zero sets as follows: Definition 2.19. Let Ω be the sample space. A set A ∈ Ω is said to have measure zero if for any given (cid:15)>0, • There exists a countable number of subsets A such that A⊆∪∞ A , and n n=1 n • (cid:80)∞ P[A ]<(cid:15). n=1 n You may need to read this definition carefully. Suppose we have an event A. We construct a set of neighbors A ,...,A such that A is included in the union ∪∞ A . If the sum of 1 ∞ n=1 n the all P[A ] is still less than (cid:15), then the set A will have a measure zero. n To understand the difference between a measure for a continuous set and a countable set,considerFigure2.21.Ontheleftsideof Figure2.21weshowanintervalΩinwhichthere isanisolatedpointx .ThemeasureforthisΩisthelengthoftheinterval(relativetowhat- 0 ever weighting function you use). We define a small neighborhood A =(x − (cid:15), x + (cid:15)) 0 0 2 0 2 surrounding x . The length of this interval is not more than (cid:15). We then shrink (cid:15). How- 0 ever, regardless of how small (cid:15) is, since x is an isolated point, it is always included in the 0 neighborhood. Therefore, the definition is satisfied, and so {x } has measure zero. 0 Example 2.37. Let Ω=[0,1]. The set {0.5}⊂Ω has measure zero, i.e., P[{0.5}]=0. To see this, we draw a small interval around 0.5, say [0.5−(cid:15)/3,0.5+(cid:15)/3]. Inside this interval, there is really nothing to measure besides the point 0.5. Thus we have found 72
2.2. PROBABILITY SPACE an interval such that it contains 0.5, and the probability is P[[0.5−(cid:15)/3,0.5+(cid:15)/3]] = 2(cid:15)/3<(cid:15). Therefore, by definition, the set {0.5} has measure 0. Thesituationisverydifferentfortheright-handsideof Figure2.21.Here,themeasure is not the length but a counter. So if we create a neighborhood surrounding the isolated point x , we can always make a count. As a result, if you shrink (cid:15) to become a very small 0 number (in this case less than 1), then P[{x }] < (cid:15) will no longer be true. Therefore, the 4 0 set {x } has a non-zero measure when we use the counter as the measure. 0 Figure 2.21: [Left]Foracontinuoussamplespace,asinglepointevent{x }canalwaysbesurrounded 0 by a neighborhood A whose size P[A ] < (cid:15). [Right] If you change the sample space to discrete 0 0 elements, then a single point event {x } can still be surrounded by a neighborhood A . However, the 0 0 size P[A ]=1/4 is a fixed number and will not work for any (cid:15). 0 When we make probabilistic claims without considering the measure zero sets, we say that an event happens almost surely. Definition 2.20. An event A∈R is said to hold almost surely (a.s.) if P[A]=1 (2.21) except for all measure zero sets in R. Therefore,ifasetAcontainsmeasurezerosubsets,wecansimplyignorethembecausethey do not affect the probability of events. In this book, we will omit “a.s.” if the context is clear. Example 2.38(a).LetΩ=[0,1].ThenP[(0,1)]=1almostsurelybecausethepoints 0 and 1 have measure zero in Ω. Example 2.38(b). Let Ω = {x | x2 ≤ 1} and let A = {x | x2 < 1}. Then P[A] = 1 almost surely because the circumference has measure zero in Ω. Practice Exercise 2.12. Let Ω={f :R→[−1,1]|f(t)=cos(ω t+θ)}, where ω is 0 0 a fixed constant and θ is random. Construct a measure zero event and an almost sure event. Solution. Let E ={f :R→[−1,1]|f(t)=cos(ω t+kπ/2)} 0 foranyintegerk.Thatis,E containsallthefunctionswithaphaseofπ/2,2π/2,3π/2, etc. Then E will have measure zero because it is a countable set of isolated functions. 73
CHAPTER 2. PROBABILITY The event Ec will have probability P[Ec] = 1 almost surely because E has measure zero. This is the end of the discussion. Please join us again. 2.2.5 Summary of the probability space After the preceding long journey through theory, let us summarize. First, it is extremely important to understand our slogan: probability is a measure of the size of a set. This slogan is precise, but it needs clarification. When we say probability isameasure,wearethinkingofitasbeingtheprobabilitylawP.Ofcourse,inpractice,we alwaysthinkofprobabilityasthenumberreturnedbythemeasure.However,thedifference is not crucial. Also, “size” not only means the number of elements in the set, but it also means the relative weight of the set in the sample space. For example, if we use a weight function to weigh the set elements, then size would refer to the overall weight of the set. When we put all these pieces together, we can understand why a probability space must consist of the three components (Ω,F,P), (2.22) whereΩisthesamplespacethatdefinesallpossibleoutcomes,F istheeventspacegenerated from Ω, and P is the probability law that maps an event to a number in [0,1]. Can we drop one or more of the three components? We cannot! If we do not specify the sample space Ω, then there is no way to define the events. If we do not have a complete event space F, then some events will become undefined, and further, if the probability law is applied only to outcomes, we will not be able to define the probability for events. Finally, if we do not specify the probability law, then we do not have a way to assign probabilities. 2.3 Axioms of Probability We now turn to a deeper examination of the properties. Our motivation is simple. While the definition of probability law has achieved its goal of assigning a probability to an event, there must be restrictions on how the assignment can be made. For example, if we set P[{H}] = 1/3, then P[{T}] must be 2/3; otherwise, the sum of having a head and a tail will be greater than 1. The necessary restrictions on assigning a probability to an event are collectively known as the axioms of probability. Definition 2.21. A probability law is a function P : F → [0,1] that maps an event A to a real number in [0, 1]. The function must satisfy the axioms of probability: I. Non-negativity: P[A]≥0, for any A⊆Ω. II. Normalization: P[Ω]=1. 74
2.3. AXIOMS OF PROBABILITY III. Additivity: For any disjoint sets {A ,A ,...}, it must be true that 1 2 (cid:34)∞ (cid:35) ∞ (cid:91) (cid:88) P A = P[A ]. (2.23) i i i=1 i=1 An axiom is a proposition that serves as a premise or starting point in a logical system. Axioms are not definitions, nor are they theorems. They are believed to be true or true within a certain context. In our case, the axioms are true within the context of Bayesian probability. The Kolmogorov probability relies on another set of axioms. We will not dive into the details of these historical issues; in this book, we will confine our discussion to the three axioms given above. 2.3.1 Why these three probability axioms? Why do we need three axioms? Why not just two axioms? Why these three particular axioms? The reasons are summarized in the box below. Why these three axioms? • Axiom I (Non-negativity) ensures that probability is never negative. • Axiom II (Normalization) ensures that probability is never greater than 1. • Axiom III (Additivity) allows us to add probabilities when two events do not overlap. Axiom I is called the non-negativity axiom. It ensures that a probability value cannot be negative. Non-negativity is a must for probability. It is meaningless to say that the probability of getting an event is a negative number. AxiomIIiscalledthenormalizationaxiom.Itensuresthattheprobabilityofobserving all possible outcomes is 1. This gives the upper limit of the probability. The upper limit does not have to be 1. It could be 10 or 100. As long as we are consistent about this upper limit, we are good. However, for historical reasons and convenience, we choose 1 to be the upper limit. Axiom III is called the additivity axiom and is the most critical one among the three. The additivity axiom defines how set operations can be translated into probability oper- ations. In a nutshell, it says that if we have a set of disjoint events, the probabilities can be added. From the measure perspective, Axiom III makes sense because if P measures the size of an event, then two disjoint events should have their probabilities added. If two dis- joint events do not allow their probabilities to be added, then there is no way to measure a combined event. Similarly, if the probabilities can somehow be added even for overlap- ping events, there will be inconsistencies because there is no systematic way to handle the overlapping regions. The countable additivity stated in Axiom III can be applied to both a finite number or an infinite number of sets. The finite case states that for any two disjoint sets A and B, we have P[A∪B]=P[A]+P[B]. (2.24) 75
CHAPTER 2. PROBABILITY In other words, if A and B are disjoint, then the probability of observing either A or B is the sum of the two individual probabilities. Figure 2.22 illustrates this idea. Example 2.39. Let’s see why Axiom III is critical. Consider throwing a fair die with Ω={1,2,3,4,5,6}. The probability of getting {4,6} is 1 1 2 P[{4,6}]=P[{4}∪{6}]=P[{4}]+P[{6}]= + = . 6 6 6 Inthisequation,thesecondequalityholdsbecausetheevents{4}and{6}aredisjoint. If we do not have Axiom III, then we cannot add probabilities. Figure 2.22: Axiom III says P[A∪B]=P[A]+P[B] if A∩B =∅. 2.3.2 Axioms through the lens of measure Axioms are “rules” we must abide by when we construct a measure. Therefore, any valid measure must be compatible with the axioms, regardless of whether we have a weighting function or not. In the following two examples, we will see how the weighting functions are used in the axioms. Example 2.40. Consider a sample space with Ω = {♣,♥,(cid:122)}. The probability for each outcome is 2 1 3 P[{♣}]= , P[{♥}]= , P[{(cid:122)}]= . 6 6 6 Suppose we construct two disjoint events E = {♣,♥} and E = {(cid:122)}. Then Axiom 1 2 III says (cid:18) (cid:19) 2 1 3 P[E ∪E ]=P[E ]+P[E ]= + + =1. 1 2 1 2 6 6 6 Note that in this calculation, the measure P is still a measure P. If we endow it with a nonuniform weight function, then P applies the corresponding weights to the corresponding outcomes. This process is compatible with the axioms. See Figure 2.23 for a pictorial illustration. 76
2.3. AXIOMS OF PROBABILITY Example 2.41. Suppose the sample space is an interval Ω = [0,1]. The two events are E = [a,b] and E = [c,d]. Assume that the measure P uses a weighting function 1 2 f(x). Then, by Axiom III, we know that P[E ∪E ]=P[E ]+P[E ] 1 2 1 2 =P[[a,b]]+P[[c,d]] (by Axiom 3) (cid:90) b (cid:90) d = f(x)dx+ f(x)dx, (apply the measure). a c As you can see, there is no conflict between the axioms and the measure. Figure 2.24 illustrates this example. Figure2.23:Applyingweightingfunctionstothemeasures:Supposewehavethreeelementsintheset. To compute the probability P[{♥,(cid:122)}∪{♣}], we can write it as the sum of P[{♥,(cid:122)}] and P[{♣}]. Figure 2.24: The axioms are compatible with the measure, even if we use a weighting function. 2.3.3 Corollaries derived from the axioms TheunionofAandB isequivalenttothelogicaloperator“OR”.Oncethelogicaloperation “OR” is defined, all other logical operations can be defined. The following corollaries are examples. Corollary 2.1. Let A∈F be an event. Then, (a) P[Ac]=1−P[A]. (b) P[A]≤1. (c) P[∅]=0. 77
CHAPTER 2. PROBABILITY Proof. (a) Since Ω=A∪Ac, by finite additivity we have P[Ω]=P[A∪Ac]=P[A]+P[Ac]. By the normalization axiom, we have P[Ω]=1. Therefore, P[Ac]=1−P[A]. (b) We prove by contradiction. Assume P[A]>1. Consider the complement Ac where A∪Ac =Ω.SinceP[Ac]=1−P[A],wemusthaveP[Ac]<0becausebyhypothesisP[A]>1. But P[Ac]<0 violates the non-negativity axiom. So we must have P[A]≤1. (c) Since Ω=Ω∪∅, by the first corollary we have P[∅]=1−P[Ω]=0. (cid:3) Corollary 2.2 (Unions of Two Non-Disjoint Sets). For any A and B in F, P[A∪B]=P[A]+P[B]−P[A∩B]. (2.25) This statement is different from Axiom III because A and B are not necessarily disjoint. Figure 2.25: For any A and B, P[A∪B]=P[A]+P[B]−P[A∩B]. Proof. First, observe that A∪B can be partitioned into three disjoint subsets as A∪B = (A\B)∪(A∩B)∪(B\A). Since A\B =A∩Bc and B\A=B∩Ac, by finite additivity we have that P[A∪B]=P[A\B]+P[A∩B]+P[B\A]=P[A∩Bc]+P[A∩B]+P[B∩Ac] ( =a)P[A∩Bc]+P[A∩B]+P[B∩Ac]+P[A∩B]−P[A∩B] ( =b)P[A∩(Bc∪B)]+P[(Ac∪A)∩B]−P[A∩B] =P[A∩Ω]+P[Ω∩B]−P[A∩B]=P[A]+P[B]−P[A∩B], where in (a) we added and subtracted a term P[A∩B], and in (b) we used finite additivity so that P[A∩Bc]+P[A∩B]=P[(A∩Bc)∪(A∩B)]=P[A∩(Bc∪B)]. (cid:3) Example 2.42. The corollary is easy to understand if we consider the following ex- ample. Let Ω={1,2,3,4,5,6} be the sample space of a fair die. Let A={1,2,3} and B ={3,4,5}. Then 5 P[A∪B]=P[{1,2,3,4,5}]= . 6 78
2.3. AXIOMS OF PROBABILITY We can also use the corollary to obtain the same result: P[A∪B]=P[A]+P[B]−P[A∩B] =P[{1,2,3}]+P[{3,4,5}]−P[{3}] 3 3 1 5 = + − = . 6 6 6 6 Corollary 2.3 (Inequalities). Let A and B be two events in F. Then, (a) P[A∪B]≤P[A]+P[B]. (Union Bound) (b) If A⊆B, then P[A]≤P[B]. Proof.(a)SinceP[A∪B]=P[A]+P[B]−P[A∩B]andbynon-negativityaxiomP[A∩B]≥0, we must have P[A∪B]≤P[A]+P[B]. (b) If A⊆B, then there exists a set B\A such that B =A∪(B\A).Therefore,byfiniteadditivitywehaveP[B]=P[A]+P[B\A]≥P[A].Since P[B\A]≥0, it follows that P[A]+P[B\A]≥P[A]. Thus we have P[B]≥P[A]. (cid:3) Unionboundisafrequentlyusedtoolforanalyzingprobabilitieswhentheintersection A∩B is difficult to evaluate. Part (b) is useful when considering two events of different “sizes.” For example, in the bus-waiting example, if we let A={t≤5}, and B ={t≤10}, then P[A] ≤ P[B] because we have to wait for the first 5 minutes to go into the remaining 5 minutes. Practice Exercise 2.13. Let the events A and B have P[A] = x, P[B] = y and P[A∪B]=z. Find the following probabilities: P[A∩B], P[Ac∪Bc], and P[A∩Bc]. Solution. (a) Note that z =P[A∪B]=P[A]+P[B]−P[A∩B]. Thus, P[A∩B]=x+y−z. (b) We can take the complement to obtain the result: P[Ac∪Bc]=1−P[(Ac∪Bc)c]=1−P[A∩B]=1−x−y+z. (c) P[A∩Bc]=P[A]−P[A∩B]=x−(x+y−z)=z−y. Practice Exercise 2.14. Consider a sample space Ω={f :R→R|f(x)=ax,for all a∈R,x∈R}. There are two events: A = {f|f(x) = ax, a ≥ 0}, and B = {f|f(x) = ax, a ≤ 0}. So, basically, A is the set of all straight lines with positive slope, and B is the set of straight lines with negative slope. Show that the union bound is tight. 79
CHAPTER 2. PROBABILITY Solution. First of all, we note that P[A∪B]=P[A]+P[B]−P[A∩B]. The intersection is P[A∩B]=P[{f|f(x)=0}]. Since this is a point set in the real line, it has measure zero. Thus, P[A∩B]=0 and hence P[A∪B]=P[A]+P[B]. So the union bound is tight. Closing remark. The development of today’s probability theory is generally credited to Andrey Kolmogorov’s 1933 book Foundations of the Theory of Probability. We close this section by citing one of the tables of the book. The table summarizes the correspondence between set theory and random events. Theory of sets Random events A and B are disjoint, i.e., A∩B =∅ Events A and B are incompatible A ∩A ···∩A =∅ Events A ,...,A are incompatible 1 2 N 1 N A ∩A ···∩A =X EventX isdefinedasthesimultaneousoccur- 1 2 N rence of events A ,...,A 1 N A ∪A ···∪A =X EventX isdefinedastheoccurrenceofatleast 1 2 N one of the events A ,...,A 1 N Ac The opposite event Ac consisting of the non- occurrence of event A A=∅ Event A is impossible A=Ω Event A must occur A ,...,A form a partition of Ω Theexperimentconsistsofdeterminingwhich 1 N of the events A ,...,A occurs 1 N B ⊂A From the occurrence of event B follows the inevitable occurrence of A Table 2.2: Kolmogorov’s summary of set theory results and random events. 2.4 Conditional Probability In many practical data science problems, we are interested in the relationship between two or more events. For example, an event A may cause B to happen, and B may cause C to happen. A legitimate question in probability is then: If A has happened, what is the probability that B also happens? Of course, if A and B are correlated events, then knowing oneeventcantellussomethingabouttheotherevent.Ifthetwoeventshavenorelationship, knowing one event will not tell us anything about the other. In this section, we study the concept of conditional probability. There are three sub- topics in this section. We summarize the key points below. 80
2.4. CONDITIONAL PROBABILITY The three main messages of this section are: • Section 2.4.1: Conditional probability. Conditional probability of A given B is P[A|B]= P[A∩B]. P[B] • Section 2.4.2: Independence. Two events are independent if the occurrence of one does not influence the occurrence of the other: P[A|B]=P[A]. • Section 2.4.3: Bayes’ theorem and the law of total probability. Bayes’ theorem allowsustoswitchtheorderoftheconditioning:P[A|B]vs.P[B|A],whereasthe law of total probability allows us to decompose an event into smaller events. 2.4.1 Definition of conditional probability We start by defining conditional probability. Definition 2.22. Consider two events A and B. Assume P[B](cid:54)=0. The conditional probability of A given B is P[A∩B] P[A|B]d =ef . (2.26) P[B] According to this definition, the conditional probability of A given B is the ratio of P[A∩B] to P[B]. It is the probability that A happens when we know that B has already happened.SinceB hasalreadyhappened,theeventthatAhasalsohappenedisrepresented byA∩B.However,sinceweareonlyinterestedintherelativeprobabilityofAwithrespect to B, we need to normalize using B. This can be seen by comparing P[A|B] and P[A∩B]: P[A∩B] P[A∩B] P[A|B]= and P[A∩B]= . (2.27) P[B] P[Ω] ThedifferenceisillustratedinFigure 2.26:TheintersectionP[A∩B]calculatestheoverlap- ping area of the two events. We make no assumptions about the cause-effect relationship. Figure 2.26: Illustration of conditional probability and its comparison with P[A∩B]. What justifies this ratio? Suppose that B has already happened. Then, anything out- side B will immediately become irrelevant as far as the relationship between A and B is concerned. So when we ask: “What is the probability that A happens given that B has happened?”, we are effectively asking for the probability that A∩B happens under the 81
CHAPTER 2. PROBABILITY condition that B has happened. Note that we need to consider A∩B because we know that B has already happened. If we take A only, then there exists a region A\B which does not contain anything about B. However, since we know that B has happened, A\B is impossible. In other words, among the elements of A, only those that appear in A∩B are meaningful. Example 2.43. Let A={Purdue gets Big Ten championship}, B ={Purdue wins 15 games consecutively}. In this example, P[A]=Prob. that Purdue gets the championship, P[B]=Prob. that Purdue wins 15 games consecutively, P[A∩B]=Prob. that Purdue gets the championship and wins 15 games, P[A|B]=Prob. that Purdue gets the championship given that Purdue won 15 games. If Purdue has won 15 games consecutively, then it is unlikely that Purdue will get thechampionshipbecausethesamplespaceofallpossiblecompetitionresultsislarge. However, if we have already won 15 games consecutively, then the denominator of the probability becomes much smaller. In this case, the conditional probability is high. Example 2.44. Consider throwing a die. Let A={getting a 3} and B ={getting an odd number}. Find P[A|B] and P[B|A]. Solution. The following probabilities are easy to calculate: 1 3 P[A]=P[{3}]= , and P[B]=P[{1,3,5}]= . 6 6 Also, the intersection is 1 P[A∩B]=P[{3}]= . 6 Given these values, the conditional probability of A given B can be calculated as P[A∩B] 1 1 P[A|B]= = 6 = . P[B] 3 3 6 In other words, if we know that we have an odd number, then the probability of obtaining a 3 has to be computed over {1,3,5}, which give us a probability 1. If we 3 82
2.4. CONDITIONAL PROBABILITY do not know that we have an odd number, then the probability of obtaining a 3 has to be computed from the sample space {1,2,3,4,5,6}, which will give us 1. 6 The other conditional probability is P[A∩B] P[B|A]= =1. P[A] Therefore, if we know that we have rolled a 3, then the probability for this number being an odd number is 1. Example 2.45. Consider the situation shown in Figure 2.27. There are 12 points with equal probabilities of happening. Find the probabilities P[A|B] and P[B|A]. Solution. In this example, we can first calculate the individual probabilities: 5 6 2 P[A]= , and P[B]= , and P[A∩B]= . 12 12 12 Then the conditional probabilities are P[A∩B] 2 1 P[A|B]= = 12 = , P[B] 6 3 12 P[A∩B] 2 2 P[B|A]= = 12 = . P[A] 5 5 12 Figure 2.27: Visualization of Example 2.45: [Left] All the sets. [Middle] P(A|B) is the ratio between dots inside the light yellow region over those in yellow, which is 2. [Right] P[A|B] is the ratio between 6 dots inside the light pink region over those in pink, which is 2. 5 Example 2.46. Consider a tetrahedral (4-sided) die. Let X be the first roll and Y be the second roll. Let B be the event that min(X,Y) = 2 and M be the event that max(X,Y)=3. Find P[M|B]. Solution. As shown in Figure 2.28, the event B is highlighted in green. (Why?) Similarly,theeventM ishighlightedinblue.(Again,why?)Therefore,theprobability 83
CHAPTER 2. PROBABILITY is P[M ∩B] 2 2 P[M|B]= = 16 = . P[B] 5 5 16 Figure 2.28: Visualization of Example 2.46. [Left] Event B. [Middle] Event M. [Right] P(M|B) is the ratioofthenumberofbluesquaresinsidethegreenregiontothetotalnumberofgreensquares,which is 2. 5 Remark.NoticethatifP[B]≤P[Ω],thenP[A|B]isalwayslargerthanorequaltoP[A∩B], i.e., P[A|B]≥P[A∩B]. Conditional probabilities are legitimate probabilities Conditional probabilities are legitimate probabilities. That is, given B, the probability P[A|B] satisfies Axioms I, II, III. Theorem 2.6. Let P[B] > 0. The conditional probability P[A|B] satisfies Axioms I, II, and III. Proof. Let’s check the axioms: • Axiom I: We want to show P[A∩B] P[A|B]= ≥0. P[B] Since P[B]>0 and Axiom I requires P[A∩B]≥0, we therefore have P[A|B]≥0. • Axiom II: P[Ω∩B] P[Ω|B]= P[B] P[B] = =1. P[B] 84
2.4. CONDITIONAL PROBABILITY • Axiom III: Consider two disjoint sets A and C. Then, P[(A∪C)∩B] P[A∪C|B]= P[B] P[(A∩B)∪(C∩B)] = P[B] (a) P[A∩B] P[C∩B] = + P[B] P[B] =P[A|B]+P[C|B], where (a) holds because if A and C are disjoint then (A∩B)∩(C∩B)=∅. (cid:3) To summarize this subsection, we highlight the essence of conditional probability. What are conditional probabilities? • Conditional probability of A given B is the ratio P[A∩B]. P[B] • It is again a measure. It measures the relative size of A inside B. • Because it is a measure, it must satisfy the three axioms. 2.4.2 Independence Conditional probability deals with situations where two events A and B are related. What if the two events are unrelated? In probability, we have a technical term for this situation: statistical independence. Definition 2.23. Two events A and B are statistically independent if P[A∩B]=P[A]P[B]. (2.28) Why define independence in this way? Recall that P[A|B]= P[A∩B]. If A and B are P[B] independent, then P[A∩B]=P[A]P[B] and so P[A∩B] P[A]P[B] P[A|B]= = =P[A]. (2.29) P[B] P[B] This suggests an interpretation of independence: If the occurrence of B provides no addi- tional information about the occurrence of A, then A and B are independent. Therefore, we can define independence via conditional probability: Definition 2.24. Let A and B be two events such that P[A]>0 and P[B]>0. Then 85
CHAPTER 2. PROBABILITY A and B are independent if P[A|B]=P[A] or P[B|A]=P[B]. (2.30) The two statements are equivalent as long as P[A] > 0 and P[B] > 0. This is because P[A|B] = P[A∩B]/P[B]. If P[A|B] = P[A] then P[A∩B] = P[A]P[B], which implies that P[B|A]=P[A∩B]/P[A]=P[B]. ApictorialillustrationofindependenceisgiveninFigure2.29.Thekeymessageisthat if two events A and B are independent, then P[A|B] = P[A]. The conditional probability P[A|B] is the ratio of P[A∩B] over P[B], which is the intersection over B (the blue set). The probability P[A] is the yellow set over the sample space Ω. Figure 2.29: Independence means that the conditional probability P[A|B] is the same as P[A]. This implies that the ratio of P[A∩B] over P[B], and the ratio of P[A∩Ω] over P[Ω] are the same. Disjoint versus independent Disjoint(cid:60)Independent. (2.31) The statement says that disjoint and independent are two completely different concepts. If A and B are disjoint, then A ∩ B = ∅. This only implies that P[A ∩ B] = 0. However, it says nothing about whether P[A∩B] can be factorized into P[A]P[B]. If A and B are independent, then we have P[A∩B] = P[A]P[B]. But this does not imply that P[A∩B]=0.TheonlyconditionunderwhichDisjoint⇔IndependenceiswhenP[A]=0or P[B]=0.Figure 2.30depictsthesituation.Whentwosetsareindependent,theconditional probability (which is a ratio) remains unchanged compared to unconditioned probability. When two sets are disjoint, they simply do not overlap. Practice Exercise 2.15. Throw a die twice. Are A and B independent, where A={1st die is 3} and B ={2nd die is 4}. Solution. We can show that P[A∩B]=P[(3,4)]= 1 , P[A]= 1, and P[B]= 1. 36 6 6 So P[A∩B]=P[A]P[B]. Thus, A and B are independent. 86
2.4. CONDITIONAL PROBABILITY Figure 2.30: Independent means that the conditional probability, which is a ratio, is the same as the unconditioned probability. Disjoint means that the two sets do not overlap. Figure 2.31: The two events A and B are independent because P[A]= 1 and P[A|B]= 1. 6 6 A pictorial illustration of this example is shown in Figure 2.31. The two events are independent because A is one row in the 2D space, which yields a probability of 1. The 6 conditional probability P[A|B] is the coordinate (3,4) over the event B, which is a column. It happens that P[A|B]= 1. Thus, the two events are independent. 6 Practice Exercise 2.16. Throw a die twice. Are A and B independent? A={1st die is 3} and B ={sum is 7}. Solution. Note that P[A∩B]=P[(3,4)]= 1 , P[A]= 1, 36 6 P[B]=P[(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)]= 1. 6 So P[A∩B]=P[A]P[B]. Thus, A and B are independent. ApictorialillustrationofthisexampleisshowninFigure2.32.Noticethatwhetherthe twoeventsintersectisnothowwedetermineindependence(thatonlydeterminesdisjointor 87
CHAPTER 2. PROBABILITY not).Thekeyiswhethertheconditionalprobability(whichistheratio)remainsunchanged compared to the unconditioned probability. Figure 2.32: The two events A and B are independent because P[A]= 1 and P[A∩B]= 1. 6 6 If we let B ={sum is 8}, then the situation is different. The intersection A∩B has a probability 1 relative to B, and therefore P[A|B] = 1. Hence, the two events A and B are 5 5 dependent. If you like a more intuitive argument, you can imagine that B has happened, i.e., the sum is 8. Then the probability for the first die to be 1 is 0 because there is no way toconstruct8whenthefirstdieis1.Asaresult,wehaveeliminatedonechoiceforthefirst die, leaving only five options. Therefore, since B has influenced the probability of A, they are dependent. Practice Exercise 2.17. Throw a die twice. Let A={max is 2} and B ={min is 2}. Are A and B independent? Solution. Let us first list out A and B: A={(1,2),(2,1),(2,2)}, B ={(2,2),(2,3),(2,4),(2,5),(2,6),(3,2),(4,2),(5,2),(6,2)}. Therefore, the probabilities are 3 9 1 P[A]= , P[B]= , and P[A∩B]=P[(2,2)]= . 36 36 36 Clearly, P[A∩B](cid:54)=P[A]P[B] and so A and B are dependent. What is independence? • Two events are independent when the ratio P[A∩B]/P[B] remains unchanged compared to P[A]. • Independence (cid:54)= disjoint. 88
2.4. CONDITIONAL PROBABILITY 2.4.3 Bayes’ theorem and the law of total probability Theorem 2.7 (Bayes’ theorem). For any two events A and B such that P[A] > 0 and P[B]>0, P[B|A]P[A] P[A|B]= . P[B] Proof. By the definition of conditional probabilities, we have P[A∩B] P[B∩A] P[A|B]= and P[B|A]= . P[B] P[A] Rearranging the terms yields P[A|B]P[B]=P[B|A]P[A], which gives the desired result by dividing both sides by P[B]. (cid:3) Bayes’theoremprovidestwoviewsoftheintersectionP[A∩B]usingtwodifferentcon- ditionalprobabilities.WecallP[B|A]theconditional probabilityandP[A|B]theposterior probability. The order of A and B is arbitrary. We can also call P[A|B] the conditional probabilityandP[B|A]theposteriorprobability.Thecontextoftheproblemwillmakethis clear. Bayes’ theorem provides a way to switch P[A|B] and P[B|A]. The next theorem helps us decompose an event into smaller events. Theorem 2.8 (Law of Total Probability). Let {A ,...,A } be a partition of Ω, i.e., 1 n A ,...,A are disjoint and Ω=A ∪···∪A . Then, for any B ⊆Ω, 1 n 1 n n (cid:88) P[B]= P[B|A ]P[A ]. (2.32) i i i=1 Proof. We start from the right-hand side. n n (cid:34) n (cid:35) (cid:88) P[B|A ]P[A ]( =a)(cid:88) P[B∩A ]( =b)P (cid:91) (B∩A ) i i i i i=1 i=1 i=1 (cid:34) (cid:32) n (cid:33)(cid:35) ( =c)P B∩ (cid:91) A ( =d)P[B∩Ω]=P[B], i i=1 where (a) follows from the definition of conditional probability, (b) is due to Axiom III, (c) holdsbecauseofthedistributivepropertyofsets,and(d)resultsfromthepartitionproperty of {A ,A ,...,A }. 1 2 n (cid:3) Interpretation.Thelawoftotalprobabilitycanbeunderstoodasfollows.Ifthesample space Ω consists of disjoint subsets A ,...,A , we can compute the probability P[B] by 1 n 89
CHAPTER 2. PROBABILITY summingoveritsportionP[B∩A ],...,P[B∩A ].However,eachintersectioncanbewritten 1 n as P[B∩A ]=P[B|A ]P[A ]. (2.33) i i i In other words, we write P[B∩A ] as the conditional probability P[B|A ] times the prior i i probability P[A ]. When we sum all these intersections, we obtain the overall probability. i See Figure 2.33 for a graphical portrayal. Figure 2.33: The law of total probability decomposes the probability P[B] into multiple conditional probabilities P[B|A ]. The probability of obtaining each P[B|A ] is P[A ]. i i i Corollary 2.4. Let {A ,A ,...,A } be a partition of Ω, i.e., A ,...,A are disjoint 1 2 n 1 n and Ω=A ∪A ∪···∪A . Then, for any B ⊆Ω, 1 2 n P[B|A ]P[A ] P[A |B]= j j . (2.34) j (cid:80)n P[B|A ]P[A ] i=1 i i Proof. The result follows directly from Bayes’ theorem: P[B|A ]P[A ] P[B|A ]P[A ] P[A |B]= j j = j j . j P[B] (cid:80)n P[B|A ]P[A ] i=1 i i (cid:3) Example 2.47. Suppose there are three types of players in a tennis tournament: A, B, and C. Fifty percent of the contestants in the tournament are A players, 25% are B players, and 25% are C players. Your chance of beating the contestants depends on the class of the player, as follows: 0.3 against an A player 0.4 against a B player 0.5 against a C player If you play a match in this tournament, what is the probability of your winning the match?Supposingthatyouhavewonamatch,whatistheprobabilitythatyouplayed against an A player? Solution.Wefirstlistalltheknownprobabilities.Weknowfromthepercentage 90
2.4. CONDITIONAL PROBABILITY of players that P[A]=0.5, P[B]=0.25, P[C]=0.25. Now, let W be the event that you win the match. Then the conditional probabilities are defined as follows: P[W|A]=0.3, P[W|B]=0.4, P[W|C]=0.5. Therefore, by the law of total probability, we can show that the probability of winning the match is P[W]=P[W |A]P[A]+P[W |B]P[B]+P[W |C]P[C] =(0.3)(0.5)+(0.4)(0.25)+(0.5)(0.25)=0.375. Given that you have won the match, the probability of A given W is P[W|A]P[A] (0.3)(0.5) P[A|W]= = =0.4. P[W] 0.375 Example 2.48. Consider the communication channel shown below. The probability of sending a 1 is p and the probability of sending a 0 is 1−p. Given that 1 is sent, the probability of receiving 1 is 1−η. Given that 0 is sent, the probability of receiving 0 is 1−ε. Find the probability that a 1 has been correctly received. Solution. Define the events S =“0 is sent”, and R =“0 is received”. 0 0 S =“1 is sent”, and R =“1 is received”. 1 1 Then,theprobabilitythat1isreceivedisP[R ].However,P[R ](cid:54)=1−η because1−η 1 1 91
CHAPTER 2. PROBABILITY is the conditional probability that 1 is received given that 1 is sent. It is possible that we receive 1 as a result of an error when 0 is sent. Therefore, we need to consider the probability that both S and S occur. Using the law of total probability we have 0 1 P[R ]=P[R |S ]P[S ]+P[R |S ]P[S ] 1 1 1 1 1 0 0 =(1−η)p+ε(1−p). Now, suppose that we have received 1. What is the probability that 1 was origi- nally sent? This is asking for the posterior probability P[S |R ], which can be found 1 1 using Bayes’ theorem P[R |S ]P[S ] (1−η)p P[S |R ]= 1 1 1 = . 1 1 P[R ] (1−η)p+ε(1−p) 1 When do we need to use Bayes’ theorem and the law of total probability? • Bayes’ theorem switches the role of the conditioning, from P[A|B] to P[B|A]. Example: P[win the game|play with A] and P[play with A|win the game]. • The law of total probability decomposes an event into smaller events. Example: P[win]=P[win|A]P[A]+P[win|B]P[B]. 2.4.4 The Three Prisoners problem Now that you are familiar with the concepts of conditional probabilities, we would like to challenge you with the following problem, known as the Three Prisoners problem. If you understand how this problem can be resolved, you have mastered conditional probability. Once upon a time, there were three prisoners A, B, and C. One day, the king decided to pardon two of them and sentence the last one, as in this figure: Figure2.34:TheThreePrisonersproblem:Thekingsaysthathewillpardontwoprisonersandsentence one. One of the prisoners, prisoner A, heard the news and wanted to ask a friendly guard abouthissituation.Theguardwashonest.HewasallowedtotellprisonerAthatprisonerB would be pardoned or that prisoner C would be pardoned, but he could not tell A whether he would be pardoned. Prisoner A thought about the problem, and he began to hesitate to ask the guard. Based on his present state of knowledge, his probability of being pardoned 92
2.4. CONDITIONAL PROBABILITY is 2. However, if he asks the guard, this probability will be reduced to 1 because the guard 3 2 would tell him that one of the two other prisoners would be pardoned, and would tell him which one it would be. Prisoner A reasons that his chance of being pardoned would then drop because there are now only two prisoners left who may be pardoned, as illustrated in Figure 2.35: Figure 2.35: The Three Prisoners problem: If you do not ask the guard, your chance of being released is 2/3. If you ask the guard, the guard will tell you which one of the other prisoners will be released. Your chance of being released apparently drops to 1/2. Should prisoner A ask the guard? What has gone wrong with his reasoning? This problem is tricky in the sense that the verbal argument of prisoner A seems flawless. If he asked the guard, indeed, the game would be reduced to two people. However, this does not seem correct, because regardless of what the guard says, the probability for A to be pardoned should remain unchanged. Let’s see how we can solve this puzzle. Let X , X , X be the events of sentencing prisoners A, B, C, respectively. Let G A B C B be the event that the guard says that the prisoner B is released. Without doing anything, we know that 1 1 1 P[X ]= , P[X ]= , P[X ]= . A 3 B 3 C 3 Conditioned on these events, we can compute the following conditional probabilities that the guard says B is pardoned: 1 P[G |X ]= , P[G |X ]=0, P[G |X ]=1. B A 2 B B B C Why are these conditional probabilities? P[G |X ]=0 quite straightforward. If the king B B decides to sentence B, the guard has no way of saying that B will be pardoned. Therefore, P[G | X ] must be zero. P[G | X ] = 1 is also not difficult. If the king decides to B B B C sentence C, then the guard has no way to tell you that B will be pardoned because the guard cannot say anything about prisoner A. Finally, P[G | X ] = 1 can be understood B A 2 as follows: If the king decides to sentence A, the guard can either tell you B or C. In other words, the guard flips a coin. Withtheseconditionalprobabilitiesready,wecandeterminetheprobability.Thisisthe conditional probability P[X |G ]. That is, supposing that the guard says B is pardoned, A B whatistheprobabilitythatAwillbesentenced?ThisistheactualscenariothatAisfacing. Solving for this conditional probability is not difficult. By Bayes’ theorem we know that P[G |X ]P[X ] P[X |G ]= B A A , A B P[G ] B 93
CHAPTER 2. PROBABILITY and P[G ]=P[G |X ]P[X ]+P[G |X ]P[X ]+P[G |X ]P[X ] according to the law of B B A A B B B B C C total probability. Substituting the numbers into these equations, we have that P[G ]=P[G |X ]P[X ]+P[G |X ]P[X ]+P[G |X ]P[X ] B B A A B B B B C C 1 1 1 1 1 = × +0× +1× = , 2 3 3 3 2 P[G |X ]P[X ] 1 × 1 1 P[X |G ]= B A A = 2 3 = . A B P[G ] 1 3 B 2 Therefore,giventhattheguardsaysB ispardoned,theprobabilitythatAwillbesentenced remains 1. In fact, what you can show in this example is that P[X | G ] = 1 = P[X ]. 3 A B 3 A Therefore,thepresenceorabsenceoftheguarddoesnotaltertheprobability.Thisisbecause what the guard says is independent of whether the prisoners will be pardoned. The lesson we learn from this problem is not to rely on verbal arguments. We need to write down the conditional probabilities and spell out the steps. Figure 2.36: The Three Prisoners problem is resolved by noting that P[X |G ] = P[X ]. Therefore, A B A the events X and G are independent. A B How to resolve the Three Prisoners problem? • The key is that G , G , G do not form a partition. See Figure 2.36. A B C • G (cid:54)=X . When G happens, the remaining set is not X ∪X . B B B A C • The ratio P[X ∩G ]/P[G ] equals P[X ]. This is independence. A B B A 94
2.5. SUMMARY 2.5 Summary By now, we hope that you have become familiar with our slogan probability is a measure of the size of a set. Let us summarize: • Probability = a probability law P. You can also view it as the value returned by P. • Measure=aruler,ascale,astopwatch,oranothermeasuringdevice.Itisatoolthat tells you how large or small a set is. The measure has to be compatible with the set. If a set is finite, then the measure can be a counter. If a set is a continuous interval, then the measure can be the length of the interval. • Size = the relative weight of the set for the sample space. Measuring the size is done by using a weighting function. Think of a fair coin versus a biased coin. The former has a uniform weight, whereas the latter has a nonuniform weight. • Set = an event. An event is a subset in the sample space. A probability law P always maps a set to a number. This is different from a typical function that maps a number to another number. If you understand what this slogan means, you will understand why probability can be appliedtodiscreteevents,continuousevents,eventsinn-Dspaces,etc.Youwillalsounder- stand the notion of measure zero and the notion of almost sure. These concepts lie at the foundation of modern data science, in particular, theoretical machine learning. The second half of this chapter discusses the concept of conditional probability. Con- ditional probability is a metaconcept that can be applied to any measure you use. The motivation of conditional probability is to restrict the probability to a subevent happening inthesamplespace.IfBhashappened,theprobabilityforAtoalsohappenisP[A∩B]/P[B]. If two events are not influencing each other, then we say that A and B are independent. Accordingto Bayes’theorem, wecanalso switchthe order ofA given B and B given A,ac- cordingtoBayes’theorem.Finally,thelawoftotalprobabilitygivesusawaytodecompose events into subevents. We end this chapter by mentioning a few terms related to conditional probabilities that will become useful later. Let us use the tennis tournament as an example: • P[W |A] = conditional probability = Given that you played with player A, what is the probability that you will win? • P[A] = prior probability = Without even entering the game, what is the chance that you will face player A? • P[A|W] = posterior probability = After you have won the game, what is the proba- bility that you have actually played with A? In many practical engineering problems, the question of interest is often the last one. That is,supposingthatyouhaveobservedsomething,whatisthemostlikelycauseofthatevent? Forexample,supposingwehaveobservedthisparticulardataset,whatisthebestGaussian model that would fit the dataset? Questions like these require some analysis of conditional probability, prior probability, and posterior probability. 95
CHAPTER 2. PROBABILITY 2.6 References Introduction to Probability 2-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 1. 2-2 Mark D. Ward and Ellen Gundlach, Introduction to Probability, W.H. Freeman and Company, 2016. Chapter 1 – Chapter 6. 2-3 Roy D. Yates and David J. Goodman, Probability and Stochastic Processes, 3rd Edi- tion, Wiley 2013, Chapter 1. 2-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 2. 2-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 2 and Chapter 3. 2-6 Ani Adhikari and Jim Pitman, Probability for Data Science, http://prob140.org/ textbook/content/README.html. Chapters 1 and 2. 2-7 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 2.1 – 2.7. 2-8 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 2. 2-9 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 1. Measure-Theoretic Probability 2-10 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 2.8 and 2.9. 2-11 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Appendix D. 2-12 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 2-13 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) 2-14 Patrick Billingsley, Probability and Measure, Wiley, 3rd Edition, 1995. Real Analysis 2-15 Tom M. Apostol, Mathematical Analysis, Pearson, 1974. 2-16 Walter Rudin, Principles of Mathematical Analysis, McGraw Hill, 1976. 96
2.7. PROBLEMS 2.7 Problems Exercise 1. A space S and three of its subsets are given by S = {1,3,5,7,9,11}, A = {1,3,5}, B = {7,9,11}, and C ={1,3,9,11}. Find A∩B∩C, Ac∩B, A−C, and (A−B)∪B. Exercise 2. Let A = (−∞,r] and B = (−∞,s] where r ≤ s. Find an expression for C = (r,s] in terms of A and B. Show that B =A∪C, and A∩C =∅. Exercise 3. (Video Solution) Simplify the following sets. (a) [1,4]∩([0,2]∪[3,5]) (b) ([0,1]∪[2,3])c (c) (cid:84)∞ (−1/n,+1/n) i=1 (d) (cid:83)∞ [5,8−(2n)−1] i=1 Exercise 4. We will sometimes deal with the relationship between two sets. We say that A implies B when A is a subset of B (why?). Show the following results. (a) Show that if A implies B, and B implies C, then A implies C. (b) Show that if A implies B, then Bc implies Ac. Exercise 5. Show that if A∪B =A and A∩B =A, then A=B. Exercise 6. A space S is defined as S = {1,3,5,7,9,22}, and three subsets as A = {1,3,5}, B = {7,9,11},C ={1,3,9,11}.Assumethateachelementhasprobability1/6.Findthefollowing probabilities: (a) P[A] (b) P[B] (c) P[C] (d) P[A∪B] (e) P[A∪C] (f) P[(A\C)∪B] 97
CHAPTER 2. PROBABILITY Exercise 7. (Video Solution) A collection of 26 letters, a-z, is mixed in a jar. Two letters are drawn at random, one after the other. What is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either order? What is the sample space? Exercise 8. Consider an experiment consisting of rolling a die twice. The outcome of this experiment is an ordered pair whose first element is the first value rolled and whose second element is the second value rolled. (a) Find the sample space. (b) Find the set A representing the event that the value on the first roll is greater than or equal to the value on the second roll. (c) Find the set B corresponding to the event that the first roll is a six. (d) Let C correspond to the event that the first valued rolled and the second value rolled differ by two. Find A∩C. Note that A, B, and C should be subsets of the sample space specified in Part (a). Exercise 9. A pair of dice are rolled. (a) Find the sample space Ω (b) Find the probabilities of the events: (i) the sum is even, (ii) the first roll is equal to the second, (iii) the first roll is larger than the second. Exercise 10. Let A, B and C be events in an event space. Find expressions for the following: (a) Exactly one of the three events occurs. (b) Exactly two of the events occurs. (c) Two or more of the events occur. (d) None of the events occur. Exercise 11. Asystemiscomposedoffivecomponents,eachofwhichiseitherworkingorfailed.Consider anexperimentthatconsistsofobservingthestatusofeachcomponent,andlettheoutcomes of the experiment be given by all vectors (x ,x ,x ,x ,x ), where x is 1 if component i is 1 2 3 4 5 i working and 0 if component i is not working. (a) How many outcomes are in the sample space of this experiment? (b) Suppose that the system will work if components 1 and 2 are both working, or if components 3 and 4 are both working, or if components 1, 3, and 5 are all working. Let W be the event that the system will work. Specify all of the outcomes in W. 98
2.7. PROBLEMS (c) Let A be the event that components 4 and 5 have both failed. How many outcomes are in the event A? (d) Write out all outcomes in the event A∩W. Exercise 12. (Video Solution) A number x is selected at random in the interval [−1,2]. Let the events A = {x|x < 0}, B = {x||x−0.5| < 0.5}, C = {x|x > 0.75}. Find (a) P[A|B], (b) P[B|C], (c) P[A|Cc], (d) P[B|Cc]. Exercise 13. (Video Solution) Let the events A and B have P[A] = x, P[B] = y and P[A∪B] = z. Find the following probabilities: (a) P[A∩B], (b) P[Ac∩Bc], (c) P[Ac∪Bc], (d) P[A∩Bc], (e) P[Ac∪B]. Exercise 14. (a) ByusingthefactthatP[A∪B]≤P[A]+P[B],showthatP[A∪B∪C]≤P[A]+P[B]+ P[C]. (b) By using the fact that P[(cid:83)n A ]≤(cid:80)n P[A ], show that k=1 k k=1 k (cid:34) n (cid:35) n (cid:92) (cid:88) P A ≥1− P[Ac]. k k k=1 k=1 Exercise 15. Usethedistributivepropertyofsetoperationstoprovethefollowinggeneralizeddistributive law: (cid:32) n (cid:33) n (cid:92) (cid:92) A∪ B = (A∪B ). i i i=1 i=1 Hint: Use mathematical induction. That is, show that the above is true for n=2 and that it is also true for n=k+1 when it is true for n=k. Exercise 16. The following result is known as the Bonferroni’s Inequality. (a) Prove that for any two events A and B, we have P(A∩B)≥P(A)+P(B)−1. (b) Generalize the above to the case of n events A ,A ,...,A , by showing that 1 2 n P(A ∩A ∩···∩A )≥P(A )+P(A )+···+P(A )−(n−1). 1 2 n 1 2 n Hint: You may use the generalized Union Bound P((cid:83)n A )≤(cid:80)n P(A ). i=1 i i=1 i Exercise 17. (Video Solution) Let A, B, C be events with probabilities P[A]=0.5, P[B]=0.2, P[C]=0.4. Find 99
CHAPTER 2. PROBABILITY (a) P[A∪B] if A and B are independent. (b) P[A∪B] if A and B are disjoint. (c) P[A∪B∪C] if A, B and C are independent. (d) P[A∪B∪C] if A, B and C are pairwise disjoint; can this happen? Exercise 18. (Video Solution) Ablockofinformationistransmittedrepeatedoveranoisychanneluntilanerror-freeblock is received. Let M ≥ 1 be the number of blocks required for a transmission. Define the following sets. (i) A = {M is even} (ii) B = {M is a multiple of 3} (iii) C = {M is less than or equal to 6} Assume that the probability of requiring one additional block is half of the probability without the additional block. That is: (cid:18) 1(cid:19)k P[M =k]= , k =1,2,.... 2 Determine the following probabilities. (a) P[A], P[B], P[C], P[Cc] (b) P[A∩B], P[A\B], P[A∩B∩C] (c) P[A|B], P[B|A] (d) P[A|B∩C], P[A∩B|C] Exercise 19. (Video Solution) A binary communication system transmits a signal X that is either a +2-voltage signal or a −2-voltage signal. A malicious channel reduces the magnitude of the received signal by thenumberofheadsitcountsintwotossesofacoin.LetY betheresultingsignal.Possible values of Y are listed below. 2 Heads 1 Head No Head X =−2 Y =0 Y =−1 Y =−2 X =+2 Y =0 Y =+1 Y =+2 Assume that the probability of having X =+2 and X =−2 is equal. (a) Find the sample space of Y, and hence the probability of each value of Y. (b) What are the probabilities P[X =+2|Y =1] and P[Y =1|X =−2]? Exercise 20. (Video Solution) A block of 100 bits is transmitted over a binary communication channel with a probability of bit error p=10−2. 100
2.7. PROBLEMS (a) If the block has 1 or fewer errors, then the receiver accepts the block. Find the prob- ability that the block is accepted. (b) If the block has more than 1 error, then the block is retransmitted. What is the probability that 4 blocks are transmitted? Exercise 21. (Video Solution) A machine makes errors in a certain operation with probability p. There are two types of errors. The fraction of errors that are type A is α and the fraction that are type B is 1−α. (a) What is the probability of k errors in n operations? (b) What is the probability of k type A errors in n operations? 1 (c) What is the probability of k type B errors in n operations? 2 (d) Whatisthejointprobabilityofk typeAerrorsandk typeBerrorsinnoperations? 1 2 Hint:Thereare(cid:0)n(cid:1)(cid:0)n−k1(cid:1) possibilitiesofhavingk typeAerrorsandk typeBerrors k1 k2 1 2 in n operations. (Why?) Exercise 22. (Video Solution) A computer manufacturer uses chips from three sources. Chips from sources A, B and C are defective with probabilities 0.005, 0.001 and 0.01, respectively. The proportions of chips from A, B and C are 0.5, 0.1 and 0.4 respectively. If a randomly selected chip is found to be defective, find (a) the probability that the chips are from A. (b) the probability that the chips are from B. (c) the probability that the chips are from C. Exercise 23. (Video Solution) In a lot of 100 items, 50 items are defective. Suppose that m items are selected for testing. Wesaythatthemanufacturingprocessismalfunctioningiftheprobabilitythatoneormore itemsaretestedtobedefective.Callthisfailureprobabilityp.Whatshouldbetheminimum m such that p≥0.99? Exercise 24. (Video Solution) Oneoftwocoinsisselectedatrandomandtossedthreetimes.Thefirstcoincomesupheads with probability p =1/3 and the second coin with probability p =2/3. 1 2 (a) What is the probability that the number of heads is k =3? (b) Repeat (a) for k =0,1,2. (c) Find the probability that coin 1 was tossed given that k heads were observed, for k =0,1,2,3. (d) In part (c), which coin is more probably when 2 heads have been observed? 101
CHAPTER 2. PROBABILITY Exercise 25. (Video Solution) Considerthefollowingcommunicationchannel.Asourcetransmitsastringofbinarysymbols through a noisy communication channel. Each symbol is 0 or 1 with probability p and 1−p, respectively, and is received incorrectly with probability ε and ε . Errors in different 0 1 symbols transmissions are independent. Denote S as the source and R as the receiver. (a) What is the probability that a symbol is correctly received? Hint: Find P[R=1∩S =1] and P[R=0∩S =0]. (b) Find the probability of receiving 1011 conditioned on that 1011 was sent, i.e., P[R=1011|S =1011]. (c) To improve reliability, each symbol is transmitted three times, and the received string is decoded by the majority rule. In other words, a 0 (or 1) is transmitted as 000 (or 111, respectively), and it is decoded at the receiver as a 0 (or 1) if and only if the received three-symbol string contains at least two 0s (or 1s, respectively). What is the probability that the symbol is correctly decoded, given that we send a 0? (d) Suppose that the scheme of part (c) is used. What is the probability that a 0 was sent if the string 101 was received? (e) Suppose the scheme of part (c) is used and given that a 0 was sent. For what value of ε is there an improvement in the probability of correct decoding? Assume that 0 ε (cid:54)=0. 0 102
Chapter 3 Discrete Random Variables When working on a data analysis problem, one of the biggest challenges is the disparity between the theoretical tools we learn in school and the actual data our boss hands to us. By actual data, we mean a collection of numbers, perhaps organized or perhaps not. When we are given the dataset, the first thing we do would certainly not be to define the Borel σ-field and then define the measure. Instead, we would normally compute the mean, the standard deviation, and perhaps some scores about the skewness. ThesituationisbestexplainedbythelandscapeshowninFigure3.1.Ontheonehand, we have well-defined probability tools, but on the other hand, we have a set of practical “battle skills” for processing data. Often we view them as two separate entities. As long as we can pull the statistics from the dataset, why bother about the theory? Alternatively, we have a set of theories, but we will never verify them using the actual datasets. How can we bridge the two? What are the missing steps in the probability theory we have learned so far? The goal of this chapter (and the next) is to fill this gap. Figure 3.1: The landscape of probability and data. Often we view probability and data analysis as two different entities. However, probability and data analysis are inseparable. The goal of this chapter is to link the two. Three concepts to bridge the gap between theory and practice Thestartingpointofourdiscussionisaprobabilityspace(Ω,F,P).Itisanabstractconcept, butwehopewehaveconvincedyouinChapter2ofitssignificance.However,theprobability space is certainly not “user friendly” because no one would write a Python program to 103
CHAPTER 3. DISCRETE RANDOM VARIABLES implement those theories. How do we make the abstract probability space more convenient so that we can model practical scenarios? The first step is to recognize that the sample space and the event space are all based on statements, for example, “getting a head when flipping a coin” or “winning the game.” These statements are not numbers, but we (engineers) love numbers. Therefore, we should ask a very basic question: How do we convert a statement to a number? The answer is the concept of random variables. Key Concept 1: What are random variables? Random variables are mappings from events to numbers. Now,supposethatwehaveconstructedarandomvariablethattranslatesstatementsto numbers. The next task is to endow the random variable with probabilities. More precisely, weneedtoassignprobabilitiestotherandomvariablesothatwecanperformcomputations. This is done using the concept called probability mass function (PMF). Key Concept 2: What are probability mass functions (PMFs)? Probability mass functions are the ideal histograms of random variables. The best way to think about a PMF is a histogram, something we are familiar with. A histogram has two axes: The x-axis denotes the set of states and the y-axis denotes the probability. For each of the states that the random variable possesses, the histogram tells us the probability of getting a particular state. The PMF is the ideal histogram of a randomvariable.Itprovidesacompletecharacterizationoftherandomvariable.Ifyouhave a random variable, you must specify its PMF. Vice versa, if you tell us the PMF, you have specified a random variable. We ask the third question about pulling information from the probability mass func- tion, such as the mean and standard deviation. How do we obtain these numbers from the PMF? We are also interested in operations on the mean and standard deviations. For ex- ample, if a professor offers ten bonus points to the entire class, how will it affect the mean and standard deviation? If a store provides 20% off on all its products, what will happen to its mean retail price and standard deviation? However, the biggest question is perhaps the difference between the mean we obtain from a PMF and the mean we obtain from a his- togram. Understanding this difference will immediately help us build a bridge from theory to practice. Key Concept 3: What is expectation? Expectation = Mean = Average computed from a PMF. Organization of this chapter The plan for this chapter is as follows. We will start with the basic concepts of random variables in Section 3.1. We will formally define the random variables and discuss their relationship with the abstract probability space. Once this linkage is built, we can put 104
3.1. RANDOM VARIABLES the abstract probability space aside and focus on the random variables. In Section 3.2 we will define the probability mass function (PMF) of a random variable, which tells us the probability of obtaining a state of the random variable. PMF is closely related to the histogramofadataset.Wewillexplaintheconnection.InSection3.3wetakeasmalldetour to consider the cumulative distribution functions (CDF). Then, we discuss the mean and standarddeviationinSection3.4.Section3.5detailsafewcommonlyusedrandomvariables, including Bernoulli, binomial, geometric, and Poisson variables. 3.1 Random Variables 3.1.1 A motivating example Consider an experiment with 4 outcomes Ω = {♣,♦,♥,♠}. We want to construct the probabilityspace(Ω,F,P).ThesamplespaceΩisalreadydefined.TheeventspaceF isthe set of all possible subsets in Ω, which, in our case, is a set of 24 subsets. For the probability law P, let us assume that the probability of obtaining each outcome is 1 2 2 1 P[{♣}]= , P[{♦}]= , P[{♥}]= , P[{♠}]= . 6 6 6 6 Therefore, we have constructed a probability space (Ω,F,P) where everything is perfectly defined. So, in principle, they can live together happily forever. Alazydatascientistcomes,andthereisa(small)problem.Thedatascientistdoesnot want to write the symbols ♣,♦,♥,♠. There is nothing wrong with his motivation because all of us want efficiency. How can we help him? Well, the easiest solution is to encode each symbol with a number, for example, ♣←1, ♦←2, ♥←3, ♠←4, where the arrow means that we assign a number to the symbol. But we can express this more formally by defining a function X :Ω→R with X(♣)=1, X(♦)=2, X(♥)=3, X(♠)=4. Thereisnothingnewhere:wehavemerelyconvertedthesymbolstonumbers,withthehelp of a function X. However, with X defined, the probabilities can be written as 1 2 2 1 P[X =1]= , P[X =2]= , P[X =3]= , P[X =4]= . 6 6 6 6 This is much more convenient, and so the data scientist is happy. 3.1.2 Definition of a random variable The story above is exactly the motivation for random variables. Let us define a random variable formally. Definition3.1. ArandomvariableX isafunctionX :Ω→Rthatmapsanoutcome ξ ∈Ω to a number X(ξ) on the real line. 105
CHAPTER 3. DISCRETE RANDOM VARIABLES This definition may be puzzling at first glance. Why should we overcomplicate things by defining a function and calling it a variable? If you recall the story above, we can map the notations of the story to the notations of the definition as follows. Symbol Meaning Ω sample space = the set containing ♣,♦,♥,♠ ξ an element in the sample space, which is one of ♣,♦,♥,♠ X afunctionthatmaps♣tothenumber1,♦tothenumber2,etc X(ξ) a number on the real line, e.g., X(♣)=1 This explains our informal definition of random variables: Key Concept 1: What are random variables? Random variables are mappings from events to numbers. TherandomvariableX isafunction.Theinputtothefunctionisanoutcomeofthesample space, whereas the output is a number on the real line. This type of function is somewhat different from an ordinary function that often translates a number to another number. Nevertheless, X is a function. Figure 3.2: Arandomvariableisamappingfromtheoutcomesinthesamplespacetonumbersonthe realline.WecanthinkofarandomvariableX asatranslatorthattranslatesastatementtoanumber. Why do we call this function X a variable? X is a variable because X has multiple states. As we illustrate in Figure 3.2, the mapping X translates every outcome ξ to a number. There are multiple numbers, which are the states of X. Each state has a certain probability for X to land on. Because X is not deterministic, we call it a random variable. Example 3.1. Suppose we flip a fair coin so that Ω={head,tail}. We can define the random variable X :Ω→R as X(head)=1, and X(tail)=0. 106
3.1. RANDOM VARIABLES Therefore,whenwewriteP[X =1]weactuallymeanP[{head}].Isthereanydifference betweenP[{Head}]andP[X =1]?No,becausetheyaredescribingtwoidenticalevents. Notethattheassignmentofthevalueistotallyuptoyou.Youcansay“head”isequal to the value 102. This is allowed and legitimate, but it isn’t very convenient. Example 3.2. Flip a coin 2 times. The sample space Ω is Ω={(head,head),(head,tail),(tail,head),(tail,tail)}. Suppose that X is a random variable that maps an outcome to a number representing the sum of “head,” i.e., X(·)=number of heads. Then,forthe4ξ’sinthesamplespacethereareonly3distinctnumbers.Moreprecisely, if we let ξ = (head,head), ξ = (head,tail), ξ = (tail,head), ξ = (tail,tail), then, 1 2 3 4 we have X(ξ )=2, X(ξ )=1, X(ξ )=1, X(ξ )=0. 1 2 3 4 A pictorial illustration of this random variable is shown in Figure 3.3. This example showsthatthemappingdefinedbytherandomvariableisnotnecessarilyaone-to-one mapping because multiple outcomes can be mapped to the same number. Figure 3.3: Arandomvariablethatmapsapairofcoinstoanumber,wherethenumberrepresentsthe number of heads. 3.1.3 Probability measure on random variables By now, we hope that you understand Key Concept 1: A random variable is a mapping from a statement to a number. However, we are now facing another difficulty. We knew how to measure the size of an event using the probability law P because P(·) takes an event E ∈F andsendsittoanumberbetween[0,1].AfterthetranslationX,wecannotsendthe outputX(ξ)toP(·)becauseP(·)“eats”asetE ∈F andnotanumberX(ξ)∈R.Therefore, when we write P[X =1], how do we measure the size of the event X =1? 107
CHAPTER 3. DISCRETE RANDOM VARIABLES This question appears difficult but is actually quite easy to answer. Since the prob- ability law P(·) is always applied to an event, we need to define an event for the random variable X. If we write the sets clearly, we note that “X =a” is equivalent to the set (cid:26) (cid:12) (cid:27) (cid:12) E = ξ ∈Ω(cid:12)X(ξ)=a . (cid:12) This is the set that contains all possible ξ’s such that X(ξ) = a. Therefore, when we say “find the probability of X = a,” we are effectively asking the size of the set E = {ξ ∈ Ω|X(ξ)=a}. How then do we measure the size of E? Since E is a subset in the sample space, E is measurable by P. All we need to do is to determine what E is for a given a. This, in turn, requires us to find the pre-image X−1(a), which is defined as (cid:26) (cid:12) (cid:27) X−1(a)d =ef ξ ∈Ω(cid:12) (cid:12)X(ξ)=a . (cid:12) Wait a minute, is this set just equal to E? Yes, the event E we are seeking is exactly the pre-image X−1(a). As such, the probability measure of E is P[X =a]=P[X−1(a)]. Figure 3.4 illustrates a situation where two outcomes ξ and ξ are mapped to the same 1 2 value a on the real line. The corresponding event is the set X−1(a)={ξ ,ξ }. 1 2 Figure 3.4: When computing the probability of P[{ξ ∈ Ω|X(ξ) = a}], we effectively take the inverse mapping X−1(a) and compute the probability of the event P[{ξ∈X−1(a)}]=P[{ξ ,ξ }]. 1 2 Example 3.3. Suppose we throw a die. The sample space is Ω={1,2,3,4,5,6}. There is a natural mapping X that maps X(1)=1, X(2)=2 and so on. Thus, 108
3.1. RANDOM VARIABLES P[X ≤3]( =a)P[X =1]+P[X =2]+P[X =3] ( =b)P[X−1(1)]+P[X−1(2)]+P[X−1(3)] ( =c)P[{1}]+P[{2}]+P[{3}]= 3 . 6 In this derivation, step (a) is based on Axiom III, where the three events are disjoint. Step (b) is the pre-image due to the random variable X. Step (c) is the list of ac- tual events in the event space. Note that there is no hand-waving argument in this derivation. Every step is justified by the concepts and theorems we have learned so far. Example 3.4. Throw a die twice. The sample space is then Ω={(1,1),(1,2),...,(6,6)}. These elements can be translated to 36 outcomes: ξ =(1,1),ξ =(1,2),...,ξ =(6,6). 1 2 36 Let X =sum of two numbers. Then, if we want to find the probability of getting X =7, we can trace back and ask: Amongthe36outcomes,whichofthose ξ ’swillgive usX(ξ)=7? Or,whatistheset i X−1(7)? To this end, we can write P[X =7]=P[{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)}] =P[(1,6)]+P[(2,5)]+P[(3,4)] +P[(4,3)]+P[(5,2)]+P[(6,1)] 1 1 1 1 1 1 1 = + + + + + = . 36 36 36 36 36 36 6 Again,inthisexample,youcanseethatallthestepsarefullyjustifiedbytheconcepts we have learned so far. Closing remark. In practice, when the problem is clearly defined, we can skip the inverse mappingX−1(a).However,thisdoesnotmeanthattheprobabilitytriplet(Ω,F,P)isgone; it is still present. The triplet is now just the background of the problem. The set of all possible values returned by X is denoted as X(Ω). Since X is not necessarily a bijection, the size of X(Ω) is not necessarily the same as the size of Ω. The elements in X(Ω) are often denoted as a or x. We call a or x one of the states of X. Be careful not to confuse x and X. The variable X is the random variable; it is a function. The variable x is a state assigned by X. A random variable X has multiple states. When we write P[X =x], we describe the probability of a random variable X taking a particular state x. It is exactly the same as P[{ξ ∈Ω|X(ξ)=x}]. 109
CHAPTER 3. DISCRETE RANDOM VARIABLES 3.2 Probability Mass Function Random variables are mappings that translate events to numbers. After the translation, we have a set of numbers denoting the states of the random variables. Each state has a different probability of occurring. The probabilities are summarized by a function known as the probability mass function (PMF). 3.2.1 Definition of probability mass function Definition 3.2. The probability mass function (PMF) of a random variable X is a function which specifies the probability of obtaining a number X(ξ)=x. We denote a PMF as p (x)=P[X =x]. (3.1) X The set of all possible states of X is denoted as X(Ω). Donotget confusedbythesamplespaceΩand the setofstates X(Ω).Thesample spaceΩ contains all the possible outcomes of the experiments, whereas X(Ω) is the translation by the mapping X. The event X =a is the set X−1(a)⊆Ω. Therefore, when we say P[X =x] we really mean P[X−1(x)]. The probability mass function is a histogram summarizing the probability of each of the states X takes. Since it is a histogram, a PMF can be easily drawn as a bar chart. Example 3.5. Flip a coin twice. The sample space is Ω = {HH, HT, TH, TT}. We can assign a random variable X = number of heads. Therefore, X(“HH”)=2,X(“TH”)=1,X(“HT”)=1,X(“TT”)=0. So the random variable X takes three states: 0, 1, 2. The PMF is therefore 1 p (0)=P[X =0]=P[{“TT”}]= , X 4 1 p (1)=P[X =1]=P[{“TH”,“HT”}]= , X 2 1 p (2)=P[X =2]=P[{“HH”}]= . X 4 3.2.2 PMF and probability measure In Chapter 2, we learned that probability is a measure of the size of a set. We introduced a weighting function that weights each of the elements in the set. The PMF is the weighing functionfordiscreterandomvariables.TworandomvariablesaredifferentwhentheirPMFs are different because they are constructing two different measures. 110
3.2. PROBABILITY MASS FUNCTION To illustrate the idea, suppose there are two dice. They each have probability masses as follows. 1 2 3 4 1 1 P[{1}]= , P[{2}]= , P[{3}]= , P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 12 12 12 2 2 2 2 2 2 P[{1}]= , P[{2}]= , P[{3}]= , P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 12 12 12 Letusdefinetworandomvariables,X andY,forthetwodice.Then,thePMFsp andp X Y can be defined as 1 2 3 4 1 1 p (1)= , p (2)= , p (3)= , p (4)= , p (5)= , p (6)= , X 12 X 12 X 12 X 12 X 12 X 12 2 2 2 2 2 2 p (1)= , p (2)= , p (3)= , p (4)= , p (5)= , p (6)= . Y 12 Y 12 Y 12 Y 12 Y 12 Y 12 Thesetwoprobabilitymassfunctionscorrespondtotwodifferentprobabilitymeasures,let’s say F and G. Define the event E = {between 2 and 3}. Then, F(E) and G(E) will lead to two different results: 1 2 3 F(E)=P[2≤X ≤3]=p (2)+p (3)= + = , X X 12 12 12 2 2 4 G(E)=P[2≤Y ≤3]=p (2)+p (3)= + = . Y Y 12 12 12 Note that even though for some particular events two final results could be the same (e.g., 2≤X ≤4 and 2≤Y ≤4), the underlying measures are completely different. Figure 3.5 shows another example of two different measures F and G on the same sample space Ω = {♣,♦,♥,♠}. Since the PMFs of the two measures are different, even when given the same event E, the resulting probabilities will be different. Figure 3.5: If we want to measure the size of a set E, using two different PMFs is equivalent to using two different measures. Therefore, the probabilities will be different. Does p =p imply X =Y? If two random variables X and Y have the same PMF, X Y doesitmeanthattherandomvariablesarethesame?Theanswerisno.Considerarandom variable with a symmetric PMF, e.g., 1 1 1 p (−1)= , p (0)= , p (1)= . (3.2) X 4 X 2 X 4 Suppose Y =−X. Then, p (−1)= 1, p (0)= 1, and p (1)= 1, which is the same as p . Y 4 Y 2 Y 4 X However, X and Y are two different random variables. If the sample space is {♣,♦,♥}, we can define the mappings X(·) and Y(·) as X(♣)=−1, X(♦)=0, X(♥)=+1, Y(♣)=+1, Y(♦)=0, Y(♥)=−1. 111
CHAPTER 3. DISCRETE RANDOM VARIABLES Therefore,whenwesayp (−1)= 1,theunderlyingeventis♣.Butwhenwesayp (−1)= 1, X 4 Y 4 theunderlyingeventis♥.Thetworandomvariablesaredifferent,althoughtheirPMFshave exactly the same shape. 3.2.3 Normalization property Herewemustmentiononeimportantpropertyofaprobabilitymassfunction.Thisproperty is known as the normalization property, which is a useful tool for a sanity check. Theorem 3.1. A PMF should satisfy the condition that (cid:88) p (x)=1. (3.3) X x∈X(Ω) Proof.TheprooffollowsdirectlyfromAxiomII,whichstatesthatP[Ω]=1.Sincexcovers all numerical values X can take, and since each x is distinct, by Axiom III we have (cid:88) (cid:88) P[X =x]= P[{ξ ∈Ω|X(ξ)=x}] x∈X(Ω) x∈X(Ω)   (cid:91) =P  {ξ ∈Ω|X(ξ)=x}=P[Ω]=1. ξ∈Ω (cid:3) Practice Exercise 3.1. Let p (k)=c(cid:0)1(cid:1)k , where k =1,2,.... Find c. X 2 (cid:80) Solution. Since p (k)=1, we must have k∈X(Ω) X (cid:88)∞ (cid:18) 1(cid:19)k =1. 2 k=1 Evaluating the geometric series on the right-hand side, we can show that (cid:88)∞ (cid:18) 1(cid:19)k c (cid:88)∞ (cid:18) 1(cid:19)k c = 2 2 2 k=1 k=0 c 1 = · 2 1− 1 2 =c =⇒ c=1. Practice Exercise 3.2. Let p (k)=c·sin(cid:0)πk(cid:1) , where k =1,2,.... Find c. X 2 Solution. The reader may might be tempted to sum p (k) over all the possible k’s: X ∞ (cid:88) (cid:16)π (cid:17) ? sin k =1+0−1+0+···=0. 2 k=1 112
3.2. PROBABILITY MASS FUNCTION However, a more careful inspection reveals that p (k) is actually negative when k = X 3,7,11,.... This cannot happen because a probability mass function must be non- negative. Therefore, the problem is not defined, and so there is no solution. 0.5 1 0.5 0.25 0 -0.5 0.125 0.0625 -1 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 (a) (b) Figure 3.6: (a) The PMF of p (k) = c(cid:0)1(cid:1)k, for k = 1,2,.... (b) The PMF of p (k) = sin(cid:0)πk(cid:1) , X 2 X 2 where k=1,2,.... Note that this is not a valid PMF because probability cannot have negative values. 3.2.4 PMF versus histogram PMFs are closely related to histograms. A histogram is a plot that shows the frequency of a state. As we see in Figure 3.6, the x-axis is a collection of states, whereas the y-axis is the frequency. So a PMF is indeed a histogram. Viewing a PMF as a histogram can help us understand a random variable. For better or worse, treating a random variable as a histogram could help you differentiate a random variable from a variable. An ordinary variable only has one state, but a random variable has multiple states. At any particular instance, we do not know which state will show up before our observation. However, we do know the probability. For example, in the coin-flip example, while we do not know whether we will get “HH,” we know that the chance of getting “HH” is 1/4. Of course, having a probability of 1/4 does not mean that we will get “HH”onceeveryfourtrials.Itonlymeansthatifwerunaninfinitenumberofexperiments, then 1/4 of the experiments will give us “HH.” The linkage between PMF and histogram can be quite practical. For example, while wedonotknowthetrueunderlyingdistributionofthe26lettersoftheEnglishalphabet,we can collect a large number of words and plot the histogram. The example below illustrates how we can empirically define a random variable from the data. Example. There are 26 English letters, but the frequencies of the letters in writing are different. If we define a random variable X as a letter we randomly draw from an English text,wecanthinkofX asanobjectwith26differentstates.Themappingassociatedwiththe randomvariableisstraightforward:X(“a”)=1,X(“b”)=2,etc.Theprobabilityoflanding onaparticularstateapproximatelyfollowsahistogramshowninFigure 3.7.Thehistogram provides meaningful values of the probabilities, e.g., p (1) = 0.0847, p (2) = 0.0149, etc. X X The true probability of the states may not be exactly these values. However, when we have enough samples, we generally expect the histogram to approach the theoretical PMF. The MATLAB and Python codes used to generate this histogram are shown below. % MATLAB code to generate the histogram load(‘ch3_data_English’); bar(f/100,‘FaceColor’,[0.9,0.6,0.0]); 113
CHAPTER 3. DISCRETE RANDOM VARIABLES 0.12 0.1 0.08 0.06 0.04 0.02 0 a b c d e f g h i j k l mn o p q r s t u v w x y z Figure 3.7: The frequency of the 26 English letters. Data source: Wikipedia. xticklabels({‘a’,‘b’,‘c’,‘d’,‘e’,‘f’,‘g’,‘h’,‘i’,‘j’,‘k’,‘l’,... ‘m’,‘n’,‘o’,‘p’,‘q’,‘r’,‘s’,‘t’,‘u’,‘v’,‘w’,‘x’,‘y’,‘z’}); xticks(1:26); yticks(0:0.02:0.2); axis([1 26 0 0.13]); # Python code generate the histogram import numpy as np import matplotlib.pyplot as plt f = np.loadtxt(‘./ch3_data_english.txt’) n = np.arange(26) plt.bar(n, f/100) ntag = [‘a’,‘b’,‘c’,‘d’,‘e’,‘f’,‘g’,‘h’,‘i’,‘j’,‘k’,‘l’,‘m’,... ‘n’,‘o’,‘p’,‘q’,‘r’,‘s’,‘t’,‘u’,‘v’,‘w’,‘x’,‘y’,‘z’] plt.xticks(n, ntag) PMF = ideal histograms Ifarandomvariableismoreorlessahistogram,whyisthePMFsuchanimportantconcept? The answer to this question has two parts. The first part is that the histogram generated from a dataset is always an empirical histogram, so-called because the dataset comes from observation or experience rather than theory. Thus the histograms may vary slightly every time we collect a dataset. As we increase the number of data points in a dataset, the histogram will eventually converge to an ideal histogram, or a distribution. For example, counting the number of headsin100coinflipswillfluctuatemoreinpercentagetermsthancountingtheheadsin10 million coin flips. The latter will almost certainly have a histogram that is closer to a 50–50 distribution. Therefore, the “histogram” generated by a random variable can be considered the ultimate histogram or the limiting histogram of the experiment. To help you visualize the difference between a PMF and a histogram, we show in Figure 3.8 an experiment in which a die is thrown N times. Assuming that the die is fair, the PMF is simply p (k) = 1/6 for k = 1,...,6, which is a uniform distribution across X the 6 states. Now, we can throw the die many times. As N increases, we observe that the 114
3.2. PROBABILITY MASS FUNCTION N = 100 N = 1000 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 1 2 3 4 5 6 1 2 3 4 5 6 (a) N =100 (b) N =1000 N = 10000 N = 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 1 2 3 4 5 6 1 2 3 4 5 6 (c) N =10000 (d) PMF Figure3.8:HistogramandPMF,whenthrowingafairdieN times.AsN increases,thehistogramsare becoming more similar to the PMF. histogram becomes more like the PMF. You can imagine that when N goes to infinity, the histogram will eventually become the PMF. Therefore, when given a dataset, one way to think ofit is totreat the dataas random realizations drawnfrom a certainPMF. The more data points you have, the closer the histogram will become to the PMF. The MATLAB and Python codes used to generate Figure 3.8 are shown below. The twocommandsweuseherearerandi(inMATLAB),whichgeneratesrandomintegernum- bers, and hist, which computes the heights and bin centers of a histogram. In Python, the corresponding commands are np.random.randint and plt.hist. Note that because of the different indexing schemes in MATLAB and Python, we offset the maximum index in np.random.randintto7insteadof6.Also,weshiftthex-axessothatthebarsarecentered at the integers. % MATLAB code to generate the histogram x = [1 2 3 4 5 6]; q = randi(6,100,1); figure; [num,val] = hist(q,x-0.5); bar(num/100,‘FaceColor’,[0.8, 0.8,0.8]); axis([0 7 0 0.24]); # Python code generate the histogram import numpy as np import matplotlib.pyplot as plt q = np.random.randint(7,size=100) 115
CHAPTER 3. DISCRETE RANDOM VARIABLES plt.hist(q+0.5,bins=6) ThisgenerativeperspectiveisillustratedinFigure3.9.Weassumethattheunderlying latent random variable has some PMF that can be described by a few parameters, e.g., the meanandvariance.Giventhedatapoints,ifwecaninfertheseparameters,wemightretrieve theentirePMF(uptotheuncertaintylevelintrinsictothedataset).Werefertothisinverse process as statistical inference. Figure 3.9: Whenanalyzingadataset,onecantreatthedatapointsaresamplesdrawnaccordingtoa latentrandomvariablewithcertainaPMF.Thedatasetweobserveisoftenfinite,andsothehistogram we obtain is empirical. A major task in data analysis is statistical inference, which tries to retrieve the model information from the available measurements. Returning to the question of why we need to understand the PMFs, the second part of the answer is the difference between synthesis and analysis. In synthesis, we start with a known random variable and generate samples according to the PMF underlying the ran- dom variable. For example, on a computer, we often start with a Gaussian random variable and generate random numbers according to the histogram specified by the Gaussian ran- dom variable. Synthesis is useful because we can predict what will happen. We can, for example, create millions of training samples to train a deep neural network. We can also evaluatealgorithmsusedtoestimatestatisticalquantitiessuchasmean,variance,moments, etc., because the synthesis approach provides us with ground truth. In supervised learning scenarios, synthesis is vital to ensuring sufficient training data. The other direction of synthesis is analysis. The goal is to start with a dataset and deduce the statistical properties of the dataset. For example, suppose we want to know whether the underlying model is indeed a Gaussian model. If we know that it is a Gaussian (or if we choose to use a Gaussian), we want to know the parameters that define this Gaussian. The analysis direction addresses this model selection and parameter estimation problem. Moving forward, once we know the model and the parameters, we can make a prediction or do recovery, both of which are ubiquitous in machine learning. We summarize our discussions below, which is Key Concept 2 of this chapter. Key Concept 2: What are probability mass functions (PMFs)? PMFs are the ideal histograms of random variables. 116
3.2. PROBABILITY MASS FUNCTION 3.2.5 Estimating histograms from real data The following discussions about histogram estimation can be skipped if it is your first time reading the book. If you have a dataset, how would you plot the histogram? Certainly, if you have access to MATLAB or Python, you can call standard functions such as hist (in MATLAB) or np.histogram (in Python). However, when plotting a histogram, you need to specify the numberofbins(orequivalentlythewidthofbins).Ifyouuselargerbins,thenyouwillhave fewer bins with many elements in each bin. Conversely, if the bin width is too small, you may not have enough samples to fill the histogram. Figure 3.10 illustrates two histograms in which the bins are respectively too large and too small. 1000 50 K = 5 K = 200 800 40 600 30 400 20 200 10 0 0 0 2 4 6 8 10 0 2 4 6 8 10 (a) 5 bins (b) 200 bins Figure 3.10: The width of the histogram has substantial influence on the information that can be extracted from the histogram. The MATLAB and Pythoncodes used to generate Figure 3.10 are shown below.Note that here we are using an exponential random variable (to be discussed in Chapter 4). In MATLAB,callinganexponentialrandomvariableisdoneusingexprnd,whereasinPython the command is np.random.exponential. For this experiment, we can specify the number ofbinsk,whichcanbesettok =200ork =5.TosuppressthePythonoutputofthearray, we can add a semicolon ;. A final note is that lambda is a reserved variable in Python. Use something else. % MATLAB code used to generate the plots lambda = 1; k = 1000; X = exprnd(1/lambda,[k,1]); [num,val] = hist(X,200); bar(val,num,‘FaceColor’,[1, 0.5,0.5]); # Python code used to generate the plots import numpy as np import matplotlib.pyplot as plt lambd = 1 117
CHAPTER 3. DISCRETE RANDOM VARIABLES k = 1000 X = np.random.exponential(1/lambd, size=k) plt.hist(X,bins=200); In statistics, there are various rules to determine the bin width of a histogram. We mention a few of them here. Let K be the number of bins and N the number of samples. √ • Square-root: K = N • Sturges’ formula: K =log N +1. √ 2 • Rice Rule: K =23N √ • Scott’s normal reference rule: K = maxX−minX, where h = 3.5 √Var[X] is the bin h 3N width. For the example data shown in Figure 3.10, the histograms obtained using the above rules aregiveninFigure 3.11.Asyoucansee,differentruleshavedifferentsuggestedbinwidths. Some are more conservative, e.g., using fewer bins, whereas some are less conservative. In any case, the suggested bin widths do seem to provide better histograms than the original ones in Figure 3.10. However, no bin width is the best for all purposes. 500 500 Square-root, K = 32 Sturges Rule, K = 11 400 400 300 300 200 200 100 100 0 0 0 1 2 3 4 5 0 1 2 3 4 5 500 500 Rice Rule, K = 20 Scott Rule, K = 22 400 400 300 300 200 200 100 100 0 0 0 1 2 3 4 5 0 1 2 3 4 5 Figure 3.11: Histograms of a dataset using different bin width rules. Beyond these predefined rules, there are also algorithmic tools to determine the bin width. One such tool is known as cross-validation. Cross-validation means defining some kind of cross-validation score that measures the statistical risk associated with the his- togram.Ahistogramhavingalowerscorehasalowerrisk,andthusitisabetterhistogram. 118
3.2. PROBABILITY MASS FUNCTION Note that the word “better” is relative to the optimality criteria associated with the cross- validationscore.Ifyoudonotagreewithourcross-validationscore,ouroptimalbinwidthis not necessarily the one you want. In this case, you need to specify your optimality criteria. Theoretically, deriving a meaningful cross-validation score is beyond the scope of this book. However, it is still possible to understand the principle. Let h be the bin width of the histogram,K thenumberofbins,andN thenumberofsamples.Givenadataset,wefollow this procedure: • Step 1: Choose a bin width h. • Step2:Constructahistogramfromthedata,usingthebinwidthh.Thehistogramwill havetheempiricalPMFvaluesp ,p ,...,p ,whicharetheheightsofthehistograms (cid:98)1 (cid:98)2 (cid:98)K normalized so that the sum is 1. • Step 3: Compute the cross-validation score (see Wasserman, All of Statistics, Section 20.2): J(h)= 2 − N +1 (cid:0) p2+p2+···+p2 (cid:1) (3.4) (N −1)h (N −1)h (cid:98)1 (cid:98)2 (cid:98)K • Repeat Steps 1, 2, 3, until we find an h that minimizes J(h). Note that when we use a different h, the PMF values p ,p ,...,p will change, and the (cid:98)1 (cid:98)2 (cid:98)K number of bins K will also change. Therefore, when changing h, we are changing not only the terms in J(h) that explicitly contain h but also terms that are implicitly influenced. -2.9 -3 -3.1 -3.2 -3.3 -3.4 -3.5 20 40 60 80 100 120 140 160 180 200 Number of Bins erocS noitadilav-ssorC 10-3 5 4 3 2 1 0 20 40 60 80 100 120 140 160 180 200 Number of Bins erocS noitadilav-ssorC 10-4 (a) One dataset (b) Average of many datasets Figure 3.12: Cross-validation score for the histogram. (a) The score of one particular dataset. (b) The scores for many different datasets generated by the same model. For the dataset we showed in Figure 3.10, the cross-validation score J(h) is shown in Figure 3.12. We can see that although the curve is noisy, there is indeed a reasonably clear minimum happening around 20≤K ≤30, which is consistent with some of the rules. The MATLAB and Python codes we used to generate Figure 3.12 are shown below. The key step is to implement Equation (3.4) inside a for-loop, where the loop goes through the range of bins we are interested in. To obtain the PMF values p ,...,p , we call hist (cid:98)1 (cid:98)K in MATLAB and np.histogram in Python. The bin width h is the number of samples n divided by the number of bins m. 119
CHAPTER 3. DISCRETE RANDOM VARIABLES % MATLAB code to perform the cross validation lambda = 1; n = 1000; X = exprnd(1/lambda,[n,1]); m = 6:200; J = zeros(1,195); for i=1:195 [num,binc] = hist(X,m(i)); h = n/m(i); J(i) = 2/((n-1)*h)-((n+1)/((n-1)*h))*sum( (num/n).^2 ); end plot(m,J,‘LineWidth’,4,‘Color’,[0.9,0.2,0.0]); # Python code to perform the cross validation import numpy as np import matplotlib.pyplot as plt lambd = 1 n = 1000 X = np.random.exponential(1/lambd, size=n) m = np.arange(5,200) J = np.zeros((195)) for i in range(0,195): hist,bins = np.histogram(X,bins=m[i]) h = n/m[i] J[i] = 2/((n-1)*h)-((n+1)/((n-1)*h))*np.sum((hist/n)**2) plt.plot(m,J); In Figure 3.12(b), we show another set of curves from the same experiment. The differencehereisthatweassumeaccesstothetruegenerativemodelsothatwecangenerate the many datasets of the same distribution. In this experiment we generated T = 1000 datasets. We compute the cross-validation score J(h) for each of the datasets, yielding T scorefunctionsJ(1)(h),...,J(T)(h).Wesubtracttheminimumbecausedifferentrealizations have different offsets. Then we compute the average: T (cid:26) (cid:27) J(h)= 1 (cid:88) J(t)(h)−min(cid:8) J(t)(h)(cid:9) . (3.5) T h t=1 This gives us a smooth red curve as shown in Figure 3.12(b). The minimum appears to be at N = 25. This is the optimal N, concerning the cross-validation score, on the average of all datasets. All rules, including cross-validation, are based on optimizing for a certain objective. Your objective could be different from our objective, and so our optimum is not necessarily youroptimum.Therefore,cross-validationmaynotbethebest.Itdependsonyourproblem. End of the discussion. 120
3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE) 3.3 Cumulative Distribution Functions (Discrete) While the probability mass function (PMF) provides a complete characterization of a dis- crete random variable, the PMFs themselves are technically not “functions” because the impulses in the histogram are essentially delta functions. More formally, a PMF p (k) X should actually be written as (cid:88) p (x)= p (k) · δ(x−k) . X X (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) k∈X(Ω) PMFvalues deltafunction Thisisatrainofdeltafunctions,wheretheheightisspecifiedbytheprobabilitymassp (k). X For example, a random variable with PMF values 1 1 1 p (0)= , p (1)= , p (2)= X 4 X 2 X 4 will be expressed as 1 1 1 p (x)= δ(x)+ δ(x−1)+ δ(x−2). X 4 2 4 Sincedeltafunctionsneedtobeintegratedtogeneratevalues,thetypicalthingswewantto do, e.g., integration and differentiation, are not as straightforward in the sense of Riemann- Stieltjes. The way to handle the unfriendliness of the delta functions is to consider mild modi- fications of the PMF. This notation of “cumulative” distribution functions will allow us to resolve the delta function problems. We will defer the technical details to the next chap- ter. For the time being, we will briefly introduce the idea to prepare you for the technical discussion later. 3.3.1 Definition of the cumulative distribution function Definition 3.3. Let X be a discrete random variable with Ω = {x ,x ,...}. The 1 2 cumulative distribution function (CDF) of X is k F (x )d =efP[X ≤x ]=(cid:88) p (x ). (3.6) X k k X (cid:96) (cid:96)=1 If Ω={...,−1,0,1,2,...}, then the CDF of X is k F (k)d =efP[X ≤k]= (cid:88) p ((cid:96)). (3.7) X X (cid:96)=−∞ A CDF is essentially the cumulative sum of a PMF from −∞ to x, where the variable x(cid:48) in the sum is a dummy variable. 121
CHAPTER 3. DISCRETE RANDOM VARIABLES Example 3.6. Consider a random variable X with PMF p (0) = 1, p (1) = 1 and X 4 X 2 p (4)= 1. The CDF of X can be computed as X 4 1 F (0)=P[X ≤0]=p (0)= , X X 4 3 F (1)=P[X ≤1]=p (0)+p (1)= , X X X 4 F (4)=P[X ≤4]=p (0)+p (1)+p (4)=1. X X X X AsshowninFigure3.13,theCDFofadiscreterandomvariableisastaircasefunction. 1 1 0.75 0.75 0.5 0.5 0.25 0.25 0 1 4 0 1 4 (a) PMF p (k) (b) CDF F (k) X X Figure 3.13: Illustration of a PMF and a CDF. The MATLAB code and the Python code used to generate Figure 3.13 are shown below. The CDF is computed using the command cumsum in MATLAB and np.cumsum in Python. % MATLAB code to generate a PMF and a CDF p = [0.25 0.5 0.25]; x = [0 1 4]; F = cumsum(p); figure(1); stem(x,p,‘.’,‘LineWidth’,4,‘MarkerSize’,50); figure(2); stairs([-4 x 10],[0 F 1],‘.-’,‘LineWidth’,4,‘MarkerSize’,50); % Python code to generate a PMF and a CDF import numpy as np import matplotlib.pyplot as plt p = np.array([0.25, 0.5, 0.25]) x = np.array([0, 1, 4]) F = np.cumsum(p) plt.stem(x,p,use_line_collection=True); plt.show() plt.step(x,F); plt.show() 122
3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE) Why is CDF a better-defined function than PMF? There are technical reasons associ- ated with whether a function is integrable. Without going into the details of these discus- sions, a short answer is that delta functions are defined through integrations; they are not functions. A delta function is defined as a function such that δ(x)=0 everywhere except at (cid:82) x = 0, and δ(x) dx = 1. On the other hand, a staircase function is always well-defined. Ω The discontinuous points of a staircase can be well defined if we specify the gap between two consecutive steps. For example, in Figure 3.13, as soon as we specify the gap 1/4, 1/2, and 1/4, the staircase function is completely defined. Example. Figure 3.14 shows the empirical histogram of the English letters and the corre- sponding empirical CDF. We want to differentiate PMF versus histogram and CDF versus empirical CDF. The empirical CDF is the CDF computed from a finite dataset. 1 0.12 0.9 0.1 0.8 0.7 0.08 0.6 0.06 0.5 0.4 0.04 0.3 0.2 0.02 0.1 0 0 abcde f gh i j k lmnopq r s t uvwx y z abcde f gh i j k lmnopq r s t uvwx y z Figure 3.14: PMF and a CDF of the frequency of English letters. 3.3.2 Properties of the CDF WeobservefromtheexampleinFigure3.13thataCDFhasseveralproperties.First,being astaircasefunction,theCDFisnon-decreasing.Itcanstayconstantforawhile,butitnever drops. Second, the minimum value of a CDF is 0, whereas the maximum value is 1. It is 0 for any value that is smaller than the first state; it is 1 for any value that is larger than the last state. Third, the gap at each jump is exactly the probability mass at that state. Let us summarize these observations in the following theorem. Theorem3.2. IfX isadiscreterandomvariable,thentheCDFofX hasthefollowing properties: (i) The CDF is a sequence of increasing unit steps. (ii) The maximum of the CDF is when x=∞: F (+∞)=1. X (iii) The minimum of the CDF is when x=−∞: F (−∞)=0. X (iv) The unit steps have jumps at positions where p (x)>0. X Proof. Statement (i) can be seen from the summation (cid:88) F (x)= p (x(cid:48)). X X x(cid:48)≤x 123
CHAPTER 3. DISCRETE RANDOM VARIABLES Sincetheprobabilitymassfunctionisnon-negative,thevalueofF islargerwhenthevalue X of the argument is larger. That is, x≤y implies F (x)≤F (y). The second statement (ii) X X is true because the summation includes all possible states. So we have ∞ (cid:88) F (+∞)= p (x(cid:48))=1. X X x(cid:48)=−∞ Similarly, for the third statement (iii), (cid:88) F (−∞)= p (x(cid:48)). X X x(cid:48)≤−∞ The summation is taken over an empty set, and so F (−∞) = 0. Statement (iv) is true X because the cumulative sum changes only when there is a non-zero mass in the PMF. (cid:3) As we can see in the proof, the basic argument of the CDF is the cumulative sum of thePMF.Bydefinition,acumulativesumalwaysaddsmass.ThisiswhytheCDFisalways increasing, has 0 at −∞, and has 1 at +∞. This last statement deserves more attention. It implies that the unit step always has a solid dot on the left-hand side and an empty dot on the right-hand side, because when the CDF jumps, the final value is specified by the “≤” sign in Equation (3.6). The technical term for this property is right continuous. 3.3.3 Converting between PMF and CDF Theorem 3.3. IfX isadiscreterandomvariable,thenthePMFofX canbeobtained from the CDF by p (x )=F (x )−F (x ), (3.8) X k X k X k−1 whereweassumedthatX hasacountablesetofstates{x ,x ,...}.Ifthesamplespace 1 2 of the random variable X contains integers from −∞ to +∞, then the PMF can be defined as p (k)=F (k)−F (k−1). (3.9) X X X Example 3.7. Continuing with the example in Figure 3.13, if we are given the CDF 1 3 F (0)= , F (1)= , F (4)=1, X 4 X 4 X how do we find the PMF? We know that the PMF will have non-negative values only at x=0,1,4. For each of these x, we can show that 1 1 p (0)=F (0)−F (−∞)= −0= , X X X 4 4 3 1 1 p (1)=F (1)−F (0)= − = , X X X 4 4 2 3 1 p (4)=F (4)−F (1)=1− = . X X X 4 4 124
3.4. EXPECTATION 3.4 Expectation When analyzing data, it is often useful to extract certain key parameters such as the mean andthestandarddeviation.Themeanandthestandarddeviationcanbeseenfromthelens of random variables. In this section, we will formalize the idea using expectation. 3.4.1 Definition of expectation Definition 3.4. The expectation of a random variable X is (cid:88) E[X]= xp (x). (3.10) X x∈X(Ω) ExpectationisthemeanoftherandomvariableX.Intuitively,wecanthinkofp (x)asthe X percentage of times that the random variable X attains the value x. When this percentage is multiplied by x, we obtain the contribution of each x. Summing over all possible values of x then yields the mean. To see this more clearly, we can write the definition as (cid:88) E[X]= x p (x) . X (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x∈X(Ω) astateX takes thepercentage (cid:124) (cid:123)(cid:122) (cid:125) sumoverallstates Figure 3.15 illustrates a PMF that contains five states x ,...,x . Corresponding to each 1 5 state are p (x ),...,p (x ). For this PMF to make sense, we must assume that p (x )+ X 1 X 5 X 1 def ···+p (x ) = 1. To simplify notation, let us define p = p (x ). Then the expectation X 5 i X i of X is just the sum of the products: value (x ) times height (p ). This gives E[X] = i i (cid:80)5 x p (x ). i=1 i X i Figure 3.15: The expectation of a random variable is the sum of x p . i i We emphasize that the definition of the expectation is exactly the same as the usual way we calculate the average of a dataset. When we calculate the average of a dataset D ={x(1),x(2),...,x(N)},wesumuptheseN samplesanddividebythenumberofsamples. This is what we called the empirical average or the sample average: N 1 (cid:88) average= x(n). (3.11) N n=1 125
CHAPTER 3. DISCRETE RANDOM VARIABLES Of course, in a typical dataset, these N samples often take distinct values. But suppose that among these N samples there are only K different values. For example, if we throw a die a million times, every sample we record will be one of the six numbers. This situation is illustrated in Figure 3.16, where we put the samples into the correct bin storing these values. In this case, to calculate the average we are effectively doing a binning: K 1 (cid:88) average= value x × number of samples with value x . (3.12) N k k k=1 Equation(3.12)isexactly thesameasEquation(3.11),aslongasthesamplescanbegrouped into K different values. With a little calculation, we can rewrite Equation (3.12) as K average= (cid:88) value x × number of samples with value x k, (cid:124) (cid:123)(cid:122) k (cid:125) N k=1 astateX takes (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) thepercentage sumofallstates which is the same as the definition of expectation. Figure 3.16: If we have a dataset D containing N samples, and if there are only K distinct values, we caneffectivelyputtheseN samplesintoK bins.Thus,the“average”(whichisthesumdividedbythe number N) is exactly the same as our definition of expectation. The difference between E[X] and the average is that E[X] is computed from the ideal histogram,whereasaverageiscomputedfromtheempirical histogram.Whenthenumberof samples N approaches infinity, we expect the average to approximate E[X]. However, when N is small, the empirical average will have random fluctuations around E[X]. Every time we experiment, the empirical average may be slightly different. Therefore, we can regard E[X]asthetrue average ofacertainrandomvariable,andtheempiricalaverageasafinite- sample average based on the particular experiment we are working with. This summarizes Key Concept 3 of this chapter. Key Concept 3: What is expectation? Expectation = Mean = Average computed from a PMF. If we are given a dataset on a computer, computing the mean can be done by calling the command mean in MATLAB and np.mean in Python. The example below shows the case of finding the mean of 10000 uniformly distributed random numbers. 126
3.4. EXPECTATION % MATLAB code to compute the mean of a dataset X = rand(10000,1); mX = mean(X); # Python code to compute the mean of a dataset import numpy as np X = np.random.rand(10000) mX = np.mean(X) Example 3.8. Let X be a random variable with PMF p (0)=1/4, p (1)=1/2 and X X p (2)=1/4. We can show that the expectation is X (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X]=(0) +(1) +(2) =1. 4 2 4 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) pX(0) pX(1) pX(2) On MATLAB and Python, if we know the PMF then computing the expectation is straight-forward. Here is the code to compute the above example. % MATLAB code to compute the expectation p = [0.25 0.5 0.25]; x = [0 1 2]; EX = sum(p.*x); # Python code to compute the expectation import numpy as np p = np.array([0.25, 0.5, 0.25]) x = np.array([0, 1, 2]) EX = np.sum(p*x) Example 3.9. Flip an unfair coin, where the probability of getting a head is 3. Let 4 X be a random variable such that X = 1 means getting a head. Then we can show that p (1)= 3 and p (0)= 1. The expectation of X is therefore X 4 X 4 (cid:18) (cid:19) (cid:18) (cid:19) 3 1 3 E[X]=(1)p (1)+(0)p (0)=(1) +(0) = . X X 4 4 4 Center of mass. How would you interpret the result of this example? Does it mean that, on average, we will get 3/4 heads (but there is not anything called 3/4 heads!). Recall the definition of a random variable: it is a translator that translates a descriptive state to a number on the real line. Thus the expectation, which is an operation defined on the real line, can only tell us what is happening on the real line, not in the original sample 127
CHAPTER 3. DISCRETE RANDOM VARIABLES Figure 3.17: Centerofmass.Ifastatex ismoreinfluentialthananotherstatex ,thecenterofmass 2 1 E[X] will lean towards x . 2 space.Ontherealline,theexpectationcanberegardedasthecenter of mass,whichisthe point where the “forces” between the two states are “balanced”. In Figure 3.17 we depict a random variable with two states x and x . The state x has less influence (because p (x ) 1 2 1 X 1 is smaller) than x . Therefore the center of mass is shifted towards x . This result shows us 2 2 that the value E[X] is not necessarily in the sample space. E[X] is a deterministic number with nothing to do with the sample space. Example3.10.LetX bearandomvariablewithPMFp (k)= 1 ,fork =1,2,3,.... X 2k The expectation is ∞ ∞ (cid:88) (cid:88) 1 E[X]= kp (k)= k· X 2k k=1 k=1 ∞ 1(cid:88) 1 1 1 = k· = · =2. 2 2k−1 2 (1− 1)2 k=1 2 On MATLAB and Python, if you want to verify this answer you can use the following code. Here, we approximate the infinite sum by a finite sum of k =1,...,100. % MATLAB code to compute the expectation k = 1:100; p = 0.5.^k; EX = sum(p.*k); # Python code to compute the expectation import numpy as np k = np.arange(100) p = np.power(0.5,k) EX = np.sum(p*k) Example 3.11. Roll a die twice. Let X be the first roll and Y be the second roll. Let Z = max(X,Y). To compute the expectation E[Z], we first construct the sample space. Since there are two rolls, we can construct a table listing all possible pairs of outcomes.Thiswillgiveus{(1,1),(1,2),...,(6,6)}.Now,wecalculateZ,whichisthe max of the two rolls. So if we have (1,3), then the max will be 3, whereas if we have (5,2), then the max will be 5. We can complete a table as shown below. 128
3.4. EXPECTATION 1 2 3 4 5 6 1 1 2 3 4 5 6 2 2 2 3 4 5 6 3 3 3 3 4 5 6 4 4 4 4 4 5 6 5 5 5 5 5 5 6 6 6 6 6 6 6 6 This table tell us that Z has 6 states. The PMF of Z can be determined by counting the number of times a state shows up in the table. Thus, we can show that 1 3 5 p (1)= , p (2)= , p (3)= , Z 36 Z 36 Z 36 7 9 11 p (4)= , p (5)= , p (6)= . Z 36 Z 36 Z 36 The expectation of Z is therefore (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 3 5 E[Z]=(1) +(2) +(3) 36 36 36 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 7 9 11 +(4) +(5) +(6) 36 36 36 161 = . 36 Example 3.12. Consider a game in which we flip a coin 3 times. The reward of the game is • $1 if there are 2 heads • $8 if there are 3 heads • $0 if there are 0 or 1 head There is a cost associated with the game. To enter the game, the player has to pay $1.50. We want to compute the net gain, on average. To answer this question, we first note that the sample space contains 8 elements: HHH, HHT, HTH, THH, THT, TTH, HTT, TTT. Let X be the number of heads. Then the PMF of X is 1 3 3 1 p (0)= , p (1)= , p (2)= , p (3)= . X 8 X 8 X 8 X 8 WethenletY bethereward.ThePMFofY canbefoundby“adding”theprobabilities of X. This yields 4 3 1 p (0)=p (0)+p (1)= , p (1)=p (2)= , p (8)=p (3)= . Y X X 8 Y X 8 Y X 8 129
CHAPTER 3. DISCRETE RANDOM VARIABLES The expectation of Y is (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 4 3 1 11 E[X]=(0) +(1) +(8) = . 8 8 8 8 Since the cost of the game is 12, the net gain (on average) is −1. 8 8 3.4.2 Existence of expectation Does every PMF have an expectation? No, because we can construct a PMF such that the expectation is undefined. Example 3.13. Consider a random variable X with the following PMF: 6 p (k)= , k =1,2,.... X π2k2 Using a result from algebra, one can show that (cid:80)∞ 1 = π2. Therefore, p (k) is a legitimatePMFbecause(cid:80)∞ p (k)=1.Howeverk ,= t1 hek2 expec6 tationdivergesX ,because k=1 X ∞ (cid:88) E[X]= kp (k) X k=1 ∞ 6 (cid:88) 1 = →∞, π2 k k=1 where the limit is due to the harmonic seriesa: 1+ 1 + 1 +···=∞. 2 3 ahttps://en.wikipedia.org/wiki/Harmonic_series_(mathematics) A PMF has an expectation when it is absolutely summable. Definition 3.5. A discrete random variable X is absolutely summable if E[|X|]d =ef (cid:88) |x|p (x)<∞. (3.13) X x∈X(Ω) This definition tells us that not all random variables have a finite expectation. This is a very important mathematical result, but its practical implication is arguably limited. Most of the random variables we use in practice are absolutely summable. Also, note that the property of absolute summability applies to discrete random variables. For continuous random variables, we have a parallel concept called absolute integrability, which will be discussed in the next chapter. 3.4.3 Properties of expectation The expectation of a random variable has several useful properties. We list them below. Note that these properties apply to both discrete and continuous random variables. 130
3.4. EXPECTATION Theorem 3.4. The expectation of a random variable X has the following properties: (i) Function. For any function g, (cid:88) E[g(X)]= g(x)p (x). X x∈X(Ω) (ii) Linearity. For any function g and h, E[g(X)+h(X)]=E[g(X)]+E[h(X)]. (iii) Scale. For any constant c, E[cX]=cE[X]. (iv) DC Shift. For any constant c, E[X+c]=E[X]+c. Proof of (i): A pictorial proof of (i) is shown in Figure 3.18. The key idea is a change of variable. Figure 3.18: By letting g(X)=Y, the PMFs are not changed. What changes are the states. When we have a function Y =g(X), the PMF of Y will have impulses moved from x (the horizontal axis) to g(x) (the vertical axis). The PMF values (i.e., the probabilities or the height of the stems), however, are not changed. If the mapping g(X) is many-to-one, multiple PMF values will add to the same position. Therefore, when we compute E[g(X)], we compute the expectation along the vertical axis. Practice Exercise 3.3. Prove statement (iii): For any constant c, E[cX]=cE[X]. Solution. Recall the definition of expectation: (cid:88) (cid:88) E[cX]= (cx)p (x)=c xp (x)=cE[X]. X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) =E[X] Statement (iii) is illustrated in Figure 3.19. Here, we assume that the original PMF has 3 131
CHAPTER 3. DISCRETE RANDOM VARIABLES states X = 0,1,2. We multiply X by a constant c = 3. This changes X to cX = 0,3,6. However, since the probabilities are not changed, the height of the PMF values remains. Therefore, when computing the expectation, we just multiply E[X] by c to get cE[X]. Figure 3.19: PictorialrepresentationofE[cX]=cE[X].WhenwemultiplyX byc,wefixtheprobabil- ities but make the spacing between states wider/narrower. Practice Exercise 3.4. Prove statement (ii): For any function g and h, E[g(X)+ h(X)]=E[g(X)]+E[h(X)]. Solution. Recall the definition of expectation: (cid:88) E[g(X)+h(X)]= [g(x)+h(x)]p (x) X x∈X(Ω) (cid:88) (cid:88) = g(x)p (x)+ h(x)p (x) X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[g(X)] =E[h(X)] =E[g(X)]+E[h(X)]. PracticeExercise3.5.Provestatement(iv):Foranyconstantc,E[X+c]=E[X]+c. Solution. Recall the definition of expectation: (cid:88) E[X+c]= (x+c)p (x) X x∈X(Ω) (cid:88) (cid:88) = xp (x)+c· p (x) X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[X] =1 =E[X]+c. This result is illustrated in Figure 3.20. As we add a constant to the random variable, itsPMFvaluesremainthesamebuttheirpositionsareshifted.Therefore,whencomputing the mean, the mean will be shifted accordingly. 132
3.4. EXPECTATION Figure3.20:PictorialrepresentationofE[X+c]=E[X]+c.WhenweaddctoX,wefixtheprobabilities and shift the entire PMF to the left or to the right. Example3.14.LetX bearandomvariablewithfourequallyprobablestates0,1,2,3. We want to compute the expectation E[cos(πX/2)]. To do so, we note that (cid:18) (cid:19) (cid:88) πX E[cos(πX/2)]= cos p (x) 2 X x∈X(Ω) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 π 1 2π 1 3π 1 =(cos0) +(cos ) +(cos ) +(cos ) 4 2 4 2 4 2 4 1+0+(−1)+0 = =0. 4 Example 3.15. Let X be a random variable with E[X]=1 and E[X2]=3. We want to find the expectation E[(aX+b)2]. To do so, we realize that E[(aX+b)2]( =a)E[a2X2+2abX+b2]( =b) a2E[X2]+2abE[X]+b2 =3a2+2ab+b2, where (a) is due to expansion of the square, and (b) holds in two steps. The first step is to apply statement (ii) for individual functions of expectations, and the second step is to apply statement (iii) for scalar multiple of the expectations. 3.4.4 Moments and variance Based on the concept of expectation, we can define a moment: Definition 3.6. The kth moment of a random variable X is (cid:88) E[Xk]= xkp (x). (3.14) X x Essentially, the kth moment is the expectation applied to Xk. The definition follows from statement (i) of the expectation’s properties. Using this definition, we note that E[X] is the first moment and E[X2] is the second moment. Higher-order moments can be defined, but in practice they are less commonly used. 133
CHAPTER 3. DISCRETE RANDOM VARIABLES Example 3.16. Flip a coin 3 times. Let X be the number of heads. Then 1 3 3 1 p (0)= , p (1)= , p (2)= , p (3)= . X 8 X 8 X 8 X 8 The second moment E[X2] is (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 3 3 1 E[X2]=(0)2 +(1)2 +(2)2 +(4)2 =3. 8 8 8 8 Example 3.17. Consider a random variable X with PMF 1 p (k)= , k =1,2,.... X 2k The second moment E[X2] is (cid:88)∞ (cid:18) 1(cid:19)k 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 E[X2]= k2 = k(k−1+1) 2 22 2 k=1 k=1 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 = k(k−1) + k 22 2 22 2 k=1 k=1 (cid:18) (cid:19) (cid:18) (cid:19) 1 2 1 1 = + =6. 22 (1− 1)3 2 (1− 1)2 2 2 Using the second moment, we can define the variance of a random variable. Definition 3.7. The variance of a random variable X is Var[X]=E[(X−µ)2], (3.15) where µ=E[X] is the expectation of X. Wedenoteσ2 byVar[X].Thesquarerootofthevariance,σ,iscalledthestandarddeviation ofX.LiketheexpectationE[X],thevarianceVar[X]iscomputedusingtheidealhistogram PMF. It is the limiting object of the usual standard deviation we calculate from a dataset. On a computer, computing the variance of a dataset is done by calling built-in com- mandssuchasvarinMATLABandnp.varinPython.Thestandarddeviationiscomputed using std and np.std, respectively. % MATLAB code to compute the variance X = rand(10000,1); vX = var(X); sX = std(X); % Python code to compute the variance import numpy as np 134
3.4. EXPECTATION X = np.random.rand(10000) vX = np.var(X) sX = np.std(X) What does the variance mean? It is a measure of the deviation of the random variable X relative to its mean. This deviation is quantified by the squared difference (X−µ)2. The expectation operator takes the average of the deviation, giving us a deterministic number E[(X−µ)2]. Theorem 3.5. The variance of a random variable X has the following properties: (i) Moment. Var[X]=E[X2]−E[X]2. (ii) Scale. For any constant c, Var[cX]=c2Var[X]. (iii) DC Shift. For any constant c, Var[X+c]=Var[X]. Figure 3.21: Pictorial representations of Var[cX]=c2Var[X] and Var[X+c]=Var[X]. Practice Exercise 3.6. Prove Theorem 3.5 above. Solution. For statement (i), we show that Var[X]=E[(X−µ)2]=E[X2−2Xµ+µ2]=E[X2]−µ2. 135
CHAPTER 3. DISCRETE RANDOM VARIABLES Statement (ii) holds because E[cX]=cµ and Var[cX]=E[(cX−E[cX])2] =E[(cX−cµ)2]=c2E[(X−µ)2]=c2Var[X]. Statement (iii) holds because Var[X+c]=E[((X+c)−E[X+c])2]=E[(X−E[X])2]=Var[X]. Thepropertiesaboveareusefulinvariousways.Thefirststatementprovidesalinkconnect- ingvarianceandthesecondmoment.Statement(ii)impliesthatwhenX isscaledbyc,the varianceshouldbescaledbyc2 becauseofthesquareinthesecondmoment.Statement(iii) says that when X is shifted by a scalar c, the variance is unchanged. This is true because no matter how we shift the mean, the fluctuation of the random variable remains the same. Practice Exercise 3.7. Flip a coin with probability p to get a head. Let X be a random variable denoting the outcome. The PMF of X is p (0)=1−p, p (1)=p. X X Find E[X], E[X2] and Var[X]. Solution. The expectation of X is E[X]=(0)p (0)+(1)p (1)=(0)(1−p)+(1)(p)=p. X X The second moment is E[X2]=(0)2p (0)+(1)2p (1)=p. X X The variance is Var[X]=E[X2]−E[X]2 =p−p2 =p(1−p). 3.5 Common Discrete Random Variables In the previous sections, we have conveyed three key concepts: one about the random vari- able, one about the PMF, and one about the mean. The next step is to introduce a few commonlyuseddiscreterandomvariablessothatyouhavesomethingconcreteinyour“tool- box.” As we have mentioned before, these predefined random variables should be studied from a synthesis perspective (sometimes called generative). The plan for this section is to introduce several models, derive their theoretical properties, and discuss examples. Note that some extra effort will be required to understand the origins of the random variables. The origins of random variablesare usually overlooked, but they are moreimpor- tant than the equations. For example, we will shortly discuss the Poisson random variable 136
3.5. COMMON DISCRETE RANDOM VARIABLES Figure 3.22: A Bernoulli random variable has two states with probability p and 1−p. and its PMF p (k) = λke−λ. Why is the Poisson random variable defined in this way? If X k! youknowhowthePoissonPMFwasoriginallyderived,youwillunderstandtheassumptions made during the derivation. Consequently, you will know why Poisson is a good model for internettraffic,recommendationscores,andimagesensorsforcomputervisionapplications. You will also know under what situation the Poisson model will fail. Understanding the physics behind the probability models is the focus of this section. 3.5.1 Bernoulli random variable We start discussing the simplest random variable, namely the Bernoulli random variable. A Bernoulli random variable is a coin-flip random variable. The random variable has two states: either 1 or 0. The probability of getting 1 is p, and the probability of getting 0 is 1−p.SeeFigure 3.22foranillustration.Bernoullirandomvariablesareusefulforallkinds ofbinarystateevents:coinflip(HorT),binarybit(1or0),trueorfalse,yesorno,present or absent, Democrat or Republican, etc. To make these notions more precise, we define a Bernoulli random variable as follows. Definition 3.8. Let X be a Bernoulli random variable. Then, the PMF of X is p (0)=1−p, p (1)=p, X X where 0<p<1 is called the Bernoulli parameter. We write X ∼Bernoulli(p) to say that X is drawn from a Bernoulli distribution with a parameter p. Inthisdefinition,theparameterpcontrolstheprobabilityofobtaining1.Inacoin-flipevent, pisusually 1,meaningthatthecoinisfair.However,forbiasedcoinspisnotnecessarily 1. 2 2 Forothersituationssuchasbinarybits(0or1),theprobabilityofobtaining1couldbevery different from the probability of obtaining 0. In MATLAB and Python, generating Bernoulli random variables can be done by call- ing the binomial random number generator np.random.binomial (Python) and binornd (MATLAB). When the parameter n is equal to 1, the binomial random variable is equiv- alent to a Bernoulli random variable. The MATLAB and Python codes to synthesize a Bernoulli random variable are shown below. 137
CHAPTER 3. DISCRETE RANDOM VARIABLES % MATLAB code to generate 1000 Bernoulli random variables p = 0.5; n = 1; X = binornd(n,p,[1000,1]); [num, ~] = hist(X, 10); bar(linspace(0,1,10), num,‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 1000 Bernoulli random variables import numpy as np import matplotlib.pyplot as plt p = 0.5 n = 1 X = np.random.binomial(n,p,size=1000) plt.hist(X,bins=‘auto’) AnalternativemethodinPythonistocallstats.bernoulli.rvstogeneraterandom Bernoulli numbers. # Python code to call scipy.stats library import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 X = stats.bernoulli.rvs(p,size=1000) plt.hist(X,bins=‘auto’); Properties of Bernoulli random variables Let us now derive a few key statistical properties of a Bernoulli random variable. Theorem 3.6. If X ∼Bernoulli(p), then E[X]=p, E[X2]=p, Var[X]=p(1−p). Proof. The expectation can be computed as E[X]=(1)p (1)+(0)p (0)=(1)(p)+(0)(1−p)=p. X X The second moment is E[X2]=(12)(p)+(02)(1−p)=p. Therefore, the variance is Var[X]=E[X2]−µ2 =p−p2 =p(1−p). (cid:3) A useful property of the Python code is that we can construct an object rv. Then we can call rv’s attributes to determine its mean, variance, etc. 138
3.5. COMMON DISCRETE RANDOM VARIABLES # Python code to generate a Bernoulli rv object import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 rv = stats.bernoulli(p) mean, var = rv.stats(moments=‘mv’) print(mean, var) In both MATLAB and Python, we can plot the PMF of a Bernoulli random variable, suchastheoneshowninFigure3.23.TodothisinMATLAB,wecallthefunctionbinopdf, with the evaluation points specified by x. 1 0.8 0.6 0.4 0.2 0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2 Figure 3.23: An example of a theoretical PMF (not the empirical histogram) plotted by MATLAB. % MATLAB code to plot the PMF of a Bernoulli p = 0.3; x = [0,1]; f = binopdf(x,1,p); stem(x, f, ‘bo’, ‘LineWidth’, 8); In Python, we construct a random variable rv. With rv, we can call its PMF rv.pmf: # Python code to plot the PMF of a Bernoulli import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.3 rv = stats.bernoulli(p) x = np.linspace(0, 1, 2) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=10); plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5); 139
CHAPTER 3. DISCRETE RANDOM VARIABLES When will a Bernoulli random variable have the maximum variance? Let us take a look at the variance of the Bernoulli random variable. For any given p, the varianceisp(1−p).Thisisaquadraticequation.IfweletV(p)=p(1−p),wecanshowthat the maximum is attained at p = 1/2. To see this, take the derivative of V(p) with respect to p. This will give us d V(p) = 1−2p. Equating to zero yields 1−2p = 0, so p = 1/2. dp We know that p = 1/2 is a maximum and not a minimum point because the second order derivative V(cid:48)(cid:48)(p) = −2, which is negative. Therefore V(p) is maximized at p = 1/2. Now, since 0 ≤ p ≤ 1, we also know that V(0) = 0 and V(1) = 0. Therefore, the variance is minimized at p=0 and p=1. Figure 3.24 shows a graph of the variance. Figure 3.24: The variance of a Bernoulli reaches maximum at p=1/2. Does this result make sense? Why is the variance maximized at p = 1/2? If we think about this problem more carefully, we realize that a Bernoulli random variable represents a coin-flip experiment. If the coin is biased such that it always gives heads, on the one hand, it is certainly a bad coin. However, on the other hand, the variance is zero because there is nothing to vary; you will certainly get heads. The same situation happens if the coin is biased towards tails. However, if the coin is fair, i.e., p = 1/2, then the variance is large because we only have a 50% chance of getting a head or a tail whenever we flip a coin. Nothing is certain in this case. Therefore, the maximum variance happening at p = 1/2 matches our intuition. Rademacher random variable A slight variation of the Bernoulli random variable is the Rademacher random variable, which has two states: +1 and −1. The probability getting +1 and −1 is 1/2. Therefore, the PMF of a Rademacher random variable is 1 1 p (−1)= , and p (+1)= . X 2 X 2 Practice Exercise 3.8. Show that if X is a Rademacher random variable then (X+1)/2∼Bernoulli(1/2).Alsoshowtheconverse:IfY ∼Bernoulli(1/2)then2Y−1 is a Rademacher random variable. Solution.SinceX caneitherbe+1or−1,weshowthatifX =+1then(X+1)/2=1 and if X =−1 then (X+1)/2=0. The probabilities of getting +1 and −1 are equal. Thus, the probabilities of getting (X+1)/2=1 and 0 are also equal. So the resulting random variable is Bernoulli(1/2). The other direction can be proved similarly. 140
3.5. COMMON DISCRETE RANDOM VARIABLES Bernoulli in social networks: the Erd˝os-R´enyi graph The study of networks is a big branch of modern data science. It includes social networks, computer networks, traffic networks, etc. The history of network science is very long, but one of the most basic models of a network is the Erd˝os-R´enyi graph, named after Paul Erd˝os and Alfr´ed R´enyi. The underlying probabilistic model of the Erd˝os-R´enyi graph is the Bernoulli random variable. To see how a graph can be constructed from a Bernoulli random variable, we first introduce the concept of a graph. A graph contains two elements: nodes and edges. For node i and node j, we denote the edge connecting i and j as A . Therefore, if we have N ij nodes, then we can construct a matrix A of size N ×N. We call this matrix the adjacency matrix. For example, the adjacency matrix   0 1 1 0 1 0 0 0 A=  1 0 0 1 0 0 1 0 will have edges for node pairs (1,2), (1,3), and (3,4). Note that in this example we assume that the adjacency matrix is symmetric, meaning that the graph is undirected. The “1” in the adjacency matrix indicates there is an edge, and “0” indicates there is no edge. So A represents a binary graph. The Erd˝os-R´enyi graph model says that the probability of getting an edge is an inde- pendent Bernoulli random variable. That is A ∼Bernoulli(p), ij for i < j. If we model the graph in this way, then the parameter p will control the density of the graph. High values of p mean that there is a higher chance for an edge to be present. p = 0.3 p = 0.5 p = 0.7 p = 0.9 4 --012 21 2 5 32 4 8 3 1 96 1 1 1 337 6 8 0 1 4 612 8 307 7 2 1 3 4 8 23 73 9 3 1 45 2 2 0 0 61 3 1 29 13 2 23 3 2 113 5 29 5 2 4 21 ---0123 321 114 33 54 3 256 2 6 1 1 9 2 9 34 03 3 2 4 1 3 93 7 20 3 1 4 28 3 6 1 2 2 1 5 1 88 1 2 1 7 23 79 2 1 3 1 262 5 8 0 10 3 3 72 ---0123 321 3 2 2 1 1 41 1 2 3 4 5 1 0 56 2 1 39 34 8 1 3 4 11 2 5 2 7 5 3 8 73 0 33 32 9 1 2 1 2 91 2 2 8 73 236 8 932 6 2 11 0 60 3 47 -02 2 7 32 220 6 2 6 3 4 15 9 1 2 5 2 2 1 3 3 6 6 1 7 1 91 345 3 1 1 3 23 1 1 2 2 338 9 74 38 9 11 3 18 38 4 2 31 0 0 2 5 4 02 72 -3 -4 -4 -4 -2 0 2 -2 0 2 -4 -2 0 2 4 -4 -2 0 2 4 Figure 3.25: The Erd˝os-R´enyi graph. [Top] The graphs. [Bottom] The adjacency matrices. 141
CHAPTER 3. DISCRETE RANDOM VARIABLES To illustrate the idea of an Erd˝os-R´enyi graph, we show in Figure 3.25 a graph of 40 nodes. The edges are randomly selected by flipping a Bernoulli random variable with parameter p=0.3,0.5,0.7,0.9. As we can see in the figure, a small value of p gives a graph with very sparse connectivity, whereas a large value of p gives a very densely connected graph. The bottom row of Figure 3.25 shows the corresponding adjacency matrices. Here, a white pixel denotes “1” in the matrix and a black pixel denotes “0” in the matrix. While Erd˝os-R´enyi graphs are elementary, their variations can be realistic models of socialnetworks.Thestochasticblockmodelisonesuchmodel.Inastochasticblockmodel, nodes form small communities within a large network. For example, there are many majors in a university. Students within the same major tend to have more interactions than with students of another major. The stochastic block model achieves this goal by partitioning the nodes into communities. Within each community, the nodes can have a high degree of connectivity.Acrossdifferentcommunities,theconnectivitywillbemuchlower.Figure3.26 illustrates a network and the corresponding adjacency matrix. In this example, the network has three communities. 4 12 10 02 2 1 7 9 1 8 1 2 31 1 1 34 9 25 1 14 8 1 75 62 2 2 0 1 6 2 1 24 53 9 76 1 9 83 6 79 6 8 8 7 00 3 8 9 2 9 8 89 9 98 4 99 2 8 859 1 8 7 6 7 7 4 9 8 8 691 4 5 1 0 7 0 7 77 5 26 9 7 7 7 8 68 706 3 43 -- 42 6 4 5 4 422 6 5 5 65 9 26 3 3 4 2 8 8 9 3 30 9 3 2 2 8 5 3 3 4 34 5 1 56 33 3 6 67 3 1 14 44 54 2 7 3 55 5 106 5 45 68 57 60 27 40 49 -6 -4 -3 -2 -1 0 1 2 3 4 Figure 3.26: A stochastic block model containing three communities. [Left] The graph. [Right] The adjacency matrix. In network analysis, one of the biggest problems is determining the community struc- ture and recovering the underlying probabilities. The former task is about grouping the nodes into blocks. This is a nontrivial problem because in practice the nodes are never arranged nicely, as shown in Figure 3.26. For example, why should Alice be node 1 and Bob be node 2? Since we never know the correct ordering of the nodes, partitioning the nodes into blocks requires various estimation techniques such as clustering or iterative esti- mation. Recovering the underlying probability is also not easy. Given an adjacency matrix, why can we assume that the underlying network is a stochastic block model? Even if the model is correct, there will be imperfect grouping in the previous step. As such, estimat- ing the underlying probability in the presence of these uncertainties would pose additional challenges. Today,networkanalysisremainsoneofthehottestareasindatascience.Itsimportance derives from its broad scope and impact. It can be used to analyze social networks, opinion polls,marketing,orevengenomeanalysis.Nevertheless,thestartingpointoftheseadvanced subjects is the Bernoulli random variable, the random variable of a coin flip! 142
3.5. COMMON DISCRETE RANDOM VARIABLES 3.5.2 Binomial random variable Supposeweflipthecoinntimescountthenumberofheads.Sinceeachcoinflipisarandom variable (Bernoulli), the sum is also a random variable. It turns out that this new random variable is the binomial random variable. Definition 3.9. Let X be a binomial random variable. Then, the PMF of X is (cid:18) (cid:19) n p (k)= pk(1−p)n−k, k =0,1,...,n, X k where 0 < p < 1 is the binomial parameter, and n is the total number of states. We write X ∼Binomial(n,p) to say that X is drawn from a binomial distribution with a parameter p of size n. To understand the meaning of a binomial random variable, consider a simple experiment consisting of flipping a coin three times. We know that all possible cases are HHH, HHT, HTH, THH, TTH, THT, HTT and TTT. Now, suppose we define X = number of heads. We want to write down the probability mass function. Effectively, we ask: What is the probability of getting 0 head, one head, two heads, and three heads? We can, of course, count and get the answer right away for a fair coin. However, suppose the coin is unfair, i.e., the probability of getting a head is p whereas that of a tail is 1−p. The probability of getting each of the 8 cases is shown in Figure 3.27 below. Figure 3.27: The probability of getting k heads out of n=3 coins. Here are the detailed calculations. Let us start with X =3. p (3)=P[{HHH}] X =P[{H}∩{H}∩{H}] ( =a)P[{H}]P[{H}]P[{H}] ( =b) p3, where (a) holds because the three events are independent. (Recall that if A and B are independent then P[A∩B] = P[A]P[B].) (b) holds because each P[{H}] = p by definition. With exactly the same argument, we can show that p (0)=P[{TTT}]=(1−p)3. X 143
CHAPTER 3. DISCRETE RANDOM VARIABLES Now, let us look at p (2), i.e., 2 heads. This probability can be calculated as follows: X p (2)=P[{HHT}∪{HTH}∪{THH}] X ( =c)P[{HHT}]+P[{HTH}]+P[{THH}] ( =d) p2(1−p)+p2(1−p)+p2(1−p)=3p2(1−p), where (c) holds because the three events HHT, HTH and THH are disjoint in the sample space. Note that we are not using the independence argument in (c) but the disjoint argu- ment. We should not confuse the two. The step in (d) uses independence, because each coin flip is independent. The above calculation shows an interesting phenomenon: Although the three events HHT, HTH, and THH are different (in fact, disjoint), the number of heads in all the cases is the same. This happens because when counting the number of heads, the ordering of the heads and tails does not matter. So the same problem can be formulated as finding the number of combinations of { 2 heads and 1 tail }, which in our case is (cid:0)3(cid:1) =3. 2 To complete the story, let us also try p (1). This probability is X p (1)=P[{TTH}∪{HTT}∪{THT}]=3p(1−p)2. X Again, we see that the combination (cid:0)3(cid:1) =3 appears in front of the p(1−p)2. 1 In general, the way to interpret the binomial random variable is to decouple the prob- abilities p, (1−p), and the number of combinations (cid:0)n(cid:1) : k (cid:18) (cid:19) n p (k)= pk (1−p)n−k . X k (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) probgettingkH’s probgettingn−kT’s numberofcombinations The running index k should go with 0,1,...,n. It starts with 0 because there could be zero heads in the sample space. Furthermore, we note that in this definition, two parameters are driving a binomial random variable: the number of Bernoulli trials n and the underlying probability for each coin flip p. As such, the notation for a binomial random variable is Binomial(n,p), with two arguments. ThehistogramofabinomialrandomvariableisshowninFigure3.28(a).Here,wecon- sidertheexamplewheren=10andp=0.5.Togeneratethehistogram,weuse5000samples. In MATLAB and Python, generating binomial random variables as in Figure 3.28(a) can be done by calling binornd and np.random.binomial. % MATLAB code to generate 5000 Binomial random variables p = 0.5; n = 10; X = binornd(n,p,[5000,1]); [num, ~] = hist(X, 10); bar( num,‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 5000 Binomial random variables import numpy as np import matplotlib.pyplot as plt 144
3.5. COMMON DISCRETE RANDOM VARIABLES 1200 0.25 1000 0.2 800 0.15 600 0.1 400 0.05 200 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.28: An example of a binomial distribution with n=10, p=0.5. p = 0.5 n = 10 X = np.random.binomial(n,p,size=5000) plt.hist(X,bins=‘auto’); Generating the ideal PMF of a binomial random variable as shown in Figure 3.28(b) can be done by calling binopdf in MATLAB. In Python, we can define a random variable rv through stats.binom, and call the PMF using rv.pmf. % MATLAB code to generate a binomial PMF p = 0.5; n = 10; x = 0:10; f = binopdf(x,n,p); stem(x, f, ’o’, ’LineWidth’, 8, ’Color’, [0.8, 0.4, 0.4]); # Python code to generate a binomial PMF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) x = np.arange(11) f = rv.pmf(x) plt.plot(x, f, ’bo’, ms=10); plt.vlines(x, 0, f, colors=’b’, lw=5, alpha=0.5); TheshapeofthebinomialPMFisshowninFigure 3.29.Inthissetoffigures,wevary one of the two parameters n and p while keeping the other fixed. In Figure 3.29(a), we fix n = 60 and plot three sets of p = 0.1,0.5,0.9. For small p the PMF is skewed towards the left, and for large p the PMF is skewed toward the right. Figure 3.29(b) shows the PMF 145
CHAPTER 3. DISCRETE RANDOM VARIABLES for a fixed p = 0.5. As we increase n, the centroid of the PMF moves towards the right. Thus we shouldexpectthe mean ofa binomialrandomvariableto increase withp. Another interestingobservationisthatasnincreases,theshapeofthePMFapproachestheGaussian function (the bell-shaped curve). We will explain the reason for this when we discuss the Central Limit Theorem. 0.4 0.2 n = 5 p = 0.1 0.15 p p = = 0 0. .5 9 0.3 n n = = 5 10 00 0.1 0.2 0.05 0.1 0 0 0 10 20 30 40 50 60 0 10 20 30 40 50 60 (a) n=60 (b) p=0.5 Figure3.29:PMFsofabinomialrandomvariableX ∼Binomial(n,p).(a)Weassumethatn=60.By varyingtheprobabilityp,weseethatthePMFshiftsfromthelefttotheright,andtheshapechanges. (b) We assume that p=0.5. By varying the number of trials, the PMF shifts and the shape becomes more “bell-shaped.” The expectation, second moment, and variance of a binomial random variable are summarized in Theorem 3.7. Theorem 3.7. If X ∼Binomial(n,p), then E[X]=np, E[X2]=np(np+(1−p)), Var[X]=np(1−p). We will prove that E[X]=np using the first principle. For E[X2] and Var[X], we will skip the proofs here and will introduce a “shortcut” later. Proof. Let us start with the definition. n (cid:18) (cid:19) (cid:88) n E[X]= k· pk(1−p)n−k k k=0 n (cid:88) n! = k· pk(1−p)n−k k!(n−k)! k=0 n n! (cid:88) n! =0· p0(1−p)n−0+ k· pk(1−p)n−k 0!(n−0)! k!(n−k)! k=1 (cid:124) (cid:123)(cid:122) (cid:125) 0 n (cid:88) n! = pk(1−p)n−k. (k−1)!(n−k)! k=1 146
3.5. COMMON DISCRETE RANDOM VARIABLES Note that we have shifted the index from k =0 to k =1. Now let us apply a trick: n (cid:88) n! E[X]= pk(1−p)n−k (k−1)!(n−k)! k=1 n (cid:88) n! = pk(1−p)n−k. (k−1)!(n−k−1+1)! k=1 Using this trick, we can show that n (cid:88) n! pk(1−p)n−k (k−1)!(n−k−1+1)! k=1 n (cid:88) n! = pk(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 n (cid:88) n(n−1)! = pk(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 n (cid:88) (n−1)! =np pk−1(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 With a simple substitution of (cid:96)=k−1, the above equation can be rewritten as n−1 (cid:88) (n−1)! E[X]=np· p(cid:96)(1−p)n−1−(cid:96) (cid:96)!((n−1)−(cid:96))! (cid:96)=0 n−1(cid:18) (cid:19) (cid:88) n−1 =np· p(cid:96)(1−p)n−1−(cid:96) =np. k (cid:96)=0 (cid:124) (cid:123)(cid:122) (cid:125) summingPMFofBinomial(n−1,p) (cid:3) In MATLAB, the mean and variance of a binomial random variable can be found by calling the command binostat(n,p) (MATLAB). In Python, the command is rv = stats.binom(n,p) followed by calling rv.stats. % MATLAB code to compute the mean and var of a binomial rv p = 0.5; n = 10; [M,V] = binostat(n, p) # Python code to compute the mean and var of a binomial rv import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) M, V = rv.stats(moments=‘mv’) print(M, V) 147
CHAPTER 3. DISCRETE RANDOM VARIABLES An alternative view of the binomial random variable. As we discussed, the origin of a binomial random variable is the sum of a sequence of Bernoulli random variables. Because ofthisintrinsicdefinition,wecanderivesomeusefulresultsbyexploitingthisfact.Todoso, let us define I ,...,I as a sequence of Bernoulli random variables with I ∼ Bernoulli(p) 1 n j for all i=1,...,n. Then the resulting variable X =I +I +···+I 1 2 n is a binomial random variable of size n and parameter p. Using this definition, we can compute the expectation as follows: E[X]=E[I +I +···+I ] 1 2 n ( =a)E[I ]+E[I ]+···+E[I ] 1 2 n =p+p+···+p =np. Inthisderivation,thestep(a)dependsonausefulfactaboutexpectation(whichwehavenot yetproved):ForanytworandomvariablesX andY,itholdsthatE[X+Y]=E[X]+E[Y]. Therefore, we can show that the expectation of X is np. This line of argument not only simplifies the proof but also provides a good intuition of the expectation. If each coin flip has an expectation of E[I ]=p, then the expectation of the sum should be simply n times i of p, given np. How about the variance? Again, we are going to use a very useful fact about variance: If two random variables X and Y are independent, then Var[X +Y] = Var[X]+Var[Y]. With this result, we can show that Var[X]=Var[I +···+I ] 1 n =Var[I ]+···+Var[I ] 1 n =p(1−p)+···+p(1−p) =np(1−p). Finally, using the fact that Var[X]=E[X2]−µ2, we can show that E[X2]=Var[X]+µ2 =np(1−p)+(np)2. Practice Exercise 3.9. Show that the binomial PMF sums to 1. Solution. We use the binomial theorem to prove this result: n n (cid:18) (cid:19) (cid:88) (cid:88) n p (k)= pk(1−p)n−k =(p+(1−p))n =1. X k k=0 k=0 The CDF of the binomial random variable is not very informative. It is basically the cumulative sum of the PMF: k (cid:18) (cid:19) (cid:88) n F (k)= p(cid:96)(1−p)n−(cid:96). X (cid:96) (cid:96)=0 148
3.5. COMMON DISCRETE RANDOM VARIABLES 0.2 1 0.8 0.15 0.6 0.1 0.4 0.05 0.2 0 0 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Figure 3.30: PMF and CDF of a binomial random variable X ∼Binomial(n,p). The shapes of the PMF and the CDF is shown in Figure 3.30. In MATLAB, plotting the CDF of a binomial can be done by calling the function binocdf. You may also call f = binopdf(x,n,p), and define F = cumsum(f) as the cumu- lativesumofthePMF.InPython,thecorrespondingcommandisrv = stats.binom(n,p) followed by rv.cdf. % MATLAB code to compute the mean and var of a binomial rv x = 0:10; p = 0.5; n = 10; F = binocdf(x,n,p); figure; stairs(x,F,‘.-’,‘LineWidth’,4,‘MarkerSize’,30); # Python code to compute the mean and var of a binomial rv import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) x = np.arange(11) F = rv.cdf(x) plt.plot(x, F, ’bo’, ms=10); plt.vlines(x, 0, F, colors=’b’, lw=5, alpha=0.5); 3.5.3 Geometric random variable In some applications, we are interested in trying a binary experiment until we succeed. For example, we may want to keep calling someone until the person picks up the call. In this case,therandomvariablecanbedefinedastheoutcomeofmanyfailuresfollowedbyafinal success. This is called the geometric random variable. Definition 3.10. Let X be a geometric random variable. Then, the PMF of X is p (k)=(1−p)k−1p, k =1,2,..., X 149
CHAPTER 3. DISCRETE RANDOM VARIABLES where 0<p<1 is the geometric parameter. We write X ∼Geometric(p) to say that X is drawn from a geometric distribution with a parameter p. A geometric random variable is easy to understand. We define it as Bernoulli trials with k−1 consecutive failures followed by one success. This can be seen from the definition: p (k)= (1−p)k−1 p . X (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) k−1failures finalsuccess Note that in geometric random variables, there is no (cid:0)n(cid:1) because we must have k − 1 k consecutivefailuresbeforeonesuccess.Thereisnoalternativecombinationofthesequence. ThehistogramandPMFofageometricrandomvariableareillustratedinFigure3.31. Here, we assume that p=0.5. 3000 0.5 2500 0.4 2000 0.3 1500 0.2 1000 0.1 500 0 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.31: An example of a geometric distribution with p=0.5. In MATLAB, generating geometric random variables can be done by calling the com- mands geornd. In Python, it is np.random.geometric. % MATLAB code to generate 1000 geometric random variables p = 0.5; X = geornd(p,[5000,1]); [num, ~] = hist(X, 0:10); bar(0:10, num, ‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 1000 geometric random variables import numpy as np import matplotlib.pyplot as plt p = 0.5 X = np.random.geometric(p,size=1000) plt.hist(X,bins=‘auto’); To generate the PMF plots, in MATLAB we call geopdf and in Python we call rv = stats.geom followed by rv.pmf. 150
3.5. COMMON DISCRETE RANDOM VARIABLES % MATLAB code to generate geometric PMF p = 0.5; x = 0:10; f = geopdf(x,p); stem(x, f, ‘o’, ‘LineWidth’, 8, ‘Color’, [0.8, 0.4, 0.4]); # Python code to generate 1000 geometric random variables import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.arange(1,11) rv = stats.geom(p) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=8, label=‘geom pmf’) plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5) Practice Exercise 3.10. Show that the geometric PMF sums to one. Solution. We can apply infinite series to show the result: ∞ ∞ (cid:88) (cid:88) p (k)= (1−p)k−1p X k=1 k=1 ∞ (cid:88) =p· (1−p)k−1, (cid:96)=k−1 k=1 ∞ (cid:88) =p· (1−p)(cid:96) (cid:96)=0 1 =p· =1. 1−(1−p) ItisinterestingtocomparetheshapeofthePMFsforvariousvaluesofp.InFigure3.32 we show the PMFs. We vary the parameter p = 0.25,0.5,0.9. For small p, the PMF starts with a low value and decays at a slow speed. The opposite happens for a large p, where the PMF starts with a high value and decays rapidly. Furthermore, we can derive the following properties of the geometric random variable. Theorem 3.8. If X ∼Geometric(p), then 1 2 1 E[X]= , E[X2]= − , (3.16) p p2 p 1−p Var[X]= . p2 Proof. We will prove that the mean is 1/p and leave the second moment and variance as 151
CHAPTER 3. DISCRETE RANDOM VARIABLES 1 1 1 p = 0.25 p = 0.5 p = 0.9 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 0.2 0.2 0 0 0 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 Figure 3.32: PMFs of a geometric random variable X ∼Geometric(p). an exercise. ∞ (cid:32) ∞ (cid:33) (cid:18) (cid:19) E[X]=(cid:88) kp(1−p)k−1 =p (cid:88) k(1−p)k−1 ( =a) p 1 = 1 , (1−(1−p))2 p k=1 k=1 where (a) follows from the infinite series identity in Chapter 1. (cid:3) 3.5.4 Poisson random variable In many physical systems, the arrivals of events are typically modeled as a Poisson ran- dom variable, e.g., photon arrivals, electron emissions, and telephone call arrivals. In social networks, the number of conversations per user can also be modeled as a Poisson. In e- commerce, the number of transactions per paying user is again modeled using a Poisson. Definition 3.11. Let X be a Poisson random variable. Then, the PMF of X is λk p (k)= e−λ, k =0,1,2,..., X k! where λ>0 is the Poisson rate. We write X ∼Poisson(λ) to say that X is drawn from a Poisson distribution with a parameter λ. In this definition, the parameter λ determines the rate of the arrival. The histogram and PMF of a Poisson random variable are illustrated in Figure 3.33. Here, we assume that λ=1. TheMATLABcodeandPythoncodeusedtogeneratethehistogramareshownbelow. % MATLAB code to generate 5000 Poisson numbers lambda = 1; X = poissrnd(lambda,[5000,1]); 152
3.5. COMMON DISCRETE RANDOM VARIABLES 2000 0.4 1500 0.3 1000 0.2 500 0.1 0 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.33: An example of a Poisson distribution with λ=1. [num, ~] = hist(X, 0:10); bar(0:10, num, ‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 5000 Poisson random variables import numpy as np import matplotlib.pyplot as plt lambd = 1 X = np.random.poisson(lambd,size=5000) plt.hist(X,bins=‘auto’); For the PMF, in MATLAB we can call poisspdf, and in Python we can call rv.pmf with rv = stats.poisson. % MATLAB code to plot the Poisson PMF lambda = 1; x = 0:10; f = poisspdf(x,lambda); stem(x, f, ‘o’, ‘LineWidth’, 8, ‘Color’, [0.8, 0.4, 0.4]); # Python code to plot the Poisson PMF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.arange(0,11) rv = stats.poisson(lambd) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=8, label=‘geom pmf’) plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5) The shape of the Poisson PMF changes with λ. As illustrated in Figure 3.34, p (k) is X more concentrated at lower values for smaller λ and becomes spread out for larger λ. Thus, we should expect that the mean and variance of a Poisson random variable will change 153
CHAPTER 3. DISCRETE RANDOM VARIABLES together as a function of λ. In the same figure, we show the CDF of a Poisson random variable. The CDF of a Poisson is (cid:88)k λ(cid:96) F (k)=P[X ≤k]= e−λ. (3.17) X (cid:96)! (cid:96)=0 0.4 1 = 1 = 4 = 10 0.8 0.3 0.6 0.2 0.4 0.1 0.2 = 1 = 4 = 10 0 0 0 5 10 15 20 0 5 10 15 20 Figure 3.34: A Poisson random variable using different λ’s. [Left] Probability mass function p (k). X [Right] Cumulative distribution function F (k). X Example 3.18.LetX beaPoissonrandomvariablewithparameterλ.FindP[X >4] and P[X ≤5]. Solution. (cid:88)4 λk P[X >4]=1−P[X ≤4]=1− e−λ, k! k=0 (cid:88)5 λk P[X ≤5]= e−λ. k! k=0 Practice Exercise 3.11. Show that the Poisson PMF sums to 1. Solution. We use the exponential series to prove this result: (cid:88)∞ (cid:88)∞ λk (cid:88)∞ λk p (k)= e−λ =e−λ· =1. X k! k! k=0 k=0 k=0 (cid:124) (cid:123)(cid:122) (cid:125) =eλ Poisson random variables in practice (1) Computational photography. In computational photography, the Poisson random vari- able is one of the most widely used models for photon arrivals. The reason pertains to the 154
3.5. COMMON DISCRETE RANDOM VARIABLES originofthePoissonrandomvariable,whichwewilldiscussshortly.Whenphotonsareemit- ted from the source, they travel through the medium as a sequence of independent events. During the integration period of the camera, the photons are accumulated to generate a voltage that is then translated to digital bits. Figure 3.35: The Poisson random variable can be used to model photon arrivals. If we assume that the photon arrival rate is α (photons per second), and suppose that thetotalamountofintegrationtimeist,thentheaveragenumberofphotonsthatthesensor can see is αt. Let X be the number of photons seen during the integration time. Then if we follow the Poisson model, we can write down the PMF of X: (αt)k P[X =k]= e−αt. k! Therefore, if a pixel is bright, meaning that α is large, then X will have a higher likelihood of landing on a large number. (2) Traffic model. The Poisson random variable can be used in many other problems. For example, we can use it to model the number of passengers on a bus or the number of spam phone calls. The required modification to Figure 3.35 is almost trivial: merely replace the photons with your favorite cartoons, e.g., a person or a phone, as shown in Figure 3.36. In theUnitedStates,shared-rideservicessuchasUberandLyftneedtomodelthevacantcars andthepassengers.Aslongastheyhaveanarrivalrateandcertaindegreesofindependence between events, the Poisson random variable will be a good model. As you can see from these examples, the Poisson random variable has broad applica- bility. Before we continue our discussion of its applications, let us introduce a few concepts related to the Poisson random variable. Properties of a Poisson random variable We now derive the mean and variance of a Poisson random variable. Theorem 3.9. If X ∼Poisson(λ), then E[X]=λ, E[X2]=λ+λ2, (3.18) Var[X]=λ. 155
CHAPTER 3. DISCRETE RANDOM VARIABLES Figure 3.36: The Poisson random variable can be used to model passenger arrivals and the number of phone calls, and can be used by Uber or Lyft to provide shared rides. Proof. Let us first prove the mean. It can be shown that (cid:88)∞ λk (cid:88)∞ λk E[X]= k· e−λ = e−λ k! (k−1)! k=0 k=1 (cid:88)∞ λk−1 (cid:88)∞ λ(cid:96) =λe−λ =λe−λ =λe−λeλ =λ. (k−1)! (cid:96)! k=1 (cid:96)=0 The second moment can be computed as (cid:88)∞ λk E[X2]= k2· e−λ k! k=0 (cid:88)∞ λk = k· e−λ (k−1)! k=0 (cid:88)∞ λk = (k−1+1)· e−λ (k−1)! k=0 (cid:88)∞ λk (cid:88)∞ λk = (k−1)· e−λ+ e−λ (k−1)! (k−1)! k=1 k=1 (cid:88)∞ λk−2e−λ (cid:88)∞ λk−1e−λ =λ2· +λ· . (k−2)! (k−1)! k=2 k=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =1 =1 The variance can be computed using Var[X]=E[X2]−µ2. (cid:3) TocomputethemeanandvarianceofaPoissonrandomvariable,wecancallpoisstat in MATLAB and rv.stats(moments=‘mv’) in Python. % MATLAB code to compute Poisson statistics lambda = 1; [M,V] = poisstat(lambda); 156
3.5. COMMON DISCRETE RANDOM VARIABLES # Python code to compute Poisson statistics import scipy.stats as stats lambd = 1 rv = stats.poisson(lambd) M, V = rv.stats(moments=’mv’) ThePoissonrandomvariableisspecialinthesensethatthemeanandthevarianceare equal. That is, if the mean arrival number is higher, the variance is also higher. This is very differentfromsomeotherrandomvariables,e.g.,thenormalrandomvariablewherethemean andvarianceareindependent.Forcertainengineeringapplicationssuchasphotography,this playsanimportantroleindefiningthesignal-to-noiseratio.Wewillcomebacktothispoint later. Origin of the Poisson random variable We now address one of the most important questions about the Poisson random variable: Where does it come from? Answering this question is useful because the derivation process will reveal the underlying assumptions that lead to the Poisson PMF. When you change the problem setting, you will know when the Poisson PMF will hold and when the Poisson PMF will fail. Our approach to addressing this problem is to consider the photon arrival process. (As we have shown, there is conceptually no difference if you replace the photons with pedestrians,passengers,orphonecalls.)OurderivationfollowstheargumentofJ.Goodman, Statistical Optics, Section 3.7.2. To begin with, we consider a photon arrival process. The total number of photons observed over an integration time t is defined as X(t). Because X(t) is a Poisson random variable, its arguments must be integers. The probability of observing X(t)=k is therefore P[X(t)=k]. Figure 3.37 illustrates the notations and concepts. Figure 3.37: Notations for deriving the Poisson PMF. We propose three hypotheses with the photon arrival process: • For sufficiently small ∆t, the probability of a small impulse occurring in the time interval [t,t+∆t] is equal to the product of ∆t and the rate λ, i.e., P[X(t+∆t)−X(t)=1]=λ∆t. This is a linearity assumption, which typically holds for a short duration of time. 157
CHAPTER 3. DISCRETE RANDOM VARIABLES • For sufficiently small ∆t, the probability that more than one impulse falls in ∆t is negligible. Thus, we have that P[X(t+∆t)−X(t)=0]=1−λ∆t. • The number of impulses in non-overlapping time intervals is independent. The significance of these three hypotheses is that if the underlying photon arrival process violates any of these assumptions, then the Poisson PMF will not hold. One example is the presence of scattering effects, where a photon has a certain probability of going off due to thescatteringmediumandacertainprobabilityofcomingback.Inthiscase,theeventswill no longer be independent. Assumingthatthesehypotheseshold,thenattimet+∆t,theprobabilityofobserving X(t+∆t)=k can be computed as P[X(t+∆t)=k] =P[X(t)=k]· (1−λ∆t) + P[X(t)=k−1]· (λ∆t) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =P[X(t+∆t)−X(t)=0] =P[X(t+∆t)−X(t)=1] =P[X(t)=k]−P[X(t)=k]λ∆t+P[X(t)=k−1]λ∆t. By rearranging the terms we show that P[X(t+∆t)=k]−P[X(t)=k] (cid:18) (cid:19) =λ P[X(t)=k−1]−P[X(t)=k] . ∆t Setting the limit of ∆t→0, we arrive at an ordinary differential equation (cid:18) (cid:19) d P[X(t)=k]=λ P[X(t)=k−1]−P[X(t)=k] . (3.19) dt We claim that the Poisson PMF, i.e., (λt)k P[X(t)=k]= e−λt, k! wouldsolvethisdifferentialequation.Toseethis,wesubstitutethePMFintotheequation. The left-hand side gives us d d (cid:18) (λt)k (cid:19) P[X(t)=k]= e−λt dt dt k! (λt)k−1 (λt)k =λk e−λt+(−λ) e−λt k! k! (λt)k−1 (λt)k =λ e−λt−λ e−λt (k−1)! k! (cid:18) (cid:19) =λ P[X(t)=k−1]−P[X(t)=k] , which is the right-hand side of the equation. To retrieve the basic form of Poisson, we can just set t=1 in the PMF so that λk P[X(1)=k]= e−λ. k! 158
3.5. COMMON DISCRETE RANDOM VARIABLES The origin of Poisson random variables • We assume independent arrivals. • Probability of seeing one event is linear with the arrival rate. • Time interval is short enough so that you see either one event or no event. • Poisson is derived by solving a differential equation based on these assumptions. • Poisson becomes invalid when these assumptions are violated, e.g., in the case of scattering of photons due to turbid medium. There is an alternative approach to deriving the Poisson PMF. The idea is to drive the parameter n in the binomial random variable to infinity while pushing p to zero. In this limit, the binomial PMF will converge to the Poisson PMF. We will discuss this shortly. However, we recommend the physics approach we have just described because it has a rich meaning and allows us to validate our assumptions. Poisson approximation to binomial We present one additional result about the Poisson random variable. The result shows that Poisson can be regarded as a limiting distribution of a binomial random variable. Theorem 3.10. (Poisson approximation to binomial). For small p and large n, (cid:18) n(cid:19) λk pk(1−p)n−k ≈ e−λ, k k! def where λ = np. Before we prove the result, let us see how close the approximation can be. In Figure 3.38, we show a binomial distribution and a Poisson approximation. The closeness of the approx- imation can easily be seen. In MATLAB, the code to approximate a binomial distribution with a Poisson formula is shown below. Here, we draw 10,000 random binomial numbers and plot their histogram. Ontopoftheplot,weusepoisspdftocomputethePoissonPMF.ThisgivesusFigure3.38. A similar set of commands can be called in Python. % MATLAB code to approximate binomial using Poisson n = 1000; p = 0.05; X = binornd(n,p,[10000,1]); t = 0:100; [num,val] = hist(X,t); lambda = n*p; f_pois = poisspdf(t,lambda); bar(num/10000,‘FaceColor’,[0.9 0.9 0],‘BarWidth’,1); hold on; plot(f_pois, ‘LineWidth’, 4); 159
CHAPTER 3. DISCRETE RANDOM VARIABLES 0.06 0.04 0.02 0 ytilibaborP Binomial, n = 5000, p = 0.01 Poisson, = 50 0 20 40 60 80 100 120 k Figure 3.38: Poisson approximation of binomial distribution. # Python code to approximate binomial using Poisson import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats n = 1000; p = 0.05 rv1 = stats.binom(n,p) X = rv1.rvs(size=10000) plt.figure(1); plt.hist(X,bins=np.arange(0,100)); rv2 = stats.poisson(n*p) f = rv2.pmf(bin) plt.figure(2); plt.plot(f); Proof. Let λ=np. Then, (cid:18) n(cid:19) n! (cid:18) λ(cid:19)k(cid:18) λ(cid:19)n−k pk(1−p)n−k = 1− k k!(n−k)! n n λk n(n−1)···(n−k+1)(cid:18) λ(cid:19)n−k = 1− k! n·n···n n λk (cid:18) 1(cid:19) (cid:18) k−1(cid:19)(cid:18) λ(cid:19)−k(cid:18) λ(cid:19)n = (1) 1− ··· 1− 1− 1− k! n n n n (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) →1asn→∞ →1asn→∞ λk (cid:18) λ(cid:19)n = 1− . k! n We claim that (cid:0) 1− λ(cid:1)n →e−λ. This can be proved by noting that n log(1+x)≈x, x(cid:28)1. It then follows that log(cid:0) 1− λ(cid:1) ≈−λ. Hence, (cid:0) 1− λ(cid:1)n ≈e−λ n n n (cid:3) 160
3.5. COMMON DISCRETE RANDOM VARIABLES Example 3.19.Consideranopticalcommunicationsystem.Thebitarrivalrateis109 bits/sec, and the probability of having one error bit is 10−9. Suppose we want to find the probability of having five error bits in one second. Let X be the number of error bits. In one second there are 109 bits. Since we do not know the location of these 5 bits, we have to enumerate all possibilities. This leads to a binomial distribution. Using the binomial distribution, we know that the probability of having k error bits is (cid:18) (cid:19) n P[X =k]= pk(1−p)n−k k (cid:18) 109(cid:19) = (10−9)k(1−10−9)109−k. k This quantity is difficult to calculate in floating-point arithmetic. UsingthePoissontobinomialapproximation,wecanseethattheprobabilitycan be approximated by λk P[X =k]≈ e−λ, k! where λ=np=109(10−9)=1. Setting k =5 yields P[X =5]≈0.003. Photon arrival statistics Poisson random variables are useful in computer vision, but you may skip this discussion if it is your first reading of the book. The strong connection between Poisson statistics and physics makes the Poisson ran- dom variable a very good fit for many physical experiments. Here we demonstrate an appli- cation in modeling photon shot noise. Animagesensorisaphotonsensitivedevicewhichisusedtodetectincomingphotons. In the simplest setting, we can model a pixel in the object plane as X , for some 2D m,n coordinate [m,n] ∈ R2. Written as an array, an M ×N image in the object plane can be visualized as   X X ··· X 1,1 1,2 1,N X =object=  . . . . . . ... . . .  . X X ··· X M,1 M,2 M,N Without loss of generality, we assume that X is normalized so that 0 ≤ X ≤ 1 for m,n m,n every coordinate [m,n]. To model the brightness, we multiply X by a scalar α > 0. If m,n a pixel αX has a large value, then it is a bright pixel; conversely, if αX has a small m,n m,n value, then it is a dark pixel. At a particular pixel location [m,n]∈R2, the observed pixel valueY isarandomvariablefollowingthePoissonstatistics.Thissituationisillustrated m,n 161
CHAPTER 3. DISCRETE RANDOM VARIABLES in Figure 3.39, where we see that an object-plane pixel will generate an observed pixel through the Poisson PMF.1 Figure 3.39: The image formation process is governed by the Poisson random variable. Given a pixel in the object plane X , the observed pixel Y is a Poisson random variable with mean αX . m,n m,n m,n Therefore, a brighter pixel will have a higher Poisson mean, whereas a darker pixel will have a lower Poisson mean. Written as an array, the image is Y =observed image (cid:26) (cid:27) =Poisson αX   Poisson{αX } Poisson{αX } ··· Poisson{αX } 1,1 1,2 1,N Poisson{αX 2,1} Poisson{αX 2,2} ··· Poisson{αX 2,N} =   . . . . . . ... . . .   . Poisson{αX } Poisson{αX } ··· Poisson{αX } M,1 M,2 M,N Here, by Poisson{αX } we mean that Y is a random integer with probability mass m,n m,n [αX ]k P[Y =k]= m,n e−αXm,n. m,n k! Note that this model implies that the images seen by our cameras are more or less an array of Poisson random variables. (We say “more or less” because of other sources of uncertainties such as read noise, dark current, etc.) Because the observed pixels Y are m,n random variables, they fluctuate about the mean values, and hence they are noisy. We refer to this type of random fluctuation as the shot noise. The impact of the shot noise can be seeninFigure 3.40.Here,wevarythesensorgainlevelα.Weseethatforsmallαtheimage is dark and has much random fluctuation. As α increases, the image becomes brighter and the fluctuation becomes smaller. In MATLAB, simulating the Poisson photon arrival process for an image requires the image-processingtoolbox.Thecommandtoreadanimageisimread.Dependingonthedata type, the input array could be unit8 integers. To convert them to floating-point numbers between0and1,weusethecommandim2double.DrawingPoissonmeasurementsfromthe clean image is done using poissrnd. Finally, we can use imshow to display the image. 1Thecolorofanimageisoftenhandledbyacolor filter array,whichcanbethoughtofasawavelength selectorthatallowsaspecificwavelengthtopassthrough. 162
3.5. COMMON DISCRETE RANDOM VARIABLES = 10 = 100 = 1000 Figure 3.40: Illustration of the Poisson random variable in photographing images. Here, α denotes the gain level of the sensor: Larger α means that there are more photons coming to the sensor. % MATLAB code to simulate a photon arrival process x0 = im2double(imread(’cameraman.tif’)); X = poissrnd(10*x0); figure(1); imshow(x0, []); figure(2); imshow(X, []); Similar commands can be found in Python with the help of the cv2 library. When reading an image, we call cv2.imread. The option 0 is used to read a gray-scale image; otherwise, we will have a 3-channel color image. The division /255 ensures that the input array ranges between 0 to 1. Generating the Poisson random numbers can be done using np.random.poisson, or by calling the statistics library with stats.poisson.rvs(10*x0). Todisplaytheimages,wecallplt.imshow,withthecolormapoptionsettocmap = ’gray’. # Python code code to simulate a photon arrival process import numpy as np import matplotlib.pyplot as plt import cv2 x0 = cv2.imread(’./cameraman.tif’, 0)/255 plt.figure(1); plt.imshow(x0,cmap=’gray’); X = np.random.poisson(10*x0) plt.figure(2); plt.imshow(X, cmap=’gray’); Why study Poisson? What is shot noise? • The Poisson random variable is used to model photon arrivals. • Shot noise is the random fluctuation of the photon counts at the pixels. Shot noise is present even if you have an ideal sensor. Signal-to-noise ratio of Poisson Now let us answer a question we asked before. A Poisson random variable has a variance equal to the mean. Thus, if the scene is brighter, the variance will be larger. How come our 163
CHAPTER 3. DISCRETE RANDOM VARIABLES simulation in Figure 3.40 shows that the fluctuation becomes smaller as the scene becomes brighter? The answer to this question lies in the signal-to-noise ratio (SNR) of the Poisson random variable. The SNR of an image defines its quality. The higher the SNR, the better the image. The mathematical definition of SNR is the ratio between the signal power and the noise power. In our case, the SNR is signal power SNR= noise power E[Y] def = (cid:112) Var[Y] (a) λ √ = √ = λ, λ whereY =Y isoneoftheobservedpixelsandλ=αX isthethecorrespondingobject m,n m,n pixel. In this equation, the step (a) uses the properties of the Poisson random variable Y √ where E[Y] = Var[Y] = λ. The result SNR = λ is very informative. It says that if the √ underlying mean photon flux (which is λ) increases, the SNR increases at a rate of λ. So, yes, the variance becomes larger when the scene is brighter. However, the gain in signal E[Y] overrides the gain in noise (cid:112) Var[Y]. As a result, the big fluctuation in bright images is compensated by the strong signal. Thus, to minimize the shot noise one has to use a longer exposure to increase the mean photon flux. When the scene is dark and the aperture is small, shot noise is unavoidable. Poissonmodelingisusefulfordescribingtheproblem.However,theactualengineering questionisthat,givenanoiseobservationY ,howwouldyoureconstructthecleanimage m,n X ? This is a very difficult inverse problem. The typical strategy is to exploit the spatial m,n correlations between nearby pixels, e.g., usually smooth except along some sharp edges. Other information about the image, e.g., the likelihood of obtaining texture patterns, can alsobeleveraged.Modernimage-processingmethodsarerich,rangingfromclassicalfiltering techniquestodeepneuralnetworks.Staticimagesareeasiertorecoverbecausewecanoften leverage multiple measurements of the same scene to boost the SNR. Dynamic scenes are substantiallyharderwhenweneedtotrackthemotionofanyunderlyingobjects.Thereare alsonewerimagesensorswithbetterphotonsensitivity.Theproblemofimaginginthedark is an important research topic in computational imaging. New solutions are developed at the intersection of optics, signal processing, and machine learning. The end of our discussions on photon statistics. 164
3.6. SUMMARY 3.6 Summary Arandomvariableissocalledbecauseitcantakemorethanonestate.Theprobabilitymass function specifies the probability for it to land on a particular state. Therefore, whenever you think of a random variable you should immediately think of its PMF (or histogram if you prefer). The PMF is a unique characterization of a random variable. Two random variables with the same PMF are effectively the same random variables. (They are not identical because there could be measure-zero sets where the two differ.) Once you have the PMF, you can derive the CDF, expectation, moments, variance, and so on. When your boss hands a dataset to you, which random variable (which model) should you use? This is a very practical and deep question. We highlight three steps for you to consider: • (i) Model selection: Which random variable is the best fit for our problem? Some- timesweknowbyphysicsthat,forexample,photonarrivalsorinternettrafficfollowa Poisson random variable. However, not all datasets can be easily described by simple models. The models we have learned in this chapter are called the parametric mod- els because they are characterized by one or two parameters. Some datasets require nonparametric models, e.g., natural images, because they are just too complex. Some data scientists refer to deep neural networks as parametric models because the net- work weights are essentially the parameters. Some do not because when the number of parameters is on the order of millions, sometimes even more than the number of training samples, it seems more reasonable to call these models nonparametric. How- ever, putting this debate aside, shortlisting a few candidate models based on prior knowledge is essential. Even if you use deep neural networks, selecting between con- volutional structures versus long short-term memory models is still a legitimate task that requires an understanding of your problem. • (ii) Parameter estimation: Suppose that you now have a candidate model; the next taskistoestimatethemodelparameterusingtheavailabletrainingdata.Forexample, forPoissonweneedtodetermineλ,andforbinomialweneedtodetermine(n,p).The estimationproblemisaninverseproblem.OftenweneedtousethePMFtoconstruct certain optimization problems. By solving the optimization problem we will find the best parameter (for that particular candidate model). Modern machine learning is doing significantly better now than in the old days because optimization methods have advanced greatly. • (iii) Validation. When each candidate model has been optimized to best fit the data, we still need to select the best model. This is done by running various testings. For example, we can construct a validation set and check which model gives us the best performance (such as classification rate or regression error). However, a model with thebestvalidationscoreisnotnecessarilythebestmodel.Yourgoalshouldbetoseek agoodmodelandnotthebestmodelbecausedeterminingthebestrequiresaccessto the testing data, which we do not have. Everything being equal, the common wisdom is to go with a simpler model because it is generally less susceptible to overfitting. 165
CHAPTER 3. DISCRETE RANDOM VARIABLES 3.7 References Probability textbooks 3-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 2. 3-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 3. 3-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 3 and 4. 3-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 2 and3. 3-5 Sheldon Ross, A First Course in Probability, Prentice Hall, 8th Edition, 2010. Chap- ter 4. 3-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 2 and 4. Advanced probability textbooks 3-7 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 3-8 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) Cross-validation 3-9 Larry Wasserman, All of Statistics, Springer 2004. Chapter 20. 3-10 Mats Rudemo, “Empirical Choice of Histograms and Kernel Density Estimators,” Scandinavian Journal of Statistics, Vol. 9, No. 2 (1982), pp. 65-78. 3-11 DavidW.Scott,MultivariateDensityEstimation:Theory,Practice,andVisualization, Wiley, 1992. Poisson statistics 3-12 Joseph Goodman, Statistical Optics, Wiley, 2015. Chapter 3. 3-13 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Section 1.10. 166
3.8. PROBLEMS 3.8 Problems Exercise 1. (Video Solution) Consider an information source that produces numbers k in the set S = {1,2,3,4}. Find X and plot the PMF in the following cases: (a) p =p /k, for k =1,2,3,4. Hint: Find p . k 1 1 (b) p =p /2 for k =1,2,3. k+1 k (c) p =p /2k for k =1,2,3. k+1 k (d) Can the random variables in parts (a)-(c) be extended to take on values in the set {1,2,...}?Whyorwhynot?Hint:Youmayusethefactthattheseries1+1+1+··· 2 3 diverges. Exercise 2. (Video Solution) Two dice are tossed. Let X be the absolute difference in the number of dots facing up. (a) Find and plot the PMF of X. (b) Find the probability that X ≤2. (c) Find E[X] and Var[X]. Exercise 3. (Video Solution) Let X be a random variable with PMF p =c/2k for k =1,2,.... k (a) Determine the value of c. (b) Find P(X >4) and P(6≤X ≤8). (c) Find E[X] and Var[X]. Exercise 4. Let X be a random variable with PMF p =c/2k for k =−1,0,1,2,3,4,5. k (a) Determine the value of c. (b) Find P(1≤X <3) and P(1<X ≤5). (c) Find P[X3 <5]. (d) Find the PMF and the CDF of X. 167
CHAPTER 3. DISCRETE RANDOM VARIABLES Exercise 5. (Video Solution) A modem transmits a +2 voltage signal into a channel. The channel adds to this sig- nal a noise term that is drawn from the set {0,−1,−2,−3} with respective probabilities {4/10,3/10,2/10,1/10}. (a) Find the PMF of the output Y of the channel. (b) Whatistheprobabilitythatthechannel’soutputisequaltotheinputofthechannel? (c) What is the probability that the channel’s output is positive? (d) Find the expected value and variance of Y. Exercise 6. On a given day, your golf score takes values from numbers 1 through 10, with equal proba- bility of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let X , X , and X be the scores that you get, and 1 2 3 let X be the minimum of these three numbers. (a) Show that for any discrete random variable X, p (k)=P(X >k−1)−P(X >k). X (b) What is the probability P(X >k) for k =1,...,10? 1 (c) Use (a), determine the PMF p (k), for k =1,...,10. X (d) What is the average score improvement if you play just for one day compared with playing for three days and taking the minimum? Exercise 7. (Video Solution) Let (cid:40) (cid:40) 1, if X >10 X−10, if X−10>0 g(X)= and h(X)= 0, otherwise. 0, otherwise. (a) Find E[g(X)] for X as in Problem 1(a) with S ={1,...,15}. X (b) Find E[h(X)] for X as in Problem 1(b) with S ={1,...,15}. X Exercise 8. (Video Solution) A voltage X is uniformly distributed in the set {−3,...,3,4}. (a) Find the mean and variance of X. (b) Find the mean and variance of Y =−2X2+3. (c) Find the mean and variance of W =cos(πX/8). (d) Find the mean and variance of Z =cos2(πX/8). Exercise 9. (Video Solution) (a) If X is Poisson(λ), compute E[1/(X+1)]. 168
3.8. PROBLEMS (b) If X is Bernoulli(p) and Y is Bernoulli(q), compute E[(X + Y)3] if X and Y are independent. (c) Let X be a random variable with mean µ and variance σ2. Let ∆(θ) = E[(X −θ)2]. Find θ that minimizes the error ∆(θ). (d) SupposethatX ,...,X areindependentuniformrandomvariablesin{0,1,...,100}. 1 n Evaluate P[min(X ,...,X )>(cid:96)] for any (cid:96)∈{0,1,...,100}. 1 n Exercise 10. (Video Solution) (a) Consider the binomial probability mass function p (k)=(cid:0)n(cid:1) pk(1−p)n−k. Show that X k the mean is E[X]=np. (b) Consider the geometric probability mass function p (k) = p(1−p)k for k = 0,1,.... X Show that the mean is E[X]=(1−p)/p. (c) Consider the Poisson probability mass function p (k) = λke−λ. Show that the vari- X k! ance is Var[X]=λ. (d) Considertheuniformprobabilitymassfunctionp (k)= 1 fork =1,...,L.Showthat X L the variance is Var[X]= L2−1. Hint: 1+2+···+n= n(n+1) and 12+22+···+n2 = 12 2 n3 + n2 + n. 3 2 6 Exercise 11. (Video Solution) Anaudioplayerusesalow-qualityharddrive.Theprobabilitythattheharddrivefailsafter being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. Theinitialcostofbuildingtheplayeris$50,andthemanufactureroffersa1-yearwarranty. Within one year, the customer can ask for a free repair up to 12 times. (a) Let X be the number of months when the player fails. What is the PMF of X? Hint: P[X = 1] may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying X = 1 is equivalent to saying that there is only one failure in the entire 12-month period. (b) What is the average cost per player? Exercise 12. (Video Solution) A binary communication channel has a probability of bit error of p = 10−6. Suppose that transmission occurs in blocks of 10,000 bits. Let N be the number of errors introduced by the channel in a transmission block. (a) What is the PMF of N? (b) Find P[N =0] and P[N ≤3]. (c) For what value of p will the probability of 1 or more errors in a block be 99%? 169
CHAPTER 3. DISCRETE RANDOM VARIABLES Hint: Use the Poisson approximation to binomial random variables. Exercise 13. (Video Solution) The number of orders waiting to be processed is given by a Poisson random variable with parameter α=λ/nµ, where λ is the average number of orders that arrive in a day, µ is the number of orders that an employee can process per day, and n is the number of employees. Let λ = 5 and µ = 1. Find the number of employees required so the probability that more than four orders are waiting is less than 10%. Hint: You need to use trial and error for a few n’s. Exercise 14. LetX bethenumberofphotonscountedbyareceiverinanopticalcommunicationsystem. ItisknownthatX isaPoissonrandomvariablewitharateλ whenasignalispresentanda 1 Poissonrandomvariablewiththerateλ <λ whenasignalisabsent.Theprobabilitythat 0 1 the signal is present is p. Suppose that we observe X =k photons. We want to determine a threshold T such that if k ≥ T we claim that the signal is present, and if k < T we claim that the signal is absent. What is the value of T? 170
Chapter 4 Continuous Random Variables If you are coming to this chapter from Chapter 3, we invite you to take a 30-second pause and switch your mind from discrete events to continuous events. Everything is continuous now. The sample space is continuous, the event space is continuous, and the probability measure is continuous. Continuous random variables are similar in many ways to discrete random variables. They are characterized by the probability density functions (the continu- ous version of the probability mass functions); they have cumulative distribution functions; theyhavemeans,moments,andvariances.Themostsignificantdifferenceisperhapstheuse of integration instead of summation, but this change is conceptually straightforward, aside from the difficulties associated with integrating functions. So why do we need a separate chapter for continuous random variables? There are several reasons. • First,howwouldyoudefinetheprobabilityofacontinuousevent?Notethatwecannot count because a continuous event is uncountable. There is also nothing called the probability mass because there are infinitely many masses. To define the probability of continuous events, we need to go back to our “slogan”: probability is a measure of the size of a set. Because probability is a measure, we can speak meaningfully about the probability of continuous events so long as we have a well-defined measure for them. Defining such a measure requires some effort. We will develop the intuitions andtheformaldefinitionsinSection4.1.InSection4.2,wewilldiscusstheexpectation and variance of continuous random variables. • The second challenge is the unification between continuous and discrete random vari- ables. Since the two types of random variables ultimately measure the size of a set, it is natural to ask whether we can unify them. Our approach to unifying them is based on the cumulative distribution functions (CDFs), which are well-defined functions for discrete and continuous random variables. Based on the CDF and the fundamental theorem of calculus, we can show that the probability density functions and proba- bility mass functions can be derived from the derivative of the CDFs. These will be discussed in Section 4.3, and in Section 4.4 we will discuss some additional results about the mode and median. • Thethirdchallengeistounderstandseveralwidelyusedcontinuousrandomvariables. We will discuss the uniform random variable and the exponential random variable in Section 4.5. Section 4.6 deals with the important topic of the Gaussian random variable.WheredoesaGaussianrandomvariablecomefrom?Whydoesithaveabell 171
CHAPTER 4. CONTINUOUS RANDOM VARIABLES shape? Why are Gaussian random variables so popular in data science? What are the useful properties of Gaussian random variables? What are the relationships between a Gaussian random variable and other random variables? These important questions will be answered in Section 4.6. • Thefinalchallengeisthetransformationofrandomvariables.Imaginethatyouhavea randomvariableX andafunctiong.Whatwilltheprobabilitymass/densityfunction ofg(X)be?Addressingthisproblemisessentialbecausealmostallpracticalengineer- ing problems involve the transformation of random variables. For example, suppose wehavevoltagemeasurementsandwewouldliketocomputethepower.Thisrequires taking the square of the voltage. We will discuss the transformation in Section 4.7, and we will also discuss an essential application in generating random numbers in Section 4.8. 4.1 Probability Density Function 4.1.1 Some intuitions about probability density functions Let’s begin by outlining some intuitive reasoning, which is needed to define the probability of continuous events properly. These intuitions are based on the fact that probability is a measure. In the following discussion you will see a sequence of logical arguments for con- structingsuchameasureforcontinuousevents.SomeargumentsarediscussedinChapter2, but now we place them in the context of continuous random variables. Suppose we are given an event A that is a subset in the sample space Ω, as illustrated in Figure 4.1. In order to calculate the probability of A, the measure perspective suggests that we consider the relative size of the set “size” of A P[{x∈A}]= . “size” of Ω The right-hand side of this equation captures everything about the probability: It is a measure of the size of a set. It is relative to the sample space. It is a number between 0 and 1. It can be applied to discrete sets, and it can be applied to continuous sets. How do we measure the “size” of a continuous set? One possible way is by means of integrating the length, area, or volume covered by the set. Consider an example: Suppose that the sample space is the interval Ω = [0,5] and the event is A = [2,3]. To measure the “size” of A, we can integrate A to determine the length. That is, (cid:82) (cid:82)3 “size” of A dx dx 1 P[{x∈[2,3]}]= “size” of Ω = (cid:82)A dx = (cid:82)2 5 dx = 5. Ω 0 Therefore, we have translated the “size” of a set to an integration. However, this definition isaveryspecialcasebecausewhenwecalculatethe“size”ofaset,wetreatalltheelements inthesetwithequalimportance.Thisisastrongassumptionthatwillberelaxedlater.But 172
4.1. PROBABILITY DENSITY FUNCTION Figure4.1:[Left]AneventAinthesamplespaceΩ.TheprobabilitythatAhappenscanbecalculated asthe“size”ofArelativetothe“size”ofΩ.[Right]Aspecificexampleontherealline.Notethatthe samedefinitionofprobabilityapplies:TheprobabilityisthesizeoftheintervalArelativetothatofthe sample space Ω. if you agree with this line of reasoning, we can rewrite the probability as (cid:82) (cid:82) dx dx P[{x∈A}]= (cid:82)A = A dx |Ω| Ω (cid:90) 1 = dx. |Ω| A (cid:124) (cid:123)(cid:122) (cid:125) equallyimportantoverΩ This equation says that under our assumption (that all elements are equiprobable), the probability of A is calculated as the integration of A using an integrand 1/|Ω| (note that 1/|Ω| is a constant with respect to x). If we evaluate the probability of another event B, all we need to do is to replace A with B and compute (cid:82) 1 dx. B |Ω| What happens if we want to relax the “equiprobable” assumption? Perhaps we can adopt something similar to the probability mass function (PMF). Recall that a PMF p X evaluated at a point x is the probability that the state x happens, i.e., p (x) = P[X = x]. X So,p (x)istherelativefrequencyofx.Followingthesamelineofthinking,wecandefinea X function f such that f (x) tells us something related to the “relative frequency”. To this X X end, we can treat f as a continuous histogram with infinitesimal bin width as shown in X Figure 4.2.Usingthisf ,wecanreplacetheconstantfunction1/|Ω|withthenewfunction X f (x). This will give us X (cid:90) P[{x∈A}]= f (x) dx. (4.1) X A (cid:124) (cid:123)(cid:122) (cid:125) replace1/|Ω| If we compare it with a PMF, we note that when X is discrete, (cid:88) P[{x∈A}]= p (x). X x∈A Hence, f can be considered a continuous version of p , although we do not recommend X X this way of thinking for the following reason: p (x) is a legitimate probability, but f (x) is X X not a probability. Rather, f is the probability per unit length, meaning that we need to X integrate f (times dx) in order to generate a probability value. If we only look at f at X X a point x, then this point is a measure-zero set because the length of this set is zero. Equation (4.1) should be familiar to you from Chapter 2. The function f (x) is pre- X cisely the weighting function we described in that chapter. 173
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.2: [Left] A probability mass function (PMF) tells us the relative frequency of a state when computing the probability. In this example, the “size” of A is p (x )+p (x ). [Right] A probability X 2 X 3 densityfunction(PDF)istheinfinitesimalversionofthePMF.Thus,the“size”ofAistheintegration over the PDF. What is a PDF? • A PDF is the continuous version of a PMF. • We integrate a PDF to compute the probability. • We integrate instead of sum because continuous events are not countable. To summarize, we have learned that when measuring the size of a continuous event, the discrete technique (counting the number of elements) does not work. Generalizing to continuous space requires us to integrate the event. However, since different elements in an event have different relative emphases, we use the probability density function f (x) to tell X us the relative frequency for a state x to happen. This PDF serves the role of the PMF. 4.1.2 More in-depth discussion about PDFs A continuous random variable X is defined by its probability density function f . This X function has to satisfy several criteria, summarized as follows. Definition 4.1. A probability density function f of a random variable X is a map- X ping f :Ω→R, with the properties X • Non-negativity: f (x)≥0 for all x∈Ω X (cid:82) • Unity: f (x)dx=1 Ω X • Measure of a set: P[{x∈A}]=(cid:82) f (x)dx A X Ifallelementsofthesamplespaceareequiprobable,thenthePDFisf(x)=1/|Ω|.Youcan easily check that it satisfies all three criteria. Let us take a closer look at the three criteria: • Non-negativity: The non-negativity criterion f (x) ≥ 0 is reminiscent of Probability X Axiom I. It says that no matter what x we are looking at, the probability density function f evaluated at x should never give a negative value. Axiom I ensures that X we will not get a negative probability. 174
4.1. PROBABILITY DENSITY FUNCTION (cid:82) • Unity: The unity criterion f(x) dx = 1 is reminiscent of Probability Axiom II, Ω which says that measuring over the entire sample space will give 1. • Measureofaset:ThethirdcriteriongivesusawaytomeasurethesizeofaneventA. It says that since each x ∈ Ω has a different emphasis when calculating the size of A, we need to scale the elements properly. This scaling is done by the PDF f (x), X which can be regarded as a histogram with a continuous x-axis. The third criterion is a consequence of Probability Axiom III, because if there are two events A and B that are disjoint, then P[{x ∈ A}∪{x ∈ B}] = (cid:82) f (x) dx+(cid:82) f (x) dx because A X B X f (x)≥0 for all x. X If the random variable X takes real numbers in 1D, then a more “user-friendly” definition of the PDF can be given. Definition 4.2. Let X be a continuous random variable. The probability density function (PDF) of X is a function f :Ω→R that, when integrated over an interval X [a, b], yields the probability of obtaining a≤X ≤b: (cid:90) b P[a≤X ≤b]= f (x)dx. (4.2) X a This definition is just a rewriting of the previous definition by explicitly writing out the definition of A as an interval [a,b]. Here are a few examples. Example 4.1.Letf (x)=3x2 withΩ=[0,1].LetA=[0,0.5].Thentheprobability X P[{X ∈A}] is (cid:90) 0.5 1 P[0≤X ≤0.5]= 3x2 dx= . 8 0 Example 4.2.Letf (x)=1/|Ω|withΩ=[0,5].LetA=[3,5].Thentheprobability X P[{X ∈A}] is (cid:90) 5 1 (cid:90) 5 1 2 P[3≤X ≤5]= dx= dx= . |Ω| 5 5 3 3 Example 4.3. Let f (x) = 2x with Ω = [0,1]. Let A = {0.5}. Then the probability X P[{X ∈A}] is (cid:90) 0.5 P[X =0.5]=P[0.5≤X ≤0.5]= 2xdx=0. 0.5 This example shows that evaluating the probability at an isolated point for a contin- uous random variable will yield 0. 175
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Practice Exercise 4.1. Let X be the phase angle of a voltage signal. Without any priorknowledgeaboutX wemayassumethatX hasanequalprobabilityofanyvalue between 0 to 2π. Find the PDF of X and compute P[0≤X ≤π/2]. Solution. Since X has an equal probability for any value between 0 to 2π, the PDF of X is 1 f (x)= , for 0≤x≤2π. X 2π Therefore, the probability P[0≤X ≤π/2] can be computed as (cid:104) π(cid:105) (cid:90) π/2 1 1 P 0≤X ≤ = dx= . 2 2π 4 0 Looking at Equation (4.2), you may wonder: If the PDF f is analogous to PMF X p , why didn’t we require 0 ≤ f (x) ≤ 1 instead of requiring only f (x) ≥ 0? This is X X X an excellent question, and it points exactly to the difference between a PMF and a PDF. Notice that f is a mapping from the sample space Ω to the real line R. It does not map X Ω to [0,1]. On the other hand, since p (x) is the actual probability, it maps Ω to [0,1]. X Thus, f (x) can take very large values but will not explode, because we have the unity X (cid:82) constraint f (x)dx=1.Eveniff (x)takesalargevalue,itwillbecompensatedbythe Ω X X small dx. If you recall, there is nothing like dx in the definition of a PMF. Whenever there is a probability mass, we need to sum or, putting it another way, the dx in the discrete case is always 1. Therefore, while the probability mass PMF must not exceed 1, a probability density PDF can exceed 1. If f (x)≥1, then what is the meaning of f (x)? Isn’t it representing the probability X X of having an element X = x? If it were a discrete random variable, then yes; p (x) is the X probability of having X = x (so the probability mass cannot go beyond 1). However, for a continuous random variable, f (x) is not the probability of having X =x. The probability X of having X = x (i.e., exactly at x) is 0 because an isolated point has zero measure in the continuous space. Thus, even though f (x) takes a value larger than 1, the probability of X X being x is zero. AtthispointyoucanseewhywecallPDFadensity,ordensityfunction,becauseeach value f (x) is the probability per unit length. If we want to calculate the probability of X x≤X ≤x+δ, for example, then according to our definition, we have (cid:90) x+δ P[x≤X ≤x+δ]= f (x)dx≈f (x)·δ. X X x Therefore, the probability of P[x ≤ X ≤ x+δ] can be regarded as the “per unit length” density f (x) multiplied with the “length” δ. As δ →0, we can see that P[X =x]=0. See X Figure 4.3 for an illustration. Why are PDFs called a density function? • Because f (x) is the probability per unit length. X • You need to integrate f (x) to obtain a probability. X 176
4.1. PROBABILITY DENSITY FUNCTION Figure 4.3: TheprobabilityP[x≤X ≤x+δ]canbeapproximatedbythedensityf (x)multipliedby X the length δ. Example 4.4. Consider a random variable X with PDF f X(x) = 2√1 x for any 0<x≤1, and is 0 otherwise. We can show that f (x) → ∞ as x → 0. However, X f (x) remains a valid PDF because X (cid:90) ∞ (cid:90) 1 1 √ (cid:12) (cid:12)1 f X(x)dx= 2√ x dx= x(cid:12) (cid:12) =1. −∞ 0 0 Remark. Since isolated points have zero measure in the continuous space, the probability of an open interval (a,b) is the same as the probability of a closed interval: P[[a,b]]=P[(a,b)]=P[(a,b]]=P[[a,b)]. The exception is that when the PDF of f (x) has a delta function at a or b. In this case, X the probability measure at a or b will be non-zero. We will discuss this when we talk about the CDFs. Practice Exercise 4.2.Letf (x)=c(1−x2)for−1≤x≤1,and0otherwise.Find X the constant c. (cid:82) Solution. Since f (x)dx=1, it follows that Ω X (cid:90) (cid:90) 1 4c f (x)dx= c(1−x2)dx= ⇒ c=3/4. X 3 Ω −1 Practice Exercise 4.3. Let f (x)=x2 for |x|≤a, and 0 otherwise. Find a. X Solution. Note that (cid:90) f X(x)dx=(cid:90) a x2 dx= x 33(cid:12) (cid:12) (cid:12) (cid:12)a = 2 3a3 . Ω −a −a (cid:113) Setting 2a3 =1 yields a= 3 3. 3 2 177
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.1.3 Connecting with the PMF The probability density function is more general than the probability mass function. To see this, consider a discrete random variable X with a PMF p (x). Because p is defined on X X a countable set Ω, we can write it as a train of delta functions and define a corresponding PDF: (cid:88) f (x)= p (x )δ(x−x ). X X k k xk∈Ω Example 4.5. If X is aBernoulli randomvariablewith PMF p (1)=p and p (0)= X X 1−p, then the corresponding PDF can be written as f (x)=pδ(x−1)+(1−p)δ(x−0). X Example4.6.IfX isabinomialrandomvariablewithPMFp (k)=(cid:0)n(cid:1) pk(1−p)n−k, X k then the corresponding PDF can be written as n (cid:88) f (x)= p (k)δ(x−k) X X k=0 n (cid:18) (cid:19) (cid:88) n = pk(1−p)n−k δ(x−k). k k=0 Strictly speaking, delta functions are not really functions. They are defined through integrations. They satisfy the properties that δ(x−x ) = ∞ if x = x , δ(x−x ) = 0 if k k k x(cid:54)=x , and k (cid:90) xk+(cid:15) δ(x−x )dx=1, k xk−(cid:15) for any (cid:15)>0. Suppose we ignore the fact that delta functions are not functions and merely treat them as ordinary functions with some interesting properties. In this case, we can imagine that for every probability mass p (x ), there exists an interval [a,b] such that X k there is one and only one state x that lies in [a,b], as shown in Figure 4.4. k Figure 4.4: We can view a PMF as a train of impulses. When computing the probability X =x , we k integrate the PMF over the interval [a,b]. 178
4.1. PROBABILITY DENSITY FUNCTION If we want to calculate the probability of obtaining X =x , we can show that k P[X =x ]( =a)P[a≤X ≤b] k (cid:90) b = f (x)dx X a (cid:90) b (b) = p (x )δ(x−x )dx X k k a (cid:90) b (c) = p (x ) δ(x−x )dx=p (x ). X k k X k a (cid:124) (cid:123)(cid:122) (cid:125) =1 Here, step (a) holds because within [a,b], there is no other event besides X = x . Step (b) k is just the definition of our f (x) (inside the interval [a,b]). Step (c) shows that the delta X function integrates to 1, thus leaving the probability mass p (x ) as the final result. Let us X k look at an example and then comment on this intuition. Example 4.7. Let X be a discrete random variable with PMF 1 p (k)= , k =1,2,... X 2k The continuous representation of the PMF can be written as ∞ ∞ (cid:18) (cid:19) (cid:88) (cid:88) 1 f (x)= p (k)δ(x−k)= δ(x−k). X X 2k k=1 k=1 Suppose we want to compute the probability P[1≤X ≤2]. This can be computed as (cid:90) 2 (cid:90) 2 (cid:88)∞ (cid:18) 1 (cid:19) P[1≤X ≤2]= f (x)dx= δ(x−k)dx X 2k 1 1 k=1 (cid:90) 2(cid:26) 1 1 (cid:27) = δ(x−1)+ δ(x−2)+··· dx 2 4 1 1(cid:90) 2 1(cid:90) 2 = δ(x−1)dx+ δ(x−2)dx 2 4 1 1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =1 =1 1(cid:90) 2 + δ(x−3)dx+ ··· 8 (cid:124)(cid:123)(cid:122)(cid:125) 1 (cid:124) (cid:123)(cid:122) (cid:125) =0 =0 1 1 3 = + = . 2 4 4 However, if we want to compute the probability P[1 < X ≤ 2], then the integration 179
CHAPTER 4. CONTINUOUS RANDOM VARIABLES limit will not include the number 1 and so the delta function will remain 0. Thus, (cid:90) 2 P[1<X ≤2]= f (x)dx X 1+ 1(cid:90) 2 1(cid:90) 2 1 = δ(x−1)dx+ δ(x−2)dx= . 2 4 4 1+ 1+ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =0 =1 Closing remark. To summarize, we see that a PMF can be “regarded” as a PDF. We are carefultoputaquotationaround“regarded”becausePMFandPDFaredefinedfordifferent events.APMFusesadiscretemeasure(i.e.,acounter)forcountableevents,whereasaPDF usesacontinuousmeasure(i.e.,integration)forcontinuousevents.Thewaywelinkthetwois byusingthedeltafunctions.Usingthedeltafunctionsisvalid,buttheargumentweprovide hereisintuitiveratherthanrigorous.Itisnotrigorousbecausetheintegrationweuseisstill the Riemann-Stieltjes integration, which does not handle delta functions. Therefore, while you can treat a discrete PDF as a train of delta functions, it is important to remember the limitations of the integrations we use. 4.2 Expectation, Moment, and Variance 4.2.1 Definition and properties As with discrete random variables, we can define expectation for continuous random vari- ables. The definition is analogous: Just replace the summation with integration. Definition 4.3. The expectation of a continuous random variable X is (cid:90) E[X]= xf (x)dx. (4.3) X Ω Example 4.8. (Uniform random variable) Let X be a continuous random variable with PDF f (x)= 1 for a≤x≤b, and 0 otherwise. The expectation is X b−a (cid:90) (cid:90) b 1 1 (cid:90) b E[X]= xf (x)dx= x· dx= xdx X b−a b−a Ω a a (cid:124) (cid:123)(cid:122) (cid:125) =x2(cid:12) (cid:12)b 2 a 1 b2−a2 a+b = · = . b−a 2 2 180
4.2. EXPECTATION, MOMENT, AND VARIANCE Example4.9.(Exponentialrandomvariable)LetX beacontinuousrandomvariable with PDF f (x)=λe−λx, for x≥0. The expectation is X (cid:90) ∞ E[X]= xλe−λx dx 0 (cid:90) ∞ =− xde−λx 0 (cid:12)∞ (cid:90) ∞ =−xe−λx(cid:12) (cid:12) + e−λx dx (cid:12) 0 0 (cid:124) (cid:123)(cid:122) (cid:125) =0 =−1 e−λx(cid:12) (cid:12) (cid:12)∞ = 1 , λ (cid:12) λ 0 (cid:124) (cid:123)(cid:122) (cid:125) =−1 where the colored step is due to integration by parts. Ifafunctiong isappliedtotherandomvariableX,theexpectationcanbefoundusing the following theorem. Theorem 4.1. Let g :Ω→R be a function and X be a continuous random variable. Then (cid:90) E[g(X)]= g(x)f (x)dx. (4.4) X Ω Example 4.10. (Uniform random variable) Let X be a continuous random variable with f (x)= 1 for a≤x≤b, and 0 otherwise. If g(·)=(·)2, then X b−a (cid:90) E[g(X)]=E[X2]= x2f (x)dx X Ω 1 (cid:90) b a2+ab+b2 = · x2 dx= . b−a 3 a (cid:124) (cid:123)(cid:122) (cid:125) =b3−a3 3 PracticeExercise4.4.LetΘbeacontinuousrandomvariablewithPDFf (θ)= 1 Θ 2π for 0≤θ ≤2π and is 0 otherwise. Let Y =cos(ωt+Θ). Find E[Y]. Solution. Referring to Equation (4.4), the function g is g(θ)=cos(ωt+θ). 181
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Therefore, the expectation E[Y] is (cid:90) 2π E[Y]= cos(ωt+θ)f (θ)dθ Θ 0 1 (cid:90) 2π = cos(ωt+θ)dθ =0, 2π 0 where the last equality holds because the integral of a sinusoid over one period is 0. Practice Exercise 4.5. Let A⊆Ω. Let I (X) be an indicator function such that A (cid:40) 1, ifX ∈A, I (X)= A 0, ifX (cid:54)∈A. Find E[I (X)]. A Solution. The expectation is (cid:90) E[I (X)]= I (x)f (x)dx A A X Ω (cid:90) = f (x)dx X x∈A =P[X ∈A]. Sotheprobabilityof{X ∈A}canbeequivalentlyrepresentedintermsofexpectation. Practice Exercise 4.6. Is it true that E[1/X]=1/E[X]? Solution. No. This is because (cid:20) 1 (cid:21) (cid:90) 1 E = f (x)dx X x X Ω 1 (cid:54)= (cid:82) xf (x)dx Ω X 1 = . E[X] All the properties of expectation we learned in the discrete case can be translated to the continuous case. Specifically, we have that • E[aX]=aE[X]: A scalar multiple of a random variable will scale the expectation. • E[X+a]=E[X]+a:Constantadditionofarandomvariablewilloffsettheexpectation. • E[aX +b] = aE[X]+b: Affine transformation of a random variable will translate to the expectation. 182
4.2. EXPECTATION, MOMENT, AND VARIANCE Practice Exercise 4.7. Prove the above three statements. Solution. The third statement is just the sum of the first two statements, so we just need to show the first two: (cid:90) (cid:90) E[aX]= axf (x)dx=a xf (x)dx=aE[X], X X Ω Ω (cid:90) (cid:90) E[X+a]= (x+a)f (x)dx= xf (x)dx+a=E[X]+a. X X Ω Ω 4.2.2 Existence of expectation As we discussed in the discrete case, not all random variables have an expectation. Definition4.4. ArandomvariableX hasanexpectationifitis absolutelyintegrable, i.e., (cid:90) E[|X|]= |x|f (x)dx<∞. (4.5) X Ω Being absolutely integrable implies that the expectation is that E[|X|] is the upper bound of E[X]. Theorem 4.2. For any random variable X, |E[X]|≤E[|X|]. (4.6) Proof. Note that f (x)≥0. Therefore, X −|x|f (x)≤xf (x)≤|x|,f (x), ∀x. X X X Thus, integrating all three terms yields (cid:90) (cid:90) (cid:90) − |x|f (x)dx≤ xf (x)dx≤ |x|f (x)dx, X X X Ω Ω Ω which is equivalent to −E[|X|]≤E[X]≤E[|X|]. (cid:3) Example 4.11. Here is a random variable whose expectation is undefined. Let X be a random variable with PDF 1 f (x)= , x∈R. X π(1+x2) This random variable is called the Cauchy random variable. We can show that (cid:90) ∞ 1 1 (cid:90) ∞ x 1 (cid:90) 0 x E[X]= x· dx= dx+ dx. π(1+x2) π (1+x2) π (1+x2) −∞ 0 −∞ 183
CHAPTER 4. CONTINUOUS RANDOM VARIABLES The first integral gives (cid:90) ∞ x 1 (cid:12)∞ dx= log(1+x2)(cid:12) =∞, 0 (1+x2) 2 (cid:12) 0 and the second integral gives −∞. Since neither integral is finite, the expectation is undefined. We can also check the absolutely integrability criterion: (cid:90) ∞ 1 E[|X|]= |x|· dx π(1+x2) −∞ (a) (cid:90) ∞ x (cid:90) ∞ x = 2 dx≥2 dx π(1+x2) π(1+x2) 0 1 (b) (cid:90) ∞ x 1 (cid:12)∞ ≥ 2 dx= log(x)(cid:12) =∞, 1 π(x2+x2) π (cid:12) 1 where in (a) we use the fact that the function being integrated is even, and in (b) we lower-bound 1 ≥ 1 if x>1. 1+x2 x2+x2 4.2.3 Moment and variance The moment and variance of a continuous random variable can be defined analogously to the moment and variance of a discrete random variable, replacing the summations with integrations. Definition 4.5. The kth moment of a continuous random variable X is (cid:90) E[Xk]= xkf (x)dx. (4.7) X Ω Definition 4.6. The variance of a continuous random variable X is (cid:90) Var[X]=E[(X−µ)2]= (x−µ)2f (x)dx, (4.8) X Ω where µd =efE[X]. It is not difficult to show that the variance can also be expressed as Var[X]=E[X2]−µ2, because Var[X]=E[(X−µ)2] =E[X2]−2E[X]µ+µ2 =E[X2]−µ2. 184
4.3. CUMULATIVE DISTRIBUTION FUNCTION Practice Exercise 4.8. (Uniform random variable) Let X be a continuous random variable with PDF f (x)= 1 for a≤x≤b, and 0 otherwise. Find Var[X]. X b−a Solution. We have shown that E[X] = a+b and E[X2] = a2+ab+b2. Therefore, the 2 3 variance is Var[X]=E[X2]−E[X]2 a2+ab+b2 (cid:18) a+b(cid:19)2 = − 3 2 (b−a)2 = . 12 Practice Exercise 4.9. (Exponential random variable) Let X be a continuous ran- dom variable with PDF f (x)=λe−λx for x≥0, and 0 otherwise. Find Var[X]. X Solution. We have shown that E[X]= 1. The second moment is λ (cid:90) ∞ E[X2]= x2 λe−λx dx 0 (cid:90) ∞ =(cid:2) −x2e−λx(cid:3)∞ + 2xe−λx dx 0 0 2 (cid:90) ∞ = xλe−λx dx λ 0 2 1 2 = · = . λ λ λ2 Therefore, Var[X]=E[X2]−E[X]2 2 1 1 = − = . λ2 λ2 λ2 4.3 Cumulative Distribution Function When we discussed discrete random variables, we introduced the concept of cumulative distributionfunctions(CDFs).OneofthemotivationswasthatifweviewaPMFasatrain ofdeltafunctions,theyaretechnicallynotwell-definedfunctions.However,itturnsoutthat theCDFisalwaysawell-definedfunction.Inthissection,wewillcompletethestorybyfirst discussing the CDF for continuous random variables. Then, we will come back and show you how the CDF can be derived for discrete random variables. 185
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.3.1 CDF for continuous random variables Definition 4.7. Let X be a continuous random variable with a sample space Ω=R. The cumulative distribution function (CDF) of X is (cid:90) x F (x)d =efP[X ≤x]= f (x(cid:48))dx(cid:48). (4.9) X X −∞ TheinterpretationoftheCDFcanbeseenfromFigure4.5.GivenaPDFf ,theCDF X F evaluated at x is the integration of f from −∞ up to a point x. The integration of f X X X from −∞ to x is nothing but the area under the curve of f . Since f is non-negative, the X X larger value x we use to evaluate in F (x), the more area under the curve we are looking X at. In the extreme when x = −∞, we can see that F (−∞) = 0, and when x = +∞ we X (cid:82)∞ have that F (+∞)= f (x)dx=1. X −∞ X Figure 4.5: A CDF is the integral of the PDF. Thus, the height of a stem in the CDF corresponds to the area under the curve of the PDF. Practice Exercise 4.10.(Uniform random variable)LetX beacontinuousrandom variablewithPDFf (x)= 1 fora≤x≤b,andis0otherwise.FindtheCDFofX. X b−a Solution. The CDF of X is given by  0, x≤a,  F (x)= (cid:82)x f (x(cid:48))dx(cid:48) =(cid:82)x 1 dx(cid:48) = x−a, a<x≤b, X −∞ X a b−a b−a 1, x>b. Asyoucanseefromthispracticeexercise,weexplicitlybreaktheCDFintothreesegments. The first segment gives F (x) = 0 because for any x ≤ a, there is nothing to integrate, X since f (x) = 0 for any x ≤ a. Similarly, for the last segment, F (x) = 1 for all x > b X X because once x goes beyond b, the integration will cover all the non-zeros of f . Figure 4.6 X illustrates the PDF and CDF for this example. In MATLAB, we can generate the PDF and CDF using the commands pdf and cdf respectively.FortheparticularexampleshowninFigure4.6,thefollowingcodecanbeused. A similar set of commands can be implemented in Python. 186
4.3. CUMULATIVE DISTRIBUTION FUNCTION Figure 4.6: Example: f (x)=1/(b−a) for a≤x≤b. The CDF has three segments. X % MATLAB code to generate the PDF and CDF unif = makedist(’Uniform’,’lower’,-3,’upper’,4); x = linspace(-5, 10, 1500)’; f = pdf(unif, x); F = cdf(unif, x); figure(1); plot(x, f, ’LineWidth’, 6); figure(2); plot(x, F, ’LineWidth’, 6); # Python code to generate the PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,10,1500) f = stats.uniform.pdf(x,-3,4) F = stats.uniform.cdf(x,-3,4) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() Practice Exercise 4.11. (Exponential random variable) Let X be a continuous random variable with PDF f (x)=λe−λx for x≥0, and 0 otherwise. Find the CDF X of X. Solution. Clearly, for x<0, we have F (x)=0. For x≥0, we can show that X (cid:90) x (cid:90) x F (x)= f (x(cid:48))dx(cid:48) = λe−λx(cid:48) dx(cid:48) =1−e−λx. X X 0 0 Therefore, the complete CDF is (see Figure 4.7 for illustration): (cid:40) 0, x<0, F (x)= X 1−e−λx, x≥0. 187
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.7: Example: f (x)=λe−λx for x≥0. The CDF has two segments. X The MATLAB code and Python code to generate this figure are shown below. % MATLAB code to generate the PDF and CDF pd = makedist(’exp’,2); x = linspace(-5, 10, 1500)’; f = pdf(pd, x); F = cdf(pd, x); figure(1); plot(x, f, ’LineWidth’, 6); figure(2); plot(x, F, ’LineWidth’, 6); # Python code to generate the PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,10,1500) f = stats.expon.pdf(x,2) F = stats.expon.cdf(x,2) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() 4.3.2 Properties of CDF LetusnowdescribethepropertiesofaCDF.Ifwecomparethesewiththoseforthediscrete cases, we see that the continuous cases simply replace the summations by integrations. Therefore, we should expect to inherit most of the properties from the discrete cases. Proposition 4.1. Let X be a random variable (either continuous or discrete), then the CDF of X has the following properties: (i) The CDF is nondecreasing. (ii) The maximum of the CDF is when x=∞: F (+∞)=1. X (iii) The minimum of the CDF is when x=−∞: F (−∞)=0. X 188
4.3. CUMULATIVE DISTRIBUTION FUNCTION Proof. For (i), we notice that F (x)=(cid:82)x f (x(cid:48))dx(cid:48). Therefore, if s≤t then X −∞ X (cid:90) s (cid:90) t F (s)= f (x(cid:48))dx(cid:48) ≤ f (x(cid:48))dx(cid:48) =F (t). X X X X −∞ −∞ Thus it shows that F is nondecreasing. (It does not need to be increasing because a CDF X can have a steady state.) For (ii) and (iii), we can show that (cid:90) +∞ (cid:90) −∞ F (+∞)= f (x(cid:48))dx(cid:48) =1, and F (−∞)= f (x(cid:48))dx(cid:48) =0. (cid:3) X X X X −∞ −∞ Example4.12.WecanshowthattheCDFwederivedfortheuniformrandomvariable satisfies these three properties. To see this, we note that x−a F (x)= , a≤x≤b. X b−a The derivative of this function F(cid:48) (x) = 1 > 0 for a ≤ x ≤ b. Also, note that X b−a F (x) = 0 for x < a and x > b, so F is nondecreasing. The other two properties X X follow because if x = b, then F (b) =1, and if x = a then F (a) =0. Together with X X the nondecreasing property, we show (ii) and (iii). Proposition 4.2. Let X be a continuous random variable. If the CDF F is contin- X uous at any a≤x≤b, then P[a≤X ≤b]=F (b)−F (a). (4.10) X X Proof. The proof follows from the definition of the CDF, which states that (cid:90) b (cid:90) a F (b)−F (a)= f (x(cid:48))dx(cid:48)− f (x(cid:48))dx(cid:48) X X X X −∞ −∞ (cid:90) b = f (x(cid:48))dx(cid:48) =P[a≤X ≤b]. (cid:3) X a This result provides a very handy tool for calculating the probability of an event a≤X ≤b using the CDF. It says that P[a ≤ X ≤ b] is the difference between F (b) and X F (a). So, if we are given F , calculating the probability of a ≤ X ≤ b just involves X X evaluating the CDF at a and b. The result also shows that for a continuous random vari- able X, P[X =x ]=F (x )−F (x )=0. This is consistent with our arguments from the 0 X 0 X 0 measure’s point of view. Example 4.13.(Exponential random variable)Weshowedthattheexponentialran- dom variable X with a PDF f (x) = λe−λx for x ≥ 0 (and f (x) = 0 for x < 0) X X has a CDF given by F (x) = 1−e−λx for x ≥ 0. Suppose we want to calculate the X 189
CHAPTER 4. CONTINUOUS RANDOM VARIABLES probability P[1≤X ≤3]. Then the PDF approach gives us (cid:90) 3 (cid:90) 3 (cid:12)3 P[1≤X ≤3]= f X(x)dx= λe−λx dx=−e−λx(cid:12) (cid:12) =e−3λ−e−λ. (cid:12) 1 1 1 If we take the CDF approach, we can show that P[1≤X ≤3]=F (3)−F (1) X X =(1−e−λ)−(1−e−3λ)=e−3λ−e−λ, which yields the same as the PDF approach. Example 4.14. Let X be a random variable with PDF f (x) = 2x for 0 ≤ x ≤ 1, X and is 0 otherwise. We can show that the CDF is (cid:90) x (cid:90) x (cid:12)x F X(x)= f X(t)dt= 2t dt=t2(cid:12) (cid:12) =x2, 0≤x≤1. (cid:12) 0 0 0 Therefore, to compute the probability P[1/3≤X ≤1/2], we have (cid:20) 1 1(cid:21) (cid:18) 1(cid:19) (cid:18) 1(cid:19) (cid:18) 1(cid:19)2 (cid:18) 1(cid:19)2 5 P ≤X ≤ =F −F = − = . 3 2 X 2 X 3 2 3 36 (cid:3) ACDFcanbeusedforbothcontinuousanddiscreterandomvariables.However,before we can do that, we need a tool to handle the discontinuities. The following definition is a summary of the three types of continuity. Definition 4.8. A function F (x) is said to be X • Left-continuous at x=b if F (b)=F (b−)d =ef lim F (b−h); X X h→0 X • Right-continuous at x=b if F (b)=F (b+)d =ef lim F (b+h); X X h→0 X • Continuous at x = b if it is both right-continuous and left-continuous at x = b. In this case, we have lim F (b−h)= lim F (b+h)=F(b). X X h→0 h→0 Inthisdefinition,thestepsizeh>0isshrinkingtozero.Thepointb−hstaysattheleftof b,andb+hstaysattherightofb.Thus,ifwesetthelimith→0,b−hwillapproachapoint b− whereas b+h will approach a point b+. If it happens that F (b−)=F (b) then we say X X that F is left-continuous at b. If F (b+)=F (b) then we say that F is right-continuous X X X X at b. These are summarized in Figure 4.8. WheneverF hasadiscontinuouspoint,itcanbeleft-continuous,right-continuous,or X neither. (“Neither” happens if F (b) take a value other than F (b+) or F (b−). You can X X X 190
4.3. CUMULATIVE DISTRIBUTION FUNCTION Figure 4.8: The definition of left- and right-continuous at a point b. always create a nasty function that satisfies this condition.) For continuous functions, it is necessary that F (b−)=F (b+). If this happens, there is no gap between the two points. X X Theorem 4.3. For any random variable X (discrete or continuous), F (x) is always X right-continuous. That is, F (b)=F (b+)d =ef lim F (b+h) (4.11) X X X h→0 Right-continuous means that if F (x) is piecewise, it must have a solid left end and an X empty right end. Figure 4.9 shows an example of a valid CDF and an invalid CDF. Figure 4.9: A CDF must be right-continuous. The reason why F is always right-continuous is that the inequality X ≤ x has a X closed right-hand limit. Imagine the following situation: A discrete random variable X has four states: 1,2,3,4. Then, “3+h” (cid:88) lim F (3+h)= lim p (k)=p (1)+p (2)+p (3)=F (3). X X X X X X h→0 h→0 k=1 Similarly, if you have a continuous random variable X with a PDF f , then X (cid:90) b+h (cid:90) b lim F (b+h)= lim f (t)dt= f (t)dt=F (b). X X X X h→0 h→0 −∞ −∞ 191
CHAPTER 4. CONTINUOUS RANDOM VARIABLES In other words, the “≤” ensures that the rightmost state is included. If we defined CDF using<,wewouldhavegottenleft-handcontinuous,butthiswouldbeinconvenientbecause the < requires us to deal with limits whenever we evaluate X <x. Theorem 4.4. For any random variable X (discrete or continuous), P[X =b] is (cid:40) F (b)−F (b−), if F is discontinuous at x=b P[X =b]= X X X (4.12) 0, otherwise. This proposition states that when F (x) is discontinuous at x = b, then P[X = b] is X the difference between F (b) and the limit from the left. In other words, the height of the X gap determines the probability at the discontinuity. If F (x) is continuous at x = b, then X F (b)=lim F (b−h) and so P[X =b]=0. X h→0 X Figure 4.10: Illustration of Equation (4.12). Since the CDF is discontinuous at a point x=b, the gap F (b)−F (b−) will define the probability P[X =b]. X X Example 4.15. Consider a random variable X with a PDF  x, 0≤x≤1,  f (x)= 1, x=3, X 2 0, otherwise. The CDF F (x) will consist of a few segments. The first segment is 0 ≤ x < 1. We X can show that (cid:90) x (cid:90) x t2(cid:12) (cid:12)x x2 F X(x)= f X(t)dt= t dt= 2(cid:12) (cid:12) = 2 , 0≤x<1. 0 0 0 The second segment is when 1 ≤ x < 3. Since there is no new f to integrate, the X CDF stays at F (x)=F (1)= 1 for 1≤x<3. The third segment is x>3. Because X X 2 this range has covered the entire sample space, we have F (x) = 1 for x > 3. How X about x=3? We can show that F (3)=F (3+)=1. X X 192
4.3. CUMULATIVE DISTRIBUTION FUNCTION Therefore, to summarize, the CDF is  0, x<0, x2, 0≤x<1, F (x)= 2 X 11 2 ,, 1 x≤ ≥x 3.<3, A graphical illustration is shown in Figure 4.11. Figure 4.11: An example of converting a PDF to a CDF. 4.3.3 Retrieving PDF from CDF Thus far, we have only seen how to obtain F (x) from f (x). In order to go in the reverse X X direction, we recall the fundamental theorem of calculus. This states that if a function f is continuous, then d (cid:90) x f(x)= f(t)dt dx a for some constant a. Using this result for CDF and PDF, we have the following: Theorem 4.5. The probability density function (PDF) is the derivative of the cu- mulative distribution function (CDF): dF (x) d (cid:90) x f (x)= X = f (x(cid:48))dx(cid:48), (4.13) X dx dx X −∞ provided F is differentiable at x. If F is not differentiable at x=x , then, X X 0 f (x )=P[X =x ]δ(x−x ). X 0 0 0 Example 4.16. Consider a CDF (cid:40) 0, x<0, F (x)= X 1− 1e−2x, x≥0. 4 We want to find the PDF f (x). To do so, we first show that F (0) = 3. This X X 4 193
CHAPTER 4. CONTINUOUS RANDOM VARIABLES corresponds to a discontinuity at x=0, as shown in Figure 4.12. Figure 4.12: An example of converting a PDF to a CDF. Because of the discontinuity, we need to consider three cases: dFX(x), x<0,  dx f (x)= P[X =0]δ(x−0), x=0, X dFX(x), x>0. dx When x<0, F (x)=0, so dFX(x) =0. X dx When x>0, F (x)=1− 1e−2x, so X 4 dF (x) 1 X = e−2x. dx 2 When x = 0, the probability P[X = 0] is determined by the gap between the solid dot and the empty dot. This yields P[X =0]=F (0)− lim F (0−h) X X h→0 3 3 = −0= . 4 4 Therefore, the overall PDF is  0, x<0,  f (x)= 3δ(x−0), x=0, X 4 1e−2x, x>0. 2 Figure 4.12 illustrates this example. 4.3.4 CDF: Unifying discrete and continuous random variables The CDF is always a well-defined function. It is integrable everywhere. If the underlying randomvariableiscontinuous,theCDFisalsocontinuous.Iftheunderlyingrandomvariable is discrete, the CDF is a staircase function. We have seen enough CDFs for continuous random variables. Let us (re)visit a few discrete random variables. 194
4.3. CUMULATIVE DISTRIBUTION FUNCTION Example 4.17. (Geometric random variable) Consider a geometric random variable with PMF p (k)=(1−p)k−1p, for k =1,2,.... X Figure 4.13: PMF and CDF of a geometric random variable. We can show that the CDF is k (cid:88) F (k)= p ((cid:96)) X X (cid:96)=1 k (cid:88) = (1−p)(cid:96)−1p (cid:96)=1 1−(1−p)k =p· 1−(1−p) =1−(1−p)k. For a sanity check, we can try to retrieve the PMF from the CDF: p (k)=F (k)−F (k−1) X X X =(1−(1−p)k)−(1−(1−p)k−1) =(1−p)k−1p. A graphical portrayal of this example is shown in Figure 4.13. If we treat the PMFs as delta functions in the above example, then the continuous definition also applies. Since the CDF is a piecewise constant function, the derivative is exactlyadeltafunction.Forsomeproblems,itiseasiertostartwithCDFandthencompute the PMF or PDF. Here is an example. Example 4.18. Let X , X and X be three independent discrete random variables 1 2 3 with sample space Ω = {1,2,...,10}. Define X = max{X ,X ,X }. We want to 1 2 3 find the PMF of X. To tackle this problem, we first observe that the PMF for X is 1 p (k)= 1 . Thus, the CDF of X is X1 10 1 k (cid:88) k F (k)= p ((cid:96))= . X1 X1 10 (cid:96)=1 195
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Then, we can show that the CDF of X is F (k)=P[X ≤k]=P[max{X ,X ,X }≤k] X 1 2 3 ( =a)P[X ≤k ∩X ≤k ∩X ≤k] 1 2 3 ( =b)P[X ≤k]P[X ≤k]P[X ≤k] 1 2 3 (cid:18) k (cid:19)3 = , 10 wherein(a)weusethefactthatmax{X ,X ,X }≤k ifandonlyifallthreeelements 1 2 3 are less than k, and in (b) we use independence. Consequently, the PMF of X is (cid:18) k (cid:19)3 (cid:18) k−1(cid:19)3 p (k)=F (k)−F (k−1)= − . X X X 10 10 What is a CDF? • CDF is F (x)=P[X ≤x]. It is the cumulative sum of the PMF/PDF. X • CDF is either a staircase function, a smooth function, or a hybrid. Unlike a PDF, which is not defined for discrete random variables, the CDF is always well defined. d • CDF −d→x PDF. (cid:82) • CDF ←− PDF. • Gap of jump in CDF = height of delta in PDF. 4.4 Median, Mode, and Mean There are three statistical quantities that we are frequently interested in: mean, mode, and median. We all know how to compute these from a dataset. For example, to compute the median of a dataset, we sort the data and pick the number that sits in the 50th percentile. However, the median computed in this way is the empirical median, i.e., it is a value computed from a particular dataset. If the data is generated from a random variable (with a given PDF), how do we compute the mean, median, and mode? 4.4.1 Median Imagine you have a sequence of numbers as shown below. n 1 2 3 4 5 6 7 8 9 ··· 100 x 1.5 2.5 3.1 1.1 −0.4 −4.1 0.5 2.2 −3.4 ··· −1.4 n How do we compute the median? We first sort the sequence (either in ascending order or descending order), and then pick the middle one. On computer, we permute the samples {x ,x ,...,x }=sort{x ,x ,...,x }, 1(cid:48) 2(cid:48) N(cid:48) 1 2 N 196
4.4. MEDIAN, MODE, AND MEAN such that x <x <...<x is ordered. The median is the one positioned at the middle. 1(cid:48) 2(cid:48) N(cid:48) There are, of course, built-in commands such as median in MATLAB and np.median in Python to perform the median operation. Now,howdowecomputethemedianifwearegivenarandomvariableX withaPDF f (x)? The answer is by integrating the PDF. X Definition 4.9. Let X be a continuous random variable with PDF f . The median X of X is a point c∈R such that (cid:90) c (cid:90) ∞ f (x)dx= f (x)dx. (4.14) X X −∞ c (cid:82)c Why is the median defined in this way? This is because f (x) dx is the area under −∞ X (cid:82)∞ the curve on the left of c, and f (x) dx is the area under the curve on the right of c. c X The area under the curve tells us the percentage of numbers that are less than the cutoff. Therefore, if the left area equals the right area, then c must be the median. How to find the median from the PDF • Find a point c that separates the PDF into two equal areas Figure 4.14: [Left] The median is computed as the point such that the two areas under the curve are equal. [Right] The median is computed as the point such that F hits 0.5. X The median can also be evaluated from the CDF as follows. Theorem 4.6. The median of a random variable X is the point c such that 1 F (c)= . (4.15) X 2 Proof. Since F (x)=(cid:82)x f (x(cid:48))dx(cid:48), we have X −∞ X (cid:90) c (cid:90) ∞ F (c)= f (x)dx= f (x)dx=1−F (c). X X X X −∞ c Rearranging the terms shows that F (c)= 1. (cid:3) X 2 197
CHAPTER 4. CONTINUOUS RANDOM VARIABLES How to find median from CDF • Find a point c such that F (c)=0.5. X Example 4.19. (Uniform random variable) Let X be a continuous random variable with PDF f (x) = 1 for a ≤ x ≤ b, and is 0 otherwise. We know that the CDF of X b−a X is F (x) = x−a for a ≤ x ≤ b. Therefore, the median of X is the number c ∈ R X b−a suchthatF (c)= 1.SubstitutingintotheCDFyields c−a = 1,whichgivesc= a+b. X 2 b−a 2 2 Example 4.20. (Exponential random variable) Let X be a continuous random vari- able with PDF f (x) = λe−λx for x ≥ 0. We know that the CDF of X is F (x) = X X 1−e−λx for x ≥ 0. The median of X is the point c such that F (c) = 1. This gives X 2 1−e−λc = 1, which is c= log2. 2 λ 4.4.2 Mode The mode is the peak of the PDF. We can see this from the definition below. Definition 4.10. Let X be a continuous random variable. The mode is the point c such that f (x) attains the maximum: X d c=argmax f (x)=argmax F (x). (4.16) X dx X x∈Ω x∈Ω The second equality holds because f (x) = F(cid:48) (x) = d (cid:82)x f (t) dt. A pictorial illustra- X X dx −∞ X tionofmodeisgiveninFigure4.15.Notethatthemodeofarandomvariableisnotunique, e.g., a mixture of two identical Gaussians with different means has two modes. Figure4.15:[Left]ThemodeappearsatthepeakofthePDF.[Right]Themodeappearsatthesteepest slope of the CDF. 198
4.4. MEDIAN, MODE, AND MEAN How to find mode from PDF • Find a point c such that f (c) is maximized. X How to find mode from CDF • Continuous: Find a point c such that F (c) has the steepest slope. X • Discrete: Find a point c such that F (c) has the biggest gap in a jump. X Example 4.21.LetX beacontinuousrandomvariablewithPDFf (x)=6x(1−x) X for 0≤x≤1. The mode of X happens at argmax f (x). To find this maximum, we X x take the derivative of f . This gives X d d 0= f (x)= 6x(1−x)=6(1−2x). dx X dx Setting this equal to zero yields x= 1. 2 To ensure that this point is a maximum, we take the second-order derivative: d2 d f (x)= 6(1−2x)=−12<0. dx2 X dx Therefore, we conclude that x = 1 is a maximum point. Hence, the mode of X is 2 x= 1. 2 4.4.3 Mean We have defined the mean as the expectation of X. Here, we show how to compute the expectation from the CDF. To simplify the demonstration, let us first assume that X >0. Lemma 4.1. Let X >0. Then E[X] can be computed from F as X (cid:90) ∞ E[X]= (1−F (t)) dt. (4.17) X 0 Proof. The trick is to change the integration order: (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ (1−F (t)) dt= [1−P[X ≤t]] dt= P[X >t]dt X 0 0 0 (cid:90) ∞(cid:90) ∞ (cid:90) ∞(cid:90) x (a) = f (x)dxdt = f (x)dtdx X X 0 t 0 0 (cid:90) ∞(cid:90) x (cid:90) ∞ = dtf (x)dx= xf (x)dx=E[X]. X X 0 0 0 Here, step (a) is due to the change of integration order. See Figure 4.16 for an illustration. (cid:3) We draw a picture to illustrate the above lemma. As shown in Figure 4.17, the mean of a positive random variable X >0 is equivalent to the area above the CDF. 199
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.16: The double integration can be evaluated by x then t, or t then x. Figure4.17:ThemeanofapositiverandomvariableX >0canbecalculatedbyintegratingtheCDF’s complement. Lemma 4.2. Let X <0. Then E[X] can be computed from F as X (cid:90) 0 E[X]= F (t)dt. (4.18) X −∞ Proof. The idea here is also to change the integration order. (cid:90) 0 (cid:90) 0 (cid:90) 0 (cid:90) t F (t)dt= P[X ≤t]dt= f (x)dxdt X X −∞ −∞ −∞ −∞ (cid:90) 0 (cid:90) 0 (cid:90) 0 = f (x)dtdx= xf (x)dx=E[X]. X X −∞ x −∞ (cid:3) Theorem 4.7. The mean of a random variable X can be computed from the CDF as (cid:90) ∞ (cid:90) 0 E[X]= (1−F (t)) dt− F (t)dt. (4.19) X X 0 −∞ 200
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Proof.ForanyrandomvariableX,wecanpartitionX =X+−X− whereX+ andX− are the positive and negative parts, respectively. Then, the above two lemmas will give us E[X]=E[X+−X−]=E[X+]−E[X−] (cid:90) ∞ (cid:90) 0 = (1−F (t)) dt− F (t)dt. X X 0 −∞ (cid:3) AsillustratedinFigure 4.18,thisequationisequivalenttocomputingtheareasabove and below the CDF and taking the difference. Figure 4.18: The mean of a random variable X can be calculated by computing the area in the CDF. How to find the mean from the CDF • A formula is given by Equation (4.20): (cid:90) ∞ (cid:90) 0 E[X]= (1−F (t)) dt− F (t)dt. (4.20) X X 0 −∞ • This result is not commonly used, but the proof technique of switching the inte- gration order is important. 4.5 Uniform and Exponential Random Variables Therearemanyusefulcontinuousrandomvariables.Inthissection,wediscusstwoofthem: uniform random variables and exponential random variables. In the next section, we will discuss the Gaussian random variables. Similarly to the way we discussed discrete random variables, we take a generative / synthesis perspective when studying continuous random variables. We assume we have access to the PDF of the random variables so we can derive the theoretical mean and variance. The opposite direction, namely inferring the underlying model parameters from a dataset, will be discussed later. 201
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.5.1 Uniform random variables Definition 4.11. Let X be a continuous uniform random variable. The PDF of X is (cid:40) 1 , a≤x≤b, f (x)= b−a (4.21) X 0, otherwise, where [a,b] is the interval on which X is defined. We write X ∼Uniform(a,b) to mean that X is drawn from a uniform distribution on an interval [a,b]. 3 1.2 2.5 1 2 0.8 1.5 0.6 1 0.4 0.5 0.2 0 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) PDF (b) CDF Figure 4.19: The PDF and CDF of X ∼Uniform(0.2,0.6). The shape of the PDF of a uniform random variable is shown in Figure 4.19. In this figure, we assume that the random variables X ∼ Uniform(0.2,0.6) are taken from the sample space Ω=[0,1]. Note that the height of the uniform distribution is greater than 1, since (cid:40) 1 =2.5, 0.2≤x≤0.6, f (x)= 0.6−0.2 X 0, otherwise. ThereisnothingwrongwiththisPDF,becausef (x)istheprobabilityperunitlength.Ifwe X integratef (x)overanysub-intervalbetween0.2and0.6,wecanshowthattheprobability X is between 0 and 1. The CDF of a uniform random variable can be determined by integrating f (x): X (cid:90) x F (x)= f (t)dt X X −∞ (cid:90) x 1 = dt b−a a x−a = , a≤x≤b. b−a 202
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Therefore, the complete CDF is  0, x<a,  F (x)= x−a, a≤x≤b, X b−a 1, x>b. ThecorrespondingCDFforthePDFweshowedinFigure4.19(a)isshowninFigure4.19(b). It can be seen that although the height of the PDF exceeds 1, the CDF grows linearly and saturates at 1. Remark. The uniform distribution can also be defined for discrete random variables. In this case, the probability mass function is given by 1 p (k)= , k =a,a+1,...,b. X b−a+1 Thepresenceof“1”inthedenominatorofthePMFisbecausek runsfromatob,including the two endpoints. InMATLABandPython,generatinguniformrandomnumberscanbedonebycalling commands unifrnd (MATLAB), and stats.uniform.rvs (Python). For discrete uniform random variables, in MATLAB the command is unidrnd, and in Python the command is stats.randint. % MATLAB code to generate 1000 uniform random numbers a = 0; b = 1; X = unifrnd(a,b,[1000,1]); hist(X); # Python code to generate 1000 uniform random numbers import scipy.stats as stats a = 0; b = 1; X = stats.uniform.rvs(a,b,size=1000) plt.hist(X); To compute the empirical average and variance of the random numbers in MATLAB wecancallthecommandmeanandvar.ThecorrespondingcommandinPythonisnp.mean and np.var. We can also compute the median and mode, as shown below. % MATLAB code to compute empirical mean, var, median, mode X = unifrnd(a,b,[1000,1]); M = mean(X); V = var(X); Med = median(X); Mod = mode(X); # Python code to compute empirical mean, var, median, mode X = stats.uniform.rvs(a,b,size=1000) M = np.mean(X) V = np.var(X) 203
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Med = np.median(X) Mod = stats.mode(X) The mean and variance of a uniform random variable are given by the theorem below. Theorem 4.8. If X ∼Uniform(a,b), then a+b (b−a)2 E[X]= and Var[X]= . (4.22) 2 12 Proof. We have derived these results before. Here is a recap for completeness: (cid:90) ∞ (cid:90) b x a+b E[X]= xf (x)dx= dx= , X b−a 2 −∞ a (cid:90) ∞ (cid:90) b x2 a2+ab+b2 E[X2]= x2f (x)dx= dx= , X b−a 3 −∞ a (b−a)2 Var[X]=E[X2]−E[X]2 = . 12 (cid:3) The result should be intuitive because it says that the mean is the midpoint of the PDF. Whenwillweencounterauniformrandomvariable?Uniformrandomvariablesareone of the most elementary continuous random variables. Given a uniform random variable, we can construct any random variable by using an appropriate transformation. We will discuss this technique as part of our discussion about generating random numbers. In MATLAB, computing the mean and variance of a uniform random variable can be done using the command unifstat. The Python coommand is stats.uniform.stats. % MATLAB code to compute mean and variance a = 0; b = 1; [M,V] = unifstat(a,b) # Python code to compute mean and variance import scipy.stats as stats a = 0; b = 1; M, V = stats.uniform.stats(a,b,moments=’mv’) To evaluate the probability P[(cid:96) ≤ X ≤ u] for a uniform random variable, we can call unifcdf in MATLAB and % MATLAB code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; F = unifcdf(0.3,a,b) - unifcdf(0.2,a,b) 204
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES # Python code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; F = stats.uniform.cdf(0.3,a,b)-stats.uniform.cdf(0.2,a,b) An alternative is to define an object rv = stats.uniform, and call the CDF attribute: # Python code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; rv = stats.uniform(a,b) F = rv.cdf(0.3)-rv.cdf(0.2) 4.5.2 Exponential random variables Definition 4.12. Let X be an exponential random variable. The PDF of X is (cid:40) λe−λx, x≥0, f (x)= (4.23) X 0, otherwise, where λ>0 is a parameter. We write X ∼Exponential(λ) to mean that X is drawn from an exponential distribution of parameter λ. In this definition, the parameter λ of the exponential random variable determines the rate of decay. A large λ implies a faster decay. The PDF of an exponential random variable is illustrated in Figure 4.20. We show two values of λ. Note that the initial value f (0) is X f (0)=λe−λ0 =λ. X Therefore, as long as λ>1, f (0) will exceed 1. X The CDF of an exponential random variable can be determined by (cid:90) x F (x)= f (t)dt X X −∞ (cid:90) x = λe−λt dt=1−e−λx, x≥0. 0 Therefore, if we consider the entire real line, the CDF is (cid:40) 0, x<0, F (x)= X 1−e−λx, x≥0. The corresponding CDFs for the PDFs shown in Figure 4.20(a) are shown in Fig- ure 4.20(b).Forlargerλ,thePDFf (x)decaysfasterbuttheCDFF (x)increasesfaster. X X 205
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 6 1.2 = 2 5 = 5 1 4 0.8 3 0.6 2 0.4 1 0.2 = 2 = 5 0 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) PDF (b) CDF Figure 4.20: (a) The PDF and (c) the CDF of X ∼Exponential(λ). In MATLAB, the code used to generate Figure 4.20(a) is shown below. There are multiplewaysofdoingthis.Analternativewayistocallexppdf,whichwillreturnthesame result. In Python, the corresponding command is stats.expon.pdf. Note that in Python the parameter λ is specified in scale option. % MATLAB code to plot the exponential PDF lambda1 = 1/2; lambda2 = 1/5; x = linspace(0,1,1000); f1 = pdf(’exp’,x, lambda1); f2 = pdf(’exp’,x, lambda2); plot(x, f1, ’LineWidth’, 4, ’Color’, [0 0.2 0.8]); hold on; plot(x, f2, ’LineWidth’, 4, ’Color’, [0.8 0.2 0]); # Python code to plot the exponential PDF lambd1 = 1/2 lambd2 = 1/5 x = np.linspace(0,1,1000) f1 = stats.expon.pdf(x,scale=lambd1) f2 = stats.expon.pdf(x,scale=lambd2) plt.plot(x, f1) plt.plot(x, f2) To plot the CDF, we replace pdf by cdf. Similarly, in Python we replace expon.pdf by expon.cdf. % MATLAB code to plot the exponential CDF F = cdf(’exp’,x, lambda1); plot(x, F, ’LineWidth’, 4, ’Color’, [0 0.2 0.8]); # Python code to plot the exponential CDF F = stats.expon.cdf(x,scale=lambd1) plt.plot(x, F) 206
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Theorem 4.9. If X ∼Exponential(λ), then 1 1 E[X]= and Var[X]= . (4.24) λ λ2 Proof. We have discussed this proof before. Here is a recap for completeness: (cid:90) ∞ (cid:90) ∞ E[X]= xf (x)dx= λxe−λx dx X −∞ 0 (cid:90) ∞ =− xde−λx 0 (cid:12)∞ (cid:90) ∞ 1 =−xe−λx(cid:12) + e−λx dx= , (cid:12) 0 0 λ (cid:90) ∞ (cid:90) ∞ E[X2]= x2f (x)dx= λx2e−λx dx X −∞ 0 (cid:90) ∞ =− x2de−λx 0 (cid:12)∞ (cid:90) ∞ =−x2e−λx(cid:12) + 2xe−λx dx (cid:12) 0 0 2 2 =0+ E[X]= . λ λ2 Thus, Var[X]=E[X2]−E[X]2 = 1 . λ2 (cid:3) ComputingthemeanandvarianceofanexponentialrandomvariableinMATLABand Python follows the similar procedures that we described above. 4.5.3 Origin of exponential random variables Exponential random variables are closely related to Poisson random variables. Recall that the definition of a Poisson random variable is a random variable that describes the number ofeventsthathappeninacertainperiod,e.g.,photonarrivals,numberofpedestrians,phone calls, etc. We summarize the origin of an exponential random variable as follows. What is the origin of exponential random variables? • Anexponentialrandomvariableistheinterarrivaltimebetweentwoconsecutive Poisson events. • Thatis,anexponentialrandomvariableishowmuchtimeittakestogofromN Poisson counts to N +1 Poisson counts. An example will clarify this concept. Imagine that you are waiting for a bus, as illus- tratedinFigure 4.21.Passengersarriveatthebusstopwithanarrivalrateλperunittime. Thus, for some time t, the average number of people that arrive is λt. Let N be a random 207
CHAPTER 4. CONTINUOUS RANDOM VARIABLES variable denoting the number of people. We assume that N is Poisson with a parameter λt. That is, for any duration t, the probability of observing n people follows the PMF (λt)n P[N =n]= e−λt. n! Figure 4.21: For any fixed period of time t, the number of people N is modeled as a Poisson random variable with a parameter λt. Figure 4.22: TheinterarrivaltimeT betweentwoconsecutivePoissoneventsisanexponentialrandom variable. LetT betheinterarrivaltimebetweentwopeople,bywhichwemeanthetimebetween twoconsecutivearrivals,asshowninFigure 4.22.NotethatT isarandomvariablebecause T depends on N, which is itself a random variable. To find the PDF of T, we first find the CDF of T. We note that P[T >t]( =a)P[interarrival time >t] ( =b)P[no arrival in t]( =c)P[N =0]= (λt)0 e−λt =e−λt. 0! In this set of arguments, (a) holds because T is the interarrival time, and (b) holds be- cause interarrival time is between two consecutive arrivals. If the interarrival time is larger than t, there is no arrival during the period. Equality (c) holds because N is the number of passengers. Since P[T >t]=1−F (t), where F (t) is the CDF of T, we can show that T T F (t)=1−e−λt, T d f (t)= F (t)=λe−λt. T dt T Therefore, the interarrival time T follows an exponential distribution. SinceexponentialrandomvariablesaretightlyconnectedtoPoissonrandomvariables, weshouldexpectthemtobeusefulformodelingtemporalevents.Wediscusstwoexamples. 208
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES 4.5.4 Applications of exponential random variables Example 4.22. (Photon arrivals) Single-photon image sensors are designed to op- erate in the photon-limited regime. The number-one goal of using these sensors is to count the number of arriving photons precisely. However, for some applications not all single-photon image sensors are used to count photons. Some are used to measure the time between two photon arrivals, such as time-of-flight systems. In this case, we are interested in measuring the time it takes for a pulse to bounce back to the sensor. The more time it takes for a pulse to come back, the greater the distance between the object and the sensor. Other applications utilize the time information. For example, high-dynamic-range imaging can be achieved by recording the time between two pho- ton arrivals because brighter regions have a higher Poisson rate λ and darker regions have a lower λ. Thefigureaboveillustratesanexampleofhigh-dynamic-rangeimaging.Whenthe sceneisbright,thelargeλwillgeneratemorephotons.Therefore,theinterarrivaltime between the consecutive photons will be relatively short. If we plot the histogram of theinterarrivaltime,weobservethatmostoftheinterarrivaltimewillbeconcentrated atsmallvalues.Darkregionsbehaveintheoppositemanner.Theinterarrivaltimewill typically be much longer. In addition, because there is more variation in the photon arrival times, the histogram will look shorter and wider. Nevertheless, both cases are modeled by the exponential random variable. Example4.23.(Energy-efficientescalator)Manyairportstodayhaveinstalledvariable- speedescalators.Theseescalatorschangetheirspeedsaccordingtothetraffic.Ifthere are no passengers for more than a certain period (say, 60 seconds), the escalator will switch from the full-speed mode to the low-speed mode. For moderately busy esca- lators, the variable-speed configuration can save energy. The interesting data-science problem is to determine, given a traffic pattern, e.g., the one shown in Figure 4.23, whether we can predict the amount of energy savings? We will not dive into the details of this problem, but we can briefly discuss the principle.Considerafixedarrivalrateλ(say,theaveragefrom07:00to08:00).Thein- terarrival time, according to our discussion above, follows an exponential distribution. 209
CHAPTER 4. CONTINUOUS RANDOM VARIABLES So we know that f (t)=λe−λt. T Supposethattheescalatorswitchestolow-speedmodewhentheinterarrivaltime exceeds τ. Then we can define a new variable Y to denote the amount of time that the escalator will operate in the low-speed mode. This new variable is (cid:40) T −τ, T >τ, Y = 0, T ≤τ. In other words, if the interarrival time T is more than τ, then the amount of time saved Y takes the value T −τ, but if the interarrival time is less than τ, then there is no saving. Figure4.23:Thevariable-speedescalatorproblem.[Left]Wemodelthepassengersasindependent Poissonarrivals.Thus,theinterarrivaltimeisexponential.[Right]Ahypotheticalpassengerarrival rate (number of people per minute), from 06:00 to 23:00. Figure 4.24: The escalator problem requires modeling the cutoff threshold τ such that if T >τ, thesavingsareY =T−τ.IfT <τ,thenY =0.Theleft-handsideofthefigureshowshowthe PDF of Y is constructed. The PDF of Y can be computed according to Figure 4.24. There are two parts to the calculation. When Y =0, there is a probability mass such that (cid:90) τ (cid:90) τ f (0)=P[Y =0]= f (t)dt= λe−λt dt=1−e−λτ. Y T 0 0 For other values of y, we can show that f (y)=f (y+τ)=λe−λ(y+τ). Y T Therefore, to summarize, we can show that the PDF of Y is (cid:40) (1−e−λτ)δ(y), y =0, f (y)= Y λe−λ(y+τ), y >0. 210
4.6. GAUSSIAN RANDOM VARIABLES Consequently, we can compute E[Y] and Var[Y] and analyze how these values change for λ (which itself changes with the time of day). Furthermore, we can analyze the amount of savings in terms of dollars. We leave these problems as an exercise. Closing remark. The photon arrival problem and the escalator problem are two of many examples we can find in which exponential random variables are useful for modeling a problem.Wedidnotgointothedetailsoftheproblemsbecauseeachofthemrequiressome additionalmodelingtoaddresstherealpracticalproblem.Weencourageyoutoexplorethese problems further. Our message is simple: Many problems can be modeled by exponential random variables, most of which are associated with time. 4.6 Gaussian Random Variables We now discuss the most important continuous random variable — the Gaussian random variable(alsoknownasthenormalrandomvariable).Wecallitthemostimportantrandom variable because it is widely used in almost all scientific disciplines. Many of us have used Gaussian random variables before, and perhaps its bell shape is the first lesson we learn in statistics. However, there are many mysteries about Gaussian random variables which you mayhavemissed,suchas:WheredoestheGaussianrandomvariablecomefrom?Whydoes ittakeabellshape?WhatarethepropertiesofaGaussianrandomvariable?Theobjective ofthissectionistoexplaineverythingyouneedtoknowaboutaGaussianrandomvariable. 4.6.1 Definition of a Gaussian random variable Definition 4.13. A Gaussian random variable is a random variable X such that its PDF is 1 (cid:26) (x−µ)2(cid:27) f (x)= √ exp − , (4.25) X 2πσ2 2σ2 where (µ,σ2) are parameters of the distribution. We write X ∼Gaussian(µ,σ2) or X ∼N(µ,σ2) to say that X is drawn from a Gaussian distribution of parameter (µ,σ2). Gaussian random variables have two parameters (µ,σ2). It is noteworthy that the mean is µ and the variance is σ2 — these two parameters are exactly the first moment and the second central moment of the random variable. Most other random variables do not have this property. Note that a Gaussian random variable is positive from −∞ to ∞. Thus, f (x) has X a non-zero value for any x, even though the value may be extremely small. A Gaussian random variable is also symmetric about µ. If µ=0, then f (x) is an even function. X The shape of the Gaussian is illustrated in Figure 4.25. When we fix the variance and change the mean, the PDF of the Gaussian moves left or right depending on the sign of the mean. When we fix the mean and change the variance, the PDF of the Gaussian changes 211
CHAPTER 4. CONTINUOUS RANDOM VARIABLES its width. Since any PDF should integrate to unity, a wider Gaussian means that the PDF is shorter. Note also that if σ is very small, it is possible that f (x) > 1 although the X integration over Ω will still be 1. 0.5 0.5 = -3 = 0.8 = -0.3 = 1 0.4 = 0 0.4 = 2 = 1.2 = 3 = 4 = 4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 -10 -5 0 5 10 -10 -5 0 5 10 µ changes, σ =1 µ=0, σ changes Figure 4.25: A Gaussian random variable with different µ and σ. On a computer, plotting the Gaussian PDF can be done by calling the function pdf(’norm’,x) in MATLAB, and stats.norm.pdf in Python. % MATLAB to generate a Gaussian PDF x = linspace(-10,10,1000); mu = 0; sigma = 1; f = pdf(’norm’,x,mu,sigma); plot(x, f); # Python to generate a Gaussian PDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-10,10,1000) mu = 0; sigma = 1; f = stats.norm.pdf(x,mu,sigma) plt.plot(x,f) Our next result concerns the mean and variance of a Gaussian random variable. You may wonder why we need this theorem when we already know that µ is the mean and σ2 is the variance. The answer is that we have not proven these two facts. Theorem 4.10. If X ∼Gaussian(µ,σ2), then E[X]=µ, and Var[X]=σ2. (4.26) 212
4.6. GAUSSIAN RANDOM VARIABLES Proof. The expectation can be derived via substitution: E[X]= √ 1 (cid:90) ∞ xe−(x 2− σµ 2)2 dx 2πσ2 −∞ ( =a) √ 1 (cid:90) ∞ (y+µ)e− 2y σ2 2 dy 2πσ2 −∞ = √ 1 (cid:90) ∞ ye− 2y σ2 2 dy+ √ 1 (cid:90) ∞ µe− 2y σ2 2 dy 2πσ2 2πσ2 −∞ −∞ ( =b) 0+µ(cid:18) √ 1 (cid:90) ∞ e− 2y σ2 2 dy(cid:19) 2πσ2 −∞ (c) = µ, where in (a) we substitute y =x−µ, in (b) we use the fact that the first integrand is odd so that the integration is 0, and in (c) we observe that integration over the entire sample space of the PDF yields 1. The variance is also derived by substitution. Var[X]= √ 1 (cid:90) ∞ (x−µ)2e−(x 2− σµ 2)2 dx 2πσ2 −∞ ( =a) √σ2 (cid:90) ∞ y2e−y 22 dy 2π −∞ = √σ2 (cid:18) −ye−y 22(cid:12) (cid:12) (cid:12)∞ (cid:19) + √σ2 (cid:90) ∞ e−y 22 dy 2π −∞ 2π −∞ =0+σ2(cid:18) √1 (cid:90) ∞ e−y 22 dy(cid:19) 2π −∞ =σ2, where in (a) we substitute y =(x−µ)/σ. 4.6.2 Standard Gaussian WeneedtoevaluatetheprobabilityP[a≤X ≤b]ofaGaussianrandomvariableX inmany practicalsituations.ThisinvolvestheintegrationoftheGaussianPDF,i.e.,determiningthe CDF. Unfortunately, there is no closed-form expression of P[a≤X ≤b] in terms of (µ,σ2). This leads to what we call the standard Gaussian. Definition 4.14. The standard Gaussian (or standard normal) random variable X has a PDF f X(x)= √1 e−x 22 . (4.27) 2π That is, X ∼N(0,1) is a Gaussian with µ=0 and σ2 =1. The CDF of the standard Gaussian can be determined by integrating the PDF. We have a special notation for this CDF. Figure 4.26 illustrates the idea. 213
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Definition 4.15. The CDF of the standard Gaussian is defined as the Φ(·) function Φ(x)d =ef F X(x)= √1 (cid:90) x e−t 22 dt. (4.28) 2π −∞ Figure 4.26: Definition of the CDF of the standard Gaussian Φ(x). % MATLAB code to generate standard Gaussian PDF and CDF x = linspace(-5,5,1000); f = normpdf(x,0,1); F = normcdf(x,0,1); figure; plot(x, f); figure; plot(x, F); # Python code to generate standard Gaussian PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-10,10,1000) f = stats.norm.pdf(x) F = stats.norm.cdf(x) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() The standard Gaussian’s CDF is related to a so-called error function defined as 2 (cid:90) x erf(x)= √ e−t2 dt. (4.29) π 0 It is easy to link Φ(x) with erf(x): 1(cid:20) (cid:18) x (cid:19)(cid:21) √ Φ(x)= 1+erf √ , and erf(x)=2Φ(x 2)−1. 2 2 WiththestandardGaussianCDF,wecandefinetheCDFofanarbitraryGaussian. 214
4.6. GAUSSIAN RANDOM VARIABLES Theorem 4.11 (CDF of an arbitrary Gaussian). Let X ∼N(µ,σ2). Then (cid:18) (cid:19) x−µ F (x)=Φ . (4.30) X σ Proof. We start by expressing F (x): X F (x)=P[X ≤x] X =(cid:90) x √ 1 e−(t 2− σµ 2)2 dt. 2πσ2 −∞ Substituting y = t−µ, and using the definition of standard Gaussian, we have σ (cid:90) x √ 1 e−(t 2− σµ 2)2 dt=(cid:90) x− σµ √1 e−y 22 dy 2πσ2 2π −∞ −∞ (cid:18) (cid:19) x−µ =Φ . (cid:3) σ If you would like to verify this on a computer, you can try the following code. % MATLAB code to verify standardized Gaussian x = linspace(-5,5,1000); mu = 3; sigma = 2; f1 = normpdf((x-mu)/sigma,0,1); % standardized f2 = normpdf(x, mu, sigma); % raw # Python code to verify standardized Gaussian import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,5,1000) mu = 3; sigma = 2; f1 = stats.norm.pdf((x-mu)/sigma,0,1) # standardized f2 = stats.norm.cdf(x,mu,sigma) # raw An immediate consequence of this result is that (cid:18) (cid:19) (cid:18) (cid:19) b−µ a−µ P[a<X ≤b]=Φ −Φ . (4.31) σ σ To see this, note that P[a<X ≤b]=P[X ≤b]−P[X ≤a] (cid:18) (cid:19) (cid:18) (cid:19) b−µ a−µ =Φ −Φ . σ σ The inequality signs of the two end points are not important. That is, the statement also holds for P[a ≤ X ≤ b] or P[a < X < b], because X is a continuous random variable at every x. Thus, P[X = a] = P[X = b] = 0 for any a and b. Besides this, Φ has several properties of interest. See if you can prove these: 215
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Corollary 4.1. Let X ∼N(µ,σ2). Then the following results hold: • Φ(y)=1−Φ(−y). (cid:16) (cid:17) • P[X ≥b]=1−Φ b−µ . σ (cid:16) (cid:17) (cid:16) (cid:17) • P[|X|≥b]=1−Φ b−µ +Φ −b−µ . σ σ 4.6.3 Skewness and kurtosis In modern data analysis we are sometimes interested in high-order moments. Here we con- sider two useful quantities: skewness and kurtosis. Definition 4.16. For a random variable X with PDF f (x), define the following X central moments as mean=E[X]d =ef µ, (cid:104) (cid:105) variance=E (X−µ)2 d =ef σ2, (cid:34)(cid:18) X−µ(cid:19)3(cid:35) skewness=E d =ef γ, σ (cid:34)(cid:18) X−µ(cid:19)4(cid:35) kurtosis=E d =ef κ, excess kurtosisd =ef κ−3. σ As you can see from the definitions above, skewness is the third central moment, whereas kurtosis is the fourth central moment. Both skewness and kurtosis can be regarded as“deviations”fromastandardGaussian—notintermsofmeanandvariancebutinterms of shape. Skewness measures the asymmetry of the distribution. Figure 4.27 shows three differ- ent distributions: one with left skewness, one with right skewness, and one symmetric. The skewness of a curve is • Skewed towards left: positive • Skewed towards right: negative • Symmetric: zero What is skewness? (cid:20)(cid:16) (cid:17)3(cid:21) • E X−µ . σ • Measures the asymmetry of the distribution. • Gaussian has skewness 0. 216
4.6. GAUSSIAN RANDOM VARIABLES 0.4 positive skewness symmetric 0.3 negative skewness 0.2 0.1 0 0 5 10 15 20 Figure 4.27: Skewness of a distribution measures the asymmetry of the distribution. In this example the skewnesses are: orange = 0.8943, black = 0, blue = -1.414. Kurtosismeasureshowheavy-tailedthedistributionis.Therearetwoformsofkurtosis: oneisthestandardkurtosis,whichisthefourthcentralmoment,andtheotheristheexcess kurtosis, which is κ = κ−3. The constant 3 comes from the kurtosis of a standard excess Gaussian.Excesskurtosisismorewidelyusedindataanalysis.Theinterpretationofkurtosis is the comparison to a Gaussian. If the kurtosis is positive, the distribution has a tail that decays faster than a Gaussian. If the kurtosis is negative, the distribution has a tail that decays more slowly than a Gaussian. Figure 4.28 illustrates the (excess) kurtosis of three different distributions. 1 kurtosis > 0 0.8 kurtosis = 0 kurtosis < 0 0.6 0.4 0.2 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 4.28: Kurtosis of a distribution measures how heavy-tailed the distribution is. In this example, the (excess) kurtoses are: orange = 2.8567, black = 0, blue = −0.1242. What is kurtosis? (cid:20)(cid:16) (cid:17)4(cid:21) • κ=E X−µ . σ • Measures how heavy-tailed the distribution is. Gaussian has kurtosis 3. • Some statisticians prefer excess kurtosis κ − 3, so that Gaussian has excess kurtosis 0. 217
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Random variable Mean Variance Skewness Excess kurtosis µ σ2 γ κ−3 Bernoulli p p(1−p) √1−2p 1 + 1 −6 p(1−p) 1−p p Binomial np np(1−p) √1−2p 6p2−6p+1 np(1−p) np(1−p) Geometric 1 1−p √2−p p2−6p+6 p p2 1−p 1−p Poisson λ λ √1 1 λ λ Uniform a+b (b−a)2 0 −6 2 12 5 Exponential 1 1 2 6 λ λ2 Gaussian µ σ2 0 0 Table 4.1: The first few moments of commonly used random variables. On a computer, computing the empirical skewness and kurtosis is done by built-in commands. Their implementations are based on the finite-sample calculations γ ≈ 1 (cid:88)N (cid:18) X n−µ(cid:19)3 , N σ n=1 κ≈ 1 (cid:88)N (cid:18) X n−µ(cid:19)4 . N σ n=1 TheMATLABandPythonbuilt-incommandsareshownbelow,usingagammadistribution as an example. % MATLAB code to compute skewness and kurtosis X = random(’gamma’,3,5,[10000,1]); s = skewness(X); k = kurtosis(X); # Python code to compute skewness and kurtosis import scipy.stats as stats X = stats.gamma.rvs(3,5,size=10000) s = stats.skew(X) k = stats.kurtosis(X) Example4.24.Tofurtherillustratethebehaviorofskewnessandkurtosis,weconsider an example using the gamma random variable X. The PDF of X is given by the equation 1 f X(x)= Γ(k)θkxk−1e−x θ, (4.32) where Γ(·) is known as the gamma function. If k is an integer, the gamma function is 218
4.6. GAUSSIAN RANDOM VARIABLES just the factorial: Γ(k)=(k−1)!. A gamma random variable is parametrized by two parameters (k,θ). As k increases or decreases, the shape of the PDF will change. For example, when k =1, the distribution is simplified to an exponential distribution. Without going through the (tedious) integration, we can show that the skewness and the (excess) kurtosis of Gamma(k,θ) are 2 skewness= √ , k 6 (excess) kurtosis= . k As we can see from these results, the skewness and kurtosis diminish as k grows. This can be confirmed from the PDF of Gamma(k,θ) as shown in Figure 4.29. 0.4 k = 2 k = 5 0.3 k = 10 k = 15 k = 20 0.2 0.1 0 0 5 10 15 20 25 30 Figure 4.29: The PDF of a gamma distribution Gamma(k,θ), where θ = 1. The skewness and the kurtosis are decaying to zero. Example 4.25. Let us look at a real example. On April 15, 1912, RMS Titanic sank after hitting an iceberg. The disaster killed 1502 out of 2224 passengers and crew. A hundred years later, we want to analyze the data. At https://www.kaggle.com/c/ titanic/thereisadatasetcollectingtheidentities,age,gender,etc.,ofthepassengers. We partition the dataset into two: one for those who died and the other one for those who survived. We plot the histograms of the ages of the two groups and compute several statistics of the dataset. Figure 4.30 shows the two datasets. 40 40 30 30 20 20 10 10 0 0 0 20 40 60 80 0 20 40 60 80 age age Group 1 (died) Group 2 (survived) Figure 4.30: The Titanic dataset https://www.kaggle.com/c/titanic/. 219
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Statistics Group 1 (Died) Group 2 (Survived) Mean 30.6262 28.3437 Standard Deviation 14.1721 14.9510 Skewness 0.5835 0.1795 Excess Kurtosis 0.2652 −0.0772 Note that the two groups of people have very similar means and standard devia- tions.Inotherwords,ifweonlycomparethemeanandstandarddeviation,itisnearly impossibletodifferentiatethetwogroups.However,theskewnessandkurtosisprovide more information related to the shape of the histograms. For example, Group 1 has morepositiveskewness,whereasGroup2isalmostsymmetrical.Oneinterpretationis that more young people offered lifeboats to children and older people. The kurtosis of Group 1 is slightly positive, whereas that of Group 2 is slightly negative. Therefore, high-order moments can sometimes be useful for data analysis. 4.6.4 Origin of Gaussian random variables The Gaussian random variable has a long history. Here, we provide one perspective on why Gaussian random variables are so useful. We give some intuitive arguments but leave the formal mathematical treatment for later when we introduce the Central Limit Theorem. Let’s begin with a numerical experiment. Consider throwing a fair die. We know that this will give us a (discrete) uniform random variable X. If we repeat the experiment many timeswecanplotthehistogram,anditwillreturnusaplotof6impulseswithequalheight, as shown in Figure 4.31(a). Now, suppose we throw two dice. Call them X and X , and let Z = X +X , i.e., 1 2 1 2 the sum of two dice. We want to find the distribution of Z. To do so, we first list out all the possible outcomes in the sample space; this gives us {(1,1),(1,2),...,(6,6)}. We then sum the numbers, which gives us a list of states of Z: {2,3,4,...,12}. The probability of gettingthesestatesisshowninFigure4.31(b),whichhasatriangularshape.Thetriangular shape makes sense because to get the state “2”, we must have the pair (1,1), which is quite unlikely. However, if we want to get the state 7, it would be much easier to get a pair, e.g., (6,1),(5,2),(4,3),(3,4),(2,5),(1,6) would all do the job. Now,whatwillhappenifwethrow5diceandconsiderZ =X +X +···+X ?Itturns 1 2 5 out that the distribution will continue to evolve and give something like Figure 4.31(c). This is starting to approximate a bell shape. Finally, if we throw 100 dice and consider Z = X +X +···+X , the distribution will look like Figure 4.31(d). The shape is 1 2 100 becoming a Gaussian! This numerical example demonstrates a fascinating phenomenon: As we sum more random variables, the distribution of the sum will converge to a Gaussian. If you are curious about how we plot the above figures, the following MATLAB and Python code can be useful. % MATLAB code to show the histogram of Z = X1+X2+X3 N = 10000; X1 = randi(6,1,N); X2 = randi(6,1,N); X3 = randi(6,1,N); Z = X1 + X2 + X3; histogram(Z, 2.5:18.5); 220
4.6. GAUSSIAN RANDOM VARIABLES # Python code to show the histogram of Z = X1+X2+X3 import numpy as np import matplotlib.pyplot as plt N = 10000 X1 = np.random.randint(1,6,size=N) X2 = np.random.randint(1,6,size=N) X3 = np.random.randint(1,6,size=N) Z = X1 + X2 + X3 plt.hist(Z,bins=np.arange(2.5,18.5)) (a) X (b) X +X (c) X +···+X (d) X +···+X 1 1 2 1 5 1 100 Figure 4.31: Whenaddinguniformrandomvariables,theoveralldistributionapproachesaGaussianas the number of summed variables increase. Can we provide a more formal description of this? Yes, but we need some new mathe- maticaltoolsthatwehavenotyetdeveloped.So,forthetimebeing,wewilloutlinetheflow of the arguments and leave the technical details to a later chapter. Suppose we have two independent random variables with identical distributions, e.g., X and X , where both are 1 2 uniform.ThisgivesusPDFsf (x)andf (x)thataretwoidenticalrectangularfunctions. X1 X2 By what operation can we combine these two rectangular functions and create a triangle function?Thekeyliesintheconceptof convolution.Ifyouconvolvetworectanglefunctions, you will get a triangle function. Here we define the convolution of f as X (cid:90) ∞ (f ∗f )(x)= f (τ)f (x−τ)dτ. X X X X −∞ In fact, for any pair of random variables X and X (not necessarily uniform random vari- 1 2 ables),thesumZ =X +X willhaveaPDFgivenbytheconvolutionofthetwoPDFs.We 1 2 have not yet proven this, but if you trust what we are saying, we can effectively generalize this argument to many random variables. If we have N random variables, then the sum Z = X +X +···+X will have a PDF that is the result of N convolutions of all the 1 2 N individual PDFs. What is the PDF of X+Y? • Summing X+Y is equivalent to convolving the PDFs f ∗f . X Y • If you sum many random variables, you convolve all their PDFs. Howdoweanalyzetheseconvolutions?WeneedasecondsetoftoolsrelatedtoFourier transforms. The Fourier transform of a PDF is known as the characteristic function, which 221
CHAPTER 4. CONTINUOUS RANDOM VARIABLES we will discuss later, but the name is not important now. What matters is the important property of the Fourier transform, that a convolution in the original space is multiplication in the Fourier space. That is, F{(f ∗f ∗···∗f )}=F{f }·F{f }·····F{f }. X X X X X X Multiplication in the Fourier space is much easier to analyze. In particular, for independent and identically distributed random variables, the multiplication will easily translate to ad- dition in the exponent. Then, by truncating the exponent to the second order, we can show that the limiting object in the Fourier space is approaching a Gaussian. Finally, since the inverseFouriertransformofaGaussianremainsaGaussian,wehaveshownthattheinfinite convolution will give us a Gaussian. Here is some numerical evidence for what we have just described. Recall that the Fourier transform of a rectangle function is the sinc function. Therefore, if we have an infiniteconvolutionofrectangularfunctions,equivalently,wehaveaninfiniteproductofsinc functionsintheFourierspace.Multiplyingsincfunctionsisreasonablyeasy.SeeFigure4.32 for the first three sincs. It is evident that with just three sinc functions, the shape closely approximates a Gaussian. 1.25 (sin x)/x 1 (sin x)2/x2 0.75 (sin x)3/x3 0.5 0.25 0 -0.25 -0.5 -10 -8 -6 -4 -2 0 2 4 6 8 10 Figure 4.32: Convolving the PDF of a uniform distribution is equivalent to multiplying their Fourier transformsintheFourierspace.Asthenumberofconvolutionsgrows,theproductisgraduallybecoming Gaussian. How about distributions that are not rectangular? We invite you to numerically visu- alizetheeffectwhenyouconvolvethefunctionmanytimes.Youwillseethatasthenumber of convolutions grows, the resulting function will become more and more like a Gaussian. Regardless of what the input random variables are, as long as you add them, the sum will have a distribution that looks like a Gaussian: X +X +···+X (cid:32)Gaussian. 1 2 N We use the notation (cid:32) to emphasize that the convergence is not the usual form of conver- gence. We will make this precise later. The implication of this line of discussion is important. Regardless of the underlying true physical process, if we are only interested in the sum (or average), the distribution will be more or less Gaussian. In most engineering problems, we are looking at the sum 222
4.7. FUNCTIONS OF RANDOM VARIABLES or average. For example, when generating an image using an image sensor, the sensor will add a certain amount of read noise. Read noise is caused by the random fluctuation of the electrons in the transistors due to thermal distortions. For high-photon-flux situations, we are typically interested in the average read noise rather than the electron-level read noise. ThusGaussianrandomvariablesbecomeareasonablemodelforthat.Inotherapplications, such as imaging through a turbulent medium, the random phase distortions (which alter thephaseofthewavefront)canalsobemodeledasaGaussianrandomvariable.Hereisthe summary of the origin of a Gaussian random variable: What is the origin of Gaussian? • When we sum many independent random variables, the resulting random vari- able is a Gaussian. • This is known as the Central Limit Theorem. The theorem applies to any ran- dom variable. • Summing random variables is equivalent to convolving the PDFs. Convolving PDFs infinitely many times yields the bell shape. 4.7 Functions of Random Variables One common question we encounter in practice is the transformation of random variables. The question can be summarized as follows: Given a random variable X with PDF f (x) X and CDF F (x), and supposing that Y = g(X) for some function g, what are f (y) and X Y F (y)? This is a prevalent question. For example, we measure the voltage V, and we want Y to analyze the power P = V2/R. This involves taking the square of a random variable. Another example: We know the distribution of the phase Θ, but we want to analyze the signal cos(ωt+Θ). This involves a cosine transformation. How do we convert one variable to another? Answering this question is the goal of this section. 4.7.1 General principle We will first outline the general principle for tackling this type of problem. In the following subsection, we will give a few concrete examples. SupposewearegivenarandomvariableX withPDFf (x)andCDFF (x).LetY = X X g(X)forsomeknownandfixedfunctiong.Forsimplicity,weassumethatgismonotonically increasing. In this case, the CDF of Y can be determined as follows. F (y)( =a)P[Y ≤y]( =b)P[g(X)≤y] Y ( =c)P[X ≤g−1(y)] ( =d) F (g−1(y)). X 223
CHAPTER 4. CONTINUOUS RANDOM VARIABLES This sequence of steps is not difficult to understand. Step (a) is the definition of CDF. Step (b) substitutes g(X) for Y. Step (c) uses the fact that since g is invertible, we can apply the inverse of g to both sides of g(X)≤y to yield X ≤g−1(y). Step (d) is the definition of the CDF, but this time applied to P[X ≤♣]=F (♣), for some ♣. X ItwillbeusefultovisualizethesituationinFigure4.33.Here,weconsiderauniformly distributed X so that the CDF F (x) is a straight line. According to F , any samples X X drawn according to F are equally likely, as illustrated by the yellow dots on the x-axis. X As we transform the X’s through Y = g(X), we increase/decrease the spacing between two samples. Therefore, some samples become more concentrated while some become less concentrated. The distribution of these transformed samples (the yellow dots on the y-axis) forms a new CDF F (y). The result F (y) = F (g−1(y)) holds when we look at Y. The Y Y X samples are traveling with g−1 in order to go back to F . Therefore, we need g−1 in the X formula. Figure 4.33: When transforming a random variable X to Y = g(X), the distributions are defined accordingtothespacingbetweensamples.Inthisfigure,auniformlydistributedX willbecomesqueezed by some parts of g and widened in other parts of g. Why should we use the CDF and not the PDF in Figure 4.33? The advantage of the CDF is that it is an increasing function. Therefore, no matter what the function g is, the input and the output functions will still be increasing. If we use the PDF, then the non- monotonic behavior of the PDF will interact with another nonlinear function g. It becomes much harder to decouple the two. We can carry out the integrations to determine F (g−1(y)). It can be shown that X (cid:90) g−1(y) F (g−1(y))= f (x(cid:48))dx(cid:48), (4.33) X X −∞ and hence, by the fundamental theorem of calculus, we have d d d (cid:90) g−1(y) f (y)= F (y)= F (g−1(y))= f (x(cid:48))dx(cid:48) Y dy Y dy X dy X −∞ (cid:18) dg−1(y)(cid:19) = ·f (g−1(y)), (4.34) dy X 224
4.7. FUNCTIONS OF RANDOM VARIABLES wherethelaststepisduetothechainrule.Basedonthislineofreasoningwecansummarize a “recipe” for this problem. How to find the PDF of Y =g(X) • Step 1: Find the CDF F (y), which is F (y)=F (g−1(y)). Y Y X (cid:16) (cid:17) • Step 2: Find the PDF f (y), which is f (y)= dg−1(y) ·f (g−1(y)). Y Y dy X Thisrecipeworkswhengisaone-to-onemapping.Ifgisnotone-to-one,e.g.,g(x)=x2 √ implies g−1(y)=± y, then we will have some issues with the above two steps. When this happens, then instead of writing X ≤g−1(y) we need to determine the set {x|g(x)≤y}. 4.7.2 Examples Example 4.26.(Lineartransform)LetX bearandomvariablewithPDFf (x)and X CDF F (x). Let Y =2X+3. Find f (y) and F (y). Express the answers in terms of X Y Y f (x) and F (x). X X Solution. We first note that F (y)=P[Y ≤y] Y =P[2X+3≤y] (cid:20) (cid:21) (cid:18) (cid:19) y−3 y−3 =P X ≤ =F . 2 X 2 Therefore, the PDF is d f (y)= F (y) Y dy Y (cid:18) (cid:19) d y−3 = F dy X 2 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) y−3 d y−3 1 y−3 =F(cid:48) = f . X 2 dy 2 2 X 2 Follow-Up. (Linear transformation of a Gaussian random variable).Suppose X is a Gaus- sianrandomvariablewithzeromeanandunitvariance,andletY =aX+b.ThentheCDF and PDF of Y are respectively (cid:18) (cid:19) (cid:18) (cid:19) y−b y−b F (y)=F =Φ , Y X a a (cid:18) (cid:19) f Y(y)= a1 f X y− a b = √ 21 πae−(y 2− ab 2)2 . Follow-Up. (Linear transformation of an exponential random variable). Suppose X is an exponential random variable with parameter λ, and let Y = aX +b. Then the CDF and 225
CHAPTER 4. CONTINUOUS RANDOM VARIABLES PDF of Y are respectively (cid:18) (cid:19) y−b F (y)=F Y X a =1−e−λ a(y−b), y ≥b, (cid:18) (cid:19) 1 y−b f (y)= f Y a X a λ = e−λ a(y−b), y ≥b. a Example 4.27. Let X be a random variable with PDF f (x) and CDF F (x). Sup- X X posing that Y = X2, find f (y) and F (y). Express the answers in terms of f (x) Y Y X and F (x). X Solution. We note that √ √ F (y)=P[Y ≤y]=P[X2 ≤y]=P[− y ≤X ≤ y] Y √ √ =F ( y)−F (− y). X X Therefore, the PDF is d f (y)= F (y) Y dy Y d √ √ = (F ( y)−F (− y)) dy X X √ d √ √ d √ =F(cid:48) ( y) y−F(cid:48) (− y) (− y) X dy X dy 1 √ √ = √ (f ( y)+f (− y)). 2 y X X Figure 4.34: When transforming a random variable X to Y =X2, the CDF becomes F (y)= √ Y by −− aa and the PDF becomes f Y(y)= √ y(1 b−a). 226
4.7. FUNCTIONS OF RANDOM VARIABLES FollowUp.(Squareofauniformrandomvariable)SupposeX isauniformrandomvariable in [a,b] (assume a>0), and let Y =X2. Then the CDF and PDF of Y are respectively √ y−a F (y)= , a2 ≤y ≤b2, Y b−a 1 f (y)= √ , a2 ≤y ≤b2. Y y(b−a) Example 4.28. Let X ∼Uniform(0,2π). Suppose Y =cosX. Find f (y) and F (y). Y Y Solution. First, we need to find the CDF of X. This can be done by noting that (cid:90) x (cid:90) x 1 x F (x)= f (x(cid:48))dx(cid:48) = dx(cid:48) = . X X 2π 2π −∞ 0 Thus, the CDF of Y is F (y)=P[Y ≤y]=P[cosX ≤y] Y =P[cos−1y ≤X ≤2π−cos−1y] =F (2π−cos−1y)−F (cos−1y) X X cos−1y =1− . π The PDF of Y is d d (cid:18) cos−1y(cid:19) f (y)= F (y)= 1− Y dy Y dy π 1 = , (cid:112) π 1−y2 where we used the fact that d cos−1y = √−1 . dy 1−y2 Example 4.29. Let X be a random variable with PDF f (x)=aexe−aex . X Let Y =eX, and find f (y). Y Solution. We first note that F (y)=P[Y ≤y]=P[eX ≤y] Y (cid:90) logy =P[X ≤logy]= aexe−aex dx. −∞ 227
CHAPTER 4. CONTINUOUS RANDOM VARIABLES To find the PDF, we recall the fundamental theorem of calculus. This gives us d (cid:90) logy f (y)= aexe−aex dx Y dy −∞ (cid:18) d (cid:19)(cid:32) d (cid:90) logy (cid:33) = logy aexe−aex dx dy dlogy −∞ 1 = aelogye−aelogy =ae−ay. y Closing remark. The transformation of random variables is a fundamental technique in datascience.Theapproachwehavepresentedisthemostrudimentaryyetthemostintuitive. The key is to visualize the transformation and how the random samples are allocated after the transformation. Note that the density of the random samples is related to the slope of the CDF. Therefore, if the transformation maps many samples to similar values, the slope of the CDF will be steep. Once you understand this picture, the transformation will be a lot easier to understand. Is it possible to replace the paper-and-pencil derivation of a transformation with a computer? If the objective is to transform random realizations, then the answer is yes because your goal is to transform numbers to numbers, which can be done on a computer. √ For example, transforming a sample x to x is straightforward on a computer. However, 1 1 if the objective is to derive the theoretical expression of the PDF, then the answer is no. Why might we want to derive the theoretical PDF? We might want to analyze the mean, variance,orotherstatisticalproperties.Wemayalsowanttoreverse-engineeranddetermine a transformation that can yield a specific PDF. This would require a paper-and-pencil derivation. In what follows, we will discuss a handy application of the transformations. What are the rules of thumb for transformation of random variables? • Always find the CDF F (y) = P[g(X) ≤ y]. Ask yourself: What are the values Y of X such that g(X)≤y? Think of the cosine example. • Sometimes you do not need to solve for F (y) explicitly. The fundamental the- Y orem of calculus can help you find f (y). Y • Drawpictures.Askyourselfwhetheryouneedtosqueezeorstretchthesamples. 4.8 Generating Random Numbers Most scientific computing software nowadays has built-in random number generators. For common types of random variables, e.g., Gaussian or exponential, these random number generatorscaneasilygeneratenumbersaccordingtothechosendistribution.However,ifwe aregivenanarbitraryPDF(orPMF)thatisnotamongthelistofpredefineddistributions, how can we generate random numbers according to the PDF or PMF we want? 228
4.8. GENERATING RANDOM NUMBERS 4.8.1 General principle Generating random numbers according to the desired distribution can be formulated as an inverse problem. Suppose that we can generate uniformly random numbers according to Uniform(0,1). This is a fragile assumption, and this process can be done on almost all computers today. Let us call this random variable U and its realization u. Suppose that we also have a desired distribution f (x) (and its CDF F (x)). We can put the two random X X variables U and X on the two axes of Figure 4.35, yielding an input-output relationship. The inverse problem is: By using what transformation g, such that X =g(U), can we make sure that X is distributed according to f (x) (or F (x))? X X Figure 4.35: Generating random numbers according to a known CDF. The idea is to first generate a uniform(0,1) random variable, then do an inverse mapping F−1. X Theorem 4.12. The transformation g that can turn a uniform random variable into a random variable following a distribution F (x) is given by X g(u)=F−1(u). (4.35) X That is, if g =F−1, then g(U) will be distributed according to f (or F ). X X X Proof. First, we know that if U ∼Uniform(0,1), then f (u)=1 for 0≤u≤1, so U (cid:90) u F (u)= f (u)du=u, U U −∞ for 0≤u≤1. Let g =F−1 and define Y =g(U). Then the CDF of Y is X F (y)=P[Y ≤y]=P[g(U)≤y] Y =P[F−1(U)≤y] X =P[U ≤F (y)]=F (y). X X Therefore, we have shown that the CDF of Y is the CDF of X. (cid:3) 229
CHAPTER 4. CONTINUOUS RANDOM VARIABLES The theorem above states that if we want a distribution F , then the transformation X should be g =F−1. This suggests a two-step process for generating random numbers. X How do we generate random numbers from an arbitrary distribution F ? X • Step 1: Generate a random number U ∼Uniform(0,1). • Step 2: Let Y =F−1(U). (4.36) X Then the distribution of Y is F . X 4.8.2 Examples Example 4.30. How can we generate Gaussian random numbers with mean µ and variance σ2 from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is (cid:18) (cid:19) x−µ F (x)=Φ . X σ Therefore, the transformation g is g(U)=F−1(U)=σΦ−1(U)+µ. X In Figure 4.36, we plot the CDF of F and the transformation g. X 1 -4 10 0.9 9 0.8 8 7 0.7 6 0.6 5 0.5 4 3 0.4 2 0.3 1 0.2 0 -1 0.1 -2 0 -3 -10 -8 -6 -4 -2 0 2 4 6 8 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) F (·) (b) g(·) X Figure 4.36: To generate random numbers according to Gaussian(0,1), we plot its CDF in (a) and the transformation g in (b). To visualize the random variables before and after the transformation, we plot the histograms in Figure 4.37. 230
4.8. GENERATING RANDOM NUMBERS 400 1200 1000 300 800 200 600 400 100 200 0 0 0 0.2 0.4 0.6 0.8 1 -5 0 5 10 (a) PDF of U (b) PDF of g(U) Figure 4.37: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. The MATLAB and Python codes used to generate the histograms above are shown below. % MATLAB code to generate Gaussian from uniform mu = 3; sigma = 2; U = rand(10000,1); gU = sigma*icdf(’norm’,U,0,1)+mu; figure; hist(U); figure; hist(gU); # Python code to generate Gaussian from uniform import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats mu = 3 sigma = 2 U = stats.uniform.rvs(0,1,size=10000) gU = sigma*stats.norm.ppf(U)+mu plt.hist(U); plt.show() plt.hist(gU); plt.show() Example 4.31. How canwegenerate exponential random numbers with parameter λ from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is F (x)=1−e−λx. X 231
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Therefore, the transformation g is 1 g(U)=F−1(U)=− log(1−U). X λ TheCDFoftheexponentialrandomvariableandthetransformationg areshown in Figure 4.38. 4 1 0.9 3 0.8 2 0.7 0.6 1 0.5 0 0.4 0.3 7 0.2 6 0.1 0 5 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) F (·) (b) g(·) X Figure 4.38: To generate random numbers according to Exponential(1), we plot its CDF in (a) and the transformation g in (b). The PDF of the uniform random variable U and the PDF of the transformed variable g(U) are shown in Figure 4.39. 400 3000 2500 300 2000 200 1500 1000 100 500 0 0 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 (a) PDF of U (b) PDF of g(U) Figure 4.39: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. The MATLAB and Python codes for this transformation are shown below. % MATLAB code to generate exponential random variables lambda = 1; U = rand(10000,1); gU = -(1/lambda)*log(1-U); # Python code to generate exponential random variables import numpy as np 232
4.8. GENERATING RANDOM NUMBERS import scipy.stats as stats lambd = 1; U = stats.uniform.rvs(0,1,size=10000) gU = -(1/lambd)*np.log(1-U) Example 4.32. How can we generate the 4 integers 1,2,3,4, according to the his- togram [0.10.50.30.1], from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is  0.1, x=1, 0.1+0.5=0.6, x=2, F (x)= X 0 0. .1 1+ +0 0. .5 5+ +0 0. .3 3+= 00 .. 19, =1.0, xx= =3 4, . This CDF is not invertible. However, we can still define the “inverse” mapping as g(U)=F−1(U) X  1, 0.0≤U ≤0.1, 2, 0.1<U ≤0.6, = 3 4, , 0 0. .6 9< <U U ≤ ≤0 1. .9 0, . For example, if 0.1 < U ≤ 0.6, then on the black curve shown in Figure 4.40(a), we are looking at the second vertical line from the left. This will go to “2” on the x-axis. Therefore, the inversely mapped value is 2 for 0.1<U ≤0.6. 4 1 0.9 3 0.6 2 1 0.1 0 0 1 2 3 4 5 0 0.1 0.6 0.9 1 (a) F (·) (b) g(·) X Figure 4.40: To generate random numbers according to a predefined histogram, we first define the CDF in (a) and the corresponding transformation in (b). The PDFs of the transformed variables, before and after, are shown in Fig- ure 4.41. 233
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 400 6000 5000 300 4000 200 3000 2000 100 1000 0 0 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 (a) PDF of U (b) PDF of g(U) Figure 4.41: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. In MATLAB, the above PDFs can be plotted using the commands below. In Python, weneedtousethelogicalcomparisonnp.logical_andtoidentifytheindices.Analternative is to use gU[((U<=0.5)*(U>=0.0)).astype(np.bool)]=1. % MATLAB code to generate the desired random variables U = rand(10000,1); gU = zeros(10000,1); gU((U>=0) & (U<=0.1)) = 1; gU((U>0.1) & (U<=0.6)) = 2; gU((U>0.6) & (U<=0.9)) = 3; gU((U>0.9) & (U<=1)) = 4; # Python code to generate the desired random variables import numpy as np import scipy.stats as stats U = stats.uniform.rvs(0,1,size=10000) gU = np.zeros(10000) gU[np.logical_and(U >= 0.0, U <= 0.1)] = 1 gU[np.logical_and(U > 0.1, U <= 0.6)] = 2 gU[np.logical_and(U > 0.6, U <= 0.9)] = 3 gU[np.logical_and(U > 0.9, U <= 1)] = 4 4.9 Summary Let us summarizethis chapter by revisiting thefour bullet pointsfrom the beginning of the chapter. • Definition of a continuous random variable. Continuous random variables are mea- sured bylengths,areas,andvolumes,whicharealldefinedbyintegrations.Thismakes 234
4.10. REFERENCE them different from discrete random variables, which are measured by counts (and summations).Becauseofthedifferentmeasuresbeingusedtodefinerandomvariables, we consequently have different ways of defining expectation, variance, moments, etc., all in terms of integrations. • Unification of discrete and continuous random variables. The unification is done by the CDF. The CDF of a discrete random variable can be written as a train of step functions. After taking the derivative, we will obtain the PDF, which is a train of impulses. • Origin of Gaussian random variables.TheoriginoftheGaussianrandomvariablelies inthefactthatmanyobservableeventsinengineeringaresumsofindependentevents. The summation of independent random variables is equivalent to taking convolutions of the PDFs. At the limit, they will converge to a bell-shaped function, which is the Gaussian. Gaussians are everywhere because we observe sums more often than we observe individual states. • Transformation of random variables. Transformation of random variables is done in the CDF space. The transformation can be used to generate random numbers according to a predefined distribution. Specifically, if we want to generate random numbers according to F , then the transformation is g =F−1. X X 4.10 Reference PDF, CDF, expectation 4-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 3.1, 3.2. 4-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.1 - 4.3. 4-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 4. 4-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 4.1, 4.2, 5.1, 5.3, 5.5. 4-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 4.10, 5.1, 5.2, 5.3. 4-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Chapter 2.4, 2.5, 4.1, 4.4. Gaussian random variables 4-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 3.3. 235
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.4. 4-9 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 5.4. 4-10 Mark D. Ward and Ellen Gundlach, Introduction to Probability, W.H. Freeman and Company, 2016. Chapter 35. Transformation of random variables 4-11 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.1. 4-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.5. 4-13 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 5. 4-14 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 5.4. 4-15 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 5.7. 4-16 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Chapter 3.1, 3.2. Advanced probability textbooks 4-17 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 4-18 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) 4.11 Problems Exercise 1. (Video Solution) Let X be a Gaussian random variable with µ=5 and σ2 =16. (a) Find P[X >4] and P[2≤X ≤7]. (b) If P[X <a]=0.8869, find a. (c) If P[X >b]=0.1131, find b. (d) If P[13<X ≤c]=0.0011, find c. 236
4.11. PROBLEMS Exercise 2. (Video Solution) Compute E[Y] and E[Y2] for the following random variables: (a) Y =Acos(ωt+θ), where A∼N(µ, σ2). (b) Y =acos(ωt+Θ), where Θ∼Uniform(0, 2π). (c) Y =acos(ωT +θ), where T ∼Uniform(cid:0) −π, π(cid:1) . ω ω Exercise 3. (Video Solution) Consider a CDF  0, if x<−1, 0.5, if −1≤x<0, F (x)= X ( 11 , +x)/2, i of th0 e≤ rwx is< e.1, (a) Find P[X <−1], P[−0.5<X <0.5] and P[X >0.5]. (b) Find f (x). X Exercise 4. (Video Solution) A random variable X has CDF: (cid:40) 0, if x<0, F (x)= X 1− 1e−2x, if x≥0. 4 (a) Find P[X ≤2], P[X =0], P[X <0], P[2<X <6] and P[X >10]. (b) Find f (x). X Exercise 5. (Video Solution) A random variable X has PDF (cid:40) cx(1−x2), 0≤x≤1, f (x)= X 0, otherwise. Find c, F (x), and E[X]. X Exercise 6. (Video Solution) A continuous random variable X has a cumulative distribution  0, x<0,  F (x)= 0.5+csin2(πx/2), 0≤x≤1, X 1, x>1. (a) What values can c assume? (b) Find f (x). X 237
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Exercise 7. (Video Solution) A continuous random variable X is uniformly distributed in [−2, 2]. (a) Let Y =sin(πX/8). Find f (y). Y (b) Let Z =−2X2+3. Find f (z). Z Hint: Compute F (y) from F (x), and use d sin−1y = √ 1 . Y X dy 1−y2 Exercise 8. Let Y =eX. (a) Find the CDF and PDF of Y in terms of the CDF and PDF of X. (b) Find the PDF of Y when X is a Gaussian random variable. In this case, Y is said to be a lognormal random variable. Exercise 9. The random variable X has the PDF (cid:40) √1 , 0≤x≤1, f (x)= 2 x X 0, otherwise. Let Y be a new random variable  0, X <0, √ Y = X, 0≤X ≤1, 1, X >1. Find F (y) and f (y), for −∞<y <∞. Y Y Exercise 10. A random variable X has the PDF (cid:40) 2xe−x2, x≥0, f (x)= X 0, x<0. Let (cid:40) 1−e−X2, X ≥0, Y =g(X)= 0, X <0. Find the PDF of Y. Exercise 11. A random variable X has the PDF 1 f (x)= e−|x|, −∞<x<∞. X 2 238
4.11. PROBLEMS Let Y =g(X)=e−X. Find the PDF of Y. Exercise 12. A random variable X has the PDF f X(x)= √ 1 e− 2x σ2 2, −∞<x<∞. 2πσ2 Find the PDF of Y where (cid:40) X, |X|>K, Y =g(X)= −X, |X|<K. Exercise 13. A random variable X has the PDF f X(x)= √1 e−x 22 , −∞<x<∞. x2 2π Let Y =g(X)= 1. Find the PDF of Y. X Exercise 14. A random variable X has the CDF  0, x<0,  F (x)= xα, 0≤x≤1, X 1, x>1, with α>0. Find the CDF of Y if Y =g(X)=−logX. Exercise 15. Energy efficiency is an important aspect of designing electrical systems. In some modern buildings(e.g.,airports),traditionalescalatorsarebeingreplacedbyanewtypeof“smart” escalator which can automatically switch between a normal operating mode and a standby mode depending on the flow of pedestrians. (a) ThearrivalofpedestrianscanbemodeledasaPoissonrandomvariable.LetN bethe number of arrivals, and let λ be the arrival rate (people per minute). For a period of t minutes, show that the probability that there are n arrivals is (λt)n P(N =n)= e−λt. n! (b) Let T be a random variable denoting the interarrival time (i.e., the time between two consecutive arrivals). Show that P(T >t)=e−λt. Also, determine F (t) and f (t). Sketch f (t). T T T (Hint: Note that P(T >t)=P(no arrival in t minutes).) 239
CHAPTER 4. CONTINUOUS RANDOM VARIABLES (c) Suppose that the escalator will go into standby mode if there are no pedestrians for t = 30 seconds. Let Y be a random variable denoting the amount of time that the 0 escalator is in standby mode. That is, let (cid:40) 0, if T ≤t , Y = 0 T −t , if T >t . 0 0 Find E[Y]. 240
Chapter 5 Joint Distributions Whenyougotoaconcerthall,sometimesyoumaywanttoseeasoloviolinconcert,butother timesyoumaywanttoseeasymphony.Symphoniesareappealingbecausemanyinstruments areplayingtogether.Randomvariablesaresimilar.Whilesinglerandomvariablesareuseful for modeling simple events, we use multiple random variables to describe complex events. Themultiplerandomvariablescanbeeitherindependentorcorrelated.Whenmanyrandom variables are present in the problem, we enter the subject of joint distribution. What are joint distributions? In the simplest sense, joint distributions are extensions of the PDFs and PMFs we studied in the previous chapters. We summarize them as follows. Joint distributions are high-dimensional PDFs (or PMFs or CDFs). What do we mean by a high-dimensional PDF? We know that a single random variable is characterized by a 1-dimensional PDF f (x). If we have a pair of random variables, then X we use a 2-dimensional function f (x,y), and if we have a triplet of random variables, X,Y we use a 3-dimensional function f (x,y,z). In general, the dimensionality of the PDF X,Y,Z grows as the number of variables: f (x) =⇒f (x ,x )=⇒···=⇒f (x ,...,x ). X X1,X2 1 2 X1,...,XN 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) onevariable twovariables N variables For busy engineers like us, f (x ,...,x ) is not a friendly notation. A more con- X1,...,XN 1 N cise way to write f (x ,...,x ) is to define a vector of random variables X = X1,...,XN 1 N [X ,X ,...,X ]T with a vector of states x=[x ,x ,...,x ]T, and to define the PDF as 1 2 N 1 2 N f (x)=f (x ,...,x ). X X1,...,XN 1 N Under what circumstance will we encounter creatures like f (x)? Believe it or not, X these high-dimensional PDFs are everywhere. In 2010, computer-vision scientists created the ImageNet dataset, containing 14 million images with ground-truth class labels. This enormous dataset has enabled a great blossoming of machine learning over the past several 241
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.1: Joint distributions are ubiquitous in modern data analysis. For example, an image from a dataset can be represented by a high-dimensional vector x. Each vector has a certain probability of being present. This probability is described by the high-dimensional joint PDF f (x). The goal of this X chapter is to understand the properties of this f . X 0.5 0.5 0.1 0.4 0.4 0.3 0.3 0.05 0.2 0.2 0 0.1 0.1 543210- y1-2-3-4-5 -5-4-3-2 x-10 1 2 3 4 5 0 -5 -4 -3 -2 -1 x0 1 2 3 4 5 0 -5 -4 -3 -2 -1 y0 1 2 3 4 5 Figure 5.2: A 2-dimensional PDF f (x,y) of a pair of random variables (X,Y) and their respective X,Y 1D PDFs f (x) and f (y). X Y decades, in which many advances in deep learning have been made. Fundamentally, the ImageNet dataset provides a large collection of samples drawn from a latent distribution thatishigh-dimensional.EachsampleintheImageNetdatasetisa224×224×3image(the three numbers stand for the image’s height, width, and color). If we convert this image into a vector, then the sample will have a dimension of 224×224×3= 150,528. In other words, the sample is a vector x ∈ R150528×1. The probability of obtaining a particular sample x is determined by probability density function f (x). For example, it is more likely to get X an image containing trees than one containing a Ferrari. The manifold generated by f (x) X can be extremely complex, as illustrated in Figure 5.1. The story of ImageNet is just one of the many instances for which we use a joint distribution f (x). Joint distributions are ubiquitous. If you do data science, you must X understand joint distributions. However, extending a 1-dimensional function f (x) to a X 2-dimensionalfunctionf (x,y)andthentoaN-dimensionalfunctionf (x)isnottrivial. X,Y X The goal of this chapter is to guide you through these important steps. Plan of Part 1 of this chapter: Two variables This chapter is broadly divided into two halves. In the first half, we will look at a pair of random variables. • Definition of f (x,y). The first thing we need to learn is the definition of a joint X,Y distribution with two variables. Since we have two variables, the joint probability density function (or probability mass function) is a 2-dimensional function. A point 242
on this 2D function is the probability density evaluated by a pair of variables X = x and Y = y, as illustrated in Figure 5.2. However, how do we formally define this 2D function? How is it related to the probability measure? Is there a way we can retrieve f (x)andf (y)fromf (x,y),asillustratedontheright-handsidesof Figure 5.2? X Y X,Y These questions will be answered in Section 5.1. • Joint expectation E[XY]. When we have a pair of random variables, how should we definetheexpectation?InSection5.2,wewillshowthatthemostnaturalwaytodefine the joint expectation is in terms of E[XY], i.e., the expectation of the product. There isasurprisingandbeautifulconnectionbetweenthis“expectationoftheproduct”and the cosine angle between two vectors, thereby showing that E[XY] is the correlation between X and Y. • The reason for studying a pair of random variables is to spell out the cause-effect relationship between the variables. This cannot be done without conditional distri- butions; this will be explained in Section 5.3. Conditional distributions provide an extremely important computational tool for decoupling complex events into simpler events. Such decomposition allows us to solve difficult joint expectation problems via simple conditional expectations; this subject will be covered in Section 5.4. • IfyourecallourdiscussionsabouttheoriginofaGaussianrandomvariable,weclaimed that the PDF of X +Y is the convolution between f and f . Why is this so? We X Y will answer this question in terms of joint distributions in Section 5.5. Plan of Part 2 of this chapter: N variables The second half of the chapter focuses on the general case of N random variables. This requires the definitions of a random vector X =[X ,...,X ]T, a joint distribution f (x), 1 N X and the corresponding expectations E[X]. To make our discussions concrete, we will focus onthecaseof high-dimensionalGaussianrandomvariablesanddiscussthefollowingtopics. • Covariance matrices/correlation matrices. If a pair of random variables can define the correlation through the expectation of the product E[X X ], then for a vector of 1 2 random variables we can consider a matrix of correlations in the form E[X X ] E[X X ] ··· E[X X ] 1 1 1 2 1 N E[X 2X 1] E[X 2X 2] ··· E[X 2X N] R=   . . . . . . ... . . .   . E[X X ] E[X X ] ··· E[X X ] N 1 N 2 N N What are the properties of the matrix? How does it affect the shape of the high- dimensionalGaussian?Ifwehaveadatasetofvectors,howdoweestimatethismatrix from the data? We will answer these questions in Section 5.6 and Section 5.7. • Principal-component analysis. Given the covariance matrix, we can perform some very useful data analyses, such as the principal-component analysis in Section 5.8. The question we will ask is: Among the many components, which one is the principal component? If we can find the principal component(s), we can effectively perform dimensionalityreductionbyprojectingahigh-dimensionalvectorintolow-dimensional representations. We will introduce an application for face detection. 243
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.3: When there is a pair of random variables, we can regard the sample space as a set of coordinates. The random variables are 2D mappings from a coordinate ω in Ω ×Ω to another X Y coordinate X(ω) in R2. 5.1 Joint PMF and Joint PDF Probability is a measure of the size of a set. This principle applies to discrete random vari- ables,continuousrandomvariables,singlerandomvariables,andmultiplerandomvariables. In situations with a pair of random variables, the measure should be applied to the coordi- nate (X,Y) represented by the random variables X and Y. Consequently, when measuring the probability, we either count these coordinates or integrate the area covered by these coordinates. In this section, we formalize this notion of measuring 2D events. 5.1.1 Probability measure in 2D Consider two random variables X and Y. Let the sample space of X and Y be Ω and X Ω , respectively. Define the Cartesian product of Ω and Ω as Ω ×Ω = {(x,y) | x ∈ Y X Y X Y Ω andy ∈Ω }. That is, Ω ×Ω contains all possible pairs (X,Y). X Y X Y Example 5.1. If Ω = {1,2} and Ω = {4,5}, then Ω × Ω = {(1,4),(1,5), X Y X Y (2,4),(2,5)}. 244
5.1. JOINT PMF AND JOINT PDF Example 5.2. If Ω = [3,4] and Ω = [1,2], then Ω ×Ω = a rectangle with two X Y X Y diagonal vertices as (3,1) and (4,2). Random variables are mappings from the sample space to the real line. If ω ∈ Ω is X mapped to X(ω)∈R, and ξ ∈Ω is mapped to Y(ξ)∈R, then a coordinate ω =(ω,ξ) in Y thesamplespaceΩ ×Ω shouldbemappedtoacoordinate(X(ω),Y(ξ))inthe2Dplane. X Y (cid:20) (cid:21) (cid:20) (cid:21) def ω X(ω) def ω = (cid:55)−→ = X(ω). ξ Y(ξ) We denote such a vector-to-vector mapping as X(·) : Ω ×Ω → R×R, as illustrated in X Y Figure 5.3. Therefore, if we have an event A∈R2, the probability that A happens is P[A]=P[{ω |X(ω)∈A}] (cid:20)(cid:26)(cid:20) (cid:21) (cid:12) (cid:20) (cid:21) (cid:27)(cid:21) =P ω (cid:12) (cid:12) X(ω) ∈A ξ (cid:12) Y(ξ) (cid:20)(cid:26)(cid:20) (cid:21) (cid:27)(cid:21) ω =P ∈X−1(A) ξ =P[ω ∈X−1(A)]. In other words, we take the coordinate X(ω) and find its inverse image X−1(A). The size of this inverse image X−1(A) in the sample space Ω ×Ω is then the probability. We X Y summarize this general principle as follows. How to measure probability in 2D For a pair of random variables X =(X,Y), the probability of an event A is measured in the product space Ω ×Ω with the size X Y P[{ω |X−1(A)}]. Thisdefinitionisquiteabstract.Tomakeitmoreconcrete,wewilllookatdiscreteand continuous random variables. 5.1.2 Discrete random variables Suppose that the random variables X and Y are discrete. Let A = {X(ω) = x, Y(ξ) = y} be a discrete event. Then the above definition tells us that the probability of A is (cid:20) (cid:12) (cid:21) P[A]=P (ω,ξ)(cid:12) (cid:12)X(ω)=x, andY(ξ)=y =P[X =xandY =y]. (cid:12) (cid:124) (cid:123)(cid:122) (cid:125) d=efpX,Y(x,y) We define this probability as the joint probability mass function (joint PMF) p (x,y). X,Y 245
CHAPTER 5. JOINT DISTRIBUTIONS Definition 5.1. Let X and Y be two discrete random variables. The joint PMF of X and Y is defined as (cid:20) (cid:12) (cid:21) p X,Y(x,y)=P[X =xandY =y]=P (ω,ξ)(cid:12) (cid:12)X(ω)=x, andY(ξ)=y . (5.1) (cid:12) We sometimes write the joint PMF as p (x,y)=P[X =x, Y =y]. X,Y Figure 5.4: A joint PMF for a pair of discrete random variables consists of an array of impulses. To measure the size of the event A, we sum all the impulses inside A. Figure 5.4 shows a graphical portrayal of the joint PMF. In a nutshell, p (x,y) X,Y can be considered as a 2D extension of a single variable PMF. The probabilities are still representedbytheimpulses,butthedomainoftheseimpulsesisnowa2Dplane.Ifwehave an event A, then the size of the event is (cid:88) P[A]= p (x,y). X,Y (x,y)∈A Example 5.3. Let X be a coin flip, Y be a die. The sample space of X is {0,1}, whereas the sample space of Y is {1,2,3,4,5,6}. The joint PMF, according to our definition, is the probability P[X =x and Y =y], where x takes a binary state and Y takes one of the 6 states. The following table summarizes all the 12 states of the joint distribution. Y 1 2 3 4 5 6 X = 0 1 1 1 1 1 1 12 12 12 12 12 12 X = 1 1 1 1 1 1 1 12 12 12 12 12 12 In this table, since there are 12 coordinates, and each coordinate has an equal chance of appearing, the probability for each coordinate becomes 1/12. Therefore, the joint PMF of X and Y is 1 p (x,y)= , x=0,1, y =1,2,3,4,5,6. X,Y 12 246
5.1. JOINT PMF AND JOINT PDF In this example, we observe that if X and Y are not interacting with each other (for- mally,independent),thejointPMFistheproductofthetwoindividualprobabilities. Example 5.4.Inthepreviousexample,ifwedefineA={X+Y =3},theprobability P[A] is (cid:88) P[A]= p (x,y)=p (0,3)+p (1,2) X,Y X,Y X,Y (x,y)∈A 2 = . 12 If B ={min(X,Y)=1}, the probability P[B] is (cid:88) P[B]= p (x,y) X,Y (x,y)∈B =p (1,1)+p (1,2)+p (1,3) X,Y X,Y X,Y +p (1,4)+p (1,5)+p (1,6) X,Y X,Y X,Y 6 = . 12 5.1.3 Continuous random variables The continuous version of the joint PMF is called the joint probability density function (joint PDF),denotedbyf (x,y).AjointPDFisanalogoustoajointPMF.Forexample, X,Y integrating it will give us the probability. Definition 5.2. Let X and Y be two continuous random variables. The joint PDF of X and Y is a function f (x,y) that can be integrated to yield a probability X,Y (cid:90) P[A]= f (x,y)dxdy, (5.2) X,Y A for any event A⊆Ω ×Ω . X Y Pictorially, we can view f as a 2D function where the height at a coordinate (x,y) is X,Y f (x,y), as can be seen from Figure 5.5. To compute the probability that (X,Y) ∈ A, X,Y we integrate the function f with respect to the area covered by the set A. For example, X,Y if the set A is a rectangular box A=[a,b]×[c,d], then the integration becomes P[A]=P[a≤X ≤b, c≤Y ≤d] (cid:90) d(cid:90) b = f (x,y)dxdy. X,Y c a 247
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.5: A joint PDF for a pair of continuous random variables is a surface in the 2D plane. To measure the size of the event A, we integrate f (x,y) inside A. X,Y Example5.5.ConsiderauniformjointPDFf (x,y)definedon[0,2]2withf (x,y)= X,Y X,Y 1. Let A=[a,b]×[c,d]. Find P[A]. 4 Solution. P[A]=P[a≤X ≤b, c≤X ≤d] (cid:90) d(cid:90) b (cid:90) d(cid:90) b 1 (d−c)(b−a) = f (x,y)dxdy = dxdy = . X,Y 4 4 c a c a Practice Exercise 5.1. In the previous example, let B ={X+Y ≤2}. Find P[B]. Solution. (cid:90) P[B]= f (x,y)dxdy X,Y B (cid:90) 2(cid:90) 2−y = f (x,y)dxdy X,Y 0 0 (cid:90) 2(cid:90) 2−y 1 = dxdy 4 0 0 (cid:90) 2 2−y 1 = dy = . 4 2 0 Here, the limits of the integration can be determined from Figure 5.6. The inner integration (with respect to x) should start from 0 and end at 2−y, which is the line defining the set x+y ≤ 2. Since the inner integration is performed for every y, we need to enumerate all the possible y’s to complete the outer integration. This leads to the outer limit from 0 to 2. 248
5.1. JOINT PMF AND JOINT PDF Figure 5.6: To calculate P[X+Y ≤2], we perform a 2D integration over a triangle. 5.1.4 Normalization Thenormalizationpropertyofatwo-dimensionalPMFandPDFisthepropertythat,when we enumerate all outcomes of the sample space, we obtain 1. Theorem 5.1. Let Ω=Ω ×Ω . All joint PMFs and joint PDFs satisfy X Y (cid:90) (cid:88) p (x,y)=1 or f (x,y)dxdy =1. (5.3) X,Y X,Y Ω (x,y)∈Ω Example 5.6. Consider a joint uniform PDF defined in the shaded area [0,3]×[0,3] with PDF defined below. Find the constant c. (cid:40) c if (x,y)∈[0,3]×[0,3], f (x,y)= X,Y 0 otherwise. Solution. To find the constant c, we note that (cid:90) 3(cid:90) 3 1= f (x,y)dxdy X,Y 0 0 (cid:90) 3(cid:90) 3 = cdxdy =9c. 0 0 Equating the two sides gives us c= 1. 9 Practice Exercise 5.2. Consider a joint PDF (cid:40) ce−xe−y 0≤y ≤x<∞, f (x,y)= X,Y 0 otherwise. Find the constant c. Tip: Consider the area of integration as shown in Figure 5.7. 249
CHAPTER 5. JOINT DISTRIBUTIONS Solution.TherearetwowaystotaketheintegrationshowninFigure 5.7.Wechoose the inner integration w.r.t. y first. (cid:90) (cid:90) ∞(cid:90) x f (x,y)dxdy = ce−xe−y dy dx X,Y Ω 0 0 (cid:90) ∞ = ce−x(1−e−x) 0 c = . 2 Therefore, c=2. Figure 5.7: To integrate the probability P[0 ≤ Y ≤ X], we perform a 2D integration over a triangle. (cid:82) (cid:82) The two subfigures show the two ways of integrating the triangle. [Left] dx first, and then dy. (cid:82) (cid:82) [Right] dy first, and then dx. 5.1.5 Marginal PMF and marginal PDF If we only sum / integrate for one random variable, we obtain the PMF / PDF of the other random variable. The resulting PMF / PDF is called the marginal PMF / PDF. Definition 5.3. The marginal PMF is defined as (cid:88) (cid:88) p (x)= p (x,y) and p (y)= p (x,y), (5.4) X X,Y Y X,Y y∈ΩY x∈ΩX and the marginal PDF is defined as (cid:90) (cid:90) f (x)= f (x,y)dy and f (y)= f (x,y)dx. (5.5) X X,Y Y X,Y ΩY ΩX Since f (x,y) is a two-dimensional function, when integrating over y from −∞ to ∞, we X,Y project f (x,y) onto the x-axis. Therefore, the resulting function depends on x only. X,Y 250
5.1. JOINT PMF AND JOINT PDF Example5.7.ConsiderthejointPDFf (x,y)= 1 shownbelow.Findthemarginal X,Y 4 PDFs. Solution. If we integrate over x and y, we have   3, if 1<x≤2, 1 2, , i if f 1 2< <x x≤ ≤2 3, , f (x)= 1, if 2<x≤3, and f (y)= X Y 0, otherwise. 1 0, , i of th3 e< rwx is≤ e.4, So the marginal PDFs are the projection of the joint PDFs onto the x- and y-axes. Practice Exercise 5.3. A joint Gaussian random variable (X,Y) has a joint PDF given by 1 (cid:26) ((x−µ )2+(y−µ )2)(cid:27) f (x,y)= exp − X Y . X,Y 2πσ2 2σ2 Find the marginal PDFs f (x) and f (y). X Y Solution. (cid:90) ∞ (cid:90) ∞ 1 (cid:26) ((x−µ )2+(y−µ )2)(cid:27) f (x)= f (x,y)dy = exp − X Y dy X X,Y 2πσ2 2σ2 −∞ −∞ 1 (cid:26) (x−µ )2(cid:27) (cid:90) ∞ 1 (cid:26) (y−µ )2(cid:27) = √ exp − X · √ exp − Y dy. 2πσ2 2σ2 2πσ2 2σ2 −∞ Recognizing that the last integral is equal to unity because it integrates a Gaussian PDF over the real line, it follows that 1 (cid:26) (x−µ )2(cid:27) f (x)= √ exp − X . X 2πσ2 2σ2 Similarly, we have 1 (cid:26) (y−µ )2(cid:27) f (y)= √ exp − Y . Y 2πσ2 2σ2 251
CHAPTER 5. JOINT DISTRIBUTIONS 5.1.6 Independent random variables Two random variables are said to be independent if and only if the joint PMF or PDF can be factorized as a product of the marginal PMF / PDFs. Definition 5.4. Random variables X and Y are independent if and only if p (x,y)=p (x)p (y), or f (x,y)=f (x)f (y). X,Y X Y X,Y X Y This definition is consistent with the definition of independence of two events. Recall that twoeventsAandBareindependentifandonlyifP[A∩B]=P[A]P[B].LettingA={X =x} and B ={Y =y}, we see that if A and B are independent then P[X = x∩Y = y] is the product P[X =x]P[Y =y]. This is precisely the relationship p (x,y)=p (x)p (y). X,Y X Y Example 5.8. Consider two random variables with a joint PDF given by 1 (cid:26) (x−µ )2+(y−µ )2(cid:27) f (x,y)= exp − X Y . X,Y 2πσ2 2σ2 Are X and Y independent? Solution. We know that 1 (cid:26) (x−µ )2(cid:27) 1 (cid:26) (y−µ )2(cid:27) f (x,y)= √ exp − X × √ exp − Y . X,Y 2πσ 2σ2 2πσ 2σ2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fX(x) fY(y) Therefore, the random variables X and Y are independent. Practice Exercise 5.4.LetX beacoinandY beadie.ThenthejointPMFisgiven by the table below. Y 1 2 3 4 5 6 X = 0 1 1 1 1 1 1 12 12 12 12 12 12 X = 1 1 1 1 1 1 1 12 12 12 12 12 12 Are X and Y independent? Solution. For any x and y, we have that 1 1 1 p (x,y)= = × . X,Y 12 2 6 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) pX(x) pY(y) Therefore, the random variables X and Y are independent. 252
5.1. JOINT PMF AND JOINT PDF Example 5.9. Consider two random variables X and Y with a joint PDF given bya f (x,y)∝exp(cid:8) −(x−y)2(cid:9) =exp(cid:8) −x2+2xy−y2(cid:9) X,Y =exp(cid:8) −x2(cid:9) exp{2xy} exp(cid:8) −y2(cid:9) . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fX(x) extraterm fY(y) This PDF cannot be factorized into a product of two marginal PDFs. Therefore, the random variables are dependent. aWeusethenotation“∝”todenote“proportionalto”.Itimpliesthatthenormalizationconstant isomitted. We can extrapolate the definition of independence to multiple random variables. If there are many random variables X ,X ,...,X , they will have a joint PDF 1 2 N f (x ,...,x ). X1,...,XN 1 N If these random variables X ,X ,...,X are independent, then the joint PDF can be 1 2 N factorized as f (x ,...,x )=f (x )·f (x )···f (x ) X1,...,XN 1 N X1 1 X2 2 XN N N (cid:89) = f (x ). Xn n n=1 This gives us the definition of independence for N random variables. Definition 5.5. A sequence of random variables X ,...,X is independent if and 1 N only if their joint PDF (or joint PMF) can be factorized. N (cid:89) f (x ,...,x )= f (x ). (5.6) X1,...,XN 1 N Xn n n=1 Example 5.10.Throwadie4times.LetX ,X ,X andX betheoutcomes.Then, 1 2 3 4 sincethesefourthrowsareindependent,theprobabilitymassfunctionofanyquadrable (x ,x ,x ,x ) is 1 2 3 4 p (x ,x ,x ,x )=p (x )p (x )p (x )p (x ). X1,X2,X3,X4 1 2 3 4 X1 1 X2 2 X3 3 X4 4 For example, the probability of getting (1,5,2,6) is (cid:18) 1(cid:19)4 p (1,5,2,6)=p (1)p (5)p (2)p (6)= . X1,X2,X3,X4 X1 X2 X3 X4 6 The example above demonstrates an interesting phenomenon. If the N random vari- ables are independent, and if they all have the same distribution, then the joint PDF/PMF is just one of the individual PDFs taken to the power N. Random variables satisfying this property are known as independent and identically distributed random variables. 253
CHAPTER 5. JOINT DISTRIBUTIONS Definition 5.6 (Independent and Identically Distributed (i.i.d.)). A collection of random variables X ,...,X is called independent and identically distributed (i.i.d.) 1 N if • All X ,...,X are independent; and 1 N • All X ,...,X have the same distribution, i.e., f (x)=···=f (x). 1 N X1 XN If X ,...,X are i.i.d., we have that 1 N N (cid:89) f (x ,...,x )= f (x ), X1,...,XN 1 1 X1 n n=1 where the particular choice of X is unimportant because f (x)=···=f (x). 1 X1 XN Why is i.i.d. so important? • If a set of random variables are i.i.d., then the joint PDF can be written as a product of PDFs. • IntegratingajointPDFisdifficult.IntegratingaproductofPDFsismucheasier. Example 5.11.LetX ,X ,...,X beasequenceofi.i.d.Gaussianrandomvariables 1 2 N where each X has a PDF i 1 (cid:26) x2(cid:27) f (x)= √ exp − . Xi 2π 2 The joint PDF of X ,X ,...,X is 1 2 N (cid:89)N (cid:26) 1 (cid:26) x2(cid:27)(cid:27) f (x ,...,x )= √ exp − i X1,...,XN 1 N 2π 2 i=1 (cid:18) 1 (cid:19)N (cid:40) (cid:88)N x2(cid:41) = √ exp − i , 2π 2 i=1 whichisafunctiondependingnotontheindividualvaluesofx ,x ,...,x butonthe 1 2 N sum(cid:80)N x2.Sowehave“compressed”anN-dimensionalfunctionintoa1Dfunction. i=1 i Example5.12.Letθbeadeterministicnumberthatwassentthroughanoisychannel. WemodelthenoiseasanadditiveGaussianrandomvariablewithmean0andvariance σ2. Supposing we have observed measurements X = θ+W , for i = 1,...,N, where i i W ∼Gaussian(0,σ2), then the PDF of each X is i i 1 (cid:26) (x−θ)2(cid:27) f (x)= √ exp − . Xi 2πσ2 2σ2 254
5.1. JOINT PMF AND JOINT PDF Thus the joint PDF of (X ,X ,...,X ) is 1 2 N f (x ,...,x )=(cid:89)N (cid:26) √ 1 exp(cid:26) −(x i−θ)2(cid:27)(cid:27) X1,...,XN 1 N 2πσ2 2σ2 i=1 =(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x i−θ)2(cid:41) . 2πσ2 2σ2 i=1 Essentially, this joint PDF tells us the probability density of seeing sample data x ,...,x . 1 N 5.1.7 Joint CDF We now introduce the cumulative distribution function (CDF) for multiple variables. Definition 5.7. Let X and Y be two random variables. The joint CDF of X and Y is the function F (x,y) such that X,Y F (x,y)=P[X ≤x ∩ Y ≤y]. (5.7) X,Y This definition can be more explicitly written as follows. Definition 5.8. If X and Y are discrete, then (cid:88) (cid:88) F (x,y)= p (x(cid:48),y(cid:48)). (5.8) X,Y X,Y y(cid:48)≤yx(cid:48)≤x If X and Y are continuous, then (cid:90) y (cid:90) x F (x,y)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48). (5.9) X,Y X,Y −∞ −∞ If the two random variables are independent, then we have (cid:90) x (cid:90) y F (x,y)= f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) =F (x)F (y). X,Y X Y X Y −∞ −∞ Example 5.13. Let X and Y be two independent uniform random variables Uniform(0,1). Find the joint CDF. Solution. (cid:90) x (cid:90) y (cid:90) x (cid:90) y F (x,y)= f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) = 1dx(cid:48) 1dy(cid:48) =xy. X,Y X Y 0 0 0 0 255
CHAPTER 5. JOINT DISTRIBUTIONS Practice Exercise 5.5. Let X and Y be two independent uniform random variables Gaussian(µ,σ2). Find the joint CDF. Solution. Let Φ(·) be the CDF of the standard Gaussian. F (x,y)=F (x)F (y) X,Y X Y (cid:90) x (cid:90) y (cid:18) x−µ(cid:19) (cid:18) y−µ(cid:19) = f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) =Φ Φ . X Y σ σ −∞ −∞ Here are a few properties of the CDF: (cid:90) −∞(cid:90) x (cid:90) x F (x,−∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) = 0dx(cid:48) =0, X,Y X,Y −∞ −∞ −∞ (cid:90) y (cid:90) −∞ (cid:90) y F (−∞,y)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) = 0dy(cid:48) =0, X,Y X,Y −∞ −∞ −∞ (cid:90) −∞(cid:90) −∞ F (−∞,−∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) =0, X,Y X,Y −∞ −∞ (cid:90) ∞ (cid:90) ∞ F (∞,∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) =1. X,Y X,Y −∞ −∞ In addition, we can obtain the marginal CDF as follows. Proposition 5.1. Let X and Y be two random variables. The marginal CDF is F (x)=F (x,∞), (5.10) X X,Y F (y)=F (∞,y). (5.11) Y X,Y Proof. We prove only the first case. The second case is similar. (cid:90) x (cid:90) ∞ (cid:90) y F (x,∞)= f (x(cid:48),y(cid:48))dy(cid:48) dx(cid:48) = f (x(cid:48))dx(cid:48) =F (x). (cid:3) X,Y X,Y X X −∞ −∞ −∞ By the fundamental theorem of calculus, we can derive the PDF from the CDF. Definition 5.9. Let F (x,y) be the joint CDF of X and Y. Then, the joint PDF X,Y is ∂2 f (x,y)= F (x,y). (5.12) X,Y ∂y ∂x X,Y The order of the partial derivatives can be switched, yielding a symmetric result: ∂2 f (x,y)= F (x,y). X,Y ∂x∂y X,Y 256
5.2. JOINT EXPECTATION Example 5.14. Let X and Y be two uniform random variables with joint CDF F (x,y)=xy for 0≤x≤1 and 0≤y ≤1. Find the joint PDF. X,Y Solution. ∂2 ∂2 f (x,y)= F (x,y)= xy =1, X,Y ∂x∂y X,Y ∂x∂y which is consistent with the definition of a joint uniform random variable. Practice Exercise 5.6.LetX andY betwoexponentialrandomvariableswithjoint CDF F (x,y)=(1−e−λx)(1−e−λy), x≥0, y ≥0. X,Y Find the joint PDF. Solution. ∂2 ∂2 f (x,y)= F (x,y)= (1−e−λx)(1−e−λy) X,Y ∂x∂y X,Y ∂x∂y = ∂ (cid:0) (1−e−λx)(λe−λy)(cid:1) =λe−λxλe−λy. ∂x which is consistent with the definition of a joint exponential random variable. 5.2 Joint Expectation 5.2.1 Definition and interpretation When we have a single random variable, the expectation is defined as (cid:90) E[X]= xf (x)dx. X Ω For a pair of random variables, what would be a good way of defining the expectation? Certainly, we cannot just replace f (x) by f (x,y) because the integration has to be- X X,Y come a double integration. However, if it is a double integration, where should we put the variable y? It turns out that a useful way of defining the expectation for X and Y is as follows. Definition 5.10. Let X and Y be two random variables. The joint expectation is (cid:88) (cid:88) E[XY]= xy · p (x,y) (5.13) X,Y y∈ΩY x∈ΩX 257
CHAPTER 5. JOINT DISTRIBUTIONS if X and Y are discrete, or (cid:90) (cid:90) E[XY]= xy · f (x,y)dxdy (5.14) X,Y y∈ΩY x∈ΩX if X and Y are continuous. Joint expectation is also called correlation. The double summation and integration on the right-hand side of the equation is nothing but the state times the probability. Here, the state is the product xy, and the probability is the joint PMF p (x,y) (or PDF). Therefore, as long as you agree that joint expectation X,Y should be defined as E[XY], the double summation and the double integration make sense. ThebiggestmysteryhereisE[XY].Youmaywonderwhythejointexpectationshould be defined as the expectation of the product E[XY]. Why not the sum E[X +Y], or the differenceE[X−Y],orthequotientE[X/Y]?WhyarewesodeeplyinterestedinX timesY? Theseareexcellentquestions.Thatthejointexpectationisdefinedastheproducthastodo with the correlation between two random variables. We will take a small detour into linear algebra. Let us consider two discrete random variables X and Y, both with N states. So X will take the states {x ,x ,...,x } and Y will take the states {y ,y ,...,y }. Let’s define 1 2 N 1 2 N them as two vectors: xd =ef [x ,...,x ]T and y d =ef [y ,...,y ]T. Since X and Y are random 1 N 1 N variables, they have a joint PMF p (x,y). The array of the PMF values can be written X,Y as a matrix:   p (x ,y ) p (x ,y ) ··· p (x ,y ) X,Y 1 1 X,Y 1 2 X,Y 1 N PMF as a matrix=P d =ef   p X,Y(x . . .2,y 1) p X,Y(x . . .2,y 2) · ..· .· p X,Y(x . . .2,y N)   . p (x ,y ) p (x ,y ) ··· p (x ,y ) X,Y N 1 X,Y N 2 X,Y N N Let’strytowritethejointexpectationintermsofmatricesandvectors.Thedefinition of a joint expectation tells us that N N (cid:88)(cid:88) E[XY]= x y · p (x ,y ), i j X,Y i j i=1j=1 which can be written as    p (x ,y ) ··· p (x ,y ) y X,Y 1 1 X,Y 1 N 1 E[XY]=(cid:2) x 1 ··· x N(cid:3)  . . . ... . . .    . . .  =xTPy. (cid:124) (cid:123)(cid:122) (cid:125) p (x ,y ) ··· p (x ,y ) y xT X,Y N 1 X,Y N N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) P y This is a weighted inner product between x and y using the weight matrix P. Why correlation is defined as E[XY] • E[XY] is a weighted inner product between the states: E[XY]=xTPy. 258
5.2. JOINT EXPECTATION • x and y are the states of the random variables X and Y. • The inner product measures the similarity between two vectors. Example 5.15. Let X be a discrete random variable with N states, where each state hasanequalprobability.Thus,p (x)=1/N forallx.LetY =X beanothervariable. X Then the joint PMF of (X,Y) is (cid:40) 1, x=y, p (x,y)= N X,Y 0, x(cid:54)=y. It follows that the joint expectation is N N N (cid:88)(cid:88) 1 (cid:88) E[XY]= x y ·p (x ,y )= x y . i j X,Y i j N i i i=1j=1 i=1 Equivalently, we can obtain the result via the inner product by defining 1 0 ··· 0 N 0 1 ··· 0 1 P =   . . . N . . . ... . . .   = NI. 0 ··· ··· 1 N In this case, the weighted inner product is xTy 1 (cid:88)N xTPy = = x y =E[XY]. N N i i i=1 How do we understand the inner product? Ignoring the matrix P for a moment, we recall an elementary result in linear algebra. Definition 5.11. Let x ∈ RN and y ∈ RN be two vectors. Define the cosine angle cosθ as xTy cosθ = , (5.15) (cid:107)x(cid:107)(cid:107)y(cid:107) (cid:113) (cid:113) where (cid:107)x(cid:107) = (cid:80)N x2 is the norm of the vector x, and (cid:107)y(cid:107) = (cid:80)N y2 is the i=1 i i=1 i norm of the vector y. This definition can be understood as the geometry between two vectors, as illustrated in Figure 5.8. If the two vectors x and y are parallel so that x = αy for some α, then the angle θ =0. If x and y are orthogonal so that xTy =0, then θ =π/2. Therefore, the inner product xTy tells us the degree of correlation between the vectors x and y. 259
CHAPTER 5. JOINT DISTRIBUTIONS Figure5.8:Thegeometryofjointexpectation.E[XY]givesusthecosineanglebetweenthetworandom variables. This, in turn, tells us the correlation between the two random variables. Now let’s come back to our discussion about the joint expectation. The cosine angle definition tells us that if E[XY]=xTPy, the following form would make sense: xTPy E[XY] cosθ = = . (cid:107)x(cid:107)(cid:107)y(cid:107) (cid:107)x(cid:107)(cid:107)y(cid:107) That is, as long as we can find out the norms (cid:107)x(cid:107) and (cid:107)y(cid:107), we will be able to interpret E[XY] from the cosine angle perspective. But what would be a reasonable definition of (cid:107)x(cid:107) and (cid:107)y(cid:107)? We define the norm by first considering the variance of the random variable X and Y: N (cid:88) E[X2]= x x · p (x ) i i X i i=1    p (x ) ··· 0 x X 1 1 =(cid:2) x 1 ··· x N(cid:3)  . . . ... . . .    . . .   (cid:124) (cid:123)(cid:122) (cid:125) 0 ··· p (x ) x xT X N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) PX x =xTP x=(cid:107)x(cid:107)2 , X PX where P is the diagonal matrix storing the probability masses of the random variable X. X It is not difficult to show that P = diag(P1) by following the definition of the marginal X distributions(whicharethecolumnandrowsumsofthejointPMF).Similarlywecandefine N (cid:88) E[Y2]= y y · p (y ) j j Y j j=1    p (y ) ··· 0 y Y 1 1 =(cid:2) y 1 ··· y N(cid:3)  . . . ... . . .    . . .   (cid:124) (cid:123)(cid:122) (cid:125) 0 ··· p (y ) y yT Y N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) PY y =yTP y =(cid:107)y(cid:107)2 . Y PY 260
5.2. JOINT EXPECTATION Therefore, one way to define the cosine angle is to start with xTP y cosθ = XY , (cid:107)x(cid:107) (cid:107)y(cid:107) PX PY (cid:112) (cid:112) where P =P, (cid:107)x(cid:107) = xTP x and (cid:107)y(cid:107) = yTP y. But writing it in terms of XY PX X PY Y the expectation, we observe that this cosine angle is exactly xTP y cosθ = XY (cid:107)x(cid:107) (cid:107)y(cid:107) PX PY E[XY] = . (cid:112)E[X2](cid:112)E[Y2] Therefore,E[XY]definesthecosineanglebetweenthetworandomvariables,which,inturn, defines the correlation between the two. A large |E[XY]| means that X and Y are highly correlated,andasmall|E[XY]|meansthatX andY arenotverycorrelated.IfE[XY]=0, then the two random variables are uncorrelated. Therefore, E[XY] tells us how the two random variables are related to each other. To further convince you that √ E[X√Y] can be interpreted as a cosine angle, we E[X2] E[Y2] show that E[XY] −1≤ ≤1, (cid:112)E[X2](cid:112)E[Y2] because if this ratio can go beyond +1 and −1, it makes no sense to call it a cosine angle. The argument follows from a very well-known inequality in probability, called the Cauchy- Schwarz inequality (for expectation), which states that −1≤ √ E[X√Y] ≤1: E[X2] E[Y2] Theorem 5.2 (Cauchy-Schwarz inequality). For any random variables X and Y, (E[XY])2 ≤E[X2]E[Y2]. (5.16) The following proof can be skipped if you are reading the book the first time. Proof. Let t∈R be a constant. Consider E[(X+tY)2]=E[X2+2tXY +t2Y2]. Since E[(X+tY)2]≥0 for any t, it follows that E[X2+2tXY +t2Y2]≥0. Expanding the left-hand side yields t2E[Y2]+2tE[XY]+E[X2]≥0. 261
CHAPTER 5. JOINT DISTRIBUTIONS Thisisaquadraticequationint,andweknowthatforanyquadraticequationat2+bt+c≥0 we must have b2−4ac≤0. Therefore, in our case, we have that (2E[XY])2−4E[Y2]E[X2]≤0, which means (E[XY])2 ≤ E[X2]E[Y2]. The equality holds when E[(X +tY)2] = 0. In this case, X = −tY for some t, i.e., the random variable X is a scaled version of Y so that the vector formed by the states of X is parallel to that of Y. (cid:3) End of the proof. 5.2.2 Covariance and correlation coefficient In many practical problems, we prefer to work with central moments, i.e., E[(X−µ )2] in- X steadofE[X2].Thisessentiallymeansthatwesubtractthemeanfromtherandomvariable. If we adopt such a centralized random variable, we can define the covariance as follows. Definition 5.12. Let X and Y be two random variables. Then the covariance of X and Y is Cov(X,Y)=E[(X−µ )(Y −µ )], (5.17) X Y where µ =E[X] and µ =E[Y]. X Y It is easy to show that if X =Y, then the covariance simplifies to the variance: Cov(X,X)=E[(X−µ )(X−µ )]=Var[X]. X X Thus, covariance is a generalization of variance. The former can handle a pair of variables, whereasthelatterisonlyforasinglevariable.Wecanalsodemonstratethefollowingresult. Theorem 5.3. Let X and Y be two random variables. Then Cov(X,Y)=E[XY]−E[X]E[Y]. (5.18) Proof. Just apply the definition of covariance: Cov(X,Y)=E[(X−µ )(Y −µ )] X Y =E[XY −Xµ −Yµ +µ µ ]=E[XY]−µ µ . (cid:3) Y X X Y X Y The next theorem concerns the sum of two random variables. Theorem 5.4. For any X and Y, a. E[X+Y]=E[X]+E[Y]. b. Var[X+Y]=Var[X]+2Cov(X,Y)+Var[Y]. 262
5.2. JOINT EXPECTATION Proof. Recall the definition of joint expectation: (cid:88)(cid:88) E[X+Y]= (x+y)p (x,y) X,Y y x (cid:88)(cid:88) (cid:88)(cid:88) = xp (x,y)+ yp (x,y) X,Y X,Y y x y x (cid:32) (cid:33) (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) (cid:88) = x p (x,y) + y p (x,y) X,Y X,Y x y y x (cid:88) (cid:88) = xp (x)+ yp (y) X Y x y =E[X]+E[Y]. Similarly, Var[X+Y]=E[(X+Y)2]−E[X+Y]2 =E[(X+Y)2]−(µ +µ )2 X Y =E[X2+2XY +Y2]−(µ2 +2µ µ +µ2) X X Y Y =E[X2]−µ2 +E[Y2]−µ2 +2(E[XY]−µ µ ) X Y X Y =Var[X]+2Cov(X,Y)+Var[Y]. (cid:3) With covariance defined, we can now define the correlation coefficient ρ, which is the cosine angle of the centralized variables. That is, ρ=cosθ E[(X−µ )(Y −µ )] = X Y . (cid:112)E[(X−µ )2]E[(Y −µ )2] X Y Recognizing that the denominator of this expression is just the variance of X and Y, we define the correlation coefficient as follows. Definition 5.13. Let X and Y be two random variables. The correlation coefficient is Cov(X,Y) ρ= . (5.19) (cid:112) Var[X]Var[Y] Since −1 ≤ cosθ ≤ 1, ρ is also between −1 and 1. The difference between ρ and E[XY] is that ρ is normalized with respect to the variance of X and Y, whereas E[XY] is not normalized. The correlation coefficient has the following properties: • ρ is always between −1 and 1, i.e., −1 ≤ ρ ≤ 1. This is due to the cosine angle definition. • When X =Y (fully correlated), ρ=+1. • When X =−Y (negatively correlated), ρ=−1. • When X and Y are uncorrelated, ρ=0. 263
CHAPTER 5. JOINT DISTRIBUTIONS 5.2.3 Independence and correlation If two random variables X and Y are independent, the joint expectation can be written as a product of two individual expectations. Theorem 5.5. If X and Y are independent, then E[XY]=E[X]E[Y]. (5.20) Proof. We only prove the discrete case because the continuous can be proved similarly. If X and Y are independent, we have p (x,y)=p (x)p (y). Therefore, X,Y X Y (cid:88)(cid:88) (cid:88)(cid:88) E[XY]= xyp (x,y)= xyp (x)p (y) X,Y X Y y x y x (cid:32) (cid:33)(cid:32) (cid:33) (cid:88) (cid:88) = xp (x) yp (y) =E[X]E[Y]. X Y x y (cid:3) In general, for any two independent random variables and two functions f and g, E[f(X)g(Y)]=E[f(X)]E[g(Y)]. The following theorem illustrates a few important relationships between independence and correlation. Theorem 5.6. Consider the following two statements: a. X and Y are independent; b. Cov(X,Y)=0. Statement (a) implies statement (b), but (b) does not imply (a). Thus, independence is a stronger condition than correlation. Proof. We first prove that (a) implies (b). If X and Y are independent, then E[XY] = E[X]E[Y]. In this case, Cov(X,Y)=E[XY]−E[X]E[Y]=E[X]E[Y]−E[X]E[Y]=0. To prove that (b) does not imply (a), we show a counterexample. Consider a discrete random variable Z with PMF p (z)=(cid:2)1 1 1 1(cid:3) . Z 4 4 4 4 Let X and Y be π π X =cos Z and Y =sin Z. 2 2 264
5.2. JOINT EXPECTATION Then we can show that E[X]=0 and E[Y]=0. The covariance is Cov(X,Y)=E[(X−0)(Y −0)] (cid:104) π π (cid:105) =E cos Zsin Z 2 2 (cid:20) (cid:21) 1 =E sinπZ 2 (cid:20) (cid:21) 1 1 1 1 1 = (sinπ0) +(sinπ1) +(sinπ2) +(sinπ3) =0. 2 4 4 4 4 The next step is to show that X and Y are dependent. To this end, we only need to show that p (x,y)(cid:54)=p (x)p (y). The joint PMF p (x,y) can be found by noting that X,Y X Y X,Y Z =0⇒X =1, Y =0, Z =1⇒X =0, Y =1, Z =2⇒X =−1, Y =0, Z =3⇒X =0, Y =−1. Thus, the PMF is  0 1 0 4 p X,Y(x,y)=1 4 0 1 4. 0 1 0 4 The marginal PMFs are p (x)=(cid:2)1 1 1(cid:3) , p (y)=(cid:2)1 1 1(cid:3) . X 4 2 4 Y 4 2 4 The product p (x)p (y) is X Y  1 1 1  16 8 16 p (x)p (y)= 1 1 1 . X Y  8 4 8  1 1 1 16 8 16 Therefore, p (x,y)(cid:54)=p (x)p (y), although E[XY]=E[X]E[Y]. X,Y X Y (cid:3) What is the relationship between independent and uncorrelated? • Independent ⇒ uncorrelated. • Independent (cid:58) uncorrelated. 5.2.4 Computing correlation from data Weclosethissectionbydiscussingaverypracticalproblem:Givenadatasetcontainingtwo columns of data points, how do we determine whether the two columns are correlated? Recall that the correlation coefficient is defined as E[XY]−µ µ ρ= X Y . σ σ X Y 265
CHAPTER 5. JOINT DISTRIBUTIONS If we have a dataset containing (x ,y )N , then the correlation coefficient can be approxi- n i n=1 mated by 1 (cid:80)N x y −xy ρ= N n=1 n n , (cid:98) (cid:113) (cid:113) 1 (cid:80)N (x −x)2 1 (cid:80)N (y −y)2 N n=1 n N n=1 n where x = 1 (cid:80)N x and y = 1 (cid:80)N y are the means. This equation should not be a N n=1 n N n=1 n surprise because essentially all terms are the empirical estimates. Thus, ρ is the empirical (cid:98) correlation coefficient determined from the dataset. As N →∞, we expect ρ→ρ. (cid:98) 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 -1 -1 -1 -2 -2 -2 -3 -3 -3 -4 -4 -4 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) ρ=−0.0038 (b) ρ=0.5321 (c) ρ=0.9656 (cid:98) (cid:98) (cid:98) Figure 5.9: Visualization of correlated variables. Each of these figures represent a scattered plot of a datasetcontaining(x ,y )N .(a)isuncorrelated.(b)issomewhatcorrelated.(c)isstronglycorrelated. n n n=1 Figure 5.9 shows three example datasets. We plot the (x ,y ) pairs as coordinates in n n the 2D plane. The first dataset contains samples that are almost uncorrelated. We can see that x does not tell us anything about y . The second dataset is moderately correlated. n n The third dataset is highly correlated: If we know x , we are almost certain to know the n corresponding y , with a small number of perturbations. n On a computer, computing the correlation coefficient can be done using built-in com- mands such as corrcoef in MATLAB and stats.pearsonr in Python. The codes to gen- erate the results in Figure 5.9(b) are shown below. % MATLAB code to compute the correlation coefficient x = mvnrnd([0,0],[3 1; 1 1],1000); figure(1); scatter(x(:,1),x(:,2)); rho = corrcoef(x) # Python code to compute the correlation coefficient import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt x = stats.multivariate_normal.rvs([0,0], [[3,1],[1,1]], 10000) plt.figure(); plt.scatter(x[:,0],x[:,1]) rho,_ = stats.pearsonr(x[:,0],x[:,1]) print(rho) 266
5.3. CONDITIONAL PMF AND PDF 5.3 Conditional PMF and PDF Whenever we have a pair of random variables X and Y that are correlated, we can define their conditional distributions, which quantify the probability of X = x given Y = y. In this section, we discuss the concepts of conditional PMF and PDF. 5.3.1 Conditional PMF We start by defining the conditional PMF for a pair of discrete random variables. Definition 5.14. Let X and Y be two discrete random variables. The conditional PMF of X given Y is p (x,y) p (x|y)= X,Y . (5.21) X|Y p (y) Y The simplest way to understand this is to view p (x|y) as P[X = x|Y = y]. That is, X|Y given that Y = y, what is the probability for X = x? To see why this perspective makes sense, let us recall the definition of a conditional probability: p (x,y) p (x|y)= X,Y X|Y p (y) Y P[X =x ∩ Y =y] = =P[X =x|Y =y]. P[Y =y] Aswecansee,thelasttwoequalitiesareessentiallythedefinitionsofconditionalprobability and the joint PMF. Howshouldweunderstandthenotationp (x|y)?Isitaone-variablefunctioninxor X|Y a two-variable function in (x,y)? What does p (x|y) tell us? To answer these questions, X|Y letusfirsttrytounderstandtherandomnessexhibitedinaconditionalPMF.Inp (x|y), X|Y the random variable Y is fixed to a specific value Y =y. Therefore there is nothing random about Y. All the possibilities of Y have already been taken care of by the denominator p (y). Only the variable x in p (x|y) has randomness. What do we mean by “fixed at a Y X|Y value Y =y”? Consider the following example. Example 5.16. Suppose there are two coins. Let X =the sum of the values of two coins, Y =the value of the first coin. Clearly, X has 3 states: 0, 1, 2, and Y has two states: either 0 or 1. When we say p (x|1), we refer to the probability mass function of X when fixing Y =1. If we do X|Y not impose this condition, the probability mass of X is simple: (cid:20) (cid:21) 1 1 1 p (x)= , , . X 4 2 4 267
CHAPTER 5. JOINT DISTRIBUTIONS However, if we include the conditioning, then p (x,1) p (x|1)= X,Y X|Y p (1) Y (cid:2) 0,2,1(cid:3) (cid:20) 2 1(cid:21) = 4 4 = 0, , . 1 3 3 6 Toputthisinplainwords,whenY =1,thereisnowayforX totakethestate0.The chance for X to take the state 1 is 2/3 because either (0,1) or (1,0) can give X = 1. The chance for X to take the state 2 is 1/3 because it has to be (1,1) in order to give X = 2. Therefore, when we say “conditioned on Y = 1”, we mean that we limit our observations to cases where Y =1. Since Y is already fixed at Y =1, there is nothing random about Y. The only variable is X. This example is illustrated in Figure 5.10. Figure5.10:SupposeX isthesumoftwocoinswithPMF0.25,0.5,0.25.LetY bethefirstcoin. When X is unconditioned, the PMF is just [0.25,0.5,0.25]. When X is conditioned on Y = 1, then“X =0”cannothappen.Therefore,theresultingPMFp (x|1)onlyhastwostates.After X|Y normalization we obtain the conditional PMF [0,0.66,0.33]. Since Y is already fixed at a particular value Y = y, p (x|y) is a probability mass X|Y function of x (we want to emphasize again that it is x and not y). So p (x|y) is a one- X|Y variable function in x. It is not the same as the usual PMF p (x). p (x|y) is conditioned X X|Y on Y =y. For example, p (x|1) is the PMF of X restricted to the condition that Y =1. X|Y In fact, it follows that (cid:88) p (x|y)= (cid:88) p X,Y(x,y) X|Y p (y) Y x∈ΩX x∈ΩX (cid:80) = x∈ΩX p X,Y(x,y) = p Y(y) =1, p (y) p (y) Y Y butthistellsusthatp (x|y)isalegitimateprobabilitymassofX.Ifwesumoverthey’s X|Y instead, then we will hit a bump: (cid:88) p (x|y)= (cid:88) p X,Y(x,y) (cid:54)=1. X|Y p (y) Y y∈ΩY y∈ΩY Therefore, while p (x|y) is a legitimate probability mass function of X, it is not a prob- X|Y ability mass function of Y. 268
5.3. CONDITIONAL PMF AND PDF Example5.17.ConsiderajointPMFgiveninthefollowingtable.Findtheconditional PMF p (x|1) and the marginal PMF p (x). X|Y X Y= 1 2 3 4 X = 1 1 1 1 0 20 20 20 20 2 1 2 3 1 20 20 20 20 3 1 2 3 1 20 20 20 20 4 0 1 1 1 20 20 20 20 Solution. To find the marginal PMF, we sum over all the y’s for every x: 4 (cid:88) 1 1 1 0 3 x=1: p (1)= p (1,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 1 2 2 1 6 x=2: p (2)= p (2,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 1 3 3 1 8 x=3: p (3)= p (3,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 0 1 1 1 3 x=4: p (4)= p (4,y)= + + + = . X X,Y 20 20 20 20 20 y=1 Hence, the marginal PMF is p (x)=(cid:2) 3 6 8 3 (cid:3) . X 20 20 20 20 The conditional PMF p (x|1) is X|Y p (x|1)= p X,Y(x,1) = (cid:2) 21 0 21 0 21 0 20 0(cid:3) =(cid:2)1 1 1 0(cid:3) . X|Y p (1) 3 3 3 3 Y 20 Practice Exercise 5.7. Consider two random variables X and Y defined as follows.  (cid:40) 102, with prob 5/6, 10−4Y, with prob 1/2, Y = X = 10−3Y, with prob 1/3, 104, with prob 1/6. 10−2Y, with prob 1/6. Find p (x|y), p (x) and p (x,y). X|Y X X,Y Solution.SinceY takestwodifferentstates,wecanenumerateY =102 andY =104. 269
CHAPTER 5. JOINT DISTRIBUTIONS This gives us  1/2, if x=0.01,  p (x|102)= 1/3, if x=0.1, X|Y 1/6, if x=1.  1/2, if x=1,  p (x|104)= 1/3, if x=10, X|Y 1/6, if x=100. The joint PMF p (x,y) is X,Y (cid:0)1(cid:1)(cid:0)5(cid:1) , x=0.01,  2 6 p (x,102)=p (x|102)p (102)= (cid:0)1(cid:1)(cid:0)5(cid:1) , x=0.1, X,Y X|Y Y 3 6 (cid:0)1(cid:1)(cid:0)5(cid:1) , x=1. 6 6 (cid:0)1(cid:1)(cid:0)1(cid:1) , x=1,  2 6 p (x,104)=p (x|104)p (104)= (cid:0)1(cid:1)(cid:0)1(cid:1) , x=10, X,Y X|Y Y 3 6 (cid:0)1(cid:1)(cid:0)1(cid:1) , x=100. 6 6 Therefore, the joint PMF is given by the following table. 104 0 0 1 1 1 12 18 36 102 5 5 5 0 0 12 18 36 0.01 0.1 1 10 100 The marginal PMF p (x) is thus X (cid:88) p (x)= p (x,y) X X,Y y =(cid:2) 5 5 2 1 1 (cid:3) . 12 18 9 18 36 In the previous two examples, what is the probability P[X ∈ A|Y = y] or the proba- bility P[X ∈A] for some events A? The answers are giving by the following theorem. Theorem 5.7. Let X and Y be two discrete random variables. Let A be an event. (cid:88) P[X ∈A|Y =y]= p (x|y) X|Y x∈A and (cid:88) (cid:88) (cid:88) P[X ∈A]= p (x|y)p (y)= P[X ∈A|Y =y]p (y). X|Y Y Y x∈Ay∈ΩY y∈ΩY Proof.ThefirststatementisbasedonthefactthatifAcontainsafinitenumberofelements, 270
5.3. CONDITIONAL PMF AND PDF then P[X ∈A] is equivalent to the sum (cid:80) P[X =x]. Thus, x∈A P[X ∈A∩Y =y] P[X ∈A|Y =y]= P[Y =y] (cid:80) P[X =x∩Y =y] = x∈A P[Y =y] (cid:88) = p (x|y). X|Y x∈A (cid:80) The second statement holds because the inner summation p (x|y)p (y) is just y∈ΩY X|Y Y the marginal PMF p (x). Thus the outer summation yields the probability. X (cid:3) Example 5.18. Let us follow up on Example 5.17. What is the probability that P[X >2|Y =1]? What is the probability that P[X >2]? Solution. Since the problem asks about the conditional probability, we know that it can be computed by using the conditional PMF. This gives us (cid:88) P[X >2|Y =1]= p (x|1) X|Y x>2 =(cid:24)p X(cid:24) |Y(cid:24) (1(cid:24) |(cid:24) 1)+(cid:24)p X(cid:24) |Y(cid:24) (2(cid:24) |(cid:24) 1)+p X|Y(3|1)+p X|Y(4|1)= 31 . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) 1 0 3 The other probability is P[X >2]=(cid:88) p (x)=(cid:24)p (cid:24) ((cid:24) 1)+(cid:24)p (cid:24) ((cid:24) 2)+p (3)+p (4)= 11 . X X X X X 20 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x>2 8 3 20 20 What is the rule of thumb for conditional distribution? • The PMF/PDF should match with the probability you are finding. • If you want to find the conditional probability P[X ∈ A|Y = y], use the condi- tional PMF p (x|y). X|Y • If you want to find the probability P[X ∈A], use the marginal PMF p (x). X Finally, we define the conditional CDF for discrete random variables. Definition 5.15. Let X and Y be discrete random variables. Then the conditional CDF of X given Y =y is (cid:88) F (x|y)=P[X ≤x|Y =y]= p (x(cid:48)|y). (5.22) X|Y X|Y x(cid:48)≤x 271
CHAPTER 5. JOINT DISTRIBUTIONS 5.3.2 Conditional PDF We now discuss the conditioning of a continuous random variable. Definition 5.16. Let X and Y be two continuous random variables. The conditional PDF of X given Y is f (x,y) f (x|y)= X,Y . (5.23) X|Y f (y) Y Example 5.19. Let X and Y be two continuous random variables with a joint PDF (cid:40) 2e−xe−y, 0≤y ≤x<∞, f (x,y)= X,Y 0, otherwise. Find the conditional PDFs f (x|y) and f (y|x). X|Y Y|X Solution. We first find the marginal PDFs. (cid:90) ∞ (cid:90) x f (x)= f (x,y)dy = 2e−xe−y dy =2e−x(1−e−x), X X,Y −∞ 0 (cid:90) ∞ (cid:90) ∞ f (y)= f (x,y)dx= 2e−xe−y dx=2e−2y. Y X,Y −∞ y Thus, the conditional PDFs are f (x,y) f (x|y)= X,Y X|Y f (y) Y 2e−xe−y = =e−(x+y), x≥y, 2e−2y f (x,y) f (y|x)= X,Y Y|X f (x) X 2e−xe−y e−y = = , 0≤y <x. 2e−x(1−e−x) 1−e−x Where does the conditional PDF come from? We cannot duplicate the argument we used for the discrete case because the denominator of a conditional PMF becomes P[Y =y]=0 when Y is continuous. To answer this question, we first define the conditional CDF for continuous random variables. Definition 5.17. Let X and Y be continuous random variables. Then the conditional CDF of X given Y =y is (cid:82)x f (x(cid:48),y)dx(cid:48) F (x|y)= −∞ X,Y . (5.24) X|Y f (y) Y 272
5.3. CONDITIONAL PMF AND PDF WhyshouldtheconditionalCDFofcontinuousrandomvariablebedefinedinthisway?One waytointerpretF (x|y)isasthelimitingperspective.WecandefinetheconditionalCDF X|Y as F (x|y)= lim P(X ≤x|y ≤Y ≤y+h) X|Y h→0 P(X ≤x∩y ≤Y ≤y+h) = lim . h→0 P[y ≤Y ≤y+h] With some calculations, we have that lim P(X ≤x∩y ≤Y ≤y+h) = lim (cid:82) −x ∞(cid:82) yy+h f X,Y(x(cid:48),y(cid:48))dy(cid:48) dx(cid:48) h→0 P[y ≤Y ≤y+h] h→0 (cid:82)y+h f (y(cid:48))dy(cid:48) y Y (cid:82)x f (x(cid:48),y(cid:48))dx(cid:48)·h = lim −∞ X,Y h→0 f Y(y)·h (cid:82)x f (x(cid:48),y(cid:48))dx(cid:48) = −∞ X,Y . f (y) Y The key here is that the small step size h in the numerator and the denominator will cancel each other out. Now, given the conditional CDF, we can verify the definition of the conditional PDF. It holds that d f (x|y)= F (x|y) X|Y dx X|Y = d (cid:40)(cid:82) −x ∞f X,Y(x(cid:48),y)dx(cid:48)(cid:41) ( =a) f X,Y(x,y) , dx f (y) f (y) Y Y where (a) follows from the fundamental theorem of calculus. JustliketheconditionalPMF,wecancalculatetheprobabilitiesusingtheconditional PDFs. In particular, if we evaluate the probability where X ∈ A given that Y takes a particular value Y =y, then we can integrate the conditional PDF f (x|y), with respect X|Y to x. Theorem 5.8. Let X and Y be continuous random variables, and let A be an event. (i) P[X ∈A|Y =y]=(cid:82) f (x|y)dx, A X|Y (ii) P[X ∈A]=(cid:82) P[X ∈A|Y =y]f (y)dy. ΩY Y Example 5.20. Let X be a random bit such that (cid:40) +1, with prob 1/2, X = −1, with prob 1/2. Suppose that X is transmitted over a noisy channel so that the observed signal is Y =X+N, 273
CHAPTER 5. JOINT DISTRIBUTIONS where N ∼Gaussian(0,1) is the noise, which is independent of the signal X. Find the probabilities P[X =+1|Y >0] and P[X =−1|Y >0]. Solution. First, we know that f Y|X(y|+1)= √1 e−(y− 21)2 and f Y|X(y|−1)= √1 e−(y+ 21)2 . 2π 2π Therefore, integrating y from 0 to ∞ gives us P[Y >0|X =+1]=(cid:90) ∞ √1 e−(y− 21)2 dy 2π 0 =1−(cid:90) 0 √1 e−(y− 21)2 dy 2π −∞ (cid:18) (cid:19) 0−1 =1−Φ =1−Φ(−1). 1 Similarly, we have P[Y >0|X =−1]=1−Φ(+1). The probability we want to find is P[X =+1|Y >0], which can be determined using Bayes’ theorem. P[Y >0|X =+1]P[X =+1] P[X =+1|Y >0]= . P[Y >0] The denominator can be found by using the law of total probability: P[Y >0]=P[Y >0|X =+1]P[X =+1] +P[Y >0|X =−1]P[X =−1] 1 =1− (Φ(+1)+Φ(−1)) 2 1 = , 2 since Φ(+1)+Φ(−1)=Φ(+1)+1−Φ(+1)=1. Therefore, P[X =+1|Y >0]=1−Φ(−1) =0.8413. The implication is that if Y > 0, the probability P[X = +1|Y > 0] = 0.8413. The complement of this result gives P[X =−1|Y >0]=1−0.8413=0.1587. Practice Exercise 5.8. Find P[Y >y], where X ∼Uniform[1,2], Y |X ∼Exponential(x). Solution.Thetrickypartofthisproblemisthetendencytoconfusethetwovariables X andY.Onceyouunderstandtheirrolestheproblembecomeseasy.Firstnoticethat Y |X ∼ Exponential(x) is a conditional distribution. It says that given X = x, the 274
5.4. CONDITIONAL EXPECTATION probabilitydistributionofY isexponential,withtheparameterx.Thus,wehavethat f (y|x)=xe−xy. Y|X Why? Recall that if Y ∼ Exponential(λ) then f (y) = λe−λy. Now if we replace λ Y with x, we have xe−xy. So the role of x in this conditional density function is as a parameter. Given this property, we can compute the conditional probability: (cid:90) ∞ P[Y >y|X =x]= f (y(cid:48)|x)dy(cid:48) Y|X y (cid:90) ∞ (cid:20) (cid:21)∞ = xe−xy(cid:48) dy(cid:48) = −e−xy(cid:48) =e−xy. y y(cid:48)=y Finally, we can compute the marginal probability: (cid:90) P[Y >y]= P[Y >0|X =x(cid:48)]f (x(cid:48))dx(cid:48) X ΩX =(cid:90) 1 e−x(cid:48)y dx(cid:48) =(cid:20) 1 e−x(cid:48)y(cid:21)x(cid:48)=1 = 1(cid:0) 1−e−y(cid:1) . y y 0 x(cid:48)=0 Wecandouble-checkthisresultbynotingthattheproblemasksabouttheprobability P[Y >y]. Thus, the answer must be a function of y but not of x. 5.4 Conditional Expectation 5.4.1 Definition When dealing with two dependent random variables, at times we would like to determine the expectation of a random variable when the second random variable takes a particular state. The conditional expectation is a formal way of doing so. Definition 5.18. The conditional expectation of X given Y =y is (cid:88) E[X|Y =y]= xp (x|y) (5.25) X|Y x for discrete random variables, and (cid:90) ∞ E[X|Y =y]= xf (x|y)dx (5.26) X|Y −∞ for continuous random variables. 275
CHAPTER 5. JOINT DISTRIBUTIONS Therearetwopointstonotehere.First,theexpectationofE[X|Y =y]istakenwithrespect to f (x|y). We assume that the random variable Y is already fixed at the state Y = y. X|Y Thus, the only source of randomness is X. Secondly, since the expectation E[X|Y =y] has eliminated the randomness of X, the resulting function is in y. What is conditional expectation? • E[X|Y =y] is the expectation using f (x|y). X|Y • The integration is taken w.r.t. x, because Y =y is given and fixed. 5.4.2 The law of total expectation Theorem 5.9. The law of total expectation states that (cid:90) ∞ (cid:88) E[X]= E[X|Y =y]p (y), or E[X]= E[X|Y =y]f (y)dy. (5.27) Y Y y −∞ Proof.Wewillprovethediscretecaseonly,asthecontinuouscasecanbeprovedbyreplacing summation with integration. (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) E[X]= xp (x)= x p (x,y) X X,Y x x y (cid:88)(cid:88) = xp (x|y)p (y) X|Y Y x y (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) = xp (x|y) p (y)= E[X|Y =y]p (y). X|Y Y Y y x y (cid:3) Figure 5.11illustratestheideabehindtheproof.Essentially,wedecompose theexpec- tation E[X] into “subexpectations” E[X|Y =y]. The probability of each subexpectation is p (y). By summing the subexpectation multiplied by p (y), we obtain the overall expecta- Y Y tion. What is the law of total expectation? • The law of total expectation is a decomposition rule. • It decomposes E[X] into smaller/easier conditional expectations. This law can also be written in a more compact form. Corollary 5.1. Let X and Y be two random variables. Then E[X]=E (cid:2)E [X|Y](cid:3) . (5.28) Y X|Y 276
5.4. CONDITIONAL EXPECTATION Figure 5.11: The expectation E[X] can be decomposed into a set of subexpectations. This gives us E[X]=(cid:80) E[X|Y =y]p (y). y Y Proof.TheprevioustheoremstatesthatE[X]=(cid:80) E[X|Y =y]p (y).IfwetreatE[X|Y = y Y y] as a function of y, for instance h(y), then (cid:88) (cid:88) E[X]= E[X|Y =y]p (y)= h(y)p (y)=E[h(Y)]=E[E[X|Y]]. (cid:3) Y Y y y Example 5.21. Suppose there are two classes of cars. Let X be the speed of a car and C be the class. When C =1, we know that X ∼Gaussian(µ ,σ ). We know that 1 1 P[C =1]=p. When C =2, X ∼Gaussian(µ ,σ ). Also, P[C =2]=1−p. If you see 2 2 a car on the freeway, what is its average speed? Solution. The problem has given us everything we need. In particular, we know that the conditional PDFs are: 1 (cid:26) (x−µ )2(cid:27) f (x|1)= exp − 1 , X|C (cid:112) 2πσ2 2σ2 1 1 1 (cid:26) (x−µ )2(cid:27) f (x|2)= exp − 2 . X|C (cid:112) 2πσ2 2σ2 2 2 Therefore, conditioned on C, we have two expectations: (cid:90) ∞ E[X|C =1]= xf (x|1)dx=µ , X|C 1 −∞ (cid:90) ∞ E[X|C =2]= xf (x|2)dx=µ . X|C 2 −∞ The overall expectation E[X] is E[X]=E[X|C =1]P[C =1]+E[X|C =2]P[C =2] =pµ +(1−p)µ . 1 2 277
CHAPTER 5. JOINT DISTRIBUTIONS Practice Exercise 5.9. Consider a joint PMF given by the following table. Find E[X|Y =102] and E[X|Y =104]. Y 104 0 0 1 1 1 12 18 36 102 5 5 5 0 0 12 18 36 0.01 0.1 1 10 100 X Solution. To find the conditional expectation, we first need to know the conditional PMF. p (x|102)=(cid:2)1 1 1 0 0(cid:3) , X|Y 2 3 6 p (x|104)=(cid:2) 0 0 1 1 1(cid:3) . X|Y 2 3 6 Therefore, the conditional expectations are (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X|Y =102]=(10−2) +(10−1) +(1) 2 3 6 123 = , 600 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X|Y =104]=(1) +(10) +(100) 2 3 6 123 = . 6 From the conditional expectations we can also find E[X]: E[X]=E[X|Y =102]p (102) Y +E[X|Y =104]p (104) Y (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19) 123 5 123 1 = + 600 6 6 6 =3.5875. Example 5.22. Consider two random variables X and Y. The random variable X is Gaussian-distributed with X ∼ Gaussian(µ,σ2). The random variable Y has a conditional distribution Y|X ∼Gaussian(X,X2). Find E[Y]. Solution. The notation Y|X ∼ Gaussian(X,X2) means that given the variable X, the other variable Y has a conditional distribution Gaussian(X,X2). That is, the variable Y is a Gaussian with mean X and variance X2. How can the mean be a random variable X and the variance be another random variable X2? Because X is the conditional variable. Y|X means that you have already chosen one state of X. Given that particular state, the distribution of Y follows f . Therefore, for this Y|X 278
5.4. CONDITIONAL EXPECTATION problem, we know the PDFs: 1 (cid:26) (x−µ)2(cid:27) f (x)= √ exp − , X 2πσ2 2σ2 1 (cid:26) (y−x)2(cid:27) f (y|x)= √ exp − . Y|X 2πx2 2x2 The conditional expectation of Y given X is (cid:90) ∞ 1 (cid:26) (y−x)2(cid:27) E[Y|X =x]= y√ exp − dy 2πx2 2x2 −∞ =E[Gaussian(x,x2)]=x. The last equality holds because we are computing the expectation of a Gaussian ran- domvariablewithmeanx.Finally,applyingthelawoftotalexpectation,wecanshow that (cid:90) ∞ E[Y]= E[Y|X =x]f (x)dx X −∞ (cid:90) ∞ 1 (cid:26) (x−µ)2(cid:27) = x√ exp − dx 2πσ2 2σ2 −∞ =E[Gaussian(µ,σ2)]=µ, where the last equality is based on the fact that it is the mean of a Gaussian. Practice Exercise 5.10. Find E[sin(X +Y)], if X ∼ Gaussian(0,1), and Y |X ∼ Uniform[x−π,x+π]. Solution. We know that the conditional density is 1 f (y|x)= , x−π ≤y ≤x+π. Y|X 2π Therefore, we can compute the probability (cid:90) x+π E[sin(X+Y)|X =x]= sin(x+y)f (y|x)dy Y|X x−π 1 (cid:90) x+π = sin(x+y)dy =0. 2π x−π (cid:124) (cid:123)(cid:122) (cid:125) =0 Hence, the overall expectation is E[sin(X+Y)]=(cid:90) 1 E[sin(X+Y)|X =x]√1 e−x 22 dx=0. 0 (cid:124) (cid:123)(cid:122) (cid:125) 2π =0 279
CHAPTER 5. JOINT DISTRIBUTIONS 5.5 Sum of Two Random Variables One typical problem we encounter in engineering is to determine the PDF of the sum of two random variables X and Y, i.e., X+Y. Such a problem arises naturally when we want to evaluate the average of many random variables, e.g., the sample mean of a collection of data points. This section will discuss a general principle for determining the PDF of a sum of two random variables. 5.5.1 Intuition through convolution First, consider two random variables, X and Y, both discrete uniform random variables in the range of 0,1,2,3. That is, p (x) = p (y) = [1/4,1/4,1/4,1/4]. Since this is such a X Y simpleproblemwecanenumerateallthepossiblecasesofthesumZ =X+Y.Theresulting probabilities are shown in the following table. Z =X+Y Cases, written in terms of (X, Y) Probability 0 (0,0) 1/16 1 (0,1), (1,0) 2/16 2 (1,1), (2,0), (0,2) 3/16 3 (3,0), (2,1), (1,2), (0,3) 4/16 4 (3,1), (2,2), (1,3) 3/16 5 (3,2), (2,3) 2/16 6 (3,3) 1/16 Clearly,thePMFofZ isnotf (z)=f (x)+f (y).(Caution!Donotwritethis.)The Z X Y PMF of Z looks like a triangle distribution. How can we get to this triangle distribution from two uniform distributions? The key is the idea of convolution. Let us start with the PMFofX,whichisp (x).Letusalsoflipp (y)overthey-axis.Asweshifttheflippedp , X Y Y we multiply and add the PMF values as shown in Figure 5.12. This gives us p (0)=P[X+Y =0] Z =P[(X,Y)=(0,0)] =p (0)p (0) X Y 1 = . 16 Now, if we shift towards the right by 1, we have p (1)=P[X+Y =1] Z =P[(X,Y)=(0,1)∪(0,1)] 2 =p (0)p (1)+p (1)p (0)= . X Y X Y 16 By continuing our argument, you can see that we will obtain the same PMF as the one shown in the table. 280
5.5. SUM OF TWO RANDOM VARIABLES Figure5.12:WhensummingtworandomvariablesX andY,weareeffectivelytakingtheconvolutions of the two respective PMF / PDFs. 5.5.2 Main result We can show that for any arbitrary random variable X and Y, the sum Z = X +Y has a distribution that is the convolution of two individual PDFs. Theorem 5.10. Let X and Y be two independent random variables with PDFs f (x) X and f (y) respectively. Let Z =X+Y. The PDF of Z is given by Y (cid:90) ∞ f (z)=(f ∗f )(z)= f (z−y)f (y)dy, (5.29) Z X Y X Y −∞ where “∗” denotes the convolution. Proof. We begin by analyzing the CDF of Z. The CDF of Z is F (z)=P[Z ≤z]=P[X+Y ≤z]. Z We now draw a picture to illustrate the line under which we want to integrate. As shown in Figure 5.13, the equation X+Y ≤z defines a straight line in the xy plane. You can think of it as Y ≤−X+z, so that the slope is −1 and the y-intercept is z. Now, shall we take the upper half of the triangle or the lower half? Since the equation is Y ≤−X+z, a value of Y has to be less than that of the line. Another easy way to check is to assume z > 0 so that we have a positive y-intercept. Then we check where the origin 281
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.13: The shaded region highlights the set X+Y ≤Z. To integrate the PDF over this region, we first take the inner integration over dx and then take the outer integration over dy. (0,0) belongs. In this case, if z >0, the origin (0,0) will satisfy the equation Y ≤−X +z, and so it must be included. Thus, we conclude that the area is below the line. Oncewehavedeterminedtheareatobeintegrated,wecanwritedowntheintegration: (cid:90) ∞ (cid:90) z−y P[X+Y ≤z]= f (x,y)dxdy X,Y −∞ −∞ (cid:90) ∞ (cid:90) z−y = f (x)f (y)dxdy, (independence) X Y −∞ −∞ where the integration limits are just a rewrite of X +Y ≤ z (in this case since we are integrating x first we have X ≤ −Y +z). Then, by the fundamental theorem of calculus, we can show that d d (cid:90) ∞ (cid:90) z−y f (z)= F (z)= f (x)f (y)dxdy Z dz Z dz X Y −∞ −∞ (cid:90) ∞ (cid:18) d (cid:90) z−y (cid:19) = f (x)f (y)dx dy dz X Y −∞ −∞ (cid:90) ∞ = f (z−y)f (y)dy =(f ∗f )(z), X Y X Y −∞ where “∗” denotes the convolution. How is convolution related to random variables? • If you sum X and Y, the resulting PDF is the convolution of f and f . X Y • E.g., convolving two uniform random variables gives you a triangle PDF. 5.5.3 Sum of common distributions Theorem 5.11 (SumoftwoPoissons). Let X ∼Poisson(λ ) and X ∼Poisson(λ ). 1 1 2 2 Then X +X ∼Poisson(λ +λ ). (5.30) 1 2 1 2 282
5.5. SUM OF TWO RANDOM VARIABLES Proof. Let us apply the convolution principle. p (k)=P[X +X =k] Y 1 2 =P[X =(cid:96) ∩ X =k−(cid:96)] 1 2 (cid:88)k λ(cid:96)e−λ1 λk−(cid:96)e−λ2 = 1 · 2 (cid:96)! (k−(cid:96))! (cid:96)=0 (cid:88)k λ(cid:96) λk−(cid:96) =e−(λ1+λ2) 1 · 2 (cid:96)! (k−(cid:96))! (cid:96)=0 k 1(cid:88) k! =e−(λ1+λ2)· λ(cid:96)λk−(cid:96) k! (cid:96)!(k−(cid:96))! 1 2 (cid:96)=0 (cid:124) (cid:123)(cid:122) (cid:125) =(cid:80)k (k)λ(cid:96)λk−(cid:96) (cid:96)=0 (cid:96) 1 2 (λ +λ )k = 1 2 e−(λ1+λ2), k! where the last step is based on the binomial identity (cid:80)k (cid:0)k(cid:1) a(cid:96)bk−(cid:96) =(a+b)k. (cid:96)=0 (cid:96) (cid:3) Theorem 5.12 (Sum of two Gaussians). Let X and X be two Gaussian random 1 2 variables such that X ∼Gaussian(µ ,σ2) and X ∼Gaussian(µ ,σ2). 1 1 1 2 2 2 Then X +X ∼Gaussian(µ +µ ,σ2+σ2). (5.31) 1 2 1 2 1 2 Proof. Let us apply the convolution principle. (cid:90) ∞ f (z)= f (t)f (z−t)dt Z X Y −∞ (cid:90) ∞ 1 (cid:26) (t−µ )2(cid:27) 1 (cid:26) (z−t−µ )2(cid:27) = √ exp − 1 · √ exp − 2 dt 2πσ2 2σ2 2πσ2 2σ2 −∞ 1 (cid:90) ∞ 1 (cid:26) (t−µ )2+(z−t−µ )2(cid:27) = √ √ exp − 1 2 dt. 2πσ2 2πσ2 2σ2 −∞ We now complete the square: (t−µ )2+(z−t−µ )2 =[t2−2µ t+µ2]+[t2+2t(µ −z)+(µ −z)2] 1 2 1 1 2 2 =2t2−2t(µ −µ +z)+µ2+(µ −z)2 1 2 1 2 (cid:20) (cid:21) µ −µ +z =2 t2−2t· 1 2 +µ2+(µ −z)2 2 1 2 (cid:20) µ −µ +z(cid:21)2 (cid:20) µ −µ +z(cid:21)2 =2 t− 1 2 −2 1 2 +µ2+(µ −z)2. 2 2 1 2 283
CHAPTER 5. JOINT DISTRIBUTIONS The last term can be simplified to (cid:20) µ −µ +z(cid:21)2 −2 1 2 +µ2+(µ −z)2 2 1 2 µ2−2µ (µ −z)+(µ −z)2 =− 1 1 2 2 +µ2+(µ −z)2 2 1 2 µ2+2µ (µ −z)+(µ −z)2 (µ +µ −z)2 = 1 1 2 2 = 1 2 . 2 2 Substituting these into the integral, we can show that 1 (cid:90) ∞ 1 (cid:40) 2(cid:2) t− µ1−µ2+z(cid:3)2 + (µ1+µ2−z)2(cid:41) f (z)= √ √ exp − 2 2 dt Z 2πσ2 2πσ2 2σ2 −∞ 1 (cid:26) (µ +µ −z)2(cid:27)(cid:90) ∞ 1 (cid:40) (cid:2) t− µ1−µ2+z(cid:3)2(cid:41) = √ exp − 1 2 √ exp − 2 dt 2πσ2 2(2σ2) 2πσ2 σ2 −∞ (cid:124) (cid:123)(cid:122) (cid:125) =√1 2 1 (cid:26) (µ +µ −z)2(cid:27) = exp − 1 2 . (cid:112) 2π(2σ)2 2(2σ2) Therefore, we have shown that the resulting distribution is a Gaussian with mean µ +µ 1 2 and variance 2σ2. (cid:3) Practice Exercise 5.11. Let X and Y be independent, and let (cid:40) (cid:40) xe−x, x≥0, ye−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Solution. Using the results derived above, we see that (cid:90) ∞ f (z)= f (z−y)f (y)dy Z X Y −∞ (cid:90) z = f (z−y)f (y)dy, X Y −∞ wheretheupperlimitz camefromthefactthatx≥0.Therefore,sinceZ =X+Y,we musthaveZ−Y =X ≥0andsoZ ≥Y.ThisisportrayedgraphicallyinFigure 5.14. Substituting the PDFs into the integration yields (cid:90) z z3 f (z)= (z−y)e−(z−y)ye−y dy = e−z, z ≥0. Z 6 0 For z <0, f (z)=0. Z The functions of two random variables are not limited to summation. The following example illustrates the case of the product of two random variables. 284
5.5. SUM OF TWO RANDOM VARIABLES Figure 5.14: [Left] The outer integral goes from 0 to z because the triangle stops at y =z. [Right] If the triangle is unbounded, then the integral goes from −∞ to ∞. Example 5.23. Let X and Y be two independent random variables such that (cid:40) (cid:40) 2x, if 0≤x≤1, 1, if 0≤y ≤1, f (x)= and f (y)= X Y 0, otherwise, 0, otherwise. Let Z =XY. Find f (z). Z Solution. The CDF of Z can be evaluated as (cid:90) ∞ (cid:90) z F (z)=P[Z ≤z]=P[XY ≤z]= y f (x)f (y)dxdy. Z X Y −∞ −∞ Taking the derivative yields d d (cid:90) ∞ (cid:90) yz f (z)= F (z)= f (x)f (y)dxdy Z dz Z dz X Y −∞ −∞ (a)(cid:90) ∞ 1 (cid:18) z(cid:19) = f f (y)dy, y X y Y −∞ where (a) holds by the fundamental theorem of calculus. The upper and lower limit of this integration can be determined by noting that z 0≤ =x≤1, y which implies that z ≤y. Since y ≤1, we have that z ≤y ≤1. Therefore, the PDF is (cid:90) 1 1 (cid:18) z(cid:19) f (z)= f f (y)dy Z y X y Y z (cid:90) 1 2z = dy =2(1−z), z ≥0. y2 z For z <0, f (z)=0. Z 285
CHAPTER 5. JOINT DISTRIBUTIONS Closing remark. For some random variables, summing two i.i.d. copies remain the same random variable (but with different parameters). For other random variables, summing two i.i.d. copies gives a different random variable. Table 5.1 summarizes some of the most commonly used random variable pairs. X X Sum X +X 1 2 1 2 Bernoulli(p) Bernoulli(p) Binomial(2,p) Binomial(n,p) Binomial(m,p) Binomial(m+n,p) Poisson(λ ) Poisson(λ ) Poisson(λ +λ ) 1 2 1 2 Exponential(λ) Exponential(λ) Erlang(2,λ) Gaussian(µ ,σ2) Gaussian(µ ,σ2) Gaussian(µ +µ ,σ2+σ2) 1 1 2 2 1 2 1 2 Table 5.1: Common distributions of the sum of two random variables. 5.6 Random Vectors and Covariance Matrices We now enter the second part of this chapter. In the first part, we were mainly interested in a pair of random variables. In the second part, however, we will study vectors of N random variables. To understand a vector of random variables, we will not drill down to the integrations of the PDFs (which you would certainly not enjoy). Instead, we will blend linear algebra tools and probabilistictools to learn afew practical data analysis techniques. 5.6.1 PDF of random vectors Joint distributions can be generalized to more than two random variables. The most conve- nient way is to consider a vector of random variables and their corresponding states.     X x 1 1 X 2 x 2 X = .  and x= . .  .   .   .   .  X x N N Ournotationhereisunconventionalsincebolduppercaselettersusuallyrepresentmatrices. Here,X denotesavector,specificallyarandomvector.Itsstateisavectorx.Inthischapter, we will use the following notational convention: X and Y represent random vectors while A represents a matrix. One way to think about X is to imagine that if you put your hand into the sample space, you will pick up a vector x. This random realization x has N entries, and so you need to specify the probability of getting all these entries simultaneously. Accordingly, we should expect that X is characterized by an N-dimensional PDF f (x)=f (x ,x ,...,x ). X X1,X2,...,XN 1 2 N 286
5.6. RANDOM VECTORS AND COVARIANCE MATRICES Essentially, this PDF tells us the probability density for random variable X =x , random 1 1 variable X =x , etc. It is a coordinate-wise description. For example, if X contains three 2 2 elements such that X = [X ,X ,X ]T, and if the state we are looking at is x = [3, 1, 7]T, 1 2 3 then f (x) is the probability density such that this 3D coordinate (X ,X ,X ) takes the X 1 2 3 value [3,1,7]T. To compute the probability, we can integrate f (x) with respect to x. Let A be the X event. Then (cid:90) P[X ∈A]= f (x)dx X A (cid:90) (cid:90) = ··· f (x ,...,x )dx ...dx . X1,...,XN 1 N 1 N A IftherandomcoordinatesX ,...,X areindependent,thePDFcanbewrittenasaprod- 1 N uct of N individual PDFs: f (x ,...,x )=f (x )f (x )···f (x ), and so X1,...,XN 1 N X1 1 X2 2 XN N (cid:90) (cid:90) P[X ∈A]= ··· f (x )f (x )···f (x )dx ···dx . X1 1 X2 2 XN N 1 N A However, this does not necessarily simplify the calculation unless A is separable, e.g., A= [a ,b ]×[a ,b ]×···×[a ,b ]. In this case the integration becomes 1 1 2 2 N N (cid:89)N (cid:34) (cid:90) bi (cid:35) P[X ∈A]= f (x )dx , Xi i i i=1 ai which is obviously manageable. Example 5.24.LetX =[X ,...,X ]T beavectorofzero-meanunitvarianceGaus- 1 N sian random vectors. Let A=[−1,2]N. Then (cid:90) P[X ∈A]= f (x)dx X A (cid:90) (cid:90) = ··· f (x ,...,x )dx ···dx X1,···,XN 1 N 1 N A (cid:20)(cid:90) 2 (cid:21)N = f (x )dx =[Φ(2)−Φ(−1)]N, X1 1 1 −1 where Φ(·) is the standard Gaussian CDF. As you can see from the definition of a vector random variable, computing the proba- bility typically involves integrating a high-dimensional function, which is tedious. However, thegoodnewsisthatinpracticeweseldomneedtoperformsuchcalculations.Oftenweare more interested in the mean and the covariance of the random vectors because they usually carry geometric meanings. The next subsection explores this topic. 287
CHAPTER 5. JOINT DISTRIBUTIONS 5.6.2 Expectation of random vectors LetX =[X ,...,X ]T bearandomvector.Wedefinetheexpectationofarandomvector 1 N as follows. Definition 5.19. Let X =[X ,...,X ]T be a random vector. The expectation is 1 N E[X ] 1 µd =efE[X]= E[X . 2] . (5.32)  .   .  E[X ] N The resulting vector is called the mean vector. Since the mean vector is a vector of individual elements, we need to compute the marginal PDFs before computing the expec- tations: E[X ]  (cid:82) x f (x )dx  1 Ω 1 X1 1 1 E[X]= . . = . . ,  .   .  E[X ] (cid:82) x f (x )dx N Ω N XN N N where the marginal PDF is determined by (cid:90) f (x )= f (x )dx . Xn n X\n \n \n Ω In the equation above, x = [x ,...,x ,x ,...,x ]T contains all the elements with- \n 1 n−1 n+1 N out x . For example, if the PDF is f (x ,x ,x ), then n X1,X2,X3 1 2 3 (cid:90) (cid:90) E[X ]= x f (x ,x ,x )dx dx dx . 1 1 X1,X2,X3 1 2 3 2 3 1 (cid:124) (cid:123)(cid:122) (cid:125) fX1(x1) Again, this will become tedious when there are many variables. While the definition of the expectation may be challenging to understand, some prob- lemsusingitarestraightforward.WewillfirstdemonstratethecaseofindependentPoisson random variables, and then we will discuss joint Gaussians. Example 5.25. Let X = [X ,...,X ]T be a random vector such that X are inde- 1 N n pendent Poissons with X ∼Poisson(λ ). Then n n E[X 1]  (cid:80)∞ k=0k· λk 1e k− !λ1   λ 1 E[X]=  . . .  =   . . .   =  . . .  . E[X N] (cid:80)∞ k· λk Ne−λN λ N k=0 k! On computers, computing the mean vector can be done using built-in commands such as mean in MATLAB and np.mean in Python. However, caution is needed when performing thecalculation.InMATLAB,meancomputesalongfirstdimension(rowsindex).Thus,ifwe 288
5.6. RANDOM VECTORS AND COVARIANCE MATRICES have an N ×2 array, applying mean will give us a 1×2 vector. To obtain the column mean vector of size N ×1, we need to specify the direction as mean(X,2). Similarly, in Python, when calling np.mean, we need to specify the axis. % MATLAB code to compute a mean vector X = randn(100,2); mX = mean(X,2); # Python code to compute a mean vector import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100) mX = np.mean(X,axis=1) 5.6.3 Covariance matrix Definition 5.20. The covariance matrix of a random vector X =[X ,...,X ]T is 1 N   Var[X ] Cov(X ,X ) ··· Cov(X ,X ) 1 1 2 1 N Σd =ef Cov(X)=   Cov[X . . .2,X 1] Var . . .[X 2] · ..· .· Cov(X . . .2,X N)   . (5.33) Cov(X ,X ) Cov(X ,X ) ··· Var[X ] N 1 N 2 N A more compact way of writing the covariance matrix is Σ=Cov(X)=E[(X−µ)(X−µ)T], where µ = E[X] is the mean vector. The notation abT means the outer product, defined as     a a b a b ··· a b 1 1 1 1 2 1 N abT =  . . .  (cid:2) b 1 ··· b N(cid:3) =  . . . . . . ... . . .  . a a b a b ··· a b N N 1 N 2 N N It is easy to show that Cov(X)=Cov(X)T, i.e., they are symmetric. Theorem 5.13. If the coordinates X ,...,X are independent, then the covariance 1 N matrix Cov(X)=Σ is a diagonal matrix:   Var[X ] 0 ··· 0 1  0 Var[X 2] ··· 0  Σ=Cov(X)=   . . . . . . ... . . .   . 0 0 ··· Var[X ] N 289
CHAPTER 5. JOINT DISTRIBUTIONS Proof. If all X ’s are independent, then Cov(X ,X ) = 0 for all i (cid:54)= j. Substituting this i i j into the definition of the covariance matrix, we obtain the result. (cid:3) If we ignore the mean vector µ, we obtain the autocorrelation matrix R. Definition 5.21. Let X = [X ,...,X ]T be a random vector. The autocorrelation 1 N matrix is E[X X ] E[X X ] ··· E[X X ] 1 1 1 2 1 N E[X 2X 1] E[X 2X 2] ··· E[X 2X N] R=E[XXT]=   . . . . . . ... . . .   . (5.34) E[X X ] E[X X ] ··· E[X X ] N 1 N 2 N N We state without proof that Σ=R−µµT, which corresponds to the single-variable case where σ2 =E[X2]−µ2. On computers, computing the covariance matrix is done using built-in commands cov inMATLABandnp.covinPython.Likethemeanvectors,whencomputingthecovariance, we need to specify the direction. For example, for an N ×2 data matrix X, the covariance needs to be a 2×2 matrix. If we compute the covariance along the wrong direction, we will obtain an N ×N matrix, which is incorrect. % MATLAB code to compute covariance matrix X = randn(100,2); covX = cov(X); # Python code to compute covariance matrix import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100) covX = np.cov(X,rowvar=False) print(covX) 5.6.4 Multidimensional Gaussian With the above tools in hand, we can now define a high-dimensional Gaussian. The PDF of a high-dimensional Gaussian is defined as follows. Definition 5.22. A d-dimensional joint Gaussian has the PDF (cid:26) (cid:27) 1 1 f (x)= exp − (x−µ)TΣ−1(x−µ) , (5.35) X (cid:112) (2π)d|Σ| 2 where d denotes the dimensionality of the vector x. 290
5.6. RANDOM VECTORS AND COVARIANCE MATRICES ThemeanvectorandthecovariancematrixofajointGaussianisreadilyavailablefromthe definition. E[X]=µ and Cov(X)=Σ. It is easy to show that if X is a scalar X, then d = 1, µ = µ, and Σ = σ2. Substituting these into the above definition returns us the familiar 1D Gaussian. Thed-dimensionalGaussianisageneralizationofthe1DGaussian(s).SupposethatX i andX areindependentforalli(cid:54)=j.ThenE[X X ]=E[X ]E[X ]andhenceCov(X ,X )= j i j i j i j 0. Consequently, the covariance matrix Σ is a diagonal matrix:  σ2 ··· 0 1 Σ=  . . . ... . . .  , 0 ··· σ2 d where σ2 =Var[X ]. When this occurs, the exponential term in the Gaussian PDF is i i  x −µ T  σ2 ··· 0−1 x −µ  (x−µ)TΣ−1(x−µ)=  1 . . . 1     . . .1 ... . . .     1 . . . 1  =(cid:88)d (x i− σ2µ i)2 . x −µ 0 ··· σ2 x −µ i=1 i d d d d d Moreover, the determinant |Σ| is |Σ|=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  σ . . .12 · ..· .· 0 . . .   (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)=(cid:89)d σ i2. (cid:12) (cid:12) (cid:12) 0 ··· σ2 (cid:12) i=1 d Substituting these results into the joint Gaussian PDF, we obtain f (x)=(cid:89)n 1 exp(cid:26) −(x−µ i)2(cid:27) , X (cid:112) (2π)σ2 2σ2 i=1 i i which is a product of individual Gaussians. The Gaussian has different offsets and orientations for different choices of µ and Σ. Figure 5.15 shows a few examples. Note that for Σ to be valid Σ has to be “symmetric positive semi-definite”, the meaning of which will be explained shortly. GeneratingrandomnumbersfromamultidimensionalGaussiancanbedonebycalling built-in commands. In MATLAB, we use mvnrnd. In Python, we have a similar command. % MATLAB code to generate random numbers from multivariate Gaussian mu = [0 0]; Sigma = [.25 .3; .3 1]; X = mvnrnd(mu,Sigma,100); # Python code to generate random numbers from multivariate Gaussian import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],100) 291
CHAPTER 5. JOINT DISTRIBUTIONS 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 -1 -1 -1 -2 -2 -2 -3 -3 -3 -4 -4 -4 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0 5 0 1 1 −0.5 0 2 1.9 (µ,Σ)= , (µ,Σ)= , (µ,Σ)= , 2 0 0.5 2 −0.5 1 0 1.9 2 Figure 5.15: Visualization of 2D Gaussians with different means and covariances. To display the data points and overlay with the contour, we can use MATLAB com- mands such as contour. The resulting plot looks like the one shown in Figure 5.16. In Python the corresponding command is plt.contour. To set up the plotting environment we use the commands np.meshgrid. The grid points are used to evaluate the PDF values, thus giving us the contour. % MATLAB code: Overlay random numbers with the Gaussian contour. X = mvnrnd([0 0],[.25 .3; .3 1],1000); x1 = -2.5:.01:2.5; x2 = -3.5:.01:3.5; [X1,X2] = meshgrid(x1,x2); F = mvnpdf([X1(:) X2(:)],[0 0],[.25 .3; .3 1]); F = reshape(F,length(x2),length(x1)); figure(1); scatter(x(:,1),x(:,2),’rx’, ’LineWidth’, 1.5); hold on; contour(x1,x2,F,[.001 .01 .05:.1:.95 .99 .999], ’LineWidth’, 2); # Python code: Overlay random numbers with the Gaussian contour. import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt X = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],1000) x1 = np.arange(-2.5, 2.5, 0.01) x2 = np.arange(-3.5, 3.5, 0.01) X1, X2 = np.meshgrid(x1,x2) Xpos = np.empty(X1.shape + (2,)) Xpos[:,:,0] = X1 Xpos[:,:,1] = X2 F = stats.multivariate_normal.pdf(Xpos,[0,0],[[0.25,0.3],[0.3,1.0]]) plt.scatter(X[:,0],X[:,1]) plt.contour(x1,x2,F) 292
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS 3 2 1 0 -1 -2 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 x y Figure 5.16: 1000 random numbers drawn from a 2D Gaussian, overlaid with the contour plot. 5.7 Transformation of Multidimensional Gaussians As we have seen in Figure 5.15, the shape and orientation of a multidimensional Gaussian are determined by the mean vector µ and the covariance matrix Σ. This means that if we can somehow transform the mean vector and the covariance matrix, we will get another Gaussian. A few practical questions are: • How do we shift and rotate a Gaussian random variable? • If we have an arbitrary Gaussian, how do we go back to zero-mean unit-variance Gaussian? • How do we generate random vectors according to a predefined Gaussian? These questions come up frequently in data analysis. Answering the first two questions will help us transform Gaussians back and forth, while answering the last question will help us with generating random samples. 5.7.1 Linear transformation of mean and covariance Supposewehaveanarbitrary(notnecessarilyaGaussian)randomvectorX =[X ,...,X ]T 1 N with mean µ and covariance Σ . Entries of X are not necessarily independent. Let X X A∈RN×N be a transformation, and let Y =AX. That is,      Y a a ··· a X 1 11 12 1N 1 Y 2 a 21 a 22 ··· a 2NX 2 Y =   . . .   =   . . . . . . ... . . .      . . .   =AX. Y a a ··· a X N N1 N2 NN N Then we can show the following result. 293
CHAPTER 5. JOINT DISTRIBUTIONS Theorem 5.14. The mean vector and covariance matrix of Y =AX are µ =Aµ , Σ =AΣ AT. (5.36) Y X Y X Proof. We first show the mean. Consider the nth element of Y: (cid:34) N (cid:35) N (cid:88) (cid:88) E[Y ]=E a X = a E[X ]. n nk k nk k k=1 k=1 Therefore, E[Y ] (cid:80)N a E[X ] 1 k=1 1k k µ Y =  E[Y . .2]  =  (cid:80)N k=1a . .2kE[X k]    .   .  E[Y N] (cid:80)N k=1a NkE[X k]  a a ··· a E[X ] 11 12 1N 1 a 21 a 22 ··· a 2NE[X 2] =   . . . . . . ... . . .      . . .   =Aµ X. a a ··· a E[X ] N1 N2 NN N The covariance matrix follows from the fact that Σ =E[(Y −µ )(Y −µ )T] Y Y Y =E[(AX−Aµ )(AX−Aµ )T] X X =E[A(X−µ )(X−µ )TAT] X X =AE[(X−µ )(X−µ )T]AT X X =AΣ AT. X (cid:3) What if we shift the random vector by defining Y = X +b? We state the following result without proof (try proving it as an exercise). Theorem 5.15. The mean vector and covariance matrix of Y =X+b are µ =µ +b, Σ =Σ . (5.37) Y X Y X ForaGaussianrandomvector,thelineartransformationseithershiftstheGaussianor rotates the Gaussian, as shown in Figure 5.17: • If we add b to X, the resulting operation is a translation. • If we multiply A by X, then the resulting operation is a rotation and scaling. 294
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Figure 5.17: TransformingaGaussian.[Left]Translationbyavectorb.[Right]Rotationandscalingby a matrix X. How to rotate, scale, and translate a Gaussian random variable • We rotate and scale a Gaussian by Y =AX. • We translate a Gaussian by Y =X+b. 5.7.2 Eigenvalues and eigenvectors As our next step, we need to understand eigendecomposition. You can easily find relevant background in any undergraduate linear algebra textbook. Here we provide a summary for completeness. When applying a matrix A to a vector x, a typical engineering question is: what x would be invariant to A? Or in other words, for what x can we make sure that Ax = λx, for some scalar λ? If we can find such a vector x, we say that x is the eigenvector of A. Eigenvectorsareusefulforseekingprincipalcomponentsofdatasetsorfindingefficientsignal representations. They are defined as follows: Definition 5.23. Given a square matrix A∈RN×N, the vector u∈RN (with u(cid:54)=0) is called the eigenvector of A if Au=λu, (5.38) for some λ∈R. The scalar λ is called the eigenvalue associated with u. AnN×N matrixhasN eigenvectorsandN eigenvalues.Therefore,theaboveequationcan be generalized to Au =λ u , i i i for i = 1,...,N, or more compactly as AU = ΛU. The eigenvalues λ ,...,λ are not 1 N necessarilydistinct.Therearematriceswithidenticaleigenvalues,theidentitymatrixbeing atrivialexample.Ontheotherhand,notallsquarematriceshaveeigenvectors.Forexample, (cid:20) (cid:21) 0 1 the matrix does not have an eigenvalue. Matrices that have eigenvalues must be 0 0 diagonalizable. 295
CHAPTER 5. JOINT DISTRIBUTIONS There are a number of equivalent conditions for λ to be an eigenvalue: • There exists u(cid:54)=0 such that Au=λu; • There exists u(cid:54)=0 such that (A−λI)u=0; • (A−λI) is not invertible; • det(A−λI)=0. We are mostly interested in symmetric matrices. If A is symmetric, then all the eigen- values are real, and the following result holds. Theorem 5.16. If A is symmetric, all the eigenvalues are real, and there exists U such that UTU =I and A=UΛUT. Then   λ  — uT —   1 1 a| |1 a| |2 ··· a| |N=    u || 1 u || 2 ··· u || N        λ 2 ...       — u . . .T 2 —   . (cid:124) (cid:123)(cid:122) (cid:125) λ N — uT N — A (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) U Λ UT (5.39) We call such a decomposition the eigendecomposition. In MATLAB, we can compute the eigenvaluesofamatrixbyusingtheeigcommand.InPython,thecorrespondingcommand is np.linalg.eig. Note that in our demonstration below we symmetrize the matrix. This step is needed, for otherwise the eigenvalues will contain complex numbers. % MATLAB Code to perform eigendecomposition A = randn(100,100); A = (A + A’)/2; % symmetrize because A is not symmetric [U,S] = eig(A); % eigendecomposition s = diag(S); % extract eigenvalue # Python Code to perform eigendecomposition import numpy as np A = np.random.randn(100,100) A = (A + np.transpose(A))/2 S, U = np.linalg.eig(A) s = np.diag(S) The condition that UTU = I is the result of an orthonormal matrix. Equivalently, uTu = 1 if i = j and uTu = 0 if i (cid:54)= j. Since {u }N is orthonormal, it can serve as a i j i j i i=1 basis of any vector in Rn: N (cid:88) x= α u , j j j=1 where α = uTx is called the basis coefficient. Basis vectors are useful in that they can j j provide alternative representations of a vector. 296
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Figure 5.18: The center and the radius of the ellipse is determined by µ and Σ. The geometry of the joint Gaussian is determined by its eigenvalues and eigenvectors. Consider the eigendecomposition of Σ: Σ=UΛUT  λ 0 ··· 0 − uT −   1 1 | | | 0 λ 2 ··· 0− uT 2 − =u |1 u |2 ··· u |d   . . . . . . ... . . .      . . .   , 0 ··· ··· λ − uT − d d for some unitary matrix U and diagonal matrix Λ. The columns of U are called the eigen- vectors, and the entries of Λ are called the eigenvalues. Since Σ is symmetric, all λ ’s are i real.Inaddition,sinceΣispositivesemi-definite,allλ ’sarenon-negative.Accordingly,the i volume defined by the multidimensional Gaussian is always a convex object, e.g., an ellipse in 2D or an ellipsoid in 3D. The orientation of the axes is defined by the column vectors u . In the case of d = 2, i themajoraxisisdefinedbyu andtheminoraxisisdefinedbyu .Thecorrespondingradii 1 2 ofeachaxisarespecifiedbytheeigenvaluesλ andλ .Figure 5.18providesanillustration. 1 2 5.7.3 Covariance matrices are always positive semi-definite The following subsection about positive semi-definite matrices can be skipped if it is your first time reading the book. Now that we understand eigendecomposition, what can we do with it? Here is one practical problem. Given a matrix Σ, how do you know whether this Σ is valid? For example, if we giveyouasingularmatrix,thenΣ−1 maynotexist.CheckingthevalidityofΣrequiresthe concept of positive semi-definite. GivenasquarematrixA∈RN×N,itisimportanttocheckthepositivesemi-definiteness of A. There are two practical scenarios where we need positive semi-definiteness. (1) If we are estimating the covariance matrix Σ from a dataset, we need to ensure that Σ = E[(X −µ)(X −µ)T] is positive semi-definite because all covariance matrices are positive 297
CHAPTER 5. JOINT DISTRIBUTIONS semi-definite. Otherwise, the matrix we estimate is not a legitimate covariance matrix. (2) If we solve an optimization problem involving a function f(x) = xTAx, then having A beingpositivesemi-definite,wecanguaranteethattheproblemisconvex.Convexproblems ensure that a local minimum is also global, and convex problems can be solved efficiently using known algorithms. Definition 5.24 (Positive Semi-Definite). A matrix A ∈ RN×N is positive semi- definite if xTAx≥0 (5.40) for any x∈RN. A is positive definite if xTAx>0 for any x∈RN. Usingeigendecomposition,itisnotdifficulttoshowthatpositivesemi-definitenessisequiv- alent to having non-negative eigenvalues. Theorem 5.17. A matrix A∈RN×N is positive semi-definite if and only if λ (A)≥0 (5.41) i for all i=1,...,N, where λ (A) denotes the ith eigenvalue of A. i Proof. By the definitions of eigenvalue and eigenvector, we have that Au =λ u , i i i where λ is the eigenvalue and u is the corresponding eigenvector. If A is positive semi- i i definite, then uTAu ≥0 since u is a particular vector in Rn. So we have i i i 0≤uTAu =λ(cid:107)u (cid:107)2, i i i and hence λ ≥ 0. Conversely, if λ ≥ 0 for all i, then since A = (cid:80)N λ u uT we can i i i=1 i i i conclude that (cid:32) N (cid:33) N (cid:88) (cid:88) xTAx=xT λ u uT x= λ (uTx)2 ≥0. i i i i i i=1 i=1 (cid:3) The following corollary shows that if A∈Rn×n is positive definite, it must be invert- ible. Being invertible also means that the columns of A are linearly independent. Corollary 5.2. If a matrix A ∈ RN×N is positive definite (but not semi-definite), then A must be invertible, i.e., there exists A−1 ∈RN×N such that A−1A=AA−1 =I. (5.42) The next theorem tells us that the covariance matrix is always positive semi-definite. 298
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Theorem 5.18. The covariance matrix Cov(X) = Σ is symmetric positive semi- definite, i.e., ΣT =Σ, and vTΣv ≥0, ∀v ∈Rd. Proof.Symmetryfollowsimmediatelyfromthedefinition,becauseCov(X ,X )=Cov(X ,X ). i j j i The positive semi-definiteness comes from the fact that vTΣv =vTE[(X−µ)(X−µ)T]v =E[vT(X−µ)(X−µ)Tv] =E[bTb]=E[(cid:107)b(cid:107)2]≥0, where b=(X−µ)Tv. (cid:3) End of the discussion. 5.7.4 Gaussian whitening Besides checking positive semi-definiteness, another typical problem we encounter is how to generate random samples according to some Gaussian distributions. FromGaussian(0,I)toGaussian(µ,Σ).Ifwearegivenzero-meanunit-varianceGaus- sian X ∼Gaussian(0,I), how do we generate Y ∼Gaussian(µ,Σ) from X? The idea is to define a transformation 1 Y =Σ2X+µ, where Σ21 =UΛ1 2UT. Then the mean of Y is E[Y]=E[Σ1 2X+µ] =Σ21E[X]+µ=Σ21 0+µ=µ, and the covariance matrix is E[(Y −µ)(Y −µ)T]=E[(Σ1 2X+µ−µ)(Σ1 2X+µ−µ)T] =E[(Σ21 X)(Σ21 X)T]=Σ1 2E[XXT]Σ21 1 1 =Σ2IΣ2 =Σ. Theorem 5.19. Let X be X ∼ Gaussian(0,I). Consider a mean vector µ and a covariance matrix Σ with eigendecomposition Σ=UΛUT. If 1 Y =Σ2X+µ, (5.43) where Σ21 =UΛ1 2UT, then Y ∼Gaussian(µ,Σ). 299
CHAPTER 5. JOINT DISTRIBUTIONS Therefore, the two steps for doing this Gaussian whitening are: • Step1:Generatesamples{x ,...,x }thataredistributedaccordingtoGaussian(0,I). 1 N • Step 2: Define y where n 1 y n =Σ2x n+µ. These two steps are portrayed in Figure 5.19. Figure 5.19: Generating an arbitrary Gaussian from Gaussian(0,I). Example 5.26.ConsiderasetofN =1000i.i.d.Gaussian(0,I)datapointsasshown in Figure 5.20, for example, (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.5377 −2.2588 0.3188 x = , x = , ... , x = . 1 1.8399 2 0.8622 1000 −1.3077 5 5 4 4 3 3 2 2 1 1 0 0 -1 -1 -2 -2 -3 -3 -4 -4 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) Before (b) After Figure 5.20: Generating arbitrary Gaussian random variables from Gaussian(0,I). Transform these data points so that the new distribution is a Gaussian with (cid:20) (cid:21) (cid:20) (cid:21) 1 3 −0.5 µ= and Σ= . −2 −0.5 1 300
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Solution.Toperformthetransformation,wefirstperformeigendecompositionofΣ= UΛUT. Then Σ1 2 =UΛ1 2UT. For our problem, we compute (cid:20) (cid:21) 1.722 −0.1848 1 Σ2 = . −0.1848 0.9828 1 Multiplying this matrix to yield y n =Σ2x n+µ, we obtain (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 1.5870 −3.0495 1.7907 y = , y = , ... , y = . 1 −0.2971 2 −0.7351 1000 −3.3441 In MATLAB, the above whitening procedure can be realized using the following com- mands. % MATLAB code to perform the whitening x = mvnrnd([0,0],[1 0; 0 1],1000); Sigma = [3 -0.5; -0.5 1]; mu = [1; -2]; y = Sigma^(0.5)*x’ + mu; The Python implementation is similar, although one needs to be careful with the more complicated syntax. For example, Sigma^(0.5) in MATLAB does the eigen-based matrix power automatically, whereas in Python we need to call a specific built-in command fractional_matrix_power. In MATLAB, broadcasting a vector to a matrix can be rec- ognized. In Python, we need to call repmat explicitly to control the shape of the mean vectors. # Python code to perform the whitening import numpy as np import scipy.stats as stats from scipy.linalg import fractional_matrix_power x = np.random.multivariate_normal([0,0],[[1,0],[0,1]],1000) mu = np.array([1,-2]) Sigma = np.array([[3, -0.5],[-0.5, 1]]) Sigma2 = fractional_matrix_power(Sigma,0.5) y = np.dot(Sigma2, x.T) + np.matlib.repmat(mu,1000,1).T From Gaussian(µ,Σ) to Gaussian(0,I). The reverse direction can be done as follows. Supposing that we have Y ∼Gaussian(µ,Σ), we define X =Σ−1 2(Y −µ). (5.44) Then E[X]=E[Σ−1 2(Y −µ)] =Σ− 21 (E[Y]−µ)=0. 301
CHAPTER 5. JOINT DISTRIBUTIONS The covariance is Cov(X)=E[(X−µ )(X−µ )T] X X =E[XXT] (cid:104) (cid:105) =E Σ− 21 (Y −µ)(Y −µ)TΣ−T 2 =Σ− 21E(cid:2) (Y −µ)(Y −µ)T(cid:3) Σ−T 2 =Σ−1 2ΣΣ−1 2 =I. The following theorem summarizes this result. Theorem 5.20. Let Y be a Gaussian Y ∼Gaussian(µ,Σ). If X =Σ−1 2(Y −µ), (5.45) then X ∼Gaussian(0,I). Thus the two steps of doing this reversed Gaussian whitening are: • Step 1: Assuming that y ,...,y are distributed as Gaussian(µ,Σ), estimate µ 1 N and Σ. • Step 2: Define x where n 1 x n =Σ2(y n−µ). (5.46) These two steps are shown pictorially in Figure 5.21. Figure 5.21: Converting an arbitrary Gaussian back to Gaussian(0,I). In practice, if we are given {y }N , we need to estimate µ and Σ. The estimations n n=1 are quite straightforward. N 1 (cid:88) µ= y , (cid:98) N n n=1 N 1 (cid:88) Σ(cid:98) = N (y n−µ (cid:98))(y n−µ (cid:98))T. n=1 302
5.8. PRINCIPAL-COMPONENT ANALYSIS On computers, these can be obtained using the command mean and cov. Once we have calculated µ (cid:98) and Σ(cid:98), we can define x n as −1 x n =Σ(cid:98) 2(y n−µ (cid:98)). On computers, the codes for the whitening procedure that uses the estimated mean and covariance are shown below. % MATLAB code to perform whitening y = mvnrnd([1; -2],[3 -0.5; -0.5 1],100); mY = mean(y); covY = cov(y); x = covY^(-0.5)*(y-mY)’; # Python code to perform whitening import numpy as np import scipy.stats as stats from scipy.linalg import fractional_matrix_power y = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],100) mY = np.mean(y,axis=0) covY = np.cov(y,rowvar=False) covY2 = fractional_matrix_power(covY,-0.5) x = np.dot(covY2, (y-np.matlib.repmat(mY,100,1)).T) 5.8 Principal-Component Analysis We have studied the covariance matrix Σ in some depth. It has many other uses besides transforming Gaussian random variables, and in this section we present one of them, called the principal-component analysis (PCA). PCA is a widely used tool for dimension reduc- tion.Insteadofusing N featurestodescribeadatapoint,PCAallowsustouse the leading p principal components to describe the same data point. In many problems in machine learning, this makes the learning task easier and the inference task more efficient. 5.8.1 The main idea: Eigendecomposition PCA can be summarized in one sentence: The key idea of PCA is the eigendecomposition of the covariance matrix Σ. This is a condensed summary of PCA: It is just the eigendecomposition of the co- variance. However, before we discuss the computational procedure, we will explain why we would want to perform the eigendecomposition of the covariance matrix. 303
CHAPTER 5. JOINT DISTRIBUTIONS Considerasetofdatapoints{x(1),...,x(N)},whereeachx(n) ∈Rd isad-dimensional vector.Thedimensiondisoftenhigh.Forexample,ifwehaveanimageofsize1024×1024×3, then d = 3,145,728 — not a huge number, but enough to make you feel dizzy. The goal of PCA is to find a low-dimensional representation in Rp where p (cid:28) d. If we can find this low-dimensional representation, we can represent the d-dimensional input using only p coefficients.Sincep(cid:28)d,wecan“compress”thedatabyusingacompactrepresentation.In modern data science, such a dimension reduction scheme is useful for handling large-scale datasets. Mathematically, we define a set of basis vector v ,...,v , where each v ∈ Rd. Our 1 p i goal is to approximate an input data point x(n) ∈Rd by these basis vectors: p (cid:88) x(n) ≈ α v , i i i=1 where {α }p are called the representation coefficients. The representation described by i i=1 thisequationisalinearrepresentation.Linearrepresentationisextremelycommoninprac- tice. For example, a data point x(n) =[7,1,4]T can be represented as       7 1 1 1= 3 −1+ 4 1. (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) 4 0 1 α1 α2 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) x(n) v1 v2 Therefore, the 3-dimensional input x(n) can now be represented by two coefficients α =3 1 and α =4. This is called dimensionality reduction. 2 Pictorially, if we have already determined the basis vectors, we can compute the co- efficients for every data point in the dataset. However, not all basis vectors are good. As illustrated in Figure 5.22, an elongated dataset will be of the greatest benefit if the basis vectors are oriented according to the data geometry. If we can find such basis vectors, then the data points will have a large coefficient and a small coefficient, corresponding to the major and the minor axes. Dimensionality reduction can thus be achieved by, for example, only keeping the larger coefficients. Figure 5.22: PCA aims at finding a low-dimensional representation of a high-dimensional dataset. In this figure, the 2D data points can be well represented by the 1D space spanned by v . 1 The challenge here is that, given the dataset {x(1),...,x(N)}, we need to determine both the basis vectors {v }p and the coefficients {α }p . Fortunately, this can be formu- i i=1 i i=1 lated as an eigendecomposition problem. 304
5.8. PRINCIPAL-COMPONENT ANALYSIS To see how this problem can be thus formulated, we consider the simplest case as illustrated in Figure 5.22, where we want to find the leading principal component. That is, we find (α,v) such that x≈αv. This amounts to solving the optimization problem (cid:13)   (cid:13)2 (cid:13) | | (cid:13) (cid:13) (cid:13) (v (cid:98),α (cid:98))= argmin (cid:13)x−αv(cid:13) . (cid:13) (cid:13) (cid:107)v(cid:107)2=1,α(cid:13) | | (cid:13) The notation “argmin” means the argument that minimizes the function. The equation says that we find the (α,v) that minimizes the distance between x and αv. The constraint (cid:107)v(cid:107) =1 limits the search to within a unit circle; otherwise our solution will not be unique. 2 Solvingtheoptimizationproblemisnotdifficult.Ifwetakethederivativew.r.t.αand set it to zero, we have that 2vT(x−αv)=0 ⇒ α=vTx. Substituting α=xTv into the objective function again, we show that (cid:26) (cid:27) (cid:8)(cid:8) argmin (cid:107)x−αv(cid:107)2 =argmin xTx−2αxTv+α2(cid:8)vTv , (cid:107)v(cid:107) =1 2 (cid:107)v(cid:107)2=1 (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmin −2αxTv+α2 , drop xTx (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmin −2(xTv)xTv+(xTv)2 , substitute α=xTv (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmax vTxxTv , change min to max. (cid:107)v(cid:107)2=1 Letuspauseforasecond.Wehaveshownthatifwehaveone datapointx,theleading principal component v can be determined by maximizing vTxxTv. What have we gained? Wehavetransformedtheoriginaloptimization,whichcontainstwovariables(v,α),toanew optimization that contains one variable v. Thus if we know how to solve the one-variable problem we are done. However, there is one more issue we need to address before we discuss how to solve for the problem. The issue is that the formulation is about one data sample, not the entire dataset. To include all the samples, we need to assume that x is a realization of a random vector X. Then the above optimization can be formulated in the expectation sense as (cid:26) (cid:27) argmin E(cid:107)X−αv(cid:107)2 =argmaxvTE XXT v (cid:107)v(cid:107)2=1 (cid:107)v(cid:107)2=1 =argmaxvTΣv, (cid:107)v(cid:107)2=1 where Σ d =ef E[XTX].1 Therefore, if we can maximize vTΣv we will be able to determine the principal component. Now comes the main result. The following theorem shows that the maximization is equivalenttoeigendecomposition.TheproofrequiresLagrangemultipliers,whicharebeyond the scope of this book. 1Here we assume that X is zero-mean, i.e., E[X] = 0. If it is not, then we can subtract the mean by (cid:26) (cid:27) consideringargmax vTE (X−µ)(X−µ)T v. (cid:107)v(cid:107)2=1 305
CHAPTER 5. JOINT DISTRIBUTIONS Theorem 5.21. Let Σ be a d×d matrix with eigendecomposition Σ=USUT. Then the optimization v =argmax vTΣv (5.47) (cid:98) (cid:107)v(cid:107)2=1 has a solution v =u , i.e., the first column of the eigenvector matrix U. (cid:98) 1 The following proof requires an understanding of Lagrange multipliers and constrained optimizations. It is not essential for understanding this chapter. We want to prove that the solution to the problem v =argmaxvTΣv (cid:98) (cid:107)v(cid:107)2=1 is the eigenvector of the matrix Σ. To show that, we first write down the Lagrangian: L(v,λ)=vTΣv−λ((cid:107)v(cid:107)2−1) Taking the derivative w.r.t. v and setting to zero yields ∇ L(v,λ)=2Σv−2λv =0. v This is equivalent to Σv =λv. So if Σ=USUT, then by letting v =u and λ=s we can i i satisfy the condition since Σu =USUTu =USe =s u . i i i i i End of the proof. This theorem can be extended to the second (and other) principal components of the covariance matrix. In fact, given the covariance matrix Σ we can follow the procedure outlinedinFigure5.23todeterminetheprincipalcomponents.Theeigendecompositionofa d×dmatrixΣwillgiveusad×deigenvectormatrixU andaneigenvaluematrixS.Tokeep thepleadingeigenvectors,wetruncatetheU matrixtoonlyusethefirstpeigenvectors.Here, we assume that the eigenvectors are ordered according to the magnitude of the eigenvalues, from large to small. In practice, ifweare given a dataset {x(1),...,x(N)}, we can firstestimate the covari- ance matrix Σ by N 1 (cid:88) Σ(cid:98) = N (x(n)−µ (cid:98))(x(n)−µ (cid:98))T, n=1 where µ = 1 (cid:80)N x(n) is the mean vector. Afterwards, we can compute the eigendecom- (cid:98) N n=1 position of Σ(cid:98) by [U,S]=eig(Σ(cid:98)). On a computer, the principal components are obtained through eigendecomposition. A MATLAB example and a Python example are shown below. We explicitly show the two principal components in this example. The magnitudes of these two vectors are determined by the eigenvalues diag(s). 306
5.8. PRINCIPAL-COMPONENT ANALYSIS Figure 5.23: The principal components are the eigenvectors of the covariance matrix. In this figure Σ denotesthecovariancematrix,u ,...,u denotethepleadingeigenvectors,andsdenotesthediagonal 1 p of the eigenvalue matrix. % MATLAB code to perform the principal-component analysis x = mvnrnd([0,0],[2 -1.9; -1.9 2],1000); covX = cov(x); [U,S] = eig(covX); u(:,1) % Principle components u(:,2) % Principle components # Python code to perform the principal-component analysis import numpy as np x = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],1000) covX = np.cov(x,rowvar=False) S, U = np.linalg.eig(covX) print(U) Example 5.27.SupposewehaveadatasetcontainingN =1000samples,drawnfrom an unknown distribution. The first few samples are (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.5254 −0.4040 1.4165 x = , x = , ... , x = . 1 −0.6930 2 0.3724 1000 −1.5463 We can compute the mean and covariance using MATLAB commands mean and cov. This will return us (cid:20) (cid:21) (cid:20) (cid:21) 0.0561 2.0460 −1.9394 µ (cid:98) = −0.0303 and Σ(cid:98) = −1.9394 2.0426 . Applying eigendecomposition on Σ(cid:98), we show that [U,S]=eig(Σ(cid:98)), (cid:20) (cid:21) (cid:20) (cid:21) −0.7068 −0.7074 0.1049 0 =⇒U = and S = . −0.7074 0.7068 0 3.9837 307
CHAPTER 5. JOINT DISTRIBUTIONS Therefore, we have obtained two principal components (cid:20) (cid:21) (cid:20) (cid:21) −0.7068 −0.7074 u = and u = . 1 −0.7074 2 0.7068 As seen in the figure below, these two principal components make sense. The vector u is the orange line and is the minor axis. The vector u is the blue line and is the 1 2 major axis. Again, the ordering of the vectors is determined by the eigenvalues. Since u has a larger eigenvalue (=3.9837), it is the leading principal component. 2 5 4 3 2 1 0 -1 -2 -3 -4 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 5.24: To determine the representation coefficients, we solve an inverse problem by finding the vector α in the equation x(n) =U α(n). p Why do we call our method principal component analysis? The analysis part comes from the fact that we can compress a data vector x(n) from a high dimension d to a low dimension p. Defining U =[u ,...,u ], a matrix containing the p leading eigenvectors of p 1 p the matrix U, we solve the inverse problem: x(n) =U α(n), p where the goal is to determine the coefficient vector α(n) ∈Rp. Since U is an orthonormal p matrix (i.e., UTU =I), it follows that p p UTx(n) =UTU α(n), p p p (cid:124) (cid:123)(cid:122) (cid:125) =I 308
5.8. PRINCIPAL-COMPONENT ANALYSIS as illustrated in Figure 5.24. Hence, α(n) =UTx(n). p This equation is a projection operation that projects a data point x(n) onto the space spanned by the p leading principal components. Repeating the procedure for all the data points x(1),...,x(N) in the dataset, we have compressed the dataset. Example 5.28. Using the example above, we can show that (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.1189 0.0221 0.0927 α(1) =UTx(1) = , α(2) = , ... , α(1000) = . −0.8615 0.5491 −2.0950 Theprincipal-componentanalysissaysthatsincetheleadingcomponentsrepresentthe data, we only need to keep the blue-colored values because they are the coefficients associated with the leading principal component. 5.8.2 The eigenface problem As a concrete example of PCA, we consider a computer vision problem called the eigen- face problem. In 2001, researchers at Yale University published the Yale Database, and a few years later they extended it to a larger one (http://vision.ucsd.edu/~leekc/ ExtYaleDatabase/ExtYaleB.html).Thedataset,nowknownastheYaleFaceDataset,con- tains 16,128 images of 28 human subjects under nine poses and 64 illumination conditions. Thesizesoftheimagesared=168×192=32,256pixels.TreatingtheseN =16,128images as vectors in R32,256×1, we have 16,128 of these vectors. Let us call them {x(1),...,x(N)}. Following the procedure we described above, we estimate the covariance matrix by computing N 1 (cid:88) Σ(cid:98) =E[(X−µ (cid:98))(X−µ (cid:98))T]≈ N (x(n)−µ (cid:98))(x(n)−µ (cid:98))T, (5.48) n=1 where µ = E[X] ≈ 1 (cid:80)N x(n) is the mean vector. Note that the size of µ is 32,256×1 (cid:98) N n=1 (cid:98) and the size of Σ(cid:98) is 32,256 × 32,256. Figure 5.25: The extended Yale Face Database B. Once we obtain an estimate of the covariance matrix, we can perform an eigendecom- position to get [U,S]=eig(Σ(cid:98)). The columns of U, i.e., {u i}d i=1, are the eigenvectors of Σ(cid:98). These eigenvectors are the basis of a testing face image. 309
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.26: Givenafaceimage,thelearnedbasisvectors(fromtheeigendecompositionofthecovari- ancematrix)canbeusedtocompresstheimagexintoafeaturevectorαwherethedimensionofαis significantly lower than that of x. With the basis vectors u ,...,u we can project every image in the dataset using a 1 p low-dimensional representation. Specifically, for an image x we compute the coefficients α =uTx, i=1,...,p i i or more compactly α = UTx. Note that the dimension of x is d×1 (which in our case is d = 32,526), and the dimensions of α can be as few as p = 100. Therefore, we are using a 100-dimensionalvectortorepresenta32,526-dimensionaldata.Thisisahugedimensionality reduction. Theprocessrepeatsforallthesamplesx(1),...,x(N).Thisgivesusacollectionofrep- resentationcoefficientsα(1),...,α(N),whereeachα(n)is100-dimensional(seeFigure5.26). Noticethatthebasisvectorsu appearmoreorless“faceimages,”buttheyarethefeatures i of the faces. PCA says that a real face can be written as a linear combination of these basis vectors. How to solve the eigenface problem • Compute the covariance matrix of all the images. • Apply eigendecomposition to the covariance matrix. • Project onto the basis vectors and find the coefficients. • The coefficients are the low-dimensional representation of the images. • We use the coefficients to perform downstream tasks, such as classification. 310
5.8. PRINCIPAL-COMPONENT ANALYSIS 5.8.3 What cannot be analyzed by PCA? PCA is a dimension reduction tool. It compresses a raw data vector x ∈ Rd into a smaller feature vector α∈Rp. The advantage is that the downstream learning problems are much easierbecausep(cid:28)d.Forexample,classificationusingαismoreefficientthanclassification using x since there is very little information loss from x to α. There are three limitations of PCA: • PCS fails when the raw data are not orthogonal. The basis vectors u returned i by PCA are orthogonal, meaning that uTu = 0 as long as i (cid:54)= j. As a result, if i j the data intrinsically have this orthogonality property, then PCA will work very well. However,ifthedataliveinaspacesuchasadonutshapeasillustratedinFigure5.27, then PCA will fail. Here, by failure, we mean that p is not much smaller than d. To handledatasetsbehavinglikeFigure 5.27weneedadvancedtools.Oneoftheseisthe kernel-PCA. The idea is to apply a nonlinear transformation to the data before you run PCA. Figure 5.27: [Left] PCA works when the data has redundant dimensions or is living on orthogonal spaces. [Right] PCA fails when the data does not have easily decomposable spaces. • Basis vectors returned by PCA are not interpretable. A temptation with PCA is to thinkthatthebasisvectorsu offermeaningfulinformationbecausetheyarethe“prin- i cipal components”. However, since PCA is the eigendecomposition of the covariance matrix,whichispurelyamathematicaloperation,thereisnoguaranteethatthebasis vectors contain any semantic meaning. If we look at the basis vectors shown in Fig- ure 5.26, there is almost no information one can draw. Therefore, in the data-science literaturealternativemethodssuchasnon-negativematrixfactorizationandthemore recentdeepneuralnetworkembeddingaremoreattractivebecausethefeaturevectors sometimes (not always) have meanings. • PCA does not return you the most influential “component”. Imagine that you are analyzing medical data for research on a disease, in which each data vector x(n) containsheight,weight,BMI,bloodpressure,etc.WhenyourunPCAonthedataset, you will obtain some “principal components”. However, these principal components will likely have everything, e.g., the height entry of the principal component will have some values, the weight will have some values, etc. If you have found a principal component, it does not mean that you have identified the leading risk factor of the disease. If you want to identify the leading risk factor of the disease, e.g., whether the height or weight is more important, you need to resort to advanced tools such as variable selection or the LASSO type of regression analysis (see Chapter 7). 311
CHAPTER 5. JOINT DISTRIBUTIONS Closing remark.PCAsarepowerfulcomputationaltoolsbasedonthesimplestconceptof covariancematricesbecause,asourderivationshowed,covariancematricesencodethe“vari- ation” of the data. Therefore, by finding a vector that aligns with the maximum variation of the data, we can find the principal component. 5.9 Summary As you were reading this chapter, you may have felt that the first and second parts discuss distinctly different subjects, and in fact many books treat them as separate topics. We take a different approach. We think that they are essentially the same thing if you understand the following chain of distributions: f (x) =⇒f (x ,x )=⇒···=⇒f (x ,...,x ). X X1,X2 1 2 X1,...,XN 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) onevariable twovariables N variables The first part exclusively deals with two variables. The generalization from two variables to N variables is straightforward for PDFs and CDFs: • PDF: f (x ,x )=⇒f (x ,...,x ). X1,X2 1 2 X1,...,XN 1 N • CDF: F (x ,x )=⇒F (x ,...,x ). X1,X2 1 2 X1,...,XN 1 N The joint expectation can also be generalized from two variables to N variables:  Var[X2] ··· Cov(X ,X ) (cid:20) CoV va (Xr[X ,12 X] ) Co Vv a(X r[X1, 2X ]2)(cid:21) =⇒  . . . 1 ... . . .1 N  . 2 1 2 Cov(X ,X ) ··· Var[X2] N 1 N Conditional PDFs and conditional expectations are powerful tools for decomposing complex events into simpler events. Specifically, the law of total expectation, (cid:90) E[X]= E[X|Y =y]f (y)dy =E [E [X|Y]], Y Y X|Y is instrumental for evaluating variables defined through conditional relationships. The idea is also extendable to more random variables, such as (cid:90) (cid:90) E[X ]= E[X |X =x ,X =x ]f (x ,x )dx dx , 1 1 2 2 3 3 X2,X3 2 3 2 3 where E[X |X =x ,X =x ] can be evaluated through 1 2 2 3 3 (cid:90) E[X |X =x ,X =x ]= x f (x |x ,x )dx . 1 2 2 3 3 1 X1|X2,X3 1 2 3 1 This type of chain relationship can generalize to other high-order cases. Itisimportanttorememberthatforanyhigh-dimensionalrandomvariables,thechar- acterizationisalwaysmadebythePDFf (x)(ortheCDF).Wedidnotgointothedetails X 312
5.10. REFERENCES of analyzing f (x) but have only discussed the mean vector E[X]=µ and the covariance X matrix Cov(X)=Σ. We have been focusing exclusively on the high-dimensional Gaussian random variables (cid:26) (cid:27) 1 1 f (x)= exp − (x−µ)TΣ(x−µ) , X (cid:112) (2π)d|Σ| 2 because they are ubiquitous in data science today. We discussed the linear transformations from a zero-mean unit-variance Gaussian to another Gaussian, and vice versa. 5.10 References Joint Distributions and Correlation 5-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 2.5, 3.4, 4.2. 5-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 5.1 – 5.6. 5-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 6.1 – 6.4. 5-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 7.1 – 7.2. 5-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapters 6.1 – 6.3. 5-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 2.6. Conditional Distributions and Expectations 5-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 2.6, 3.5, 3.6, 4.3. 5-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 5.7. 5-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 6.6 – 6.7. 5-10 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 7.3 – 7.5. 5-11 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapters 7.5 – 7.6. 5-12 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 4.2. 313
CHAPTER 5. JOINT DISTRIBUTIONS Sum of Random Variables 5-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.5. 5-14 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 7.1. 5-15 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 3.3 and 3.4. Vector Random Variables 5-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 6.1 – 6.6. 5-17 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 8.1 – 8.3, 9. 5-18 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 5.1 – 5.6. Principal-Component Analysis PCAisoftentaughtinmachinelearningcourses.Forfirst-timereaders,wesuggestreviewing thelinearalgebraictoolsinMoonandStirling.Then,thetutorialbyShlensandthechapter in Bishop would be sufficient to cover most of the materials. More advanced topics, such as kernel PCA, can be found in the following references. 5-19 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Chapter 7. 5-20 ChristopherBishop,PatternRecognitionandMachineLeanring,Springer,2006.Chap- ter 12. 5-21 Jonathon Shlens (2014) “A Tutorial on Principal Component Analysis”, https:// arxiv.org/pdf/1404.1100.pdf 5-22 PaulHoneine(2014),“Aneigenanalysisofdatacenteringinmachinelearning”,https: //arxiv.org/pdf/1407.2904.pdf 5-23 Quan Wang (2012), “Kernel Principal Component Analysis and its Applications”, https://arxiv.org/abs/1207.3538 5-24 Schlkopfetal.(2005),“KernelPrincipalComponentAnalysis”,https://link.springer. com/chapter/10.1007/BFb0020217 314
5.11. PROBLEMS 5.11 Problems Exercise 1. (Video Solution) Alex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets. (a) Find and sketch the joint PMF p (x,y). X,Y (b) Find the marginal PMF p (x) and p (y). X Y (c) Find the conditional PMF P (x|y). Does P (x|y)=P (x)? Why or why not? X|Y X|Y X Exercise 2. Two fair dice are rolled. Find the joint PMF of X and Y when (a) X is the larger value rolled, and Y is the sum of the two values. (b) X is the smaller, and Y is the larger value rolled. Exercise 3. The amplitudes of two signals X and Y have joint PDF f (x,y)=e−x/2ye−y2 XY for x>0,y >0. (a) Find the joint CDF. (b) Find P(X1/2 >Y). (c) Find the marginal PDFs. Exercise 4. (Video Solution) Find the marginal CDFs F (x) and F (y) and determine whether or not X and Y are X Y independent, if  x−1− e−y−e−xy, if 1≤x≤2,y ≥0  y F (x,y)= 1− e−y−e−2y, if x>2,y ≥0, XY y 0, otherwise. Exercise 5. (Video Solution) (a) Find the marginal PDF f (x) if X exp{−|y−x|−x2/2} f (x,y)= √ . XY 2 2π 315
CHAPTER 5. JOINT DISTRIBUTIONS (b) Find the marginal PDF f (y) if Y 4e−(x−y)2/2 f (x,y)= √ . XY y2 2π Exercise 6. (Video Solution) Let X,Y be two random variables with joint CDF y+e−x(y+1) F (x,y)= . X,Y y+1 Show that ∂2 ∂2 F (x,y)= F (x,y). ∂x∂y X,Y ∂y∂x X,Y What is the implication of this result? Exercise 7. (Video Solution) Let X and Y be two random variables with joint PDF 1 f X,Y(x,y)= 2πe−1 2(x2+y2). (a) Find the PDF of Z =max(X,Y). (b) Find the PDF of Z =min(X,Y). You may leave your answers in terms of the Φ(·) function. Exercise 8. The random vector (X,Y) has a joint PDF f (x,y)=2e−xe−2y XY for x>0, y >0. Find the probability of the following events: (a) {X+Y ≤8}. (b) {X−Y ≤10}. (c) {X2 <Y}. Exercise 9. LetX andY bezero-mean,unit-varianceindependentGaussianrandomvariables.Findthe value of r for which the probability that (X,Y) falls inside a circle of radius r is 1/2. Exercise 10. The input X to a communication channel is +1 or −1 with probabilities p and 1 − p, respectively. The received signal Y is the sum of X and noise N, which has a Gaussian distribution with zero mean and variance σ2 =0.25. 316
5.11. PROBLEMS (a) Find the joint probability P(X =j, Y ≤y). (b) Find the marginal PMF of X and the marginal PDF of Y. (c) Suppose we are given that Y >0. Which is more likely, X =1 or X =−1? Exercise 11. (Video Solution) Let (cid:40) ce−xe−y, if 0≤y ≤x<∞, f (x,y)= X,Y 0, otherwise. (a) Find c. (b) Find f (x) and f (y). X Y (c) Find E[X] and E[Y], Var[X] and Var[Y]. (d) Find E[XY], Cov(X,Y) and ρ. Exercise 12. (Video Solution) Inclass,wehaveusedtheCauchy-Schwarzinequalitytoshowthat−1≤ρ≤1.Thisexercise asks you to prove the Cauchy-Schwarz inequality: (E[XY])2 ≤E[X2]E[Y2]. Hint:ConsidertheexpectationE[(tX+Y)2].Notethatthisisaquadraticequationintand E[(tX+Y)2]≥0 for all t. Consider the discriminant of this quadratic equation. Exercise 13. (Video Solution) Let Θ∼Uniform[0,2π]. (a) If X =cosΘ, Y =sinΘ. Are X and Y uncorrelated? (b) If X =cos(Θ/4), Y =sin(Θ/4). Are X and Y uncorrelated? Exercise 14. (Video Solution) Let X and Y have a joint PDF f (x,y)=c(x+y), X,Y for 0≤x≤1 and 0≤y ≤1. (a) Find c, f (x), f (y), and E[Y]. X Y (b) Find f (y|x). Y|X (c) Find P[Y >X|X >1/2]. (d) Find E[Y|X =x]. 317
CHAPTER 5. JOINT DISTRIBUTIONS (e) Find E[E[Y|X]], and compare with the E[Y] computed in (a). Exercise 15. (Video Solution) Use the law of total expectation to compute the following: 1. E[sin(X+Y)], where X ∼N(0,1), and Y |X ∼Uniform[x−π,x+π] 2. P[Y <y], where X ∼Uniform[0,1], and Y |X ∼Exponential(x) 3. E[XeY], where X ∼Uniform[−1,1], and Y |X ∼N(0,x2) Exercise 16. LetY =X+N,whereX istheinput,N isthenoise,andY istheoutputofasystem.Assume that X and N are independent random variables. It is given that E[X] = 0, Var[X] = σ2 , X E[N]=0, and Var[N]=σ2 . N (a) Find the correlation coefficient ρ between the input X and the output Y. (b) Suppose we estimate the input X by a linear function g(Y) = aY. Find the value of a that minimizes the mean squared error E[(X−aY)2]. (c) Express the resulting mean squared error in terms of η =σ2 /σ2 . X N Exercise 17. (Video Solution) Two independent random variables X and Y have PDFs (cid:40) (cid:40) e−x, x≥0, 0, y >0, f (x)= f (y)= X 0, x<0, Y ey, y ≤0. Find the PDF of Z =X−Y. Exercise 18. Let X and Y be two independent random variables with densities (cid:40) (cid:40) xe−x, x≥0, ye−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Exercise 19. The random variables X and Y have the joint PDF f (x,y)=e−(x+y) XY for 0<y <x<1. Find the PDF of Z =X+Y. Exercise 20. The joint density function of X and Y is given by f (x,y)=e−(x+y) XY for x>0,y >0. Find the PDF of the random variable Z =X/Y. 318
Chapter 6 Sample Statistics Whenwethinkaboutprobability,thefirstthingthatlikelycomestomindisflippingacoin, throwingadie,orplayingacardgame.Theseareexcellentexamplesofthesubject.However, they seldom fit in the context of modern data science, which is concerned with drawing conclusions from data. In our opinion, the power of probability is its ability to summarize microstates using macro descriptions.Thisstatementwilltakeussomeefforttoelaborate. We study probability because we want to analyze the uncertainties. However, when we have many data points, analyzing the uncertainties of each data point (the microstates) is computationally very difficult. Probability is useful here because it allows us to bypass the microstates and summarize the macro behavior. Instead of reporting the states of each individual, we report their sample average. Instead of offering the worst-case guarantee, we offer a probabilistic guarantee. You ask: so what? If we can offer you a performance guaranteeat99.99%confidencebutone-tenthofthecostofa100%performanceguarantee, would you consider our offer? The goal of this chapter is to outline the concepts of these probabilistic arguments. The significance of sample average Imaginethatyouhaveaboxcontainingmanytinymagnets.(Youcanalsothinkofadataset containing two classes of labels.) In condensed matter physics, these are known as the spin glasses. The orientations of the magnets depend on the magnetic field. Under an extreme condition where the magnetic field is strong, all magnets will point in the same direction. When the magnetic field is not as strong, some will align with the field but some will not, as we show in Figure 6.1. If we try to study every single magnet in this box, the correlation of the magnets will force us to consider a joint distribution, since if one magnet points to the right it is likely thatanothermagnetwillalsopointtotheright.Thesimultaneousdescriptionofallmagnets is modeled through a joint probability distribution f (x ,x ,...,x ). X1,X2,...,XN 1 2 N Like any joint PDF, this PDF tells us the probability density that the magnets will take a collection of states simultaneously. If N is large (say, on the order of millions), this joint distribution will be very complicated. 319
CHAPTER 6. SAMPLE STATISTICS Figure 6.1: Imagine that we have a box of magnets and we want to measure their orientation angles. The data points have individual randomness and correlations. Studying each one individually could be computationally infeasible, as we need to estimate the joint PDF f (x ,...,x ) across all the X1,...,XN 1 N data points. Probability offers a tool to summarize these individual states using a macro description. For example, we can analyze the sample average X of the data points and derive conclusions from N the PDF of X , i.e., f (x). The objective of this chapter is to present a few probabilistic tools to N XN analyze macro descriptions, such as the sample average. Since the joint PDF is very difficult to obtain computationally, physicists proposed to study the sample statistics. Instead of looking at the individual states, they look at the sample average of the states. If we define X ,...,X as the states of the magnets, then 1 N the sample average is N 1 (cid:88) X = X . N N n n=1 Sinceeachmagnetisrandom,thesampleaverageisalsorandom,andthereforeitisgranted a PDF: f (x). XN Thus, X has a PDF, a mean, a variance, and so on. N We call X a sample statistic. It is called a statistic because it is a summary of the N microstates,andasamplestatisticbecausethestatisticisbasedonrandomsamples,noton the underlying theoretical distributions. We are interested in knowing the behavior of X N because it is the summary of the observations. If we know the PDF of X , we will know N themean,thevariance,andthevalueofX whenthemagneticfieldincreasesordecreases. N Why study the sample average X ? N • Analyzing individual variables is not feasible because the joint PDF can be ex- tremely high-dimensional. • Sample average is a macro description of the data. • If you know the behavior of the sample average, you know most of the data. Probabilistic guarantee versus worst-case guarantee Besides the sample average, we are also interested in the difference between a probabilistic guarantee and a deterministic guarantee. 320
Consider the birthday paradox (see Chapter 1 for details). Suppose there are 50 stu- dentsinaroom.Whatistheprobabilitythatatleasttwostudentshavethesamebirthday? A naive thought would suggest that we need 366 students to guarantee a pair of the same birthday because there are 365 days. So, with only 50 students, it would seem unlikely to have a pair with the same birthday. However, it turns out that with just 50 students, the probabilityofhavingatleastonepairwiththesamebirthdayismorethan97%.Figure 6.2 belowshowsacalculationbyacomputer,whereweplottheestimatedprobabilityasafunc- tion of the number of students. What is more surprising is that with as few as 23 students, the probability is greater than 50%. There is no need for there to be 365 students in order to offer a guarantee. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 10 20 30 40 50 60 70 80 90 100 Number of people ytilibaborP Figure6.2:Thebirthdayparadoxasksthequestionofhowmanypeopleweneedtoaskinordertohave atleasttwoofthemhavingthesamebirthday.Whilewetendtothinkthattheansweris366(because there are 365 days), the actual probability, as we have calculated (see Chapter 1), is more than 97%, evenifwehaveonlyasked50people.Thecurveaboveshowstheprobabilityofhavingatleastonepair of people having the same birthday as a function of the number of people. The plot highlights the gap between the worst-case performance and an average-case performance. Why does this happen? Certainly, we can trace back to the formulae in Chapter 1 and argue through the lens of combinations and permutations. However, the more important message is about the difference between the worst-case guarantee and the average-case guarantee. Worst case versus average case • Worst-case guarantee: You need to ensure that the worst one is protected. This requires an exhaustive search until hitting 100%. It is a deterministic guarantee. • Average-caseguarantee:Youguaranteethatwithahighprobability(e.g.,99.99%), the undesirable event does not happen. This is a probabilistic guarantee. Is there a difference between 99.99% and 100%? If the probability is 99.99%, there is one failure every 10,000 trials on average. You are unlikely to fail, but it is still possible. A 100% guarantee says that no matter how many trials you make you will not fail. The 99.99% guarantee is much weaker (yes, much weaker, not just a little bit weaker) than the deterministic guarantee. However, in practice, people might be willing to pay for the risk in exchange for efficiency. This is the principle behind insurance. Automobile manufacturing 321
CHAPTER 6. SAMPLE STATISTICS also uses this principle — your chance of purchasing a defective car is non-zero, but if the manufacturer can sell enough cars to compensate for the maintenance cost of fixing your car, they might be willing to offer a limited warranty in exchange for a lower selling price. Howdoweanalyzetheprobabilisticguarantee,e.g.,forthesampleaverage?Remember that the sample average X is a random variable. Since it is a random variable, it has a N mean, variance, and PDF.1 To measure the probabilistic guarantee, we consider the event def B = {|X −µ|≥(cid:15)}, N where µ = E[X ] is the true population mean, and (cid:15) > 0 is a very small number. This N probability is illustrated in Figure 6.3, assuming that X has the PDF of a Gaussian. The N probability of B is the two tails under the PDF. Therefore, B is a bad event because in principle X should be close to µ. The probability P[B] measures situations where X N N stays very far from µ. If we can show that P[B] is small (e.g., < 0.01%), then we can say that we have obtained a probabilistic guarantee at 99.99%. Figure 6.3: The probabilistic guarantee of a sample average X is established by computing the N probability of the tails. In this example, we assume that f (x) take a Gaussian shape, and we define XN (cid:15) = 1. Anything belonging to |X −µ| ≥ (cid:15) is called a undesired event B. If the probability of a N undesired event is small, we say that we can offer a probabilistic guarantee. ThemomentwecomputeP[|X −µ|≥(cid:15)],weentertheraceofprobabilisticguarantee N (e.g., 99.99%). Why? If the probability P[|X −µ|≥(cid:15)] is less than 0.01%, it still does not N exclude the possibility that something bad will happen once every 10,000 trials on average. Thechanceislow,butitisstillpossible.Wewilllearnsomemathematicaltoolsforanalyzing this type of probabilistic guarantee. Plan for this chapter Withthesetwomainthemesinmind,wenowdiscusstheorganizationofthischapter.There are four sections: two for mathematical tools and two for main results. • Moment-generating functions: We have seen in Chapter 5 that the PDF of a sum of tworandomvariablesX+Y istheconvolutionofthetwoPDFsf ∗f .Convolutions X Y are non-trivial, especially when we have more random variables to sum. The moment- generating functions provide a convenient way of summing N random variables. They are the transform domain techniques (e.g., Fourier transforms). Since convolutions in 1Not all random variables have mean and variance, e.g., a Cauchy random variable, but most of them do. 322
time are multiplications in frequency, the moment-generating functions allow us to multiply PDFs in the transformed space. In this way, we can sum as many random variables as we want. We will discuss this idea in Section 6.1. Key Concept 1: Why study moment-generating functions? Moment-generatingfunctionshelpusdeterminethePDFofX +X +···+X . 1 2 N • Probability inequalities:WhenanalyzingsamplestatisticssuchasX ,evaluatingthe N exactprobabilitycouldbedifficultbecauseitrequiresintegratingthePDFs.However, if our ultimate goal is to estimate the probability, deriving an upper bound might be sufficienttoachievethegoal.Theprobabilityinequalitiesaredesignedforthispurpose. In Section 6.2, we discuss several of the most basic probability inequalities. We will use some of them to prove the law of large numbers. Key Concept 2: How can probability inequalities be useful? Probability inequalities help us upper-bound the bad event P[|X −µ|≥(cid:15)]. N • Law of large numbers: This is the first main result of the chapter. The law of large numbers says that the sample average X converges to the population mean µ when N the number of samples grows to infinity. The law of large numbers comes in two versions: the weak law of large numbers and the strong law of large numbers. The difference is the type of convergence they guarantee. The weak law is based on con- vergence in probability,whereasthestronglawisbasedonalmost sure convergence. We will discuss these types of convergence in Section 6.3. Key Concept 3: What is the law of large numbers? There is a weak law and a strong law of large numbers. The weak law of large numbers says that X converges to the true mean µ, as N grows: N lim P[|X −µ|>(cid:15)]=0. N N→∞ • Central Limit Theorem: The Central Limit Theorem says that the probability of X can be approximated by the probability of a Gaussian. You can also think of N this as saying that the PDF of X is converging to a distribution that can be well N approximatedbyabell-shapedGaussian.Ifwehavemanyrandomvariablesandtheir sum is becoming a Gaussian, we can ignore the individual PDFs and focus on the Gaussian. Thus it explains why Gaussian is so popular. We will discuss this theorem in detail in Section 6.4. Key Concept 4: What is the Central Limit Theorem? The CDF of X can be approximated by the CDF of a Gaussian, as N grows. N 323
CHAPTER 6. SAMPLE STATISTICS 6.1 Moment-Generating and Characteristic Functions Consider two independent random variables X and Y with PDFs f (x) and f (y), respec- X Y tively. Let Z = X +Y be the sum of the two random variables. We know from Chapter 5 that the PDF of Z, f , is the convolution of f and f . However, we think you will agree Z X Y that convolutions are not easy to compute. Especially when the sum involves more random variables, computing the convolution would be tedious. So how should we proceed in this case? One approach is to use some kind of “frequency domain” method that transforms the PDFs to another domain and then perform multiplication instead of the convolution to make the calculations easy or at least easier. The moment-generating functions and the characteristic functions are designed for this purpose. 6.1.1 Moment-generating function Definition6.1. ForanyrandomvariableX,the moment-generatingfunction(MGF) M (s) is X M (s)=E(cid:2) esX(cid:3) . (6.1) X The definition says that the moment-generating function (MGF) is the expectation of the random variable taken to the power esX for some s. Effectively, it is the expectation of a function of random variables. The meaning of the expectation can be seen by writing out the definition. For the discrete case, the MGF is (cid:88) M (s)= esxp (x), (6.2) X X x∈Ω whereas in the continuous case, the MGF is (cid:90) ∞ M (s)= esxf (x)dx. (6.3) X X −∞ The continuous case should remind us of the definition of a Laplace transform. For any function f(t), the Laplace transform is (cid:90) ∞ L[f](s)= f(t)est dt. −∞ From this perspective, we can interpret the MGF as the Laplace transform of the PDF. The argument s of the output can be regarded as the coordinate in the Laplace space. If s=−jω, then M (jω) becomes the Fourier transform of the PDF. X Example 6.1. Consider a random variable X with three states 0,1,2 and with prob- ability masses 2,3,1 respectively. Find the MGF. 6 6 6 324
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Solution. The moment-generating function is 2 3 1 M (s)=E[esX]=es0· +es1· +es2· X 6 6 6 1 es e2s = + + . 3 2 6 Practice Exercise 6.1. Find the MGF for a Poisson random variable. Solution. The MGF of Poisson random variable can be found as M (s)=E[esX]=(cid:88)∞ esxλxe−λ =(cid:88)∞ (λes)x e−λ =eλes e−λ. X x! x! x=0 x=0 Practice Exercise 6.2. Find the MGF for an exponential random variable. Solution. The MGF of an exponential random variable can be found as (cid:90) ∞ (cid:90) ∞ λ M (s)=E[esX]= esxλe−λx dx= λe(s−λ)x dx= , if λ>s. X λ−s 0 0 Why are moment-generating functions so called? The following theorem reveals the reason. Theorem 6.1. The MGF has the properties that • M (0)=1, X • d M (s)| =E[X], d2 M (s)| =E[X2], ds X s=0 ds2 X s=0 • dk M (s)| =E[Xk], for any positive integer k. dsk X s=0 Proof. The first property can be proved by noting that M (0)=E[e0X]=E[1]=1. X The third property holds because dk (cid:90) ∞ dk (cid:90) ∞ M (s)= esxf (x)dx= xkesxf (x)dx. dsk X dsk X X −∞ −∞ Setting s=0 yields dk (cid:90) ∞ M (s)| = xkf (x)dx=E[Xk]. dsk X s=0 X −∞ The second property is a special case of the third property. (cid:3) 325
CHAPTER 6. SAMPLE STATISTICS The theorem tells us that if we take the derivative of the MGF and set s=0, we will obtain the moment. The order of the moment depends on the order of the derivative. As a result, the MGF can “generate moments” by taking derivatives. This happens because of the exponential function esx. Since d esx =xesx, the variable x appears whenever we take ds the derivative. Practice Exercise 6.3. Let X be a Bernoulli random variable with parameter p. Find the first two moments using MGF. Solution. The MGF of a Bernoulli random variable is M (s)=E[esX] X =es0p (0)+es1p (1) X X =(1)(1−p)+(es)(p) =1−p+pes. The first and the second moment, using the derivative approach, are (cid:12) (cid:18) (cid:19)(cid:12) (cid:12) E[X]= dd sM X(s)(cid:12) (cid:12) (cid:12) = dd s 1−p+pes (cid:12) (cid:12) (cid:12) =pes(cid:12) (cid:12) (cid:12) =p, s=0 s=0 s=0 E[X2]= dd s2 2M X(s)(cid:12) (cid:12) (cid:12) (cid:12) = dd s2 2(cid:18) 1−p+pes(cid:19)(cid:12) (cid:12) (cid:12) (cid:12) =pes(cid:12) (cid:12) (cid:12) (cid:12) =p. s=0 s=0 s=0 To facilitate our discussions of MGF, we summarize a few MGFs in the table below. Distribution PMF/PDF E[X] Var[X] M (s) X Bernoulli p (1)=pandp (0)=1−p p p(1−p) 1−p+pes X X Binomial p (k)=(cid:0)n(cid:1) pk(1−p)n−k np np(1−p) (1−p+pes)n X k 1 1−p pes Geometric p (k)=p(1−p)k−1 X p p2 1−(1−p)es λke−λ Poisson p (k)= λ λ eλ(es−1) X k! 1 (cid:26) (x−µ)2(cid:27) (cid:26) σ2s2(cid:27) Gaussian f (x)= √ exp − µ σ2 exp µs+ X 2πσ2 2σ2 2 1 1 λ Exponential f (x)=λexp{−λx} X λ λ2 λ−s 1 a+b (b−a)2 esb−esa Uniform f (x)= X b−a 2 12 s(b−a) Table 6.1: Moment-generating functions of common random variables. 326
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS 6.1.2 Sum of independent variables via MGF MGFs are most useful when analyzing the PDF of a sum of two random variables. The following theorem highlights the result. Theorem 6.2. Let X and Y be independent random variables. Let Z =X+Y. Then M (s)=M (s)M (s). (6.4) Z X Y Proof. By the definition of MGF, we have that M (s)=E(cid:104) es(X+Y)(cid:105)( =a)E(cid:2) esX(cid:3)E(cid:2) esY(cid:3) =M (s)M (s), Z X Y where (a) is valid because X and Y are independent. (cid:3) Corollary6.1. ConsiderindependentrandomvariablesX ,...,X .LetZ =(cid:80)N X 1 N n=1 n be the sum of random variables. Then the MGF of Z is N (cid:89) M (s)= M (s). (6.5) Z Xn n=1 If these random variables are further assumed to be identically distributed, the MGF is M (s)=(M (s))N. (6.6) Z X1 Proof. This follows immediately from the previous theorem: N (cid:89) M (s)=E[es(X1+···+XN)]=E[esX1]E[esX2]···E[esXN]= M (s). Z Xn n=1 If the random variables X ,...,X are i.i.d., then the product simplifies to 1 N N N (cid:89) M (s)= (cid:89) M (s)=(M (s))N. Xn X1 X1 n=1 n=1 (cid:3) Theorem 6.3 (Sum of Bernoulli = binomial). Let X , ..., X be a sequence of 1 N i.i.d. Bernoulli random variables with parameter p. Let Z =X +···+X be the sum. 1 N Then Z is a binomial random variable with parameters (N,p). Proof. Let us consider a sequence of i.i.d. Bernoulli random variables X ∼ Bernoulli(p) n for n=1,...,N. Let Z =X +···+X . The moment-generating function of Z is 1 N N (cid:89) M (s)=E[es(X1+···+XN)]= E[esXn] Z n=1 N = (cid:89)(cid:0) pes1+(1−p)es0(cid:1) =(pes+(1−p))N. n=1 327
CHAPTER 6. SAMPLE STATISTICS Now, let us check the moment-generating function of a binomial random variable: If Z ∼ Binomial(N,p), then N (cid:18) (cid:19) (cid:88) N M (s)=E[esZ]= esk pk(1−p)N−k Z k n=0 N (cid:18) (cid:19) = (cid:88) N (pes)k(1−p)N−k =(pes+(1−p))N, k n=0 where the last equality holds because (cid:80)N (cid:0)N(cid:1) akbN−k = (a + b)N. Therefore, the two n=0 k moment-generating functions are identical. (cid:3) Theorem 6.4 (Sum of binomial = binomial). Let X , ..., X be a sequence of 1 N i.i.d. binomial random variables with parameters (n,p). Let Z =X +···+X be the 1 N sum. Then Z is a binomial random variable with parameters (Nn,p). Proof. The MGF of a binomial random variable is M (s)=(pes+(1−p))n. Xi If we have N of these random variables, then Z =X +···+X will have the MGF 1 N N M (s)=(cid:89) M (s)=(pes+(1−p))Nn. Z Xi i=1 NotethatthisisjusttheMGFofanotherbinomialrandomvariablewithparameter(Nn,p). (cid:3) Theorem 6.5 (Sum of Poisson = Poisson). Let X , ..., X be a sequence of 1 N i.i.d. Poisson random variables with parameter λ. Let Z =X +···+X be the sum. 1 N Then Z is a Poisson random variable with parameters Nλ. Proof. The MGF of a Poisson random variable is (cid:88)∞ λk M (s)=E[esX]= esk e−λ X k! k=0 (cid:88)∞ (λes)k =e−λ k! k=0 =e−λeλes =eλ(es−1). AssumethatwehaveasumofN i.i.d.Poissonrandomvariables.Then,bythemaintheorem, we have that M (s)=[M (s)]N =eNλ(es−1). Z X Therefore, the resulting random variable Z is a Poisson with parameter Nλ. (cid:3) 328
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Theorem 6.6 (Sum of Gaussian = Gaussian). Let X , ..., X be a sequence of 1 N independent Gaussian random variables with parameters (µ ,σ2), ..., (µ ,σ2 ). Let 1 1 N N Z =X +···+X be the sum. Then Z is a Gaussian random variable: 1 N (cid:18) N N (cid:19) (cid:88) (cid:88) Z =Gaussian µ , σ2 . (6.7) n n n=1 n=1 Proof. We skip the proof of the MGF of a Gaussian. It can be shown that (cid:26) σ2s2(cid:27) M (s)=exp µs+ . X 2 When we have a sequence of Gaussian random variables, then M (s)=E[es(X1+···+XN)] Z =M (s)···M (s) X1 XN (cid:18) (cid:26) σ2s2(cid:27)(cid:19) (cid:18) (cid:26) σ2 s2(cid:27)(cid:19) = exp µ s+ 1 ··· exp µ s+ N 1 2 N 2 (cid:40)(cid:32) (cid:88)N (cid:33) (cid:32) (cid:88)N (cid:33) s2(cid:41) =exp µ s+ σ2 . n n 2 n=1 n=1 Therefore, the resulting random variable Z is also a Gaussian. The mean and variance of Z are (cid:80)N µ and (cid:80)N σ2, respectively. n=1 n n=1 n (cid:3) 6.1.3 Characteristic functions Moment-generating functions are the Laplace transforms of the PDFs. However, since the Laplacetransformisdefinedontheentirerighthalf-plane,notallPDFscanbetransformed. One way to mitigate this problem is to restrict s to the imaginary axis, s = jω. This will give us the characteristic function. Definition 6.2 (Usual definition). The characteristic function of a random variable X is Φ (jω)=E[ejωX]. (6.8) X However, we note that since ω can take any value in (−∞,∞), it does not matter if we consider E[e−jωX] or E[ejωX]. This leads to the following equivalent definition of the char- acteristic function: Definition 6.3 (Alternative definition (for this book)). The characteristic function of a random variable X is Φ (jω)=E[e−jωX]. (6.9) X 329
CHAPTER 6. SAMPLE STATISTICS If we follow this definition, we see that the characteristic function can be written as (cid:90) ∞ Φ (jω)=E[e−jωX]= e−jωxf (x)dx. (6.10) X X −∞ ThisisexactlytheFouriertransform ofthePDF.Thereasonforintroducingthisalternative characteristicfunctionisthatE[e−jωX]istheFouriertransformoff (x)butE[ejωX]isthe X inverse Fourier transform of f (x). The former is more convenient (in terms of notation) X forstudentswhohavetakenacourseinsignalsandsystems.However,weshouldstressthat the usual way of defining the characteristic function is E[ejωX]. A list of common Fourier transforms is shown in the table below. Additional identities can be found in standard signals and systems textbooks. Fourier Transforms f(t)←→F(ω) f(t)←→F(ω) 1. e−atu(t)←→ 1 , a>0 10. sinc2(Wt)←→ 2π∆( ω ) a+jω 2 W 2W 2. eatu(−t)←→ a−1 jω, a>0 11. e−atsin(ω 0t)u(t)←→ (a+jωω )0 2+ω2 0 3. e−a|t| ←→ 2a , a>0 12. e−atcos(ω t)u(t)←→ a+jω a2+ω2 0 (a+jω)2+ω2 0 √ 4. a2a +2 t2 ←→πae−a|ω|, a>0 13. e− 2t σ2 2 ←→ 2πσe−σ2 2ω2 5. te−atu(t)←→ 1 , a>0 14. δ(t)←→1 (a+jω)2 6. tne−atu(t)←→ n! , a>0 15. 1←→2πδ(ω) (a+jω)n+1 7. rect( τt)←→τsinc(ω 2τ) 16. δ(t−t 0)←→e−jωt0 8. sinc(Wt)←→ Wπ rect( 2w W) 17. ejω0t ←→2πδ(ω−ω 0) 9. ∆( τt)←→ τ 2sinc2(ω 4τ) 18. f(t)ejω0t ←→F(ω−ω 0) Table 6.2: Fourier transform pairs of commonly used functions. Example 6.2. Let X be a random variable with PDF f (x)=λe−λx for x≥0. Find X the characteristic function. Solution. The Fourier transform pair is (cid:26) (cid:27) 1 λe−λx −→λ·F e−λx =λ· . λ+jω Therefore, the characteristic function is Φ (jω)= λ . X λ+jω 330
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Example 6.3. Let X and Y be independent, and let (cid:40) (cid:40) λe−λx, x≥0, λe−λy, y ≥0, f (x)= f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Solution.ThecharacteristicfunctionofX andY canbefoundfromtheFouriertable: λ λ Φ (jω)= and Φ (jω)= . X λ+jω Y λ+jω Therefore, the characteristic function of Z is λ2 Φ (jω)=Φ (jω)Φ (jω)= . Z X Y (λ+jω)2 By inverse Fourier transform, we have that (cid:26) λ2 (cid:27) f (z)=F−1 =λ2ze−λz, z ≥0. Z (λ+jω)2 Why Φ (jω) but not M (s)? As we said, the function is not always defined. Recall X X that the expectation E[X] exists only when f (x) is absolutely integrable, or E[|X|] < ∞. X Foracharacteristicfunction,theexpectationisvalidbecauseE[|ejωX|]=E[1]=1.However, for a function, E[|esX|] could be unbounded. To see a counterexample, we consider the Cauchy distribution. Theorem 6.7. Consider the Cauchy distribution with PDF 1 f (x)= . (6.11) X π(x2+1) The MGF of X is undefined but the characteristic function is well defined. Proof. The MGF is (cid:90) ∞ 1 (cid:90) ∞ 1 M (s)= esx dx≥ esx dx X π(x2+1) π(x2+1) −∞ 1 (cid:90) ∞ (sx)3 (sx)3 ≥ dx, because esx ≥ 6π(x2+1) 6 1 (cid:90) ∞ (sx)3 s3 (cid:90) ∞ ≥ dx= xdx=∞. 6π(2x2) 12π 1 1 Therefore, the MGF is undefined. On the other hand, by the Fourier table we know that (cid:26) (cid:27) 1 Φ (jω)=F =e−|ω|. X π(x2+1) (cid:3) 331
CHAPTER 6. SAMPLE STATISTICS Example 6.4. Let X ,X ,... be a sequence of independent random variables with 0 1 PDF a 1 f (x)= k , a = for k =0,1,.... Xk π(a2 +x2) k 2k+1 k Find the PDF of Y, where Y =(cid:80)∞ X . k=0 k Solution. From the Fourier transform table, we know that a 1 a2 1 π(a2 +k x2) = a π · (a2 +k x2) ←F → a π ·πa ke−ak|ω| =e−ak|ω|. k k k k The characteristic function of Y is ∞ (cid:40) ∞ (cid:41) (cid:89) (cid:88) Φ (jω)= Φ (jω)=exp −|ω| a . Y Xk k k=0 k=0 Since (cid:80)∞ a = (cid:80)∞ 1 = 1 + 1 +··· = 1, the characteristic function becomes k=0 k k=0 2k+1 2 4 Φ (jω)=e−|ω|. The inverse Fourier transform gives us Y 1 1 1 e−|ω| = ·πe−|ω| ←F → · . π π 1+x2 Therefore the PDF of Y is 1 f (y)= . Y π(1+y2) Example 6.5. Two random variables X and Y have the PDFs (cid:40) (cid:40) e−x, x≥0, e−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =max(X,Y)−min(X,Y). Solution. We first show that Z =max(X,Y)−min(X,Y)=|X−Y|. Suppose X >Y, then max(X,Y)=X and min(X,Y)=Y. So Z =X−Y. If X <Y, then max(X,Y) = Y and min(X,Y) = X. So Z = Y −X. Combining the two cases gives us Z =|X−Y|. Now, consider the Fourier transform of the PDFs: 1 e−x ←F → . 1+jω 332
6.2. PROBABILITY INEQUALITIES Let U =X−Y, and let Z =|U|. The characteristic function is Φ (jω)=E[e−jω(X−Y)]=E[e−jωX]E[ejωY] U 1 1 1 1 = · = ←F → f (u)= e−|u|. 1+jω 1−jω 1+ω2 U 2 With the PDF of U, we can find the CDF of Z: F (z)=P[Z ≤z]=P[|U|≤z] Z (cid:90) z = f (u)du U −z (cid:90) z 1 = e−|u| du 2 −z (cid:90) z 1 =2 e−u du=1−e−z. 2 0 Hence, the PDF is d f (z)= F (z)=e−z. Z dz Z Closing remark. Moment-generating functions and characteristic functions are useful mathematical tools. In this section, we have confined our discussion to using them to com- pute the sum of two random variables. Later sections and chapters will explain further uses for these functions. For example, we use the MGFs when proving Chernoff’s bound and proving the Central Limit Theorem. 6.2 Probability Inequalities Moment-generatingfunctionsandcharacteristicfunctionsarepowerfultoolsforhandlingthe sum of random variables. We now introduce another set of tools, known as the probability inequalities, that allow us to do approximations. We will highlight a few basic probability inequalities in this section. 6.2.1 Union bound The first inequality is the union bound we had introduced when we discussed the axioms of probabilities. The union bound states the following: Theorem 6.8 (Union Bound). Let A ,...,A be a collection of sets. Then 1 N (cid:34) N (cid:35) N (cid:91) (cid:88) P A ≤ P[A ]. (6.12) n n n=1 n=1 333
CHAPTER 6. SAMPLE STATISTICS Proof. We can prove this by induction. First, if N =2, P[A ∪A ]=P[A ]+P[A ]−P[A ∩A ]≤P[A ]+P[A ], 1 2 1 2 1 2 1 2 because P[A ∩A ] is a probability and so it must be non-negative. Thus we have proved 1 2 the base case. Assume that the statement is true for N = K. We need to prove that the statement is also true for N =K+1. To this end, we note that (cid:34)K+1 (cid:35) (cid:34)(cid:32) K (cid:33) (cid:35) (cid:91) (cid:91) P A =P A ∪A n n K+1 n=1 n=1 (cid:34) K (cid:35) (cid:34)(cid:32) K (cid:33) (cid:35) (cid:91) (cid:91) =P A +P[A ]−P A ∩A n K+1 n K+1 n=1 n=1 (cid:34) K (cid:35) (cid:91) ≤P A +P[A ]. n K+1 n=1 Then, according to our hypothesis for N =K, it follows that (cid:34) K (cid:35) K (cid:91) (cid:88) P A ≤ P[A ]. n n n=1 n=1 Putting these together, (cid:34)K+1 (cid:35) K K+1 (cid:91) (cid:88) (cid:88) P A ≤ P[A ]+P[A ]= P[A ]. n n K+1 n n=1 n=1 n=1 Therefore, by the principle of induction, we have proved the statement. (cid:3) Remark.Thetightnessoftheunionbounddependsontheamountof overlappingbetween theeventsA ,...,A ,asillustratedinFigure6.4.Iftheeventsaredisjoint,theunionbound 1 n is tight. If the events are overlapping significantly, the union is loose. The idea of the union bound is the principle of divide and conquer. We decompose the system into smaller events forasystemofnvariablesandusetheunionboundtoupper-limittheoverallprobability.If the probability of each event is small, the union bound tells us that the overall probability of the system will also be small. Figure 6.4: Conditions under which the union bound is loose or tight. [Left] The union bound is loose when the sets are overlapping. [Right] The union bound is tight when the sets are (nearly) disjoint. 334
6.2. PROBABILITY INEQUALITIES Example 6.6. Let X ,...,X be a sequence of i.i.d. random variables with CDF 1 N F (x) and let Z =min(X ,...,X ). Find an upper bound on the CDF. Xn 1 N Solution. Note that Z = min(X ,...,X ) ≤ z is equivalent to at least one of the 1 N X ’s being less than z. Thus, we have that n Z =min(X ,...,X )≤z ⇔ X ≤z∪···∪X ≤z. 1 N 1 N Substituting this result into the CDF, F (z)=P[Z ≤z] Z =P[min(X ,...,X )≤z] 1 N =P[X ≤z∪···∪X ≤z] 1 N ≤P[X ≤z]+···+P[X ≤z] 1 N =N ·F (z). X 6.2.2 The Cauchy-Schwarz inequality ThesecondinequalitywestudyhereistheCauchy-Schwarzinequality,whichwepreviously mentioned in Chapter 5. We review it for the sake of completeness. Theorem 6.9 (Cauchy-Schwarz inequality). Let X and Y be two random variables. Then E[XY]2 ≤E[X2]E[Y2]. (6.13) Proof. Let f(s)=E[(sX+Y)2] for any real s. Then f(s)=E[(sX+Y)2] =E[s2X2+2sXY +Y2] =E[X2]s2+2E[XY]s+E[Y2]. This is a quadratic equation, and f(s)≥0 for all s because E[(sX+Y)2]≥0. Recall that for a quadratic equation φ(x)=ax2+bx+c, the function φ(x)≥0 if and only if b2−4ac≤0. Substituting this result into our problem, we show that (2E[XY])2−4E[X2]E[Y2]≤0. This implies that E[XY]2 ≤E[X2]E[Y2], which completes the proof. (cid:3) Remark. As shown in Chapter 5, the Cauchy-Schwarz inequality is useful in analyzing E[XY].Forexample,wecanusetheCauchy-Schwarzinequalitytoprovethatthecorrelation coefficient ρ is bounded between −1 and 1. 335
CHAPTER 6. SAMPLE STATISTICS 6.2.3 Jensen’s inequality Our next inequality is Jensen’s inequality. To motivate the inequality, we recall that Var[X]=E[X2]−E[X]2. Since Var[X]≥0 for any X, it follows that E[X2] ≥ E[X]2 . (6.14) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[g(X)] =g(E[X]) Jensen’s inequality is a generalization of the above result by recognizing that the inequality does not only hold for the function g(X) = X2 but also for any convex function g. The theorem is stated as follows: Theorem 6.10 (Jensen’s inequality). Let X be a random variable, and let g :R→R be a convex function. Then E[g(X)]≥g(E[X]). (6.15) If the function g is concave, then the inequality sign is flipped: E[g(X)] ≤ g(E[X]). The way to remember this result is to remember that E[X2]−E[X]2 =Var[X]≥0. Now,whatisaconvexfunction?Informally,afunctiongisconvexif,whenwepickany two points on the function and connect them with a straight line, the line will be above the function for that segment. This definition is illustrated in Figure 6.5. Consider an interval [x,y], and the line segment connecting g(x) and g(y). If the function g(·) is convex, then the entire line segment should be above the curve. Figure 6.5: Illustration of a convex function, a concave function, and a function that is neither convex nor concave. The definition of a convex function essentially follows the above picture: Definition 6.4. A function g is convex if g(λx+(1−λ)y)≤λg(x)+(1−λ)g(y), (6.16) for any 0≤λ≤1. Hereλrepresentsa“sweeping”constantthatgoesfromxtoy.Whenλ=1thenλx+(1−λ)y simplifies to x, and when λ=0 then λx+(1−λ)y simplifies to y. 336
6.2. PROBABILITY INEQUALITIES Thedefinitioniseasytounderstand.Theleft-handsideg(λx+(1−λ)y)isthefunction evaluated at any points in the interval [x,y]. The right-hand side is the red straight line we plotted in Figure 6.5. It connects the two points g(x) and g(y). Convexity means that the red line is entirely above the curve. For twice-differentiable 1D functions, convexity can be described by the curvature of the function. A function is convex if g(cid:48)(cid:48)(x)≥0. (6.17) This is self-explanatory because if the curvature is non-negative for all x, then the slope of g has to keep increasing. Example 6.7. The following functions are convex or concave: • g(x)=logx is concave, because g(cid:48)(x)= 1 and g(cid:48)(cid:48)(x)=− 1 ≤0 for all x. x x2 • g(x)=x2 is convex, because g(cid:48)(x)=2x and g(cid:48)(cid:48)(x)=2 is positive. • g(x)=e−x is convex, because g(cid:48)(x)=−e−x and g(cid:48)(cid:48)(x)=e−x ≥0. Why is Jensen inequality valid for a convex function? Consider the illustration in Figure 6.6. Suppose we have a random variable X taking some PDF f (x). There is a X convexfunctiong(·)thatmapstherandomvariableX tog(X).Sinceg(·)isconvex,aPDF like the one we see in Figure 6.6 will become skewed. (You can map the left tail to the new left tail, the peak to the new peak, and the right tail to the new right tail.) As you can see from the figure, the new random variable g(X) has a mean E[g(X)] that is greater than the mapped old mean g(E[X]). Jensen’s inequality captures this phenomenon by stating that E[g(X)]≥g(E[X]) for any convex function g(·). Figure6.6:Jensen’sinequalitystatesthatifthereisaconvexfunctiong(·)thatmapsarandomvariable X to a new random variable g(X), the new mean E[g(X)] will be greater than the mapped old mean g(E[X]). ProvingJensen’sinequalityisstraightforwardforatwo-statediscreterandomvariable. Define a random variable X with states x and y. The probabilities for these two states are P[X =x]=λ and P[X =y]=1−λ. Then (cid:88) E[X]= x(cid:48)p (x(cid:48))=λx+(1−λ)y. X x(cid:48)∈{x,y} 337
CHAPTER 6. SAMPLE STATISTICS Now, let g(·) be a convex function. We know from the expectation that (cid:88) E[g(X)]= g(x(cid:48))p (x(cid:48))=g(x)λ+(1−λ)g(y). X x(cid:48)∈{x,y} By convexity of the function g(·), it follows that g(λx+(1−λ)y)≤λf(x)+(1−λ)g(y), (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =g(E[X]) =E[g(X)] where in the underbrace we substitute the definitions using the expectation. Therefore, foranytwo-statediscreterandomvariables,theproofofJensen’sinequalityfollowsdirectly fromtheconvexity.Ifthediscreterandomvariabletakesmorethantwostates,wecanprove thetheorembyinduction.Forcontinuousrandomvariables,wecanprovethetheoremusing the following approach. You may skip the proof of Jensen’s inequality if this is your first time reading the book. Here we present an alternative proof of Jensen’s inequality that does not require proof by induction. The idea is to recognize that if the function g is convex we can find a tangent lineL(X)=aX+batthepointE[X]thatisuniformlylowerthan g(X),i.e.,g(X)≥L(X) for all X. Then we can prove the result with a simple geometric argument. Figure 6.7 illustrates this idea. Figure6.7:GeometricillustrationoftheproofofJensen’sinequality.Supposeg(·)isaconvexfunction. For any point X on g(·), we can find a tangent line L(X) = aX+b. Since the black curve is always above the tangent, it follows that E[g(X)]≥E[L(X)] for any X. Also, note that at a particular point E[X], the black curve and the red line touch, and so we have L(E[X])=g(E[X]). Proof of Jensen’s inequality.ConsiderL(X)asdefinedabove.Sinceg isconvex,g(X)≥ L(X) for all X. Therefore, E[g(X)]≥E[L(X)] =E[aX+b] =aE[X]+b =L(E[X])=g(E[X]), where the last equality holds because L is a tangent line to g where they meet at E[X]. (cid:3) 338
6.2. PROBABILITY INEQUALITIES What are (a,b) in the proof? By Taylor expansion, g(X)≈g(E[X])+g(cid:48)(E[X])(X−E[X]) def = L(X). Therefore, if we want to be precise, then a=g(cid:48)(E[X]) and b=g(E[X])−g(cid:48)(E[X])E[X]. The end of the proof. Example 6.8. By Jensen’s inequality, we have that (a) E[X2]≥E[X]2, because g(x)=x2 is convex. (b) E(cid:2)1(cid:3) ≥ 1 , because g(x)= 1 is convex. X E[X] x (c) E[logX]≤logE[X], because g(x)=logx is concave. 6.2.4 Markov’s inequality Our next inequality, Markov’s inequality, is an elementary inequality that links probability and expectation. Theorem 6.11 (Markov’s inequality). Let X ≥0 be a non-negative random variable. Then, for any ε>0, we have E[X] P[X ≥ε]≤ . (6.18) ε Markov’s inequality concerns the tail of the random variable. As illustrated in Figure 6.8, P[X ≥ ε] measures the probability that the random variable takes a value greater than ε. Markov’s inequality asserts that this probability P[X ≥ ε] is upper-bounded by the ratio E[X]/ε.Thisresultisusefulbecauseitrelatestheprobabilityandtheexpectation.Inmany problems the probability P[X ≥ε] could be difficult to evaluate if the PDF is complicated. The expectation, on the other hand, is usually easier to evaluate. Proof. Consider εP[X ≥ε]. It follows that (cid:90) ∞ (cid:90) ∞ εP[X ≥ε]= ε f (x)dx≤ xf (x)dx, X X ε ε where the inequality is valid because for any x ≥ ε the integrand (which is non-negative) will always increase (or at least not decrease). It then follows that (cid:90) ∞ (cid:90) ∞ xf (x)dx≤ xf (x)dx=E[X]. (cid:3) X X ε 0 A pictorial interpretation of Markov’s inequality is shown in Figure 6.9. For X >0, it is not difficult to show that E[X] = (cid:82)∞ 1−F (x) dx. Then, in the CDF plot, we see that 0 X ε·P[X ≥ε] is a rectangle covering the top left corner. This area is clearly smaller than the area covered by the function 1−F (x). X 339
CHAPTER 6. SAMPLE STATISTICS Figure6.8:Markov’sinequalityprovidesanupperboundtothetailofarandomvariable.Theinequality states that the probability P[X ≥ε] is upper bounded by the ratio E[X]/ε. Figure 6.9: The proof of Markov’s inequality follows from the fact that ε·P[X ≥ ε] occupies the top left corner marked by the yellow rectangle. The expectation is the area above the CDF so that E[X]=(cid:82)∞1−F (x)dx.Sincetheyellowrectangleissmallerthantheorangeshadedarea,itfollows 0 X that ε·P[X ≥ε]≤E[X], which is Markov’s inequality. Practice Exercise 6.4. Prove that if X >0, then E[X]=(cid:82)∞ 1−F (x)dx. 0 X Solution. We start from the right-hand side: (cid:90) ∞ (cid:90) ∞ 1−F (x)dx= 1−P[X ≤x]dx X 0 0 (cid:90) ∞ = P[X ≥x]dx 0 (cid:90) ∞(cid:90) ∞ = f (t)dtdx X 0 x (cid:90) ∞(cid:90) t = f (t)dxdt X 0 0 (cid:90) ∞ = tf (t)dt=E[X]. X 0 The change in the integration order is illustrated below. 340
6.2. PROBABILITY INEQUALITIES How tight is Markov’s inequality? It is possible to create a random variable such that the equality is met (see Exercise 6.14). However, in general, the estimate provided by the upper bound is not tight. Here is an example. Practice Exercise 6.5.LetX ∼Uniform(0,4).VerifyMarkov’sinequalityforP[X ≥ 2], P[X ≥3] and P[X ≥4]. Solution. First, we observe that E[X]=2. Then E[X] P[X ≥2]=0.5, =1, 2 E[X] P[X ≥3]=0.25, =0.67, 3 E[X] P[X ≥4]=0, =0.5. 4 Therefore, although the upper bounds are all valid, they are very loose. If Markov’s inequality is not tight, why is it useful? It turns out that while Markov’s inequality is not tight, its variations can be powerful. We will come back to this point when we discuss Chernoff’s bound. 6.2.5 Chebyshev’s inequality The next inequality is a simple extension of Markov’s inequality. The result is known as Chebyshev’s inequality. Theorem 6.12 (Chebyshev’s inequality). Let X be a random variable with mean µ. Then for any ε>0 we have Var[X] P[|X−µ|≥ε]≤ . (6.19) ε2 The tail measured by Chebyshev’s inequality is illustrated in Figure 6.10. Since the event |X −µ| ≥ ε involves an absolute value, the probability measures the two-sided tail. Chebyshev’s inequality states that this tail probability is upper-bounded by Var[X]/ε2. 341
CHAPTER 6. SAMPLE STATISTICS Figure 6.10: Chebyshev’s inequality states that the two-sided tail probability P[|X−µ|≥ε] is upper- bounded by Var[X]/ε2 Proof. We apply Markov’s inequality to show that P[|X−µ|≥ε]=P[(X−µ)2 ≥ε2] E[(X−µ)2] Var[X] ≤ = . ε2 ε2 (cid:3) An alternative form of Chebyshev’s inequality is obtained by letting ε = kσ. In this case, we have σ2 1 P[|X−µ|≥kσ]≤ = . k2σ2 k2 Therefore,ifarandomvariableisk timesthestandarddeviationawayfromthemean,then the probability bound drops to 1/k2. Practice Exercise 6.6. Let X ∼ Uniform(0,4). Find the bound of Chebyshev’s inequality for the probability P[|X−µ|≥1]. Solution. Note that E[X]=2 and σ2 =42/12=4/3. Therefore, we have σ2 4 P[|X−µ|≥1]≤ = , ε2 3 which is a valid upper bound, but quite conservative. Practice Exercise 6.7. Let X ∼ Exponential(1). Find the bound of Chebyshev’s inequality for the probability P[X ≥ε]. Solution. Note that E[X]=1 and σ2 =1. Thus we have P[X ≥ε]=P[X−µ≥ε−µ]≤P[|X−µ|≥ε−µ] σ2 1 ≤ = . (ε−µ)2 (ε−1)2 342
6.2. PROBABILITY INEQUALITIES We can compare this with the exact probability, which is P[X ≥ε]=1−F (ε)=e−ε. X Again,theestimategivenbyChebyshev’sinequalityisacceptablebuttooconservative. Corollary 6.2. Let X ,...,X be i.i.d. random variables with mean E[X ]=µ and 1 N n variance Var[X ]=σ2. Let X = 1 (cid:80)N X be the sample mean. Then n N N n=1 n P(cid:20) (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:21) ≤ Nσ (cid:15)2 2. (6.20) Proof. We can first show that E[X ]=µ and Var[X ] satisfies N N 1 (cid:88)N σ2 Var[X ]= Var[X ]= . N N2 n N n=1 Then by Chebyshev’s inequality, P(cid:20) (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:21) ≤ Var (cid:15)[X 2 N] = Nσ (cid:15)2 2. (cid:3) Theconsequenceofthiscorollaryisthattheupperboundσ2N/(cid:15)2 willconvergetozero (cid:12) (cid:12) as N → ∞. Therefore, the probability of getting the event {(cid:12)X N −µ(cid:12) > (cid:15)} is vanishing. It means that the sample average X is converging to the true population mean µ, in the N sense that the probability of failing is shrinking. 6.2.6 Chernoff’s bound We now introduce a powerful inequality or a set of general procedures that gives us some highly useful inequalities. The idea is named for Herman Chernoff, although it was actually due to his colleague Herman Rubin. Theorem 6.13 (Chernoff’s bound). Let X be a random variable. Then, for any ε≥0, we have that P[X ≥ε]≤e−ϕ(ε), (6.21) wherea (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) , (6.22) X s>0 and M (s) is the moment-generating function. X aϕ(ε)iscalledtheFenchel-LegendredualfunctionoflogMX.Seereferences[6-14]. 343
CHAPTER 6. SAMPLE STATISTICS Proof. There are two tricks in the proof of Chernoff’s bound. The first trick is a nonlinear transformation. Since esx is an increasing function for any s>0 and x, we have that P[X ≥ε]=P[esX ≥esε] (a) E[esX] ≤ esε ( =b) e−sεM (s) X =e−sε+logMX(s), where the inequality (a) is due to Markov’s inequality. Step (b) just uses the definition of MGF that E[esX]=M (s). X Now for the second trick. Note that the above result holds for all s. That means it must also hold for the s that minimizes e−sε+logMX(s). This implies that (cid:110) (cid:111) P[X ≥ε]≤min e−sε+logMX(s) . s>0 Again, since ex is increasing, the minimizer of the above probability is also the maximizer of this function: (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) . X s>0 Thus, we conclude that P[X ≥ε]≤e−ϕ(ε). (cid:3) 6.2.7 Comparing Chernoff and Chebyshev Let’s consider an example of how Chernoff’s bound can be useful. Suppose that we have a random variable X ∼Gaussian(0,σ2/N). The number N can be regarded as the number of samples. For example, if Y ,...,Y are N Gaussian random 1 N variables with mean 0 and variance σ2, then the average X = 1 (cid:80)N Y will have mean N n=1 n 0 and variance σ2/N. Therefore, as N grows, the variance of X will become smaller and smaller. First, since the random variable is Gaussian, we can show the following: Lemma 6.1. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for any N ε>0, (cid:32)√ (cid:33) Nε P[X ≥ε]=1−Φ , (6.23) σ where Φ is the standard Gaussian’s CDF. Note that this is the exact result: If you tell me ε, N, and σ, then the probability P[X ≥ε] is exactly the one shown on the right-hand side. No approximation, no randomness. 344
6.2. PROBABILITY INEQUALITIES Proof. Since X is Gaussian, the probability is (cid:90) ∞ 1 (cid:26) x2 (cid:27) P[X ≥ε]= exp − dx (cid:112) 2π(σ2/N) 2(σ2/N) ε (cid:90) ε 1 (cid:26) x2 (cid:27) =1− exp − dx (cid:112) 2π(σ2/N) 2(σ2/N) −∞ =1−(cid:90) √ σε 2/N √1 exp(cid:26) −x2(cid:27) dx 2π 2 −∞ (cid:32) (cid:33) (cid:32)√ (cid:33) ε Nε =1−Φ =1−Φ . (cid:112) σ2/N σ (cid:3) Let us compute the bound given by Chebyshev’s inequality. Lemma 6.2. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for any N ε>0, Chebyshev’s inequality implies that σ2 P[X ≥ε]≤ . (6.24) Nε2 Proof. We apply Chebyshev’s inequality by assuming that µ=0: P[X ≥ε]=P[X−µ≥ε−µ]≤P[|X−µ|≥ε−µ] E[(X−µ)2] σ2 ≤ = . (ε−µ)2 Nε2 (cid:3) We now compute Chernoff’s bound. Theorem 6.14. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for N any ε>0, Chernoff’s bound implies that (cid:26) ε2N(cid:27) P[X ≥ε]≤exp − . (6.25) 2σ2 Proof.TheMGFofazero-meanGaussianrandomvariablewithvarianceσ2/N isM (s)= X (cid:110) (cid:111) exp σ2s2 . Therefore, the function ϕ can be written as 2N (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) X s>0 (cid:26) σ2s2(cid:27) =max sε− . s>0 2N To maximize the function we take the derivative and set it to zero. This yields d (cid:26) σ2s2(cid:27) Nε sε− =0 ⇒ s∗ = . ds 2N σ2 345
CHAPTER 6. SAMPLE STATISTICS Note that this s∗ is a maximizer because sε− σ2s2 is a concave function. 2N Substituting s∗ into ϕ(ε), (cid:26) sε−σ2s2(cid:27) ϕ(ε)=max s>0 2N σ2(s∗)2 (cid:18) Nε(cid:19) σ2 (cid:18) Nε(cid:19)2 ε2N =s∗ε− = ε− = , 2N σ2 2N σ2 2σ2 and hence (cid:26) ε2N(cid:27) P[X ≥ε]≤e−ϕ(ε) =exp − . 2σ2 (cid:3) Figure 6.11 shows the comparison between the exact probability, the bound provided by Chebyshev’s inequality, and Chernoff’s bound: (cid:16)√ (cid:17) • Exact: P[X ≥ε]=1−Φ Nε . σ • Chebyshev: P[X ≥ε]≤ σ2 , Nε2 (cid:110) (cid:111) • Chernoff: P[X ≥ε]≤exp −ε2N . 2σ2 Inthisnumerical experiment,weset ε=0.1,and σ =1.WevarythenumberN.Aswe can see from the figure, the bound provided by Chebyshev is valid but very loose. It does not even capture the tail as N grows. On the other hand, Chernoff’s bound is reasonably tight. However, one should note that the tightness of Chernoff is only valid for large N. When N is small, it is possible to construct random variables such that Chebyshev is tighter. The MATLAB code used to generate this plot is illustrated below. % MATLAB code to compare the probability bounds epsilon = 0.1; sigma = 1; N = logspace(1,3.9,50); p_exact = 1-normcdf(sqrt(N)*epsilon/sigma); p_cheby = sigma^2./(epsilon^2*N); p_chern = exp(-epsilon^2*N/(2*sigma^2)); loglog(N, p_exact, ’-o’, ’Color’, [1 0.5 0], ’LineWidth’, 2); hold on; loglog(N, p_cheby, ’-’, ’Color’, [0.2 0.7 0.1], ’LineWidth’, 2); loglog(N, p_chern, ’-’, ’Color’, [0.2 0.0 0.8], ’LineWidth’, 2); What could go wrong if we insist on using Chebyshev’s inequality? Consider the fol- lowing example. Example 6.9. Let X ∼Gaussian(0,σ2/N). Suppose that we want the probability to be no greater than a confidence level of α: P[X ≥ε]≤α. 346
6.2. PROBABILITY INEQUALITIES 0 10 -5 10 -10 10 -15 10 1 2 3 10 10 10 N ytilibaborP Exact Chebyshev Chernoff Figure 6.11: Comparison between Chernoff’s bound and Chebyshev’s bound. The random variable we use is X ∼ Gaussian(0,σ2/N). As N grows, we show the probability bounds predicted by the two methods. Letα=0.05,ε=0.1,andσ =1.FindtheN using(i)Chebyshev’sinequalityand(ii) Chernoff’s inequality. Solution: (i) Chebyshev’s inequality implies that σ2 P[X ≥ε]≤ ≤α, Nε2 which means that σ2 N ≥ . αε2 If we plug in α=0.05, ε=0.1, and σ =1, then N ≥2000. (ii) For Chernoff’s inequality, it holds that (cid:26) ε2N(cid:27) P[X ≥ε]≤exp − ≤α, 2σ2 which means that 2σ2 N ≥− logα ε2 Plugging in α=0.05, ε=0.1, and σ =1, we have that N ≥600. This is more than 3 timessmallerthantheonepredictedbyChebyshev’sinequality.Whichoneiscorrect? Both are correct but Chebyshev’s inequality is overly conservative. If N ≥ 600 can makeP[X ≥ε]≤α,thencertainlyN ≥2000willworktoo.However,N ≥2000istoo loose. 347
CHAPTER 6. SAMPLE STATISTICS 6.2.8 Hoeffding’s inequality Chernoff’s bound can be used to derive many powerful inequalities. Here we present an inequality for bounded random variables. This result is known as Hoeffding’s inequality. Theorem 6.15 (Hoeffding’s inequality). Let X ,...,X be i.i.d. random variables 1 N with 0≤X ≤1, and E[X ]=µ. Then n n (cid:20) (cid:21) P (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15) ≤2e−2(cid:15)2N, (6.26) where X = 1 (cid:80)N X . N N n=1 n YoumayskiptheproofofHoeffding’sinequalityifthisisyourfirsttime readingthebook. Proof. (Hoeffding’s inequality) First, we show that (cid:34) N (cid:35) (cid:34) N (cid:35) P(cid:2) X −µ>(cid:15)(cid:3) =P 1 (cid:88) X −µ>(cid:15) =P (cid:88) (X −µ)>N(cid:15) N N n n n=1 n=1 =P(cid:104) es(cid:80)N n=1(Xn−µ) ≥es(cid:15)N(cid:105) E[es(cid:80)N n=1(Xn−µ)] (cid:18)E[es(Xn−µ)](cid:19)N ≤ = . es(cid:15)N es(cid:15) Let Z = X −µ. Then −µ ≤ Z ≤ 1−µ. At this point we use Hoeffding Lemma (see n n n below) that E[esZn]≤es 82 because b−a=(1−µ)−(−µ)=1. Thus, P(cid:2) X N −µ>(cid:15)(cid:3) ≤(cid:18)E[e es sZ (cid:15)n](cid:19)N ≤(cid:32) e es s82 (cid:15)(cid:33)N =es2 8N−s(cid:15)N, ∀s. This result holds for all s, and thus it holds for the s that minimizes the right-hand side. This implies that P(cid:2) X −µ>(cid:15)(cid:3) ≤min(cid:26) exp(cid:26) s2N −s(cid:15)N(cid:27)(cid:27) . N s 8 (cid:110) (cid:111) Minimizing the exponent gives d s2N −s(cid:15)N = sN −(cid:15)N = 0. Thus we have s = 4(cid:15). ds 8 4 Hence, P(cid:2) X −µ>(cid:15)(cid:3) ≤exp(cid:26) (4(cid:15))2N −(4(cid:15))(cid:15)N(cid:27) =e−2(cid:15)2N. N 8 By symmetry, P(cid:2) X −µ<−(cid:15)(cid:3) ≤e−2(cid:15)2N. Then by union bound we show that N P(cid:2) |X −µ|>(cid:15)(cid:3) =P(cid:2) X −µ>(cid:15)(cid:3) +P(cid:2) X −µ<−(cid:15)(cid:3) N N N ≤e−2(cid:15)2N +e−2(cid:15)2N =2e−2(cid:15)2N. (cid:3) 348
6.2. PROBABILITY INEQUALITIES Lemma 6.3 (Hoeffding’s lemma). Let a ≤ X ≤ b be a random variable with E[X]=0. Then M (s)d =efE(cid:2) esX(cid:3) ≤exp(cid:26) s2(b−a)2(cid:27) . (6.27) X 8 Proof. Since a≤X ≤b, we can write X as a linear combination of a and b: X =λb+(1−λ)a, whereλ= X−a.Sinceexp(·)isaconvexfunction,itfollowsthateλb+(1−λ)a ≤λeb+(1−λ)ea. b−a (Recall that h is convex if h(λx+(1−λ)y)≤λh(x)+(1−λ)h(y).) Therefore, we have esX ≤λesb+(1−λ)esa X−a b−X = esb+ esa. b−a b−a Taking expectations on both sides of the equation, −a b E[esX]≤ esb+ esa, b−a b−a because E[X]=0. Now, if we let θ =− a , then b−a −a b esb+ esa =θesb+(1−θ)esa b−a b−a (cid:16) (cid:17) (cid:16) (cid:17) =esa 1−θ+θes(b−a) = 1−θ+θes(b−a) e−sθ(b−a) =(1−θ+θeu)e−θu =e−θu+log(1−θ+θeu), where we let u=s(b−a). This can be simplified as E[esX]≤E[eφ(u)] by defining φ(u)=−θu+log(1−θ+θeu). The final step is to approximate φ(u). To this end, we use Taylor approximation: u2 φ(u)=φ(0)+uφ(cid:48)(0)+ φ(cid:48)(cid:48)(ξ), 2 for some ξ ∈[a,b]. Since φ(0)=0, φ(cid:48)(0)=0, and φ(cid:48)(cid:48)(u)≤ 1 for all u, it follows that 4 u2 u2 s2(b−a)2 φ(u)= φ(cid:48)(cid:48)(ξ)≤ = . (cid:3) 2 8 8 End of the proof. 349
CHAPTER 6. SAMPLE STATISTICS What is so special about the Hoeffding’s inequality? • Since Hoeffding’s inequality is derived from Chernoff’s bound, it inherits the tightness. Hoeffding’s inequality is much stronger than Chebyshev’s inequality in bounding the tail distributions. • Hoeffding’sinequalityisoneofthefewinequalitiesthatdonotrequireE[X]and Var[X] on the right-hand side. • A downside of the inequality is that boundedness is not always easy to satisfy. For example, if X is a Gaussian random variable, Hoeffding does not apply. n There are more advanced inequalities for situations like these. Interpreting Hoeffding’s inequality. One way to interpret Hoeffding’s inequality is to write the equation as P(cid:2)(cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:3) ≤2e−2(cid:15)2N, (cid:124) (cid:123)(cid:122) (cid:125) δ which is equivalent to P(cid:2)(cid:12) (cid:12)X N −µ(cid:12) (cid:12)≤(cid:15)(cid:3) ≥1−δ. This means that with a probability at least 1−δ, we have X −(cid:15)≤µ≤X +(cid:15). N N If we let δ =2e−2(cid:15)2N, this becomes (cid:114) (cid:114) 1 2 1 2 X − log ≤µ≤X + log . (6.28) N 2N δ N 2N δ This inequality is a confidence interval (see Chapter 9). It says that with probability at least 1−δ, the interval [X −(cid:15), X +(cid:15)] includes the true population mean µ. N N There are two questions one can ask about the confidence interval: • Given N and δ, what is the confidence interval? Equation (6.28) tells us that if we know N, to achieve a probability of at least 1−δ the confidence interval will follow (cid:113) Equation (6.28). For example, if N = 10,000 and δ = 0.01, 1 log2 = 0.016. 2N δ Therefore,withaprobabilityatleast99%,thetruepopulationmeanµwillbeincluded in the interval X −0.16≤µ≤X +0.16. N N • If we want to achieve a certain confidence interval, what is the N we need? If we are given (cid:15) and δ, the N we need is log2 δ ≤2e−2(cid:15)2N ⇒ N ≥ δ. 2(cid:15)2 For example, if δ =0.01 and (cid:15)=0.01, the N we need is N ≥26,500. WhenisHoeffding’sinequalityused?Hoeffding’sinequalityisfundamentalinmodern machine learning theory. In this field, one often wants to quantify how well a learning 350
6.3. LAW OF LARGE NUMBERS algorithmperforms withrespectto thecomplexityof the modeland thenumber oftraining samples.Forexample,ifwechooseacomplexmodel,weshouldexpecttousemoretraining samples or overfit otherwise. Hoeffding’s inequality provides an asymptotic description of the training error, testing error, and the number of training samples. The inequality is oftenusedtocomparethetheoreticalperformancelimitofonemodelversusanothermodel. Therefore, although we do not need to use Hoeffding’s inequality in this book, we hope you appreciate its tightness. Closing Remark. We close this section by providing the historic context of Chernoff’s inequality. Herman Chernoff, the discoverer of Chernoff’s inequality, wrote the following many years after the publication of the original paper in 1952. “In working on an artificial example, I discovered that I was using the Central Limit Theorem for large deviations where it did not apply. This led me to derive the asymptotic upper and lower bounds that were needed for the tail probabilities. [Herman] Rubin claimed he could get these bounds with much less work, and I challenged him. He produced a rather simple argument, using Markov’s inequality, for the upper bound. Since that seemed to be a minor lemma in the ensuing paper I published (Chernoff, 1952), I neglected to give him credit. I now consider it a serious error in judgment, especially because his result is stronger for the upper bound than the asymptotic result I had derived.” — Herman Chernoff, “A career in statistics,” in Lin et al., Past, Present, and Future of Statistical Science (2014), p. 35. 6.3 Law of Large Numbers In this section, we present our first main result: the law of large numbers. We will discuss two versions of the law: the weak law and the strong law. We will also introduce two forms of convergence: convergence in probability and almost sure convergence. 6.3.1 Sample average The law of large numbers is a probabilistic statement about the sample average. Suppose thatwehaveacollectionofi.i.d.randomvariablesX ,...,X .Thesampleaverageofthese 1 N N random variables is defined as follows: Definition 6.5. The sample average of a sequence of random variables X ,...,X 1 N is N 1 (cid:88) X = X . (6.29) N N n n=1 If the random variables X ,...,X are i.i.d. so that they have the same population 1 N mean E[X ]=µ (for n=1,...,N), then by the linearity of the expectation, n N E(cid:2) X (cid:3) = 1 (cid:88) E[X ]=µ. N N n n=1 351
CHAPTER 6. SAMPLE STATISTICS Therefore, the mean of X is the population mean µ. N The sample average, X , plays an important role in statistics. For example, by sur- N veying 10,000 Americans, we can find a sample average of their ages. Since we never have accesstothetruepopulationmean,thesampleaverageisanestimate,andsinceX isonly N an estimate, we need to ask how good the estimate is. One reason we ask this question is that X is a finite-sample “approximation” of µ. N More importantly, the root of the problem is that X itself is a random variable because N X ,...,X are all random variables. Since X is a random variable, there is a PDF of 1 N N X ; there is a CDF of X ; there is E[X ]; and there is Var[X ]. Since X is a random N N N N N variable, it has uncertainty. To say that we are confident about X , we need to ensure that N the uncertainty is within some tolerable range. How do we control the uncertainty? We can compute the variance. If X ,...,X are 1 N i.i.d. random variables with the same variance Var[X ]=σ2 (for n=1,...,N), then n Var(cid:2) X (cid:3) = 1 (cid:88)N Var[X ]= 1 (cid:88)N σ2 = σ2 . N N2 n N2 N n=1 n=1 Therefore, the variance will shrink to 0 as N grows. In other words, the more samples we use to construct the sample average, the less deviation the random variable X will have. N Visualizing the sample average To help you visualize the randomness of X , we consider an experiment of drawing N N Bernoulli random variables X ,...,X with parameter p = 1/2. Since X is Bernoulli, it 1 N n follows that E[X ]=p and Var[X ]=p(1−p). n n WeconstructasampleaverageX = 1 (cid:80)N X .SinceX isaBernoullirandomvariable, N N n=1 n n we know everything about X . First, X is a binomial random variable, since X is the N N N sum of Bernoulli random variables. Second, the mean and variance of X are respectively N N µ d =efE[X ]= 1 (cid:88) E[X ]=p, XN N N n n=1 N σ2 d =ef Var[X ]= 1 (cid:88) Var[X ]= p(1−p) . XN N N2 n N n=1 In Figure 6.12, we plot the random variables X (the black crosses) for every N. You N canseethatateachN,e.g.,N =100,therearemanypossibleobservationsforX because N X itself is a random variable. As N increases, we see that the deviation of the random N variablesbecomessmaller.Inthesameplot,weshowtheboundsµ±3σ ,whicharethree XN standarddeviationsfromthemean.Wecanseeclearlythattheboundsprovideaverygood envelope coveringthe random variables.As N goestoinfinity,wecansee thatthestandard deviation goes to zero, and so X approaches the true mean. N For your reference, the MATLAB code and the Python code we used to generate the plot are shown below. % MATLAB code to illustrate the weak law of large numbers Nset = round(logspace(2,5,100)); 352
6.3. LAW OF LARGE NUMBERS 0.7 0.6 0.5 0.4 0.3 102 103 104 105 N egareva elpmas Figure6.12:Theweaklawoflargenumbers.Inthisplot,weassumethatX ,...,X arei.i.d.Bernoulli 1 N random variables with a parameter p. The black crosses in the plot are the sample averages X = 1 (cid:80)N X . The red curves are the ideal bounds µ ± 3σ , where µ = p and N N (cid:112) n=1 n XN XN XN σ = p(1−p)/N. As N grows, we observe that the variance shrinks to zero. Therefore, the XN sample average is converging to the true population mean. for i=1:length(Nset) N = Nset(i); p = 0.5; x(:,i) = binornd(N, p, 1000,1)/N; end y = x(1:10:end,:)’; semilogx(Nset, y, ’kx’); hold on; semilogx(Nset, p+3*sqrt(p*(1-p)./Nset), ’r’, ’LineWidth’, 4); semilogx(Nset, p-3*sqrt(p*(1-p)./Nset), ’r’, ’LineWidth’, 4); # Python code to illustrate the weak law of large numbers import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats import numpy.matlib p = 0.5 Nset = np.round(np.logspace(2,5,100)).astype(int) x = np.zeros((1000,Nset.size)) for i in range(Nset.size): N = Nset[i] x[:,i] = stats.binom.rvs(N, p, size=1000)/N Nset_grid = np.matlib.repmat(Nset, 1000, 1) plt.semilogx(Nset_grid, x,’ko’); plt.semilogx(Nset, p + 3*np.sqrt((p*(1-p))/Nset), ’r’, linewidth=6) plt.semilogx(Nset, p - 3*np.sqrt((p*(1-p))/Nset), ’r’, linewidth=6) 353
CHAPTER 6. SAMPLE STATISTICS Note the outliers for each N in Figure 6.12. For example, at N = 102 we see a point locatednear0.7onthey-axis.Thispointisoutsidethreestandarddeviations.Isitnormal? Yes. Being outside three standard deviations only says that the probability of having this outlierissmall.Itdoesnotsaythattheoutlierisimpossible.Havingasmallprobabilitydoes notexcludethepossibility.Bycontrast,ifyousaythatsomethingwillsurelynothappenyou mean that there is not even a small probability. The former is a weaker statement than the latter. Therefore, even though we establish a three standard deviation envelope, there are pointsfallingoutsidetheenvelope.AsN grows,thechanceofhavingabadoutlierbecomes smaller. Therefore, the greater the N, the smaller the chance we will get an outlier. If the random variables X are i.i.d., the above phenomenon is universal. Below is an n example of the Poisson case. Practice Exercise 6.8. Let X ∼ Poisson(λ). Define the sample average as X = n N 1 (cid:80)N X . Find the mean and variance of X . N n=1 n N Solution. Since X is Poisson, we know that E[X ]=λ and Var[X ]=λ. So n n n N N 1 (cid:88) 1 (cid:88) E[X ]= E[X ]= λ=λ, N N n N n=1 n=1 N N 1 (cid:88) 1 (cid:88) λ Var[X ]= Var[X ]= λ= . N N2 n N2 N n=1 n=1 Therefore, as N →∞, the variance Var[X ]→0. N 6.3.2 Weak law of large numbers (WLLN) The analysis of Figure 6.12 shows us something important, namely that the convergence in a probabilistic way is different from that in a deterministic way. We now describe one fundamental result related to probabilistic convergence, known as the weak law of large numbers. Theorem 6.16 (Weak law of large numbers). Let X ,...,X be a set of i.i.d. ran- 1 N domvariableswithmeanµandvarianceσ2.AssumeE[X2]<∞.LetX = 1 (cid:80)N X . N N n=1 n Then for any ε>0, (cid:20) (cid:21) lim P |X −µ|>ε =0. (6.30) N N→∞ Proof. By Chebyshev’s inequality, P(cid:2) |X −µ|>ε(cid:3) ≤ Var[X N] = Var[X n] . N ε2 Nε2 Therefore, setting N →∞ we have lim P(cid:2) |X −µ|>ε(cid:3) = lim Var[X n] =0. N→∞ N N→∞ Nε2 (cid:3) 354
6.3. LAW OF LARGE NUMBERS Example 6.10. Consider a set of i.i.d. random variables X ,...,X where 1 N X ∼Gaussian(µ,σ2). n Verify that the sample average X = 1 (cid:80)N X follows the weak law of large num- N N n=1 n bers. Solution: Since X is a Gaussian, the sample average X is also a Gaussian: n N (cid:18) σ2(cid:19) X ∼Gaussian µ, . N N Consider the probability P(cid:2) |X −µ|>ε(cid:3) for each N: N (cid:20) (cid:21) δ d =efP |X −µ|>ε N N (cid:20) (cid:21) (cid:20) (cid:21) =P X −µ>ε +P X −µ<−ε N N (cid:32) √ (cid:33) (cid:32) √ (cid:33) ε N ε N =1−Φ +Φ − σ σ (cid:32) √ (cid:33) ε N =2Φ − . σ If we set σ =1 and ε=0.1, then (cid:18) (cid:19) (cid:32) √ (cid:33) 0.1·1 0.1· 5 δ =2Φ − =0.9203, δ =2Φ − =0.8231, 1 1 5 1 (cid:32) √ (cid:33) (cid:32) √ (cid:33) 0.1· 10 0.1· 100 δ =2Φ − =0.7518, δ =2Φ − =0.3173, 10 1 100 1 (cid:32) √ (cid:33) 0.1· 1000 δ =2Φ − =0.0016. 1000 1 As you can see, the the sequence δ ,δ ,...,δ ,... rapidly converges to 0 as N grows. 1 2 N In fact, since Φ(z) is a increasing function for z <0 with Φ(−∞)=0, it follows that (cid:20) (cid:21) (cid:32) √ (cid:33) ε N lim P |X −µ|>ε = lim 2Φ − =0. N→∞ N N→∞ σ The weak law of large numbers is portrayed graphically in Figure 6.13. In this figure we draw several PDFs of the sample average X . The shapes of the PDFs are getting N narrower as the variance of the random variable shrinks. Since the PDFs become narrower, the probability P[|X −µ| > ε] becomes more unlikely. At the limit when N → ∞, the N probabilityvanishes.Theweaklawoflargenumbersassertsthatthishappensforanysetof i.i.d.randomvariables.Itsaysthatthesequenceofprobabilityvaluesδ d =efP[|X −µ|>ε] N N 355
CHAPTER 6. SAMPLE STATISTICS will converge to zero. Figure 6.13: The weak law of large numbers states that as N increases, the variance of the sample average X shrinks. As a result, the probability P[|X −µ| > ε] decreases and eventually vanishes. N N Note that the convergence here is that of the sequence of probabilities P[|X −µ|>ε], which is just N a sequence of numbers. What is the weak law of large numbers? Let X be the sample average of i.i.d. random variables X ,...,X . N 1 N (cid:20) (cid:21) lim P |X −µ|>ε =0. (6.31) N N→∞ • For details, see Theorem 6.16. • The WLLN concerns the sequence of probability values δ =P[|X −µ|>ε]. N N • The probabilities converge to zero as N grows. • It is weak because having a small probability does not exclude the possibility of happening. 6.3.3 Convergence in probability The example above tells us that in order to show convergence, we need to first compute the probability δ of each event and then take the limit of the sequence, e.g., the one shown in n the table below: δ δ δ δ δ δ 1 5 10 100 1000 10000 0.9203 0.8231 0.7518 0.3173 0.0016 1.5240×10−23 Therefore, the convergence is the convergence of the probability. Since {δ ,δ ,...} is a 1 2 sequenceofrealnumbers(between0and1),anyconvergenceresultsforrealnumbersapply here. Note that the convergence controls only the probabilities. Probability means chance. Therefore, having the limit converging to zero only means that the chance of happening is becoming smaller and smaller. However, at any N, there is still a chance that some bad event can happen. 356
6.3. LAW OF LARGE NUMBERS Whatdowemeanbyabadevent?AssumethatX arefaircoins.Thesampleaverage n X = (1/N)(cid:80)N X is more or less equal to 1/2 as N grows. However, even if N is a N n=1 n largenumber,sayN =1000,wearestillnotcertainthatthesampleaverageisexactly1/2. It is possible, though very unlikely, that we obtain 1000 heads or 1000 tails (so that the sample average is “1” or “0”). The bottom line is: Having a probability converging to zero only means that for any tolerance level we can always find an N large enough so that the probability is smaller than that tolerance. The type of convergence described by the weak law of large numbers is known as the convergence in probability. Definition 6.6. A sequence of random variables A ,...,A converges in probability 1 N to a deterministic number α if for every ε>0, lim P[|A −α|>ε]=0. (6.32) N N→∞ p We write A →α to denote convergence in probability. N The following two examples illustrate how to prove convergence in probability. Example 6.11. Let X ,...,X be i.i.d. random variables with X ∼ Uniform(0,1). 1 N n Define A =min(X ,...,X ). Show that A converges in probability to zero. N 1 N N Solution. (Without determining the PDF of A , we notice that as N increases, the N value of A will likely decrease. Therefore, we should expect A to converge to zero.) N N Pick an ε>0. It follows that P[|A −0|≥ε]=P[min(X ,...,X )≥ε], because X ≥0 N 1 N n =P[X ≥εand ··· andX ≥ε] 1 N =P(cid:0) X ≥ε(cid:1) ···P(cid:0) X ≥ε(cid:1) =(1−ε)N. 1 N Setting the limit of N →∞, we conclude that lim P[|A −0|≥ε]= lim (1−ε)N =0. N N→∞ N→∞ Therefore, A converges to zero in probability. N Practice Exercise 6.9. Let X ∼ Exponential(1). By evaluating the CDF, we know thatP[X ≥x]=e−x.LetA =X/N.ProvethatA convergestozeroinprobability. N N Solution. For any ε>0, P[|A −0|≥ε]=P[A ≥ε] N N =P[X ≥Nε] =e−Nε. 357
CHAPTER 6. SAMPLE STATISTICS Putting N →∞ on both sides of the equation gives us lim P[|A −0|≥ε]= lim e−Nε =0. N N→∞ N→∞ Thus, A converges to zero in probability. N Example 6.12.ConstructanexamplesuchthatA convergesinprobabilitytosome- N thing, but E[A ] does not converge to the same thing. N Solution. Consider a sequence of random variables A such that N  1− 1, α=0,  N P[A =α]= 1, α=N2, N N 0, otherwise. The PDF of the random variable A is shown in Figure 6.14. N Figure 6.14: Probability density function of the random variable A . N We first show that A converges in probability to zero. Let ε > 0 be a fixed N constant. Since ε>0, 1 P[A ≥ε]= N N √ for any N > ε. Therefore, we have that lim P[|A −0|≥ε]= lim P[A ≥ε] N N N→∞ N→∞ 1 = lim =0. N→∞N Hence, A converges to 0 in probability. N However, E[A ] does not converge to zero, because N (cid:18) (cid:19) 1 1 E[A ]=0· 1− +N2· N N N =N. So E[A ] goes to infinity as N grows. N 358
6.3. LAW OF LARGE NUMBERS 6.3.4 Can we prove WLLN using Chernoff’s bound? The following discussion of using Chernoff’s bound to prove WLLN can be skipped if this is your first time reading the book. InprovingWLLNweuseChebyshev’sinequality.CanweuseChernoff’sinequality(or Hoeffding’s)toprovetheresult?Yes,wecanusethem.However,noticethatthetaskhereis to prove convergence, not to find the best convergence. Finding the best convergence means findingthefastestdecayrateoftheprobabilitysequence.Chernoff’sbound(andHoeffding’s inequality)offersabetterdecayrate.However,Chernoff’sboundneedstobecustomizedfor individual random variables. For example, Chernoff’s bound for Gaussian is different from Chernoff’s bound for exponential. This result makes Chebyshev the most convenient bound because it only requires the variance to be bounded. WhatifweinsistonusingChernoff’sboundinprovingtheWLLN?Wecandothatfor specific random variables. Let’s consider two examples. The first example is the Gaussian randomvariablewhereX ∼N(0,σ2).WeknowthatX ∼N(0,σ2/N).Chernoff’sbound n N shows that P(cid:2) |X −µ|>ε(cid:3) ≤2exp(cid:26) −ε2N(cid:27) , N 2σ2 Taking the limit on both sides, we have lim P(cid:2) |X −µ|>ε(cid:3) = lim 2exp(cid:26) −ε2N(cid:27) =0. N→∞ N N→∞ 2σ2 Note that the rate of convergence here is exponential. The rate of convergence offered by Chebyshev is only linear. Of course, you may argue that since X is Gaussian we have n closed-form expressions about the probability, so we do not need Chernoff’s bound. This is alegitimatepoint,andsohereisanexamplewherewedonothaveaclosed-formexpression for the probability. Consider a sequence of arbitrary i.i.d. random variables X ,...,X with 0≤X ≤1. 1 N n Then Hoeffding’s inequality tells us that P(cid:2) |X −µ|>ε(cid:3) ≤2exp(cid:8) −2ε2N(cid:9) . N Taking the limit on both sides, we have lim P(cid:2) |X −µ|>ε(cid:3) = lim 2exp(cid:8) −2ε2N(cid:9) =0. N N→∞ N→∞ Again, we obtain a WLLN result, this time for i.i.d. random variables X ,...,X with 1 N 0≤X ≤1. n As you can see from these two examples, WLLN can be proved in multiple ways depending on how general the random variables need to be. End of the discussions. 359
CHAPTER 6. SAMPLE STATISTICS 6.3.5 Does the weak law of large numbers always hold? The following discussion of the failure of the weak law of large numbers can be skipped if this is your first time reading the book. The weak law of large numbers does not always hold. Recall that when we prove the weak law of large numbers using Chebyshev’s inequality, we implicitly require that the variance Var[X ] is finite. (Look at the condition that E[X2]<∞.) Thus for distributions N whose variance is unbounded, Chebyshev’s inequality does not hold. One example is the Cauchy distribution. The PDF of a Cauchy distribution is γ f (x)= , X π(γ2+x2) where γ is a parameter. Letting γ =1, (cid:90) ∞ x2 1 (cid:90) ∞ 1 E[X2]= dx= 1− dx π(1+x2) π 1+x2 −∞ −∞ = 1 (cid:90) ∞ dx− 1 (cid:90) ∞ 1 dx= 1(cid:20) x−tan−1(x)(cid:21)(cid:12) (cid:12) (cid:12)∞ =∞. π π 1+x2 π (cid:12) −∞ −∞ x=−∞ Since the second moment is unbounded, the variance of X will also be unbounded. A perceptive reader may observe that even if E[X2] is unbounded, it does not mean that the tail probability is unbounded. This is correct. However, for Cauchy distributions, we can show that the sample average X does not converge to the mean when N → ∞ N (and so the WLLN fails). To see this, we note that 1 ↔e−|ω|. π(1+x2) So for the sample average X = 1 (cid:80)N X , the characteristic function is N N n=1 n N E[e−jωXN]=E[e−j Nω(cid:80)N n=1Xn]= (cid:89) E[e−j NωXn]=(cid:104) e−| Nω|(cid:105)N =e−|ω|, n=1 which remains a Cauchy distribution with γ =1. Therefore, we have that (cid:90) ε 1 P[|X |≤ε]= dx N π(1+x2) −∞ (cid:90) 0 1 (cid:90) ε 1 1 1 = dx+ dx= + tan−1(ε). π(1+x2) π(1+x2) 2 π −∞ 0 Thus no matter how many samples we have, P[|X | ≤ ε] will never converge to 1 (so N P[|X |>ε] will never converge to 0). Therefore, WLLN does not hold. N End of the discussion. 360
6.3. LAW OF LARGE NUMBERS 6.3.6 Strong law of large numbers Since there is a “weak” law of large numbers, you will not be surprised to learn that there is a strong law of large numbers. The strong law is more restrictive than the weak law. Any sequence satisfying the strong law will satisfy the weak law, but not vice versa. Since the strong law is “stronger”, the proof is more involved. Theorem 6.17 (Strong law of large numbers). Let X ,...,X be a sequence of 1 N i.i.d. random variables with common mean µ and variance σ2. Assume E[X4] < ∞. Let X = 1 (cid:80)N X be the sample average. Then N N n=1 n (cid:104) (cid:105) P lim X =µ =1. (6.33) N N→∞ The strong law flips the order of limit and probability. Asyou can see,the difference betweenthestronglawandtheweaklawistheorderofthelimitandtheprobability.Inthe weak law, the limit is outside the probability, whereas, in the strong law, the limit is inside the probability. This switch in order makes the interpretation of the result fundamentally different.Inthefinalanalysis,theweaklawconcernsthelimitofasequenceofprobabilities (which are just real numbers between 0 and 1). However, the strong law concerns the limit ofasequenceofrandomvariables.Thestronglawanswersthequestion,whatisthelimiting object of the sample average as N grows? The strong law concerns the limiting object, not a sequence of numbers. What is the “limiting object”? If we denote X as the sample average using N samples, then N we know that X is a random variable, X is a random variable, and all X ’s are random 1 2 n variables.Sowehaveasequenceofrandomvariables.AsN goestoinfinity,wecanaskabout the limiting object lim X . However, even without any deep analysis, you should be N→∞ N able to see that lim X is another random variable. The strong law says that this N→∞ N limiting object will “successfully” become a deterministic number µ, after a finite number of “failures”. The strong law asserts that there are a finite number of failures. Let us explain “success” and “failure”. X is a random variable, so it fluctuates. However, as N goes to N infinity, the strong law says that the number of times where X (cid:54)= µ will be zero. That N is, there is a finite number of times where X (cid:54)= µ (i.e., fail), and afterward, you will be N perfectly fine (i.e., success). Yes, perfectly fine means 100%. The weak law only guarantees 99.99%. A good example for differentiating the weak law and the strong law is an electronic dictionary that improves itself every time you use it. The weak law says that if you use the dictionary for a long period, the probability of making an error will become small. You will still get an error once in a while, but the probability is very small. This is a 99.99% guarantee, and it is the weak law. The strong law says that the number of failures is finite. After you have gone through this finite number of failures, you will be completely free of error. This is a 100% guarantee by the strong law. When will you hit this magical number? Thestronglawdoesnotsaywhen;itonlyassertstheexistenceofthisnumber.However,this existence is already good enough in many ways. It gives a certificate of assurance, whereas the weak law still has uncertainty. 361
CHAPTER 6. SAMPLE STATISTICS Strong law (cid:54)= deterministic. If the strong law offers a 100% guarantee, does it mean that it is a deterministic guarantee? No, the strong law is still a probabilistic statement because we are still using P[·] to measure an event. The event can include measure-zero subsets,andthemeasure-zerosubsetscanbehuge.Forexample,thesetofrationalnumbers on the real line is a measure-zero set when measuring the probability using an integration. The strong law does not handle those measure-zero subsets. 6.3.7 Almost sure convergence The discussion below can be skipped if this is your first time reading the book. The type of convergence used by the strong law of large numbers is the almost sure convergence. It is defined formally as follows. Definition 6.7. A sequence of random variables A ,...,A converges almost surely 1 N to α if (cid:104) (cid:105) P lim A =α =1. (6.34) N N→∞ a.s. We write A → α to denote almost sure convergence. N Toprovealmostsureconvergence,oneneedstoshowthatthesequenceA willdemonstrate N A (cid:54)=α for a finite number of times. Afterward, A needs to demonstrate A =α. N N N Example 6.13.a Construct a sequence of events that converges almost surely. Solution. Let X ,...,X be i.i.d. random variables such that X ∼ Uniform(0,1). 1 N n Define A = min(X ,...,X ). Since A is nonincreasing and is bounded below by N 1 N N zero, it must have a limit. Let us call this limit def A = lim A . N N→∞ Then we can show that P[A≥(cid:15)]=P[min(X ,X ,...)≥(cid:15)] 1 2 (a) ≤ P[min(X ,X ,...,X )≥(cid:15)] 1 2 N ( =b)P[X ≥(cid:15)andX ≥(cid:15)and ··· andX ≥(cid:15)] 1 2 N =(1−(cid:15))N, where (a) holds because there are more elements in (X ,X ,...) than in 1 2 (X ,X ,...,X ). Therefore, the minimum value of the former is less than the mini- 1 2 N mum value of the latter. (b) holds because if min(X ,X ,...,X ) ≥ (cid:15), then X ≥ (cid:15) 1 2 N n for all n. 362
6.3. LAW OF LARGE NUMBERS Since P[A≥(cid:15)]≤(1−(cid:15))N for any N, the statement still holds as N →∞. Thus, P[A≥(cid:15)]≤ lim (1−(cid:15))N =0. N→∞ This shows P[A≥(cid:15)]=0 for any positive (cid:15). So P[A>(cid:15)]=0, and hence P[A=0]=1. Since A is the limit of A , we conclude that N (cid:104) (cid:105) P lim A =0 =P[A=0]=1. N N→∞ So A converges to 0 almost surely. N aThisexampleismodifiedfromBertsekasandTsitsiklis,IntroductiontoProbability,Chapter5.5. Example 6.14.a Constructanexamplewhereasequenceofeventsconvergesinprob- ability but does not converge almost surely. Solution.Consideradiscretetimearrivalprocess.Thesetoftimesispartitionedinto consecutive intervals of the form I ={2,3}, 1 I ={4,5,6,7}, 2 I ={8,9,10,...,15}, 3 . . . I ={2k,2k+1,...,2k+1−1}. k Therefore, the length of each interval is |I |=2, |I |=4, ..., |I |=2k. 1 2 k During each interval, there is exactly one arrival. Define Y as a binary random n variable such that for every n∈I , k (cid:40) 1, with probability 1 , Y = |Ik| n 0, with probability 1− 1 . |Ik| For example, if n∈{2,3}, then P[Y =1]= 1. If n∈{4,5,6,7}, then P[Y =1]= 1. n 2 n 4 In general, we have that 1 1 lim P[Y =1]= lim = lim =0, n→∞ n n→∞|I k| n→∞2k and hence 1 lim P[Y =0]= lim 1− =1. n→∞ n n→∞ 2k Therefore, Y converges to 0 in probability. n However, when we carry out the experiment, there is exactly one arrival per interval according to the problem conditions. Since we have an infinite number of 363
CHAPTER 6. SAMPLE STATISTICS intervals I ,I ,..., we will have an infinite number of arrivals in total. As a result, 1 2 Y = 1 for infinitely many times. We do not know which Y will equal 1 and which n n Y will equal to 0. However, we know that there are infinitely many Y that are equal n n to 1. Therefore, in the sequence Y ,Y ,...,Y ,..., we must have that the tail of the 1 2 n sequence is 1. (If Y stops being 1 after some n, then we will not have an infinite n number of arrivals in total.) Since Y =1 when n is large enough, it follows that n (cid:104) (cid:105) P lim Y =1 =1. n n→∞ Equivalently, we can say that the sequence Y will never take the value 0 when n is n large enough. Thus, (cid:104) (cid:105) P lim Y =0 =0. n n→∞ Therefore, Y does not converge to 0 almost surely. n aThisexampleismodifiedfromBertsekasandTsitsiklis,IntroductiontoProbability,Chapter5.5. End of the discussions. 6.3.8 Proof of the strong law of large numbers Thestronglawoflargenumberscanbeprovedinseveralways.Wepresentaproofbasedon Bertsekas and Tsitsiklis, Introduction to Probability, Problems 5.16 and 5.17, which require a finite fourth moment E[X4] < ∞. An alternative proof that requires only E[X ] < ∞ is n n from Billingsley, Probability and Measure, Theorem 22.1. The proof of the strong law of large numbers is beyond the scope of this book. This section is optional. Lemma 6.4. Consider non-negative random variables X ,...,X . Assume that 1 N (cid:34) ∞ (cid:35) (cid:88) E X <∞. (6.35) n n=1 a.s. Then X → 0. n Proof. Let S = (cid:80)N X . Note that S is a random variable, and our assumption is that n=1 n E[S]<∞.Thus,wearguethatS <∞withprobability1.Ifnot,thenS willhaveapositive probability of being ∞. But if this happens, we will have E[S]=∞ because (by the law of total expectation): E[S]=E[S |S =infinite]P[S =infinite]+E[S |S =finite]P[S =finite]. (cid:124) (cid:123)(cid:122) (cid:125) =∞ 364
6.3. LAW OF LARGE NUMBERS Now,sinceSisfinite,thesequence{X ,...,X ,...}mustconvergetozero.Otherwise, 1 N if X is converging to some constants c>0, then summing the tail of the sequence (which n contains infinitely many terms) gives infinity: S =X +···+X +···+. 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =finite =infinite Since the probability of S being finite is 1, it follows that {X ,...,X } is converging 1 N to zero with probability 1. (cid:3) Theorem 6.18 (Strong law of large numbers). Let X ,...,X be a sequence of 1 N i.i.d. random variables with common mean µ and variance σ2. Assume E[X4] < ∞. n Let X = 1 (cid:80)N X be the sample average. Then N N n=1 n (cid:104) (cid:105) P lim X =µ =1. (6.36) N N→∞ Proof. We first prove the case where E[X ]=0. To establish that X →0 with probabil- n N ity 1, we use the lemma to show that (cid:34) ∞ (cid:35) (cid:88) E |X | <∞. N N=1 But to show E[(cid:80)∞ |X |] < ∞, we note that |x| ≤ 1+x4. Therefore, E[(cid:80)∞ |X |] ≤ N=1 N N=1 N 1+E[(cid:80)∞ X4 ], and hence we just need to show that N=1 N (cid:34) ∞ (cid:35) E (cid:88) X4 <∞. N N=1 Let us expand the term E[X4 ] as follows: N N N N N E[X4 ]= 1 (cid:88) (cid:88) (cid:88) (cid:88) E[X X X X ]. N N4 n1 n2 n3 n4 n1=1n2=1n3=1n4=1 There are five possibilities for E[X X X X ]: n1 n2 n3 n4 • All indices are different. Then E[X X X X ]=E[X ]E[X ]E[X ]E[X ]=0·0·0·0=0. n1 n2 n3 n4 n1 n2 n3 n4 • One index is different from other three indices. For example, if n is different from 1 n ,n ,n , then 2 3 4 E[X X X X ]=E[X ]E[X X X ]=0·E[X X X ]=0. n1 n2 n3 n4 n1 n2 n3 n4 n2 n3 n4 • Two indices are identical. For example, if n =n , and n =n , then 1 3 2 4 E[X X X X ]=E[X X ]E[X X ]=E[X2 X2 ]. n1 n2 n3 n4 n1 n3 n2 n4 n1 n2 There are altogether 3N(N −1) of these cases: N(N −1) comes from choosing N followedbychoosingN−1,and3accountsforn =n (cid:54)=n =n ,n =n (cid:54)=n =n , 1 2 3 4 1 3 2 4 and n =n (cid:54)=n =n . 1 4 2 3 365
CHAPTER 6. SAMPLE STATISTICS • Two indices are identical, and two indices are different. For example, if n = n but 1 3 n and n are different. Then 2 4 E[X X X X ]=E[X X ]E[X ]E[X ] n1 n2 n3 n4 n1 n3 n2 n4 =E[X2 ]·0·0=0. n1 • All indices are identical. If n =n =n =n , then 1 2 3 4 E[X X X X ]=E[X4 ]. n1 n2 n3 n4 n1 There are altogether N cases of this. Therefore, it follows that E[X4 ]= NE[X 14]+3N(N −1)E[X 12X 22] . N N4 Since xy ≤(x2+y2)/2, it follows that E[X2X2]≤E[(X2)2+(X2)2]/2 1 2 1 2 =E[X4+X4]/2 1 2 =E[X4]. 1 Substituting into the previous result, E[X4 ]≤ NE[X 14]+3N(N −1)E[X 14] N N4 3N2 ≤ E[X4] N4 1 3 = E[X4]. N2 1 Now, let us complete the proof. (cid:34) ∞ (cid:35) (cid:34) ∞ (cid:35) E (cid:88) X4 ≤E (cid:88) 3 E[X4] <∞, N N2 1 N=1 N=1 because (cid:80)∞ (1/N2) is the Bassel problem with a solution that (cid:80)∞ (1/N2) = π2/6. N=1 N=1 Consequently,wehaveshownthatE(cid:104) (cid:80)∞ X4 (cid:105) <∞,whichimpliesE(cid:2)(cid:80)∞ |X |(cid:3) <∞. N=1 N N=1 N Then,bythelemma,wehaveX convergingto0withprobability1,whichprovestheresult. N IfE[X ]=µ,thenjustreplaceX withY =X −µintheabovearguments.Thenwe n n n n can show that Y converges to 0 with probability 1, which is equivalent to X converging N N to µ with probability 1. End of the proof of strong law of large numbers. 366
6.4. CENTRAL LIMIT THEOREM 6.4 Central Limit Theorem The law of large numbers tells us the mean of the sample average X = (1/N)(cid:80)N X . N n=1 n However, if you recall our experiment of throwing N dice and inspecting the PDF of the sum of the numbers, you may remember that the convolution of an infinite number of uniform distributions gives us a Gaussian distribution. For example, we show a sequence of experiments in Figure 6.15. In each experiment, we throw N dice and count the sum. Therefore, if each face of the die is denoted as X , then the sum is X +···+X . We plot n 1 N the PDF of the sum. As you can see in the figure, X +···+X converges to a Gaussian. 1 N This phenomenon is explained by the Central Limit Theorem (CLT). Figure6.15:PictorialillustrationoftheCentralLimitTheorem.Supposewethrowadieandrecordthe face. [Left] If we only have one die, then the distribution of the face is uniform. [Middle] If we throw two dice, the distribution is the convolution of two uniform distributions. This will give us a triangle distribution.[Right]Ifwethrowfivedice,thedistributionisbecomingsimilartoaGaussian.TheCentral Limit Theorem says that as N goes to infinity, the distribution of the sum will converge to a Gaussian. What does the Central Limit Theorem say? Let X be the sample average, and let √ (cid:16) (cid:17) N Z = N XN−µ be the normalized variable. The Central Limit Theorem is as follows: N σ Central Limit Theorem: The CDF of Z is converging pointwise to the CDF of Gaussian(0,1). N Note that we are very careful here. We are not saying that the PDF of Z is converging to N the PDF of a Gaussian, nor are we saying that the random variable Z is converging to a N Gaussian random variable. We are only saying that the values of the CDF are converging pointwise. The difference is subtle but important. To understand the difficulty and the core ideas, we first present the concept of conver- gence in distribution. 367
CHAPTER 6. SAMPLE STATISTICS 6.4.1 Convergence in distribution Definition 6.8. Let Z ,...,Z be random variables with CDFs F ,...,F respec- 1 N Z1 ZN tively. We say that a sequence of Z ,...,Z converges in distribution to a random 1 N variable Z with CDF F if Z lim F (z)=F (z), (6.37) N→∞ ZN Z d for every continuous point z of F . We write Z → Z to denote convergence in Z N distribution. Thisdefinitioninvolvesmanyconcepts,whichwewilldiscussonebyone.However,the definition can be summarized in a nutshell as follows. Convergence in distribution = values of the CDF converge. Example 1. (Bernoulli) Consider flipping a fair coin N times. Denote each coin flip as a Bernoulli random variable X ∼Bernoulli(p), where n=1,2,...,N. Define Z as the sum n N of N Bernoulli random variables, so that N (cid:88) Z = X . N n n=1 We know that the resulting random variable Z is a binomial random variable with mean N Np and variance Np(1−p). Let us plot the PDF f (z) as shown in Figure 6.16. ZN Figure 6.16: Convergence in distribution. The convergence in distribution concerns the convergence of the values of the CDF (not the PDF). In this figure, we let Z = X +···+X , where X is a N 1 N N Bernoulli random variable with parameter p. Since a sum of Bernoulli random variables is a binomial, Z is a binomial random variable with parameters (N,p). We plot the PDF of Z , which is a train of N N deltafunctions,andcompareitwiththeGaussianPDF.Observethattheerror,max |f (z)−f (z)|, z ZN Z doesnot convergeto0.ThePDFofZ isabinomial.Abinomialisalwaysabinomial.Itwillnotturn N into a Gaussian. The first thing we notice in the figure is that as N increases, the PDF of the binomial has an envelope that is “very Gaussian”. So one temptation is to say that the random 368
6.4. CENTRAL LIMIT THEOREM variable Z is converging to another random variable Z. In addition, we would think that N the PDFs converge in the sense that for all z, (cid:18) N(cid:19) 1 (cid:26) (z−µ)2(cid:27) f (z)= pz(1−p)N−z −→ f (z)= √ exp − , ZN z Z 2πσ2 2σ2 where µ=Np and σ2 =Np(1−p). Unfortunately this argument does not work, because f (z) is continuous but f (z) Z ZN is discrete. The sample space of Z and the sample space of Z are completely different. In N fact, if we write f as an impulse train, we observe that ZN N (cid:18) (cid:19) (cid:88) N f (z)= pi(1−p)N−iδ(z−i). ZN i i=0 Clearly, no matter how big the N is, the difference |f (z)−f (z)| will never go to zero ZN Z for non-integer values of z. Mathematically, we can show that max |f (z)−f (z)|(cid:54)−→0, z ZN Z asN →∞.Z isabinomialrandomvariableregardlessofN.ItwillnotbecomeaGaussian. N If f (z) is not converging to a Gaussian PDF, how do we explain the convergence? ZN The answer is to look at the CDF. For discrete PDFs such as a binomial random variable, the CDF is a staircase function. What we can show is that (cid:88)z (cid:18) N(cid:19) (cid:90) z 1 (cid:26) (t−µ)2(cid:27) F (z)= pi(1−p)N−i −→ F (z)= √ exp − dt. ZN i Z 2πσ2 2σ2 i=0 −∞ The difference between the PDF convergence and the CDF convergence is that the PDF doesnotallowameaningful“distance”betweenadiscretefunctionandcontinuousfunction. ForCDF,thedistanceiswelldefinedbytakingthedifferencebetweenthestaircasefunction and the continuous function. For example, we can compute |F (z)−F (z)|, for all continuous points z of F , ZN Z Z and show that max|F (z)−F (z)|−→0. z ZN Z We need to pay attention to the set of z’s. We do not evaluate all z’s but only the z’s that are continuous points of F . If F is Gaussian, this does not matter because all z’s Z Z are continuous. However, for CDFs containing discontinuous points, our definition of con- vergence in distribution will ignore these discontinuous points because they have a measure zero. Example 2. (Poisson) Consider X ∼Poisson(λ), and consider X ,...,X . Define Z = n 1 N N (cid:80)N X .ItfollowsthatE[Z ]=(cid:80)N E[X ]=NλandVar[Z ]=(cid:80)N Var[X ]=Nλ. n=1 n N n=1 n N n=1 n Moreover, we know that the sum of Poissons remains a Poisson. Therefore, the PDF of Z N is (cid:88)∞ (Nλ)k 1 (cid:26) (z−µ)2(cid:27) f (z)= e−Nλδ(z−k) and f (z)= √ exp − , ZN k! Z 2πσ2 2σ2 k=0 369
CHAPTER 6. SAMPLE STATISTICS Figure 6.17: Convergence in distribution. This is the same as Figure 6.16, but this time we plot the CDFofZ .TheCDFisastaircasefunction.WecompareitwiththeGaussianCDF.Observethatthe N error,max |F (z)−F (z)|,convergestozeroasN grows.Convergenceindistributionsaysthatthe z ZN Z sequence of CDFs F (z) will converge to the limiting CDF F (z), at all continuous points of F (z). ZN Z Z where µ=Nλ and σ2 =Nλ. Again, f does not converge to f . However, if we compare ZN Z the CDF, we can see from Figure 6.18 that the CDF of the Poisson is becoming better approximated by the Gaussian. Interpreting “convergence in distribution”. After seeing two examples, you should now have some idea of what “convergence in distribution” means. This concept applies to the CDFs. When we write lim F (z)=F (z), (6.38) N→∞ ZN Z we mean that F (z) is converging to the value F (z), and this relationship holds for all ZN Z the continuous z’s of F . It does not say that the random variable Z is becoming another Z N random variable Z. d Z −→Z is equivalent to lim F (z)=F (z). N N→∞ ZN Z Example 3. (Exponential) So far, we have studied the sum of discrete random variables. Now, let’s take a look at continuous random variables. Consider X ∼Exponential(λ), and n let X ,...,X be i.i.d. copies. Define Z =(cid:80)N X . Then E[Z ]=(cid:80)N E[X ]=N/λ 1 N N n=1 n N n=1 n andVar[Z ]= N.HowaboutthePDFofZ ?Usingthecharacteristicfunctions,weknow N λ2 N that λ f (x)=λe−λx ←F → Φ (jω)= . Xn Xn λ+jω Therefore, the product is (cid:89)N λN λN (N −1)! Φ (jω)= Φ (jω)= = × ZN Xn (λ+jω)N (λ+jω)N (N −1)! n=1 λN (N −1)! λN = · ←F → zN−1e−λz =f (z). (N −1)! (λ+jω)N (N −1)! ZN 370
6.4. CENTRAL LIMIT THEOREM 0.2 0.15 0.06 Poisson Poisson Poisson Gaussian Gaussian Gaussian 0.15 0.1 0.04 0.1 0.05 0.02 0.05 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 1 1 1 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 Poisson 0.2 Poisson 0.2 Poisson Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 (a) N =4 (b) N =10 (c) N =50 Figure 6.18: Convergence in distribution for a sum of Poisson random variables. Here we assume that X ,...,X arei.i.d.Poissonwithaparameterλ.WeletZ =(cid:80)N X bethesum,andcomputethe 1 N N n=1 n correspondingPDF(toprow)andCDFs(bottomrow).Justaswiththebinomialexample,thePDFsof the Poisson do not converge but the CDFs of the Poisson converge to the CDF of a Gaussian. This resulting PDF f (z) = λN zN−1e−λz is known as the Erlang distribution. The ZN (N−1)! CDF of the Erlang distribution is (cid:90) z F (z)= f (t)dt ZN ZN −∞ (cid:90) z λN = tN−1e−λt dt (N −1)! 0 =Gamma function(z,N), where the last integral is known as the incomplete gamma function, evaluated at z. Givenallthese,wecannowcomparethePDFandtheCDFofZ versusZ.Figure6.19 N showsthePDFsandtheCDFsofZ forvariousN values.Inthisexperimentwesetλ=1. N As we can see from the experiment, the Erlang distribution’s PDF and CDF converge to a Gaussian. In fact, for continuous random variables such as exponential random variables, we indeed have the random variable Z converging to the random variable Z. This is quite N different from discrete random variables, where Z does not converge to Z but only F N ZN converges to F . Z d p Is −→ stronger than −→? Convergence in distribution is actually weaker than con- vergence in probability. Consider a continuous random variable X with a symmetric PDF f (x) such that f (x) = f (−x). It holds that the PDF of −X has the same PDF. If X X X we define the sequence Z = X if N is odd and Z = −X if N is even, and let Z = X, N N then F (z) = F (z) for every z because the PDF of X and −X are identical. There- ZN Z d p fore, Z → Z. However, Z (cid:54)→ Z because Z oscillates between the random variables X N N N and −X. These two random variables are different (although they have the same CDF) because P[X =−X]=P[{ω :X(ω)=−X(ω)}]=P[{ω :X(ω)=0}]=0. 371
CHAPTER 6. SAMPLE STATISTICS 0.25 0.15 0.06 0.2 0.1 0.04 0.15 0.1 0.05 0.02 0.05 Sum of Exponential Sum of Exponential Sum of Exponential Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 1 1 1 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 Sum of Exponential 0.2 Sum of Exponential 0.2 Sum of Exponential Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 (a) N =4 (b) N =10 (c) N =50 Figure 6.19: Convergence in distribution for a sum of exponential random variables. Here we assume that X ,...,X are i.i.d. exponentials with a parameter λ. We define Z = (cid:80)N X be the sum. 1 N N n=1 n It is known that the sum of exponentials is an Erlang. We compute the corresponding PDF (top row) andCDFs(bottomrow).Unliketheprevioustwoexamples,inthisexampleweseethatbothPDFsand CDFs of the Erlang distribution are converging to a Gaussian. 6.4.2 Central Limit Theorem Theorem 6.19 (Central Limit Theorem). Let X ,...,X be i.i.d. random variables 1 N of mean E[X ] = µ and variance Var[X ] = σ2. Also, assume that E[|X3|]<∞. Let n n √ (cid:16) (cid:17)n X =(1/N)(cid:80)N X be the sample average, and let Z = N XN−µ . Then N n=1 n N σ lim F (z)=F (z), (6.39) N→∞ ZN Z where Z =Gaussian(0,1). In plain words, the Central Limit Theorem says that the sample average (which is a random variable) has a CDF converging to the CDF of a Gaussian. Therefore, if we want to evaluate probabilities associated with the sample average, we can approximate the probability by the probability of a Gaussian. As we discussed above, the Central Limit Theorem does not mean that the random variable Z is converging to a Gaussian random variable, nor does it mean that the PDF N of Z is converging to the PDF of a Gaussian. It only means that the CDF of Z is N N converging to the CDF of a Gaussian. Many people think that the Central Limit Theorem means “sample average converges to Gaussian”. This is incorrect for the above reasons. However, it is not completely wrong. For continuous random variables where both PDF and CDF are continuous, we will not run into situations where the PDF is a train of delta functions. In this case, convergence in CDF can be translated to convergence in PDF. The power of the Central Limit Theorem is that the result holds for any distribution of X ,...,X . That is, regardless of the distribution of X ,...,X , the CDF of X is 1 N 1 N N 372
6.4. CENTRAL LIMIT THEOREM approaching a Gaussian. Summary of the Central Limit Theorem • X ,...,X are i.i.d. random variables, with mean µ and variance σ2. They are 1 N not necessarily Gaussians. √ (cid:16) (cid:17) • DefinethesampleaverageasX =(1/N)(cid:80)N X ,andletZ = N X−µ . N n=1 n N σ d • The Central Limit Theorem says Z −→ Gaussian(0,1). Equivalently, the the- N orem says that NX −d →Gaussian(µ,σ2). N • So if we want to evaluate the probability of X ∈ A for some set A, we can N approximate the probability by evaluating the Gaussian: (cid:90) 1 (cid:26) (y−µ)2(cid:27) P[X ∈A]≈ exp − dy. N (cid:112) 2π(σ2/N) 2(σ2/N) A • CLT does not say that the PDF of X is becoming a Gaussian PDF. N • CLT only says that the CDF of X is becoming a Gaussian CDF. N If the set A is an interval, we can use the standard Gaussian CDF to compute the probability. Corollary 6.3. Let X ,...,X be i.i.d. random variables with mean µ and vari- 1 N ance σ2. Define the sample average as X =(1/N)(cid:80)N X . Then N n=1 n (cid:18)√ b−µ(cid:19) (cid:18)√ a−µ(cid:19) P[a≤X ≤b]≈Φ N −Φ N , (6.40) N σ σ where Φ(z)=(cid:82)z √1 e−x 22 dx is the CDF of the standard Gaussian. −∞ 2π Proof. By the Central Limit Theorem, we know that X −d →Gaussian(µ,σ2). Therefore, N N (cid:90) b 1 (cid:26) (y−µ)2(cid:27) P[a≤X ≤b]≈ exp − dy N (cid:112) 2π(σ2/N) 2(σ2/N) a √ =(cid:90) Nb− σµ √1 e−y 22 dy =Φ(cid:18)√ Nb−µ(cid:19) −Φ(cid:18)√ Na−µ(cid:19) . √ Na−µ 2π σ σ σ (cid:3) A graphical illustration of the CLT is shown in Figure 6.20, where we use a binomial randomvariable(which isthe sumof i.i.d.Bernoulli)as anexample. TheCLT does notsay thatthebinomialrandomvariableisbecomingaGaussian.Itonlysaysthattheprobability covered by the binomial can be approximated by the Gaussian. 373
CHAPTER 6. SAMPLE STATISTICS Figure6.20:TheCentralLimitTheoremsaysthatifwewanttoevaluatetheprobabilityP[a≤X ≤b], N where X = (1/N)(cid:80)N X is the sample average of i.i.d. random variables X ,...,X , we can N n=1 n 1 N approximate the probability by integrating the Gaussian PDF. The following proof of the Central Limit Theorem can be skipped if this is your first time reading the book. Proof of the Central Limit Theorem. We now give a “proof” of the Central Limit Theorem.Technicallyspeaking,thisproofdoesnotprovetheconvergenceoftheCDFasthe theorem claims; it only proves that the moment-generating function converges. The actual proof of the CDF convergence is based on the Berry-Esseen Theorem, which is beyond the scope of this book. However, what we prove below is still useful because it gives us some intuitionaboutwhyGaussianisthelimitingrandomvariableweshouldconsiderinthefirst place. √ (cid:16) (cid:17) Let Z = N XN−µ . It follows that E[Z ] = 0 and Var[Z ] = 1. Therefore, if we N σ N N canshowthatZ isconvergingtoastandardGaussianrandomvariableZ ∼Gaussian(0,1), N thenbythelineartransformationpropertyofGaussian,Y = √σ Z+µwillbeGaussian(µ,σ2/N). N Our proof is based on analyzing the moment-generating function of Z . In particular, N M ZN(s)d =efE[esZN]=E(cid:20) es√ N(cid:16)XN σ−µ(cid:17)(cid:21) = (cid:89)N E(cid:104) eσ√s N(Xn−µ)(cid:105) . n=1 Expanding the exponential term using the Taylor expansion (Chapter 1.2), N (cid:89) E(cid:104) eσ√s N(Xn−µ)(cid:105) n=1 = (cid:89)N E(cid:20) 1+ √s (X −µ)+ s2 (X −µ)2+O(cid:18) (X n− √µ)3(cid:19)(cid:21) σ N n 2σ2N n σ3N N n=1 = (cid:89)N (cid:20) 1+ √s E[X −µ]+ s2 E(cid:2) (X −µ)2(cid:3)(cid:21) =(cid:18) 1+ s2 (cid:19)N . σ N n 2σ2N n 2N n=1 374
6.4. CENTRAL LIMIT THEOREM (cid:16) (cid:17)N It remains to show that 1+ s2 →es2/2. If we can show that, we have shown that the 2N MGF of Z is also the MGF of Gaussian(0,1). To this end, we consider log(1+x). By the N Taylor approximation, we have that (cid:18) d (cid:19) (cid:18) d2 (cid:19) x2 log(1+x)≈log(1)+ logx| x+ logx| +O(x3). dx x=1 dx2 x=1 2 (cid:16) (cid:17) Therefore, we have log 1+ s2 ≈ s2 − s4 . As N →∞, the limit becomes 2N 2N 4N2 (cid:18) s2 (cid:19) s2 s4 s2 lim Nlog 1+ ≈ − lim = , N→∞ 2N 2 N→∞4N 2 and so taking the exponential on both sides yields lim N→∞(cid:16) 1+ 2s N2 (cid:17)N = es 22 . Therefore, we conclude that lim N→∞M ZN(s)=es 22 , and so Z N is converging to a Gaussian. (cid:3) Limitation of our proof. The limitation of our proof lies in the issue of whether the integration and the limit are interchangeable: (cid:26)(cid:90) (cid:27) lim M (s)= lim f (z)esz dz N→∞ ZN N→∞ ZN (cid:90) (cid:16) (cid:17) =? lim f (z) esz dz. N→∞ ZN If they were, then proving lim M (s)=M (s) is sufficient to claim f (z)→f (z). N→∞ ZN Z ZN Z However, we know that the latter is not true in general. For example, if f (z) is a train of ZN delta functions, then the limit and the integration are not interchangeable. Berry-Esseen Theorem. The formal way of proving the Central Limit Theorem is to prove the Berry-Esseen Theorem. The theorem states that (cid:12) (cid:12) (cid:12) (cid:12) β s zu ∈p R (cid:12) (cid:12)F ZN(z)−F Z(z)(cid:12) (cid:12)≤C σ3√ N, where β and C are universal constants. Here, you can more or less treat the supremum operator as the maximum. The left-hand side represents the worst-case error of the CDF F compared to the limiting CDF F . The right-hand side involves several constants C, ZN Z β, and σ, but they are fixed. As N goes to infinity, the right-hand side will converge to zero. Therefore, if we can prove this result, then we have proved the actual Central Limit Theorem. In addition, we have found the rate of convergence since the right-hand side tells us that the error √ drops at the rate of 1/ N, which is not particularly fast but is sufficient for our purpose. Unfortunately, proving the Berry-Esseen theorem is not easy. One of the difficulties, for example, is that one needs to deal with the infinite convolutions in the time domain or the frequency domain. Interpreting our proof. If our proof is not completely valid, why do we mention it? For one thing, it provides us with some useful intuition. For most of the (well-behaving) randomvariableswhosemomentsarefinite,theexponentialterminthemoment-generating 375
CHAPTER 6. SAMPLE STATISTICS function can be truncated to the second-order polynomial. Since a second-order polynomial is a Gaussian, it naturally concludes that as long as we can perform such truncation the truncated random variable will be Gaussian. To convince you that the Gaussian MGF is the second-order approximation to other MGFs, we use Bernoulli as an example. Let X ,...,X be i.i.d. Bernoulli with a parame- 1 N ter p. Then the moment-generating function of X =(1/N)(cid:80)N X would be: N n=1 n N M (s)=E[esX]=E[es N1 (cid:80)N n=1Xn]= (cid:89) E[eNsXn] XN n=1 (cid:18) (cid:18) s s2 (cid:19)(cid:19)N =(1−p+peNs)N ≈ 1−p+p 1+ + N 2N2 (cid:18) sp s2p (cid:19)N = 1+ + . N 2N2 Using the logarithmic approximation, it follows that (cid:18) sp s2p (cid:19) logM (s)=Nlog 1+ + XN N 2N2 (cid:18) sp s2p (cid:19) N (cid:18) sp s2p (cid:19)2 ≈N + − + N 2N2 2 N 2N2 s2p(1−p) def ≈sp+ = logM (s). 2N Y Taking the exponential on both sides, we have that (cid:26) s2p(1−p)(cid:27) M (s)=exp sp+ , Y 2N (cid:16) (cid:17) which is the MGF of a Gaussian random variable Y ∼Gaussian p,p(1−p) . N Figure 6.21 shows several MGFs. In each of the subfigures we plot the exact MGF M XN(s)=(1−p+peNs)N asafunctionofs.(Theparameterpinthisexampleisp=0.5.)We vary the number N, and we inspect how the shape of M (s) changes. On top of the exact XN (cid:110) (cid:111) MGFs, we plot the Gaussian approximations M (s) = exp sp+ s2p(1−p) . According to Y 2N ourcalculation,thisGaussianapproximationisthesecond-orderapproximationtotheexact MGF. The figures show the effect of the second-order approximation. For example, in (a) when N =2 the Gaussian is a quadratic approximation of the exact MGF. For (b) and (c), as N increases, the approximation improves. The reason why the second-order approximation works for Gaussian is that when N increases, the higher order moments of X vanish and only the leading first two moments N (cid:110) (cid:111) survive. The MGFs are becoming flat because M (s) = exp sp+ s2p(1−p) converges to Y 2N exp{sp}whenN →∞.TakingtheinverseLaplacetransform,M (s)=exp{sp}corresponds Y to a delta function. This makes sense because as N grows, the variance of the X shrinks. End of the discussion. 376
6.4. CENTRAL LIMIT THEOREM 105 105 105 104 Binomial MGF 104 Binomial MGF 104 Binomial MGF Gaussian MGF Gaussian MGF Gaussian MGF 103 103 103 102 102 102 101 101 101 100 100 100 10-1 10-1 10-1 10-2 10-2 10-2 -10 -5 0 5 10 -10 -5 0 5 10 -10 -5 0 5 10 (a) N =2 (b) N =4 (c) N =10 Figure6.21:ExplanationoftheCentralLimitTheoremusingthefunction.Inthissetofplots,weshow theMGFoftherandomvariableX =(1/N)(cid:80)N X ,whereX ,...,X arei.i.d.Bernoullirandom N n=1 n 1 N variables. The exact MGF of X is the binomial, whereas the approximated MGF is the Gaussian. We N observe that as N increases, the Gaussian approximation to the exact MGF improves. 6.4.3 Examples Example 6.15. Prove the equivalence of a few statements. √ (cid:16) (cid:17) • N XN−µ →d Gaussian(0,1) σ √ • N(X −µ)→d Gaussian(0,σ2) N √ • NX →d Gaussian(µ,σ2) N Solution. The proof is based on the linear transformation property of Gaussian ran- dom variables. For example, if the first statement is true, then the second statement is also true because √ (cid:20)√ (cid:18) X −µ(cid:19) z(cid:21) lim F√ (z)= lim P[ N(X −µ)≤z]= lim P N N ≤ N→∞ N(XN−µ) N→∞ N N→∞ σ σ =(cid:90) z/σ √1 e−t 22 dt=(cid:90) z √ 1 e− 2t σ2 2 dt. 2π 2πσ2 −∞ −∞ The other results can be proved similarly. Example 6.16. Suppose X ∼ Poisson(10) for n = 1,...,N, and let X be the n N sample average. Use the Central Limit Theorem to approximate P[9 ≤ X ≤ 11] for N N =20. Solution. We first show that (cid:34) N (cid:35) N 1 (cid:88) 1 (cid:88) E[X ]=E X = E[X ]=10, N N n N n n=1 n=1 N 1 (cid:88) 1 10 1 Var[X ]= Var[X ]= Var[X ]= = . N N2 n N n 20 2 n=1 377
CHAPTER 6. SAMPLE STATISTICS Therefore, the Central Limit Theorem implies that X −d → Gaussian(cid:0) 10,1(cid:1) . The N 2 probability is (cid:32) (cid:33) (cid:32) (cid:33) 11−10 9−10 P[9≤X ≤11]≈Φ −Φ N (cid:112) (cid:112) 1/2 1/2 (cid:18) (cid:19) (cid:18) (cid:19) 1 1 =Φ √ −Φ −√ =0.9214−0.0786=0.8427. 0.5 0.5 We can also do an exact calculation to verify our approximation. Let S = N (cid:80)N n=1X n so that X N = S NN. Since a sum of Poisson remains a Poisson, it follows that S ∼Poisson(10N)=Poisson(200). N Consequently, P[9≤X ≤11]=P[180≤S ≤220] N N (cid:88)220 200(cid:96)e−200 (cid:88)180 200(cid:96)e−200 = − =0.9247−0.0822=0.8425. (cid:96)! (cid:96)! (cid:96)=0 (cid:96)=0 Note that this is an exact calculation subject to numerical errors when evaluating the finite sums. Example 6.17. Suppose you have collected N = 100 data points from an unknown distribution. The only thing you know is that the true population mean is µ = 500 and the standard deviation is σ =80. (Note that this distribution is not necessarily a Gaussian.) (a) Find the probability that the sample mean will be inside the interval (490,510). (b) Find an interval such that 95% of the sample average is covered. (cid:18) (cid:16) (cid:17)2(cid:19) Solution. To solve (a), we note that X N →d Gaussian 500, √80 . Therefore, 100 (cid:32) (cid:33) (cid:32) (cid:33) 510−500 490−500 P[490≤X ≤510]=Φ −Φ N √80 √80 100 100 =Φ(1.25)−Φ(−1.25)=0.7888. To solve (b), we know that Φ(x) = 0.025 implies that x = −1.96, and Φ(x) = 0.975 implies that x=+1.96. So y−500 =±1.96 ⇒ y =484.32 or y =515.68. √80 100 Therefore, P[484.32≤X ≤515.68]=0.95. N 378
6.4. CENTRAL LIMIT THEOREM 6.4.4 Limitation of the Central Limit Theorem If we recall the statement of the Central Limit Theorem (Berry-Esseen), we observe that the theorem states only that (cid:20)√ (cid:18) X −µ(cid:19) (cid:21) lim P N N ≤ε = lim F (ε)=F (ε)=Φ(ε). N→∞ σ N→∞ ZN Z Rearranging the terms, (cid:20) (cid:21) σε lim P X ≤µ+ √ =Φ(ε). N N→∞ N This implies that the approximation is good only when the deviation ε is small. Let us consider an example to illustrate this idea. Consider a set of i.i.d. exponential random variables X ,...,X , where X ∼ Exponential(λ). Let S = X +···+X be 1 N n N 1 N the sum, and let X =S /N be the sample average. Then, according to Chapter 6.4.1, S N N is an Erlang distribution S ∼Erlang(N,λ) with a PDF N λN f (x)= xN−1e−λx. SN (N −1)! Practice Exercise 6.10. Let S ∼ Erlang(N,λ) with a PDF f (x). Show that if N SN Y =aS +b for any constants a and b, then N N (cid:18) (cid:19) 1 y−b f (y)= f . YN a SN a Solution: This is a simple transformation of random variables: F (y)=P[Y ≤y]=P[aS +b≤y]=P(cid:20) S ≤ y−b(cid:21) =(cid:90) y− ab f (x)dx. YN N N a SN −∞ Hence, using the fundamental theorem of calculus, d (cid:90) y− ab 1 (cid:18) y−b(cid:19) f (y)= f (x)dx= f . YN dy SN a SN a −∞ We are interested in knowing the statistics of X and comparing it with a Gaussian. N To this end, we construct a normalized variable X −µ Z = N√ , N σ/ N where µ=E[X ]= 1 and σ2 =Var[X ]= 1 . Then n λ n λ2 S /N −µ S −Nµ λ √ Z = N √ = N√ = √ S − N N N σ/ N σ N N 379
CHAPTER 6. SAMPLE STATISTICS √ Usingtheresultofthepracticeexercise,bymapping a= √λ andb=− N,itfollowsthat N √ (cid:32) √ (cid:33) N z+ N f (z)= f . ZN λ SN √λ N NowwecompareZ withthestandardGaussianZ ∼Gaussian(0,1).Accordingtothe N Central Limit Theorem, the standard Gaussian is a good approximation to the normalized sample average Z . To compare the two results, we conduct a numerical experiment. We N let λ = 1 and we vary N. We plot the PDF f (z) as a function of z, for different N’s, in ZN Figure 6.22. In addition, we plot the PDF f (z), which is the standard Gaussian. Z The plot in Figure 6.22 shows that while the Central Limit Theorem provides a good approximation, the approximation is only good for values that are close to the mean. For the tails, the Gaussian approximation is not as good. 100 10-2 N = 1 10-4 N = 10 N = 100 N = 1000 Gaussian 10-6 -1 0 1 2 3 4 5 Figure 6.22: CLT fails at the tails. We note that X ,...,X are i.i.d. exponential with a parameter 1 N λ=1.WeplotthePDFsofthenormalizedsampleaverageZ N = XN√−µ byvaryingN.WeplotthePDF σ/ N of the standard Gaussian Z ∼Gaussian(0,1) on the same grid. Note that the Gaussian approximation is good for values that are close to the mean. For the tails, the Gaussian approximation is not very accurate. The limitation of the Central Limit Theorem is attributable to the fact that Gaussian is a second-order approximation. If a random variable has a very large third moment, the second-order approximation may not be sufficient. In this case, we need a much larger N to drive the third moment to a small value and make the Gaussian approximation valid. When will the Central Limit Theorem fail? • The Central Limit Theorem fails when N is small. • The Central Limit Theorem fails if the third moment is large. As an extreme case,aCauchyrandomvariabledoesnothaveafinitethirdmoment.TheCentral Limit Theorem is not valid for this case. • The Central Limit Theorem can only approximate the probability for input val- ues near the mean. It does not approximate the tails, for which we need to use Chernoff’s bound. 380
6.5. SUMMARY 6.5 Summary Whydoweneedtostudythesampleaverage?Becauseitisthesummaryofthedataset.In machinelearning,oneofthemostfrequentlyaskedquestionsisaboutthenumberoftraining samplesrequiredtotrainamodel.Theanswercanbefoundbyanalyzingtheaveragenumber of successes and failures as the number of training samples grows. For example, if we define f as the classifier that takes a data point x and predicts a label f(x ), we hope that it n n will match with the true label y . If we define an error n (cid:40) 1, f(x )=y correct classification, E = n n n 0, f(x )(cid:54)=y incorrect classification, n n then E is a Bernoulli random variable, and the total loss E = 1 (cid:80)N E will be the n N n=1 n trainingloss.Butwhatis 1 (cid:80)N E ?ItisexactlythesampleaverageofE .Therefore,by N n=1 n n analyzing the sample average E we will learn something about the generalization capability of our model. Howshouldwestudythesampleaverage?Byunderstandingthelawoflargenumbers and the Central Limit Theorem, as we have seen in this chapter. • Law of large numbers: X converges to the true mean µ as N grows. • Central Limit Theorem: The CDF of X can be approximated by the CDF of a Gaussian, as N grows. Performance guarantee? The other topic we discussed in this chapter is the concept of convergence type. There are essentially four types of convergence, ranked in the order of restrictions. • Deterministic convergence: A sequence of deterministic numbers converges to anotherdeterministicnumber.Forexample,thesequence1,1,1,1,...converges 2 3 4 to 0 deterministically. There is nothing random about it. • Almost sure convergence: Randomness exists, and there is a probabilistic con- vergence.Almostsureconvergencemeansthatthereiszeroprobabilityoffailure after a finite number of failures. • Convergence in probability: The sequence of probability values converges, i.e., the chance of failure is going to zero. However, you can still fail even if your N is large. • Convergence in distribution: The probability values can be approximated by the CDF of a Gaussian. 381
CHAPTER 6. SAMPLE STATISTICS 6.6 References Moment-Generating and Characteristic Functions 6-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.4. 6-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 4.5 and 4.7. 6-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 5.5 and 7.2. 6-4 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 4.5 and 4.7. 6-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 7.7. 6-6 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 4.3. Basic probability inequality 6-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 5.1. 6-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 6 and 8. 6-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-10 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.2. 6-11 Larry Wasserman, All of Statistics, Springer 2003. Chapter 4. Concentration inequalities 6-12 Larry Wasserman, All of Statistics, Springer 2003. Chapter 4. 6-13 Martin Wainwright, High-Dimensional Statistics, Cambridge University Press, 2019. Chapter 2.1. 6-14 Stephane Boucheron, Gabor Lugosi and Pascal Massart, Concentration Inequalities, Oxford University Press, 2013. Chapters 2.1 and 2.2. 382
6.6. REFERENCES Law of large numbers 6-15 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 5.2, 5.3, 5.5. 6-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 7.1, 7.2, 7.4 6-17 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-18 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.2, 8.4. 6-19 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 3.3, 14.1, 14.3. 6-20 Larry Wasserman, All of Statistics, Springer 2003. Chapter 5.1 - 5.3. 6-21 Patrick Billingsley, Probability and Measure, Wiley 1995. Section 22. Central Limit Theorem 6-22 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 5.4. 6-23 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 7.3. 6-24 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-25 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.3. 6-26 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 5.6, 14.2. 6-27 Larry Wasserman, All of Statistics, Springer 2003. Chapter 5.4. 6-28 Patrick Billingsley, Probability and Measure, Wiley 1995. Section 27. 383
CHAPTER 6. SAMPLE STATISTICS 6.7 Problems Exercise 1. (Video Solution) Let X, Y, Z be three independent random variables: X ∼Bernoulli(p), Y ∼Exponential(α), Z ∼Poisson(λ) Find the function for the following random variables. (a) U =Y +Z (b) U =2Z+3 (c) U =XY (d) U =2XY +(1−X)Z Exercise 2. (Video Solution) Two random variables X and Y have the joint PMF λn+mλm P(X =n,Y =m)= 1 2 e−(λ1+λ2), m=0,1,2,..., n≥−m. (n+m)!m! Let Z =X+Y. Find the function M (s) and the PMF of Z. Z Exercise 3. (Video Solution) Let X ,X ,... be a sequence of independent random variables with PDF 0 1 a 1 f (x)= k , a = , Xk π(a2 +x2) k 2k+1 k for k =0,1,.... Find the PDF of Y, where ∞ (cid:88) Y = X . k k=0 Hint: You may find the characteristic function useful. Exercise 4. The random variables X and Y are independent and have PDFs (cid:40) (cid:40) e−x, x≥0, 0, y >0, f (x)= and f (y)= X 0, x<0, Y et, y ≤0. 384
6.7. PROBLEMS Find the PDF of Z = X + Y. (Hint: Use the characteristic function and the moment- generating function.) Exercise 5. A discrete random variable X has a PMF 1 p (k)= , k =1,2,.... X 2k Find the characteristic function Φ (jω). X Exercise 6. Let T ,T ,... be i.i.d. random variables with PDF 1 2 (cid:40) λe−λt, t≥0, f (t)= Tk 0, t<0, for k =1,2,3,.... Let S =(cid:80)n T . Find the PDF of S . n k=1 k n Exercise 7. (Video Solution) In this exercise we will prove a variant of Chebyshev when the variance σ2 is unknown but X is bounded between a≤X ≤b. (a) Letγ ∈R.FindaγthatminimizesE[(X−γ)2].Hence,showthatE[(X−γ)2]≥Var[X] for any γ. (b) Let γ =(a+b)/2. Show that (b−a)2 E[(X−γ)2]=E[(X−a)(X−b)]+ . 4 (c) From (a) and (b), show that Var[X]≤ (b−a)2 . 4 (d) Show that for any ε>0, (b−a)2 P[|X−µ|>ε]≤ . 4ε2 Exercise 8. The random variables X and Y are independent with PDFs 1 1 f (x)= and f (y)= , X π(1+x2) Y π(1+y2) respectively. Find the PDF of Z =X−Y. (Hint: Use the characteristic function.) Exercise 9. A random variable X has the characteristic function Φ (jω)=e−jω/(1−jω). X 385
CHAPTER 6. SAMPLE STATISTICS Find the mean and variance of X. Exercise 10. Show that for any random variables X and Y, 1 P[|X−Y|>(cid:15)]≤ E[(X−Y)2]. (cid:15)2 Exercise 11. Let X be an exponential random variable with a parameter λ. Let µ = E[X] and σ2 = Var[X]. Compute P[|X −µ| ≥ kσ] for any k > 1. Compare this to the bound obtained by Chebyshev’s inequality. Exercise 12. Let X ,...,X be i.i.d. Bernoulli with a parameter p. Let α>0 and define 1 N (cid:115) (cid:18) (cid:19) 1 2 (cid:15)= log . 2N α Let X = 1 (cid:80)N X . Define an interval N N n=1 n (cid:2) (cid:3) I = X −(cid:15), X +(cid:15) . N N Use Hoeffding’s inequality to show that P[I contains p]≥1−α. Exercise 13. Let Z ∼Gaussian(0,1). Prove that for any (cid:15)>0, P[|Z|>(cid:15)]≤(cid:114) 2e−(cid:15) 22 . π (cid:15) Hint: Note that (cid:15)P[|Z| > (cid:15)] = 2(cid:15)P[Z > (cid:15)], and then follow the procedure we used to prove Markov’s inequality. Exercise 14. (a) Giveanon-negativerandomvariableX ≥0suchthatMarkov’sinequalityismetwith equality. Hint: Consider a discrete random variable. (b) Give a random variable X such that Chebyshev’s inequality is met with equality. Exercise 15. Consider a random variable X such that E[esX]≤es2 2σ2 . 386
6.7. PROBLEMS (a) Show that for any t, (cid:26) t2 (cid:27) P[X ≥t]≤exp − . 2σ2 Hint: Use Chernoff’s bound. (b) Show that E[X2]≤4σ2. Hint: First prove that E[X2]=(cid:82)∞P[X2 ≥t]dt. Then use part (a) above. 0 Exercise 16. LetX ,...,X bei.i.d.uniformrandomvariablesdistributedover[0,1].SupposeY ,...,Y 1 N 1 N are defined as follows. (a) Y =X /n n n (b) Y =(X )n n n (c) Y =max(X ,...,X ) n 1 n (d) Y =min(X ,...,X ) n 1 n For (a), (b), (c), and (d), show that Y converges in probability to some limit. Identify the n limit in each case. Exercise 17. Let λ = 1 for n=1,2,.... Let X ∼Poisson(λ ). Show that X converges in probability n n n n n to 0. Exercise 18. Let Y ,Y ,... be a sequence of random variables such that 1 2 (cid:40) 0, with probability 1− 1, Y = n n 2n, with probability 1. n Does Y converge in probability to 0? n Exercise 19. (Video Solution) A Laplace random variable has a PDF λ f (x)= e−λ|x|, λ>0, X 2 and the variance is Var[X] = 2 . Let X ,...,X be a sequence of i.i.d. Laplace random λ2 1 500 variables. Let X +···+X M = 1 500. 500 500 (a) Find E[X]. Express your answer in terms of λ. 387
CHAPTER 6. SAMPLE STATISTICS (b) Let λ=10. Using Chebyshev’s inequality, find a lower bound of P[−0.1≤M ≤0.1]. 500 (c) Let λ=10. Using the Central Limit Theorem, find the probability P[−0.1≤M ≤0.1]. 500 You may leave your answer in terms of the Φ(·) function. Exercise 20. (Video Solution) Let X ,...,X be a sequence of i.i.d. random variables such that X = ±1 with equal 1 N n probability. Let N 1 (cid:88) X = √ X . N n N n=1 ProvetheCentralLimitTheoremforthisparticularsequenceofrandomvariablesbyshowing that (a) E[X ]=0, Var[X ]=1. N N (b) The moment-generating function of X N is M XN(s)→es 22 as N →∞. Exercise 21. (Video Solution) Let X ,...,X be a sequence of i.i.d. random variables with mean and variance 1 N E[X ]=µ and Var[X ]=σ2, n=1,...,N. n n The distribution of X is, unknown. Let n N 1 (cid:88) M = X . N N n n=1 Use the Central Limit Theorem to estimate the probability P[M >2µ]. N 388
Chapter 7 Regression Starting with this chapter, we will discuss several combat skills — techniques that we use to do the actual data analysis. The theme of this topic is learning and inference, which are both at the core of modern data science. The word “learning” can be broadly interpreted as seeking the best model to explain the data, and the word “inference” refers to prediction andrecovery.Here,predictionmeansthatweusetheobserveddatatoforecastorgeneralize to unseen situations, whereas recovery means that we try to restore the missing data in our current observations. In this chapter we will learn regression, one of the most widely used learning and inference techniques. Regressionisaprocessforfindingtherelationshipbetweentheinputsandtheoutputs. In a regression problem, we consider a set of input data {x ,...,x } and a set of output 1 N def data {y ,...,y }. We call the set of these input-output pairs D = {(x ,y ),...,(x ,y )} 1 N 1 1 N N the training data. The true relationship between an x and a y is unknown. We do not n n know,youdonotknow,onlyGodknows.Wedenotethisunknownrelationshipasamapping f(·) that takes x and maps it to y , n n y =f(x ), n n as illustrated in Figure 7.1. Figure7.1:Aregressionproblemisaboutfindingthebestapproximationtotheinput-outputrelationship of the data. Since we do not know f(·), finding it from a set of finite number of data points D = {(x ,y ),...,(x ,y )} is infeasible — there are infinitely many ways we can make y = 1 1 N N n f(x ) for every n = 1,...,N. The idea of regression is to add a structure to the problem. n Insteadoflookingforf(·),wefindaproxyg (·).Thisproxyg (·)takesacertainparametric θ θ form. For example, we can postulate that (x ,y ) has a linear relationship, and so n n g (x )= θ x + θ , n=1,...,N. θ n 1 n 0 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) parameter parameter 389
CHAPTER 7. REGRESSION This equation is a straight line with a slope θ and a y-intercept θ . We call θ = [θ ,θ ] 1 0 1 0 the parameter of the model f(·). To emphasize that the function we are using here is parameterized by θ, we denote the function by g (·). θ Of course, any model we choose is our guess. It will never be the true model. There is alwaysadifferencebetweenwhatourmodeltellsusandwhatwehaveobserved.Wedenote this “difference” or “error” by e and define it as: n e =y −g (x ), n=1,...,N. n n θ n Thepurposeofregressionistofindthebestθsuchthattheerrorisminimized.Forexample, consider a minimization of the sum-square error: N θ(cid:98)=argmin (cid:88) (y n−g θ(x n))2. θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) Thesumofthesquarederrorisjustoneofthemanypossiblewayswecandefinethetraining lossE (θ).Wewilldiscussdifferentwaystodefinethetraininglossinthischapter,butthe train point should be evident. For a given dataset D = {(x ,y ),...,(x ,y )}, regression tries 1 1 N N to find a function g (·) such that the training loss is minimized. The optimization variable θ is the parameter θ. If the function g (·) is a linear function in θ, we call the regression a θ linear regression. Figure 7.2: A regression problem involves several steps: picking a model g , defining the training loss θ E (θ), and solving the optimization to update θ. train A summary of the regression process is shown in Figure 7.2. Given the training data D = {(x ,y ),...,(x ,y )}, the user picks a model g (·) to make a prediction. We com- 1 1 N N θ pare the predicted value g (x ) with the observed value y , and compute the training loss θ n n E (θ).ThetraininglossE (θ)isafunctionofthemodelparameterθ.Differentmodel train train parameters θ give different training loss. We solve an optimization problem to find the best model parameter. In practice, we often iterate the process for a few times until the training loss is settled down. 390
What is regression? Given the data points (x ,y ),...,(x ,y ), regression is the process of finding 1 1 N N the parameter θ of a function g (·) such that the training loss is minimized: θ N (cid:88) θ(cid:98)=argmin L(y n, g θ(x n)), (7.1) θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) whereL(·,·)isthelossbetweenapairoftrueobservationy andthepredictiong (x ). n θ n One common choice of L(·,·) is L(g (x ),y )=(g (x )−y )2. θ n n θ n n Example 1. Fitting the data Suppose we have a set of data points (x ,y ), (x ,y ), ..., (x ,y ), where x ’s are the 1 1 2 2 N N n inputs and y ’s are the outputs. These pairs of data points can be plotted in a scatter plot, n as shown in Figure 7.3. We want to find the curve that best fits the data. To solve this problem, we first need to choose a model, for example g (x )=θ +θ x +θ x2 +θ x3 +θ x4. θ n 0 1 n 2 n 3 n 4 n Wecallthecoefficientsθ =[θ ,θ ,θ ,θ ,θ ]theregression coefficients.Theycanbefound 0 1 2 3 4 by solving the optimization problem N (cid:18) (cid:19)2 (cid:88) minimize y −(θ +θ x +θ x2 +θ x3 +θ x4) . n 0 1 n 2 n 3 n 4 n θ0,θ1,θ2,θ3,θ4 n=1 4 3 2 1 0 -1 -2 data fitted curve -3 -1 -0.5 0 0.5 1 Figure 7.3: Regression can be used to fit the dataset using curves. In this example, we use a fourth-th order polynomial g (x)=(cid:80)4 θ xp to fit a 50-point dataset. θ p=0 p n This optimization asks for the best θ = [θ ,...,θ ]T such that the training loss is 0 4 minimized. Solving the minimization problem would require some effort, but if we imagine that we have solved it we can find the best curve, which is g (x) = (cid:80)4 θ xp with the θ p=0 p n optimal θ plugged in. The red curve in Figure 7.3 shows an example in which we have used a fourth-order polynomial to fit a dataset comprising 50 data points. We will learn how to solve the problem in this chapter. 391
CHAPTER 7. REGRESSION Example 2. Predicting the stock market Imaginethatyouhaveboughtsomesharesinthestockmarket.Youhavelookedatthepast data, and you want to predict the price of the shares over the next few days. How would you do it besides just eyeballing the data? First,youwouldplotthedatapointsonagraph.Mathematically,wecandenotethese data points as {x ,x ,...,x }, where the indices n = 1,2,...,N can be treated as time 1 2 N stamps. We assume a simple model to describe the relationship between the x ’s, say n x ≈ax +bx , n n−1 n−2 for some parameters θ = (a,b).1 This model assumes that the current value x can be n approximated by a linear combination of two previous values x and x . Therefore, if n−1 n−2 we have x and x we should be able to predict x , and if we have x and x we should be 1 2 3 2 3 able to predict x , etc. The magic of this prediction comes from the parameters a and b. If 4 we know a and b, the prediction can be done by simply plugging in the numbers. Theregressionproblemhereistoestimatetheparametersaandbfromthedata.Since we are given a set of training data {x ,x ,...,x }, we can check whether our predicted 1 2 N value x is close to the true x , and whether our predicted value x is close to the true x , (cid:98)3 3 (cid:98)4 4 etc. This leads to the optimization N (cid:18) (cid:19)2 (cid:88) ( (cid:98)a,(cid:98)b)=argmin x n−(ax n−1+bx n−2) , a,b (cid:124) (cid:123)(cid:122) (cid:125) n=1 =prediction where we use initial conditions that x = x = 0. The optimization problem requires us 0 −1 to minimize the disparity between x and the predicted value ax +bx , for all n. n n−1 n−2 By finding the (a,b) that minimizes this objective function, we will accomplish our goal of estimating the best (a,b). Figure 7.4 shows an example of predicting a random process using the above model. If the parameters a and b are properly determined, we will obtain a reasonably well-fitted curve to the data. A simple extrapolation to the future timestamp would suffice for the forecast task. Plan for this chapter What are the key ingredients of regression? • Learning: Formulate the regression problem as an optimization problem, and solve it by finding the best parameters. • Inference: Use the estimated parameters and models to predict the unseen data points. Regression is too broad a topic to be covered adequately in a single chapter. Accord- ingly, we will present a few principles and a few practical algorithmic techniques that are broadlyapplicabletomany(definitelynotall)regressiontasks.Theseincludethefollowing. 1Caution: If you lose money in the stock market by following this naive model, please do not cry. This modelisgreatlyoversimplifiedandprobablywrong. 392
1.5 1 0.5 0 -0.5 data best fit candidate -1 0 0.2 0.4 0.6 0.8 1 Figure 7.4: An autoregression model aims at learning the model parameters based on the previous samples.Thisexampleillustratesfittingthedatausingthemodelx =ax +bx ,forn=1,...,N. n n−1 n−2 • The principle of regression (Section 7.1). We explain the formulation of a regression problem via optimization. There are a few steps involved in developing this concept. First, we will exclusively focus on linear models because these models are easier to analyze than nonlinear models but are still rich enough for many practical problems. We will discuss how to solve the linear regression problem and some applications of the solutions. We then address the issue of outliers using a concept called the robust linear regression. • Overfitting (Section 7.2). The biggest practical challenge of regression is overfitting. Overfitting occurs when a model fits too closely to the training samples so that it fails to generalize. We will delve deeply into the roots of overfitting and show that overfitting depends on three factors: the number of training samples N, the model complexity d, and the magnitude of noise σ2. • Bias-variance trade-off (Section 7.3). We will present one of the most fundamental resultsinlearningtheory,knownasthebias-variancetrade-off.Itappliestoall regres- sion problems, not just to linear models. Understanding this trade-off will help you understand the fundamental limits of your problem so that you know what to expect from the model. • Regularization (Section 7.4). In this section we discuss a technique for combatting overfitting known as regularization. Regularization is carried out by adding an extra term to the regression objective function. By solving the modified optimization, the regression solution is improved in two ways: (i) regularization makes the regression solution less sensitive to noise perturbations, and (ii) it alleviates the fitting difficulty whenwehaveonlyafewtrainingsamples.Wewilldiscusstworegularizationstrategies: the ridge regression and the LASSO regression. Much of this chapter deals with optimization. If this is your first time reading this book, we encourage you to have a reference book on linear algebra at hand. 393
CHAPTER 7. REGRESSION 7.1 Principles of Regression We start by recalling our discussion in the introduction. The purpose of regression can be summarized in a simple statement: Given the data points (x ,y ),...,(x ,y ), find the parameter θ of a function g (·) 1 1 N N θ such that the training loss is minimized: N (cid:88) θ(cid:98)=argmin L(y n, g θ(x n)), (7.2) θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) whereL(·,·)isthelossbetweenapairoftrueobservationy andthepredictiong (x ). n θ n Whenthecontextmakesitclear,wewilldropthesubscriptθing (·)withtheunderstanding θ that the function g(·) is parameterized by θ. Asyoucansee,regressionfindsafunctiong(·)thatbestapproximates theinput-output relationship between x and y . There are two choices we need to make when formulating n n a regression problem: • Function g(·): What is the family of functions we want to use? This could be a line, a polynomial,orasetofbasisfunctions.Ifitisapolynomial,whatisitsorder?Weneed to make all these decisions before running the regression. A poor choice of function family can lead to a poor regression result. • Loss“L(·,·)”:Howdowemeasuretheclosenessbetweeny andg(x )?Arewemeasur- n n ingintermsofthesquarederror(y −g(x ))2,ortheabsolutedifference|y −g(x )|, n n n n or something else? Again, a poor choice of distance function can create a false sense of closeness because you might be optimizing for a wrong objective. Before we delve into the details, we need to discuss briefly the connection between regressionandprobability.Aregressionproblemcanbesolvedwithoutknowingprobability, so why is regression discussed in a book on probability? This question is related to how much we know about the statistical model and what kind of optimality we are seeking. A full answer requires some understanding of maximum likelihood estimation and maximum a posteriori estimation, which will be explained in Chapter 8. As a quick preview of our results, we summarize the key ideas below: How is regression related to probability? • Ifyouknowthestatisticalrelationshipbetweenx andy ,thenwecanconstruct n n a regression problem that maximizes the likelihood of the underlying distribu- tion. Such regression solution is optimal with respect to the likelihood. • We can construct a regression problem that can minimize the expectation of the 394
7.1. PRINCIPLES OF REGRESSION squared error. This regression solution is mean-squared optimal. • If you are a Bayesian and you know the prior distribution of x , then we can n construct a regression problem that maximizes the posterior distribution. The solution to this regression problem is Bayesian optimal. • If you know nothing about the statistics of x and y , you can still run the n n regressionandgetsomething,andthis“something”canbeveryuseful.However, you cannot claim statistical optimality of this “something”. See Chapter 8 for additional discussion. It is important to understand that a regression problem is at the intersection of op- timization and statistics. The need for optimization is clear because we need to minimize the error. The statistical need is to generalize to unknown data. If there is no statistical relationship between x and y (for all n), whatever model we obtain from the regression n n will only work for the N training samples. The model will not generalize because knowing x will not help us know y . In other words, if there is no statistical relationship between n n x and y , you can fit perfectly to the training data but you will fail miserably to fit the n n testing data. 7.1.1 Intuition: How to fit a straight line? In this subsection we want to give you the basic idea of how regression is formulated. To keep things simple, we will discuss how to fit data using a straight line. Consider a collection of data points D = {(x ,y ),...,(x ,y )}, where x ’s are the 1 1 N N n inputs and y ’s are the observations, for example, in the table below. n n x y n n 1 0.6700 3.0237 2 0.3474 2.3937 3 0.6695 3.5548 . . . . . . . . . N −1 0.2953 2.6396 N 0.6804 3.2536 Let us consider the linear regression problem. The goal of linear regression is to find the straight line that best fits the datasets. All straight lines on a 2D graph are plots of the equation g(x)=ax+b, where a is the slope of the line and b is the y-intercept of the line. We denote this line by g(·). Note that this function g is characterized by two parameters (a,b) because once (a,b) are known the line is determined. If we change (a,b), the line will change as well. Therefore, by finding the best line we are essentially searching for the best (a,b) such that the training error is minimized. The pictorial meaning of linear regression can easily be seen in Figure 7.5, which shows N = 50 data points according to some latent distributions. Given these 50 data points, we construct several possible candidates for the regression model. These candidates 395
CHAPTER 7. REGRESSION are characterized by the parameters (a,b). For example, the parameters (a,b) = (1,2) and (a,b)=(−2,3) represent two different straight lines in the candidate pool. The goal of the regression is to find the best line from these candidates. Note that since we limit ourselves to straight lines, the candidate set will not include polynomials or trigonometric functions. These functions are outside the family we are considering. 4.5 4 3.5 3 2.5 data 2 best fit candidate 1.5 0 0.2 0.4 0.6 0.8 1 Figure 7.5: Theobjectiveofleastsquaresfitting(orlinearregression)istofindalinethatbestfitsthe dataset. Given these candidate functions, we need to measure the the training loss. This can be defined in multiple ways, such as • Sum-squared loss E (θ)=(cid:80)N (y −g(x ))2. train n=1 n n • Sum-absolute loss E (θ)=(cid:80)N |y −g(x )|. train n=1 n n • Cross-entropy loss E (θ)=−(cid:80)N (y logg(x )+(1−y )log(1−g(x ))). train n=1 n n n n • Perceptual loss E (θ) = (cid:80)N max(−y g(x ),0), when y and g(x ) are binary train n=1 n n n n takingvalues±1.Thisisareasonabletrainingerrorbecauseify matcheswithg(x ), n n then y g(x )=1 and so max(−y g(x ),0)=0. But if y does not match with g(x ), n n n n n n theny g(x )=−1andhencemax(−y g(x ),0)=1.Thus,thelosscapturesthesum n n n n of all the mismatched pairs. Choosingthelossfunctionisproblem-specific.Itisalsowhereprobabilityentersthepicture because, without any knowledge about the distributions of x and y , there is no way to n n choosethebesttrainingloss.Youcanstillpickone,aswewilldo,butitwillnotbegranted any probabilistic guarantees. Among these possible choices of the training error, we are going to focus on the sum- squared loss because it is convex and differentiable. This makes the computation easy, since we can run any textbook optimization algorithm. The regression problem under the sum-squared loss is: (cid:16) (cid:17) (cid:88)N (cid:18) (cid:19)2 (cid:98)a,(cid:98)b =argmin y n−(ax n+b) . (7.3) (a,b) n=1 (cid:124) (cid:123)(cid:122) (cid:125) =g(xn) In this equation, the symbol “argmin” means “argument minimize”, which returns the ar- gument that minimizes the cost function on the right. The interpretation of the equation is 396
7.1. PRINCIPLES OF REGRESSION that we seek the (a,b) that minimize the sum (cid:80)N (y −(ax +b))2. Since we are mini- n=1 n n mizing the squared error, this linear regression problem is also known as the least squares fitting problem. The idea is summarized in the following box. What is linear least squares fitting? • Find a line g(x)=ax+b that best fits the training data {(x ,y )}N . n n n=1 • The optimality criterion is to minimize the squared error N (cid:18) (cid:19)2 (cid:88) E (θ)= y −g(x ) , (7.4) train n n n=1 where θ =(a,b) is the model parameter. • There exist other optimality criteria. Squared error is convex and differentiable. 7.1.2 Solving the linear regression problem Let’s consider how to solve the linear regression problem given by Equation (7.3). The problem is the following: (cid:16) (cid:17) (cid:98)a,(cid:98)b =argmin E train(a,b). (7.5) (a,b) As with any two-dimensional optimization problem, the optimal point ( (cid:98)a,(cid:98)b) should have a zero gradient, meaning that ∂ ∂ E (a,b)=0 and E (a,b)=0. ∂a train ∂b train This should be familiar to you, even if you have only learned basic calculus. This pair of equations says that, at a minimum point, the directional slopes should be zero no matter which direction you are looking at. The derivative with respect to a is ∂ E (a,b) ∂a train ∂ (cid:26) (cid:88)N (cid:18) (cid:19)2(cid:27) = y −(ax +b) ∂a n n n=1 ∂ (cid:26)(cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2(cid:27) = y −(ax +b) + y −(ax +b) +···+ y −(ax +b) ∂a 1 1 2 2 N N (cid:18) (cid:19) (cid:18) (cid:19) =2 y −(ax +b) (−x )+···+2 y −(ax +b) (−x ) 1 1 1 N N N (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) =2 − x y +a x2 +b x . n n n n n=1 n=1 n=1 397
CHAPTER 7. REGRESSION Similarly, the derivative with respect to b is ∂ ∂ (cid:26) (cid:88)N (cid:18) (cid:19)2(cid:27) E (a,b)= y −(ax +b) ∂b train ∂b n n n=1 (cid:18) (cid:19) (cid:18) (cid:19) =2 y −(ax +b) (−1)+···+2 y −(ax +b) (−1) 1 1 N N (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) =2 − y +a x +b 1 . n n n=1 n=1 n=1 Setting these two equations to zero, we have that (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) 2 − y x +a x2 +b x =0, n n n n n=1 n=1 n=1 (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) 2 − y +a x +b 1 =0. n n n=1 n=1 n=1 Rearranging the terms, the pair can be equivalently written as  N N   N  (cid:80) x2 n (cid:80) x n (cid:20) a(cid:21) (cid:80) x ny n  n=1 n=1  =n=1 . (cid:80)N  b  (cid:80)N  x N y n n n=1 n=1 Therefore, if we can solve this system of linear equations, we will have the linear regression solution. Remark.Itiseasytoseethatthesolutionachievestheminimuminsteadofthemaximum, since the second-order derivatives are positive: ∂2 (cid:88)N ∂2 (cid:88)N E (a,b)= x2 ≥0 and E (a,b)= 1>0. ∂a2 train n ∂b2 train n=1 n=1 The following theorem summarizes this intermediate result. Theorem 7.1. The solution of the problem Equation (7.5) (cid:16) (cid:17) (cid:88)N (cid:18) (cid:19)2 (cid:98)a,(cid:98)b =argmin y n−(ax n+b) (a,b) n=1 satisfies the equation  N N   N   n(cid:80) =1x2 n n(cid:80) =1x n  (cid:20) (cid:98)a(cid:21) = n(cid:80) =1x ny n  . (7.6) (cid:80)N  (cid:98)b  (cid:80)N  x N y n n n=1 n=1 398
7.1. PRINCIPLES OF REGRESSION Matrix-vector form of linear regression Solving this linear regression requires some basic linear algebra. The regression can be written as       y x 1 e 1 1 (cid:20) (cid:21) 1 . . . a .  .  =  . . +  .  .  .   . . b  .  y N x N 1 (cid:124) (cid:123)(cid:122) (cid:125) e N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) θ (cid:124) (cid:123)(cid:122) (cid:125) y X e With X, y, θ and e, we can write the linear regression problem compactly as y =Xθ+e. Therefore, the training loss E (θ) can be defined as train E (θ)=(cid:107)y−Xθ(cid:107)2 train (cid:13)    (cid:13)2 (cid:13) (cid:13) y .1 x .1 1 . (cid:20) a(cid:21)(cid:13) (cid:13) (cid:88)N (cid:18) (cid:19)2 =(cid:13) . − . . (cid:13) = y −(ax +b) . (cid:13) .   . . b (cid:13) n n (cid:13) (cid:13) (cid:13) y N x N 1 (cid:13) n=1 Now, taking the gradient with respect to θ yields2 (cid:26) (cid:27) ∇ E (θ)=∇ (cid:107)y−Xθ(cid:107)2 θ train θ =−2XT(y−Xθ). Equating this to zero, we obtain XT(y−Xθ)=0 ⇐⇒ XTXθ =XTy. (7.7) Equation (7.7) is called the normal equation. Thenormalequationisaconvenientwayofconstructingthesystemoflinearequations. Using the 2-by-2 system shown in Equation (7.6) as an example, we note that    N N  XTX =(cid:20) x 11 · ·· ·· · x 1N(cid:21)  x . . .1 1 . . . =  n(cid:80) (cid:80)N=1x2 n n(cid:80) =1x n   , x N 1 x n N n=1    N  y (cid:80) XTy =(cid:20) x 11 · ·· ·· · x 1N(cid:21)   . . .1  =  n= (cid:80)N1x ny n   . y N y n n=1 Therefore, as long as you can construct the X matrix, forming the 2-by-2 system in Equa- tion (7.6) is straightforward: start with y = Xθ and then multiply the matrix transpose XT to both sides. The resulting system is what you need. There is nothing to memorize. 2Thisisabasicvectorcalculusresult.Fordetails,youmayconsultstandardtextssuchastheUniversity ofWaterloo’smatrixcookbook.https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf 399
CHAPTER 7. REGRESSION Running linear regression on a computer On a computer, solving the linear regression for a line is straightforward. Let us look at the MATLAB code first. % MATLAB code to fit data points using a straight line N = 50; x = rand(N,1)*1; a = 2.5; % true parameter b = 1.3; % true parameter y = a*x + b + 0.2*rand(size(x)); % Synthesize training data X = [x(:) ones(N,1)]; % construct the X matrix theta = X\y(:); % solve y = X theta t = linspace(0, 1, 200); % interpolate and plot yhat = theta(1)*t + theta(2); plot(x,y,’o’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,4); InthispieceofMATLABcode,weneedtodefinethedatamatrixX.Here,x(:)isthe column vector that stores all the values (x ,...,x ). The all-one vector ones(N,1) is the 1 N second column in our X matrix. The command X\y(:) is equivalent to solving the normal equation XTXθ =XTy. The last few lines are used to plot the predicted curve. Note that theta(1) and theta(2) are the entries of the solution θ. The result of this program is exactly the plot shown in Figure 7.5 above. In Python, the program is quite similar. The command we use to solve the inversion is np.linalg.lstsq. # Python code to fit data points using a straight line import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.rand(N) a = 2.5 # true parameter b = 1.3 # true parameter y = a*x + b + 0.2*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) # construct the X matrix theta = np.linalg.lstsq(X, y, rcond=None)[0] # solve y = X theta t = np.linspace(0,1,200) # interpolate and plot yhat = theta[0]*t + theta[1] plt.plot(x,y,’o’) plt.plot(t,yhat,’r’,linewidth=4) 400
7.1. PRINCIPLES OF REGRESSION 7.1.3 Extension: Beyond a straight line Regression is a powerful technique. Although we have discussed its usefulness for fitting straight lines, the same concept can fit other curves. To generalize the regression formulation, we consider a d-dimensional regression coef- ficient vector θ =[θ ,...,θ ]T ∈Rd and a general linear model 0 d−1 d−1 (cid:88) g (x )= θ φ (x ). θ n p p n p=0 Here, the mappings {φ (·)}d−1 can be considered as a nonlinear transformation that takes p p=0 the input x and maps it to another value. For example, φ (·) = (·)p will map an input x n p to a pth power xp. We can now write the system of linear equations as        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . . (7.8)  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) y X θ e Let us look at some examples. Example 7.1. (Quadratic fitting) Consider the linear regression problem using a quadratic equation: y =ax2 +bx +c, n=1,...,N. n n n Express this equation in matrix-vector form. Solution. The matrix-vector expression is  y   x2 x 1  e  1 1 1   1 y 2 x2 2 x 2 1 a e 2   . .  =  . . . . . . b+  . .  .  .   . . . c  .  y x2 x 1 e N N N N This is again in the form of y =Xθ+e. The MATLAB and Python programs for Example 7.1 are shown below. A numerical example is illustrated in Figure 7.6. % MATLAB code to fit data using a quadratic equation N = 50; x = rand(N,1)*1; a = -2.5; b = 1.3; c = 1.2; 401
CHAPTER 7. REGRESSION 2.5 2 1.5 1 0.5 data fitted curve 0 0 0.2 0.4 0.6 0.8 1 Figure 7.6: Example: Our goal is to fit the dataset of 50 data points shown above. The model we use is g (x )=ax2 +bx +c, for n=1,...,N. θ n n n y = a*x.^2 + b*x + c + 1*rand(size(x)); N = length(x); X = [ones(N,1) x(:) x(:).^2]; beta = X\y(:); t = linspace(0, 1, 200); yhat = theta(3)*t.^2 + theta(2)*t + theta(1); plot(x,y, ’o’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,6); # Python code to fit data using a quadratic equation import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.rand(N) a = -2.5 b = 1.3 c = 1.2 y = a*x**2 + b*x + c + 0.2*np.random.randn(N) X = np.column_stack((np.ones(N), x, x**2)) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(0,1,200) yhat = theta[0] + theta[1]*t + theta[2]*t**2 plt.plot(x,y,’o’) plt.plot(t,yhat,’r’,linewidth=4) The generalization to polynomials of arbitrary order is to replace the model with d−1 (cid:88) g (x )= θ xp, θ n p p=0 402
7.1. PRINCIPLES OF REGRESSION where p = 0,1,...,d−1 represent the orders of the polynomials and θ ,...,θ are the 0 d−1 regression coefficients. In this case, the matrix system is  y   1 x ··· xd−1 θ   e  1 1 1 0 1   y . .2  =  1 . . x 2 · . .·· x 2d . .−1      θ . .1   +  e . .2  ,  .  . ··· . .  .   .  y N 1 x N ··· x Nd−1 θ d−1 e N which again is in the form of y =Xθ+e. Example 7.2. (Legendre polynomial fitting) Let {L (·)}d−1 be a set of Legendre p p=0 polynomials (see discussions below), and consider the linear regression problem using d−1 (cid:88) y = θ L (x), n=1,...,N. n p p p=0 Express this equation in matrix-vector form. Solution. The matrix-vector expression is        y L (x ) L (x ) ··· L (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 L 0(x 2) L 1(x 2) ··· L d−1(x 2) θ 1  e 2  . = . . .  . + . .  .   . . .  .   .   .   . ··· . .  .   .  y L (x ) L (x ) ··· L (x ) θ e N 0 N 1 N d−1 N d−1 N Legendre polynomials are orthogonal polynomials. In conventional polynomials, the functions {x,x2,x3,...,xp} are not orthogonal. As we increase p, the set of functions {x,x2,x3,...,xp} will have redundancy, which will eventually result in the matrix X being noninvertible. The pth-order Legendre polynomial is denoted by L (x). Using the Legendre polyno- p mials as the building block of the regression problem, the model is expressed as d−1 def(cid:88) g (x) = θ L (x) θ p p p=0 =θ L (x)+θ L (x)+θ L (x) +···+θ L (x), 0 0 1 1 2 2 d−1 d−1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =x =1(3x2−1) 2 where L (·), L (·) and L (·) are the Legendre polynomials of order 0, 1 and 2, respectively. 0 1 2 As an example, the first few leading Legendre polynomials are L (x)=1, 0 L (x)=x, 1 1 L (x)= (3x2−1), 2 2 1 L (x)= (5x3−3x). 3 2 403
CHAPTER 7. REGRESSION The order of the Legendre polynomials is always the same as that of the ordinary polyno- mials. The shapes of these polynomials are shown in Figure 7.7(a). 4 1 data 3 Legendre basis Polynomial basis 0.5 2 0 1 L (x) 0 L 1(x) 0 -0.5 L 2(x) L (x) -1 3 L (x) 4 -1 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) (b) Figure7.7:(a)Thefirst5leadingLegendrepolynomialsplottedintherangeof−1≤x≤1.(b)Fitting the data using an ordinary polynomial and a Legendre polynomial. Figure 7.7(b) demonstrates a fitting problem using the Legendre polynomials. You can see that the fitting is just as good as that of the ordinary polynomials (which should be the case). However, if we compare the coefficients, we observe that the magnitude of the Legendre coefficients is smaller (see Table 7.1). In general, as the order of polynomials increasesandthenoisegrows,theordinarypolynomialswillbecomeincreasinglydifficultto fit the data. θ θ θ θ θ 4 3 2 1 0 Ordinary polynomials 5.3061 3.3519 −3.6285 −1.8729 0.1540 Legendre polynomials 1.2128 1.3408 0.6131 0.1382 0.0057 Table 7.1: Theregressioncoefficientsof anordinary polynomialandaLegendrepolynomial. Notethat whilebothpolynomialscanfitthedata,theLegendrepolynomialcoefficientshavesmallermagnitudes. Calling Legendre polynomials for regression is not difficult in MATLAB and Python. Specifically, one can call legendreP in MATLAB and scipy.special.eval_legendre in Python. % MATLAB code to fit data using Legendre polynomials N = 50; x = 1*(rand(N,1)*2-1); a = [-0.001 0.01 +0.55 1.5 1.2]; y = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ... + a(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ... + a(5)*legendreP(4,x) + 0.5*randn(N,1); X = [legendreP(0,x(:)) legendreP(1,x(:)) ... legendreP(2,x(:)) legendreP(3,x(:)) ... 404
7.1. PRINCIPLES OF REGRESSION legendreP(4,x(:))]; beta = X\y(:); t = linspace(-1, 1, 200); yhat = beta(1)*legendreP(0,t) + beta(2)*legendreP(1,t) + ... + beta(3)*legendreP(2,t) + beta(4)*legendreP(3,t) + ... + beta(5)*legendreP(4,t); plot(x,y,’ko’,’LineWidth’,2,’MarkerSize’,10); hold on; plot(t,yhat,’LineWidth’,6,’Color’,[0.9 0 0]); import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre N = 50 x = np.linspace(-1,1,N) a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.2*np.random.randn(N) X = np.column_stack((eval_legendre(0,x), eval_legendre(1,x), \ eval_legendre(2,x), eval_legendre(3,x), \ eval_legendre(4,x))) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(-1, 1, 50); yhat = theta[0]*eval_legendre(0,t) + theta[1]*eval_legendre(1,t) + \ theta[2]*eval_legendre(2,t) + theta[3]*eval_legendre(3,t) + \ theta[4]*eval_legendre(4,t) plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=8) plt.show() The idea of fitting a set of data using the Legendre polynomials belongs to the larger family of basis functions. In general, we can use a set of basis functions to model the data: d−1 def(cid:88) g (x) = θ φ (x), θ p p p=0 where {φ (x)}d−1 are the basis functions and {θ }d−1 are the regression coefficients. The p p=0 p p=0 constant θ is often called the bias of the regression. 0 Choiceoftheφ (x)canbeextremelybroad.Onecanchoosetheordinarypolynomials p φ (x)=xp or the Legendre polynomial φ (x)=L (x). Other choices are also available: p p p • Fourier basis: φ p(x)=ejωpx, where ω p is the pth carrier frequency. • Sinusoid basis: φ (x) = sin(ω x), which is same as the Fourier basis but taking the p p imaginary part. 405
CHAPTER 7. REGRESSION • Gaussian basis: φ (x)= √1 exp(cid:110) −(x−µp)2(cid:111) , where (µ ,σ ) are the model param- p 2πσ p2 2σ p2 p p eters. Evidently,bychoosingdifferentbasisfunctionswehavedifferentwaystofitthedata.There isnodefinitiveanswerastowhichfunctionsarebetter.Statisticaltechniquessuchasmodel selections are available, but experience will tell you to align with one and not the other. It is frequently more useful to have some domain knowledge rather than resorting to various computational techniques. How to fit data using basis functions • Construct this equation:        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . , (7.9)  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) y X θ e • The functions φ (x) are the basis functions, e.g., φ (x) = xp for ordinary poly- p p nomials. • You can replace the polynomials with the Legendre polynomials. • You can also replace the polynomials with other basis functions. • Solve for θ by θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. θ Example 7.3. (Autoregressive model) Consider a two-tap autoregressive model: y =ay +by , n=1,2,...,N n n−1 n−2 where we assume y =y =0. Express this equation in the matrix-vector form. 0 −1 Solution. The matrix-vector form of the equation is       y y y e 1 0 −1 1 y 2  y 1 y 0 (cid:20) a(cid:21) e 2  . = . .  + . .  .   . .  b  .   .   . .   .  (cid:124)(cid:123)(cid:122)(cid:125) y y y e N N−1 N−2 =θ N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X In general, we can append more previous samples to predict the future. The general 406
7.1. PRINCIPLES OF REGRESSION expression is L (cid:88) y = θ y , n=1,2,...,N, n (cid:96) n−(cid:96) (cid:96)=1 where (cid:96) = 1,2,...,L denote the previous L samples of the data and {θ ,...,θ } are the 1 L regression coefficients. If we do this we see that the matrix expression is    y y y ··· y    y 0 −1 −2 1−L e 1 1        y y y . . .2 3 4       =         y y y . . .1 2 3 y y y . . .0 1 2 y y y− . . .0 11 · · · .. . .· · ·· · · y y y2 3 4− − − . . . L L L             θθ θ . . . L1 2    +       e e e . . .2 3 4       . y . (cid:124)(cid:123)(cid:122)(cid:125) e N y y y . y N N−1 N−2 N−3 N−L =θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X Observe the pattern associated with this matrix X. Each column is a one-entry shifted version of the previous column. This matrix is called a Toeplitz matrix. The MATLAB (and Python) code for calling and using the Toeplitz matrix is shown below. % MATLAB code for auto-regressive model N = 500; y = cumsum(0.2*randn(N,1)) + 0.05*randn(N,1); % generate data L = 100; % use previous 100 samples c = [0; y(1:400-1)]; r = zeros(1,L); X = toeplitz(c,r); % Toeplitz matrix theta = X\y(1:400); % solve y = X theta yhat = X*theta; % prediction plot(y(1:400), ’ko’,’LineWidth’,2);hold on; plot(yhat(1:400),’r’,’LineWidth’,4); # Python code for auto-regressive model import numpy as np import matplotlib.pyplot as plt from scipy.linalg import toeplitz N = 500 y = np.cumsum(0.2*np.random.randn(N)) + 0.05*np.random.randn(N) L = 100 c = np.hstack((0, y[0:400-1])) r = np.zeros(L) X = toeplitz(c,r) theta = np.linalg.lstsq(X, y[0:400], rcond=None)[0] yhat = np.dot(X, theta) 407
CHAPTER 7. REGRESSION plt.plot(y[0:400], ’o’) plt.plot(yhat[0:400],linewidth=4) The plots generated by the above programs are shown in Figure 7.8(a). Note that we aredoinganinterpolation,becausewearepredictingthevalueswithinthetrainingdataset. 2 2 0 0 -2 -2 -4 -4 -6 -6 0 100 200 300 400 500 0 100 200 300 400 500 (a) (b) Figure7.8:Autoregressivemodelonasimulateddataset,usingL=100coefficients.(a)Trainingdata. Notethatthemodeltrainsverywellonthisdataset.(b)Testingdata.Whentestedonfuturedata,the autoregressive model can still predict for a few samples but loses track when the time elapsed grows. We now consider extrapolation. Given the training data, we can find the regression coefficients by solving the above linear equation. This gives us θ. To predict the future samples we need to return to the equation L (cid:88) y = θ y , n=1,2,...,N, (cid:98)n (cid:96) (cid:98)n−(cid:96) (cid:124) (cid:123)(cid:122) (cid:125) (cid:96)=1 =previousestimate wherey arethepreviousestimates.Forexample,ifwearegiven100daysofstockprices, (cid:98)n−(cid:96) then predicting the 101st day’s price should be based on the L days before the 101st. A simple for-loop suffices for such a calculation. Figure7.8(b)showsanumericalexampleofextrapolatingdatausingtheautoregressive model.InthisexperimentweuseN =400samplestotrainanautoregressivemodeloforder L = 100. We then predict the data for another 100 data points. As you can see from the figure, the first few samples still look reasonable. However, as time increases, the model starts to lose track of the real trend. Isthereanywaywecanimprovetheautoregressivemodel?Asimplewayistoincrease the memory L so that we can use a long history to predict the future. This boils down to the long-term running average of the curve, which works well in many cases. However, if the testing data does not follow the same distribution as the training data (which is often the case in the real stock market because unexpected news can change the stock price), then even the long-term average will not be a good forecast. That is why data scientists on Wall Street make so much money: they have advanced mathematical tools for modeling the stockmarket.Nevertheless,wehopethattheautoregressivemodelprovidesyouwithanew perspective for analyzing data. The summary below highlights the main ideas of the autoregressive model. 408
7.1. PRINCIPLES OF REGRESSION What is the autoregressive model? • It solves this problem    y y y ··· y          yy yy . . .1 2 3     =       yy . . .0 21 y y− . . .0 11 y y− − . . .02 1 · · . . . . .· ·· · y y1 2 3− − − . . . L L L           θθ θ . . . L1 2    +      ee e e . . .1 2 3     . (7.10) N y N−1 y N−2 y N−3 . y N−L (cid:124)(cid:123)(cid:122)(cid:125) N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =θ (cid:124) (cid:123)(cid:122) (cid:125) =y =e =X • The number of taps in the past history would affect the memory and hence the long-term forecast. • Solve for θ by θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. (7.11) θ∈Rd 7.1.4 Overdetermined and underdetermined systems The sub-section requires knowledge of some concepts in linear algebra that can be found in standard references.a aCarlMeyer,Matrix Analysis and Applied Linear Algebra,SIAM,2000. Let us now consider the theoretical properties of the least squares linear regression problem, which is an optimization: θ(cid:98)=argmin(cid:107)y−Xθ(cid:107)2. (P1) θ∈Rd We observe that the objective value of this optimization problem can go to zero if and only if the minimizer θ(cid:98)is the solution of the system of linear equations Findθ such thaty =Xθ. (P2) We emphasize that Problem (P1) and Problem (P2) are two different problems. Even if we cannot solve Problem (P2), Problem (P1) is still well defined, but the objective value will not go to zero. This subsection aims to draw the connection between the two problems and discuss the respective solutions. We will start with Problem (P2) by considering two shapes of the matrix X. Overdetermined system Problem(P2)iscalledoverdeterminedifX ∈RN×distallandskinny,i.e.,N >d.Thishap- penswhenyouhavemorerowsthancolumns,orequivalentlywhenyouhavemoreequations than unknowns. When N > d, Problem (P2) has a unique solution θ(cid:98)= (XTX)−1XTy if 409
CHAPTER 7. REGRESSION andonlyifXTX isinvertible,orequivalentlyifandonlyifthecolumnsofX arelinearlyin- dependent.AtechnicaldescriptionofthisisthatX hasafullrank,denotedbyrank(X)=d. Whenrank(X)=d,Problem(P1)hasauniqueglobalminimizerθ(cid:98)=(XTX)−1XTy,which is the same as the unique solution of Problem (P2). Figure 7.9: Hierarchy of the solutions of an overdetermined system. An overdetermined system uses a tall and skinny matrix X. The rank of a matrix X is defined as the largest number of independent columnswecanfindinX.Ifrank(X)=d,thematrixXTX isinvertible,andProblem(P2)willhave a unique solution. If rank(X) < d, then the solution depends on whether the particular observation y lives in the range space of X. If yes, Problem (P2) will have infinitely many solutions because there is a nontrivial null space. If no, Problem (P2) will have no solution because the system is incompatible. If the columns of X are linearly dependent so that XTX is not invertible, we say that X is rank-deficient (denoted as rank(X) < d). In this case, Problem (P2) may not have a solution. We say that it may not have a solution because it is still possible to have a solution. It all depends on whether y can be written as a linear combination of the linearly independent columns of X. If yes, we say that y lives in the range space of X. The range space of X is defined as the set of vectors {z|z =Xα, for someα}. If rank(X)=d, all y will live in the range space of X. But if rank(X) < d, only some of the y will live in the range space of X. When this happens, the matrix X must have a nontrivial null space. The null space of X is defined as the set of vectors {z|Xz = 0}. A nontrivial null space will give us infinitely many solutions to Problem (P2). This is because if α is the solution found in the range space so that y = Xα, then we can pick any z from the null space such that Xz = 0. This will lead to another solution α+z such that X(α+z)=Xα+0=y. Since we have infinitely many choices of such z’s, there will be infinitely many solutions to Problem (P2). AlthoughthereareinfinitelymanysolutionstoProblem(P2),allofthemaretheglobal minimizers of Problem (P1). They can make the objective value equal to zero because the equality y = Xθ holds. However, the solutions to Problem (P2) are not unique since the objective function is convex but not strictly convex. If y does not live in the range space of X, we say that Problem (P2) is incompatible. If a system of linear equations is incompatible, there is no solution. However, even when this happens, we can still solve the optimization Problem (P1), but the objective value will not reach 0. The minimizer is a global minimizer because the objective function is convex, 410
7.1. PRINCIPLES OF REGRESSION but the minimizer is not unique. Underdetermined system Problem (P2) is called underdetermined if X is fat and short, i.e., N < d. This happens whenyouhavemorecolumnsthanrows,orequivalentlywhenyouhavemoreunknownsthan equations. In this case, XTX is not invertible, and so we cannot use θ(cid:98)= (XTX)−1XTy asthesolution.However,ifrank(X)=N,thenany y willliveintherangespaceofX.But because X is fat and short, there exists a nontrivial null space. Therefore, Problem (P2) will have infinitely many solutions, attributed to the vectors generated by the null space. For this set of infinitely many solutions, the corresponding Problem (P1) will have a global minimizer, and the objective value will be zero. However, the minimizer is not unique. This is the first case in Figure 7.10. Figure7.10:Hierarchyofthesolutionsofanunderdeterminedsystem.Anunderdeterminedsystemuses a fat and short matrix X. The rank of a matrix X is defined as the largest number of independent columns we can find in X. If rank(X) = N, we will have infinitely many solutions. If rank(X) < N, thenthesolutionsdependsonwhethertheparticularobservationy livesintherangespaceofX.Ifyes, Problem(P2)willhaveinfinitelymanysolutionsbecausethereisanontrivialnullspace.Ifno,Problem (P2) will have no solution because the system is incompatible. There are two other cases in Figure 7.10, which occur when rank(X)<N: • (i) If y is in the range space of X, Problem (P2) will have infinitely many solutions. Since Problem (P2) remains feasible, the objective function of Problem (P1) will go to zero. • (ii) If y is not in the range space of X, the system in Problem (P2) is incompatible andtherewillbenosolution.TheobjectivevalueofProblem(P1)willnotgotozero. Ifanunderdeterminedsystemhasinfinitelymanysolutions,weneedtopickandchoose. One of the possible approaches is to consider the optimization θ(cid:98)=argmin (cid:107)θ(cid:107)2 subjectto Xθ =y. (P3) θ∈Rd This optimization is different from Problem (P1), which is an unconstrained optimization. OurgoalistominimizethedeviationbetweenXθandy.Problem(P3)isconstrained.Since 411
CHAPTER 7. REGRESSION we assume that Problem (P2) has infinitely many solutions, the constraint set y = Xθ is feasible. Among all the feasible choices, we pick the one that minimizes the squared norm. Therefore, the solution to Problem (P3) is called the minimum-norm least squares. Theorem 7.2 below summarizes the solution. If y does not live in the range space of X, then Problem (P2) does not have a solution. Therefore, the constraint in P3 is infeasible, and hence the optimization problem does not have a minimizer. Theorem7.2. Considerthe underdeterminedlinearregressionproblemwhereN <d: θ(cid:98)=argmin (cid:107)θ(cid:107)2 subject to y =Xθ, θ∈Rd where X ∈ RN×d, θ ∈ Rd, and y ∈ RN. If rank(X) = N, then the linear regression problem will have a unique global minimum θ(cid:98)=XT(XXT)−1y. (7.12) This solution is called the minimum-norm least-squares solution. Proof. The proof of the theorem requires some knowledge of constrained optimization. Consider the Lagrangian of the problem: L(θ,λ)=(cid:107)θ(cid:107)2+λT(Xθ−y), where λ is called the Lagrange multiplier. The solution of the constrained optimization is thestationarypointoftheLagrangian.Tofindthestationarypoint,wetakethederivatives with respect to θ and λ. This yields ∇ L=2θ+XTλ=0, θ ∇ L=Xθ−y =0. λ The first equation gives us θ = −XTλ/2. Substituting it into the second equation, and assuming that rank(X)=N so that XTX is invertible, we have (cid:16) (cid:17) X −XTλ/2 −y =0, which implies that λ=−2(XXT)−1y. Therefore, θ =XT(XXT)−1y. (cid:3) The end of this subsection. Please join us again. 7.1.5 Robust linear regression This subsection is optional for a first reading of the book. The linear regression we have discussed so far is based on an important criterion, namely the squared error criterion. We chose the squared error as the training loss because 412
7.1. PRINCIPLES OF REGRESSION it is differentiable and convex. Differentiability allows us to take the derivative and locate the minimum point. Convexity allows us to claim a global minimizer (also unique if the objective function is strictly convex). However, such a nice criterion suffers from a serious drawback: the issue of outliers. Consider Figure 7.11. In Figure 7.11(a), we show a regression problem for N = 50 datapoints.Ourbasisfunctionsaretheordinarypolynomialsinthefourthorder.Everything looksfineinthefigure.Weinterveneinthedatabyrandomlyalteringafewofthemsothat their values are off. There are only a handful of these outliers. We run the same regression analysis again, but we observe (see Figure 7.11(b)) that our fitted curve has been distorted quite significantly. 4 5 4 3 3 2 2 1 1 0 0 -1 data -1 data fitted curve fitted curve -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) (·)2 without outlier (b) (·)2 with outlier Figure 7.11: Linearregressionusingthesquarederrorasthetraininglosssuffersfromoutliers.(a)The regressionperformswellwhenthereisnooutlier.(b)Byaddingonlyafewoutliers,theregressioncurve has already been distorted. This occurs because of the squared error. By the definition of a squared error, our training loss is N (cid:18) (cid:19)2 (cid:88) E (θ)= y −g (x ) . train n θ n n=1 Withoutlossofgenerality,letusassumethatoneoftheseerrortermsislargebecauseofan outlier. Then the training loss becomes (cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2 E (θ)= y −g (x ) + y −g (x ) + y −g (x ) +···+ y −g (x ) . train 1 θ 1 2 θ 2 3 θ 3 N θ N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) small small large small Here is the daunting fact: If one or a few of these individual error terms are large, the squareoperationwillamplify them.Asaresult,theerroryouseeisnotjustlargebutlarge2. Moreover, since we put the squares to the small errors as well, we have small2 instead of small. When you try to weigh the relative significance between the outliers and the normal datapoints,theoutlierssuddenlyhaveaverylargecontributiontotheerror.Sincethegoal of linear regression is to minimize the total loss, the presence of the outliers will drive the optimization solution to compensate for the large error. 413
CHAPTER 7. REGRESSION One possible solution is to replace the squared error by the absolute error, such that N (cid:12) (cid:12) (cid:88)(cid:12) (cid:12) E train(θ)= (cid:12)y n−g θ(x n)(cid:12). (cid:12) (cid:12) n=1 This is a simple modification, but it is very effective. The reason is that the absolute error keeps the small just as small, and keeps the large just as large. There is no amplification. Therefore, while the outliers still contribute to the overall loss, their contributions are less prominent. (If you have a lot of strong outliers, even the absolute error will fail. If this happens, you should go back to your data collection process and find out what has gone wrong.) Whenweusetheabsoluteerrorasthetrainingloss,theresultingregressionproblemis the least absolute deviation regression (or simply the robust regression). The tricky thing about the least absolute deviation is that the training loss is not differentiable. In other words, we cannot take the derivative and find the optimal solution. The good news is that there exists an alternative approach for solving this problem: using linear programming (implemented via the simplex method). Solving the robust regression problem Let us focus on the linear model g (x )=xTθ, θ n n where x = [φ (x ),...,φ (x )]T ∈ Rd is the nth input vector for some basis functions n 0 n d−1 n {φ }d−1, and θ =[θ ,...,θ ]T ∈Rd is the parameter. Substituting this into the training p p=0 0 d−1 loss, the optimization problem is N (cid:12) (cid:12) mi θn ∈im Rdize (cid:88)(cid:12) (cid:12) (cid:12)y n−xT nθ(cid:12) (cid:12) (cid:12). n=1 Hereisanimportanttrick.Theideaistoexpresstheproblemasanequivalentproblem N (cid:88) minimize u n θ∈Rd,u∈RN n=1 subjectto u =|y −xTθ|, n=1,...,N. n n n Thereisasmallbutimportantdifferencebetweenthisproblemandthepreviousone.Inthe firstproblem,thereisonlyoneoptimizationvariableθ.Inthenewproblem,weintroducean additionalvariableu=[u ,...,u ]T andaddaconstraintu =|y −xTθ|forn=1,...,N. 1 N n n n We introduce u so that we can have some additional degrees of freedom. At the optimal solution, u must equal to |y −xTθ|, and so the corresponding θ is the solution of the n n n original problem. Now we note that x=|a| is equivalent to x≥a and x≥−a. Therefore, the constraint can be equivalently written as N (cid:88) minimize u , (7.13) n θ∈Rd,u∈RN n=1 subjectto u ≥−(y −xTθ), n=1,...,N n n n u ≥(y −xTθ), n=1,...,N. n n n 414
7.1. PRINCIPLES OF REGRESSION In other words, we have rewritten the equality constraint as a pair of inequality constraints by removing the absolute signs. The optimization in Equation (7.13) is in the form of a standard linear programming problem. A linear programming problem takes the form of minimize cTx (7.14) x∈Rk subjectto Ax≤b, for some vectors c ∈ Rk, b ∈ Rm, and matrix A ∈ Rm×k. Linear programming is a stan- dard optimization problem that you can find in most optimization textbooks. On a com- puter, if we know c, b and A, solving the linear programming problem can be done using built-in commands. For MATLAB, the command is linprog. For Python, the command is scipy.optimize.linprog. We will discuss a concrete example shortly. % MATLAB command for linear programming x = linprog(c, A, b); # Python command for linear programming linprog(c, A, b, bounds=(None,None), method="revised simplex") GivenEquation(7.13),thequestionbecomeshowtoconvertitintothestandardlinear programming format. This requires two steps. The first step uses the objective function: N d−1 N (cid:88) (cid:88) (cid:88) u = (0)(θ )+ (1)(u ) n p n n=1 p=0 n=1 (cid:20) (cid:21) (cid:2) (cid:3) θ = 0 0 ··· 0 1 1 ··· 1 . u (cid:124) (cid:123)(cid:122) (cid:125) =cT Therefore, the vector c has d 0’s followed by N 1’s. The second step concerns the constraint. It can be shown that u ≥ −(y −xTθ) is n n n equivalent to xTθ−u ≤y . Written in the matrix form, we have n n n  xT −1 0 ··· 0  θ   y  1 1 xT 2 0 −1 ··· 0 u 1 y 2  . . . .  . ≤ . ,  . . . .  .   .   . . . ··· .  .   .  xT 0 0 ··· −1 u y N N N which is equivalent to (cid:20) (cid:21) (cid:2) (cid:3) θ X −I ≤y, (7.15) u where I ∈RN×N is the identity matrix. Similarly, the other constraint u ≥ (y −xTθ) is equivalent to −xTθ−u ≤ −y . n n n n n n Written in the matrix form, we have  −xT −1 0 ··· 0  θ   −y  1 1 −xT 2 0 −1 ··· 0 u 1 −y 2  . . . .  . ≤ . ,  . . . .  .   .   . . . ··· .  .   .  −xT 0 0 ··· −1 u −y N N N 415
CHAPTER 7. REGRESSION which is equivalent to (cid:20) (cid:21) (cid:2) (cid:3) θ −X −I ≤−y u Puttingeverythingtogether,wehavefinallyarrivedatthelinearprogrammingproblem (cid:20) (cid:21) (cid:2) (cid:3) θ minimize 0 1 θ∈Rd,u∈RN d N u (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) X −I θ y subject to ≤ , −X −I u −y where 0 ∈ Rd is an all-zero vector, and 1 ∈ RN is an all-one vector. It is this problem d N that solves the robust linear regression. Let us look at how to implement linear programming to solve the robust regression optimization. As an example, we continue with the polynomial fitting problem in which there are outliers. We choose the ordinary polynomials as the basis functions. To construct the linear programming problem, we need to define the matrix A and the vectors c and b according to the linear programming form. This is done using the following MATLAB program. % MATLAB code to demonstrate robust regression N = 50; x = linspace(-1,1,N)’; a = [-0.001 0.01 0.55 1.5 1.2]; y = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ... a(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ... a(5)*legendreP(4,x) + 0.2*randn(N,1); idx = [10, 16, 23, 37, 45]; y(idx) = 5; X = [x(:).^0 x(:).^1 x(:).^2 x(:).^3 x(:).^4]; A = [X -eye(N); -X -eye(N)]; b = [y(:); -y(:)]; c = [zeros(1,5) ones(1,N)]’; theta = linprog(c, A, b); t = linspace(-1,1,200)’; yhat = theta(1) + theta(2)*t(:) + ... theta(3)*t(:).^2 + theta(4)*t(:).^3 + ... theta(5)*t(:).^4; plot(x,y, ’ko’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,4); In this set of commands, the basis vectors are defined as xT = [φ (x ),...,φ (x )]T, for n 4 n 0 n n=1,...,N.ThematrixI isconstructedbyusingthecommandeye(N),whichconstructs theidentitymatrixofsizeN×N.Therestofthecommandsareself-explanatory.Notethat the solution to the linear programming problem consists of both θ and u. To squeeze θ we need to locate the first d entries. The remainder is u. CommandsforPythonaresimilar,althoughweneedtocallnp.hstackandnp.vstack toconstructthematricesandvectors.Themainroutineislinproginthescipy.optimize 416
7.1. PRINCIPLES OF REGRESSION library. Note that for this particular example, the bounds are bounds=(None,None), or otherwise Python will search in the positive quadrant. # Python code to demonstrate robust regression import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre from scipy.optimize import linprog N = 50 x = np.linspace(-1,1,N) a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.2*np.random.randn(N) idx = [10,16,23,37,45] y[idx] = 5 X = np.column_stack((np.ones(N), x, x**2, x**3, x**4)) A = np.vstack((np.hstack((X, -np.eye(N))), np.hstack((-X, -np.eye(N))))) b = np.hstack((y,-y)) c = np.hstack((np.zeros(5), np.ones(N))) res = linprog(c, A, b, bounds=(None,None), method="revised simplex") theta = res.x t = np.linspace(-1,1,200) yhat = theta[0]*np.ones(200) + theta[1]*t + theta[2]*t**2 + \ theta[3]*t**3 + theta[4]*t**4 plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=8) plt.show() The result of this experiment is shown in Figure 7.12. It is remarkable to see that the robust regression result is almost as good as the result would be without outliers. If robust linear regression performs so well, why don’t we use it all the time? Why is least squares regression still more popular? The answer has a lot to do with the com- putational complexity and the uniqueness of the solution. Linear programming requires an algorithmforasolution.Whilewehaveveryfastlinearprogrammingsolverstoday,thecom- putational cost of solving a linear program is still much higher than solving a least-squares problem (which is essentially a matrix inversion). The other issue with robust linear regression is the uniqueness of the solution. Lin- ear programming is known to have degenerate solutions when the constraint set (a high- dimensional polygon) touches the objective function (which is a line) at one of its edges. The least-squares fitting does not have this problem because the optimization surface is a parabola. Unless the matrix XTX is noninvertible, the solution is guaranteed to be the unique global minimum. Linear programming does not have this convenient property. We canhavemultiplesolutionsθ thatgivethesameobjectivevalue.Ifyoutrytointerpretyour resultbyinspectingthemagnitudeoftheθ’s,thenonuniquenessofthesolutionwouldcause problems because your interpretation can be swiped immediately if the linear programming gives you a nonunique solution. 417
CHAPTER 7. REGRESSION 5 5 4 4 3 3 2 2 1 1 0 0 -1 -1 -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ordinary (·)2 regression with outliers (b) Robust |·| regression with outliers Figure 7.12: (a)Ordinarylinearregressionusing(·)2 asthetrainingloss.Intheabsenceofanyoutlier, the regression performs well. (b) Robust linear regression using |·| as the training loss. Note that even in the presence of outliers, the robustness regression perform reasonably well. End of this subsection. Please join us again. Closing remark. The principle of linear regression is primarily to set up a function to fit the data. This, in turn, is about finding a set of good basis functions and minimizing the appropriate training loss. Selecting the basis is usually done in several ways: • The problem forces you to choose certain basis functions. For example, suppose you are working on a disease dataset. The variates are height, weight, and BMI. You do nothaveanychoiceherebecauseyourgoalistoseewhichfactorcontributesthemost to the cause of the disease. • There are known basis functions that work. For example, suppose you are working on a speech dataset. Physics tells us that Fourier bases are excellent representations of these sinusoidal functions. So it would make more sense to use the Fourier basis than the polynomials. • Sometimesthebasiscanbelearnedfromthedata.Forexample,youcanrunprincipal- componentanalysis(PCA)toextractthebasis.Thenyoucanrunthelinearregression to compute the coefficients. This is a data-driven approach and could apply to some problems. 7.2 Overfitting The regression principle we have discussed in the previous section is a powerful technique for data analysis. However, there are many ways in which things can fall apart. We have seen the problem of outliers, where perturbations of one or a few data points would result in a big change in the regression result, and we discussed some techniques to overcome the 418
7.2. OVERFITTING outlier problem, e.g., using robust regression. In addition to outliers, there are other causes of the failure of the regression. In this section, we examine the relationship between the number of training samples and the complexity of the model. For example, if we decide to use polynomials as the basis functionsandwehaveonlyN =20datapoints,whatshouldbetheorderofthepolynomials? Shallweusethe5th-orderpolynomial,orshallweusethe20th-order?Ourgoalinthissection is to acquire an understanding of the general problem known as overfitting. Then we will discuss methods for mitigating overfitting in Section 7.4. 7.2.1 Overview of overfitting ImaginethatwehaveadatasetcontainingN =20trainingsamples.Weknowthatthedata are generated from a fourth-order polynomial with Legendre polynomials as the basis. On top of these samples, we also know that a small amount of noise corrupts each sample, for example, Gaussian noise of standard deviation σ =0.1. We have two options here for fitting the data: • Option 1: h(x)=(cid:80)4 θ L (x), which is a 4th-order polynomial. p=0 p p • Option 2: g(x)=(cid:80)50 θ L (x), which is a 50th-order polynomial. p=0 p p Model2ismoreexpressivebecauseithasmoredegreesoffreedom.Letusfitthedatausing these two models. Figure 7.13 shows the results. However, what is going on with the 50th- order polynomial? It has gone completely wild. How can the regression ever choose such a terrible model? 3 3 2 2 1 1 0 0 -1 -1 -2 data -2 data fitted curve fitted curve -3 -3 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) 4th-order polynomial (b) 50th-order polynomial Figure 7.13: Fitting data using a 4th-order polynomial and a 50th-order polynomial. Here is an even bigger surprise: If we compute the training loss, we get 1 (cid:88)N (cid:18) (cid:19)2 E (h)= y −h(x ) =0.0063, train N n n n=1 1 (cid:88)N (cid:18) (cid:19)2 E (g)= y −g(x ) =5.7811×10−24. train N n n n=1 419
CHAPTER 7. REGRESSION Thus,whileModel2lookswildinthefigure,ithasamuchlowertraininglossthanModel1. So according to the training loss, Model 2 fits better. Any sensible person at this point will object, since Model 2 cannot possibly be better, for the following reason. It is not because it “looks bad”, but because if you test the model with an unseen sample it is almost certain that the testing error will explode. For example, in Figure 7.13(a) if we look at x = 0, we would expect the predicted value to be close to y = 0. However, Figure 7.13(b) suggests that the predicted value is going to negative infinity.Itwouldbehardtobelievethatthenegativeinfinityisabetterpredictionthanthe other one. We refer to this general phenomenon of fitting very well to the training data but generalizing poorly to the testing data as overfitting. What is overfitting? Overfitting means that a model fits too closely to the training samples so that it fails to generalize. Overfittingoccursasaconsequenceofanimbalancebetweenthefollowingthreefactors: • Number of training samples N. If you have many training samples, you should learn very well, even if the model is complex. Conversely, if the model is complex but does not have enough training samples, you will overfit it. The most serious problem in regression is often insufficient training data. • Model orderd.Thisreferstothecomplexityofthemodel.Forexample,ifyourmodel uses a polynomial, d refers to the order of the polynomial. If your training set is too small, you need to use a less complex model. The general rule of thumb is: “less is more”. • Noise variance σ2. This refers to the variance of the error e you add to the data. n The model we assumed in the previous numerical experiment is that y =g(x )+e , n=1,...,N. n n n where e ∼ Gaussian(0,σ2). If σ increases, it is inevitable that the fitting will be- n come more difficult. Hence it would require more training samples, and perhaps a less complex model would work better. 7.2.2 Analysis of the linear case Let us spell out the details of these factors one by one. To make our discussion concrete, we will use linear regression as a case study. The general analysis will be presented in the next section. Notations • Ground Truth Model. To start with, we assume that we have a population set D containinginfinitelymanysamples(x,y)drawnaccordingtosomelatentdistributions. The relationship between x and y is defined through an unknown target function y =f(x)+e, 420
7.2. OVERFITTING wheree∼Gaussian(0,σ2)isthenoise.Forouranalysis,weassumethatf(·)islinear, so that f(x)=xTθ∗, whereθ∗ ∈Rd isthegroundtruthmodelparameter.Noticethatf(·)isdeterministic, but e is random. Therefore, any randomness we see in y is due to e. • Training and Testing Set. From D, we construct two datasets: the training data set D that contains training samples {(x ,y ),...,(x ,y )} and the testing dataset train 1 1 N N D that contains {(x ,y ),...,(x ,y )}. Both D and D are subsets of D. test 1 1 M M train test • Predictive Model. We consider a predictive model g (·). For simplicity, we assume θ that g (·) is also linear: θ g (x)=xTθ. θ Given the training dataset D ={(x ,y ),...,(x ,y )}, we construct a linear regres- 1 1 N N sion problem: θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2. θ∈Rd Throughout our analysis, we assume that N ≥ d so that we have more training data than the number of unknowns. We further assume that XTX is invertible, and so there is a unique global minimizer θ(cid:98)=(XTX)−1XTy. • Training Error. Given the estimated model parameter θ(cid:98), we define the in-sample prediction as y (cid:98)train =X trainθ(cid:98), where X = X is the training data matrix. The in-sample prediction is the pre- train dicted value using the trained model for the training data. The corresponding error with respect to the ground truth is called the training error: (cid:20) (cid:21) 1 E train(θ(cid:98))=E e N(cid:107)y (cid:98)train−y(cid:107)2 , where N is the number of training samples in the training dataset. Note that the expectation is taken with respect to the noise vector e, which follows the distribution Gaussian(0,σ2I). • Testing Error. During testing, we construct a testing matrix X . This gives us the test estimated values y : (cid:98)test y (cid:98)test =X testθ(cid:98). Theout-samplepredictionisthepredictedvalueusingthetrainedmodelforthetesting data. The corresponding error with respect to the ground truth is called the testing error: (cid:20) (cid:21) 1 E test(θ(cid:98))=E e M(cid:107)y (cid:98)test−y(cid:107)2 , where M is the number of testing samples in the testing dataset. 421
CHAPTER 7. REGRESSION Analysis of the training error We first analyze the training error, which we defined as (cid:20) (cid:21) 1 E =E (cid:107)y−y(cid:107)2 d =ef MSE(y,y). (7.16) train e N (cid:98) (cid:98) For this particular choice of the training error, we call it the mean squared error (MSE). It measures the difference between y and y. (cid:98) Theorem 7.3. Let θ∗ ∈ Rd be the ground truth linear model parameter, and X ∈ RN×d be a matrix such that N ≥ d and XTX is invertible. Assume that the data follows the linear model y =Xθ∗+e where e∼Gaussian(0,σ2I). Consider the linear regression problem θ(cid:98) = argmin (cid:107)Xθ −y(cid:107)2, and the predicted value y (cid:98) = Xθ(cid:98). The θ∈Rd mean squared training error of this linear model is (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y)=E (cid:107)y−y(cid:107)2 =σ2 1− . (7.17) train (cid:98) e N (cid:98) N The proof below depends on some results from linear algebra that may be difficult for first-time readers. We recommend you read the proof later. Proof. Recall that the least squares linear regression solution is θ(cid:98)=(XTX)−1XTy. Since y =Xθ∗+e, we can substitute this into the predicted value y to show that (cid:98) y (cid:98)=Xθ(cid:98)=X(XTX)−1XTy =X(XTX)−1XT(Xθ∗+e)=Xθ∗+He. (cid:124) (cid:123)(cid:122) (cid:125) =H Therefore, substituting y =Xθ∗+He into the MSE, (cid:98) (cid:20) (cid:21) (cid:20) (cid:21) 1 1 MSE(y,y)d =efE (cid:107)y−y(cid:107)2 =E (cid:107)Xθ∗+He−Xθ∗−e(cid:107)2 (cid:98) e N (cid:98) e N (cid:20) (cid:21) 1 =E (cid:107)(H −I)e(cid:107)2 . e N At this point we need to use a tool from linear algebra. One useful identity3 is that for any v ∈RN, (cid:107)v(cid:107)2 =Tr(vvT). 3Thereasonforthisidentityisthat v= n(cid:88)N =1v n2 =Tr       vv Nv 2 . . .1 v2 v1 1 vv Nv1 . . .2v 2 v2 2 ·· · . ·· · .. ·· · v v1 2 v Nv v . . . 2N N      =Tr(cid:110) vvT(cid:111) . 422
7.2. OVERFITTING Using this identity, we have that (cid:20) (cid:21) (cid:20) (cid:26) (cid:27)(cid:21) 1 1 E (cid:107)(H −I)e(cid:107)2 = E Tr (H −I)eeT(H −I)T e N N e (cid:26) (cid:27) = 1 Tr (H −I)E (cid:2) eeT(cid:3) (H −I)T N e σ2 (cid:26) (cid:27) = Tr (H −I)(H −I)T , N whereweusedthefactthatE[eeT]=σ2I.ThespecialstructureofH tellsusthatHT =H and HTH = H. Thus, we have (H −I)T(H −I) = I −H. In addition, using the cyclic property of trace Tr(AB)=Tr(BA), we have that Tr(H)=Tr(X(XTX)−1XT) =Tr((XTX)−1XTX)=Tr(I)=d. Consequently, σ2 (cid:26) (cid:27) σ2 (cid:26) (cid:27) Tr (H −I)(H −I)T = Tr I−H N N (cid:18) (cid:19) d =σ2 1− . N This completes the proof. (cid:3) The end of the proof. Please join us again. Practice Exercise 1. In the theorem above, we proved the MSE of the prediction y. In this example, we would like to prove the MSE for the parameter. Prove that (cid:20)(cid:13) (cid:13)2(cid:21) (cid:26) (cid:27) MSE(θ(cid:98),θ∗)d =efE e (cid:13) (cid:13)θ(cid:98)−θ∗(cid:13) (cid:13) =σ2Tr (XTX)−1 . Solution. Let us start with the definition: (cid:20)(cid:13) (cid:13)2(cid:21) MSE(θ(cid:98),θ∗)=E e (cid:13) (cid:13)(XTX)−1XTy−θ∗(cid:13) (cid:13) (cid:20)(cid:13) (cid:13)2(cid:21) =E (cid:13)(XTX)−1XT(Xθ∗+e)−θ∗(cid:13) e (cid:13) (cid:13) (cid:20)(cid:13) (cid:13)2(cid:21) (cid:20)(cid:13) (cid:13)2(cid:21) =E (cid:13)θ∗+(XTX)−1XTe−θ∗(cid:13) =E (cid:13)(XTX)−1XTe(cid:13) . e (cid:13) (cid:13) e (cid:13) (cid:13) 423
CHAPTER 7. REGRESSION Continuing the calculation, (cid:20)(cid:13) (cid:13)2(cid:21) (cid:20) (cid:26) (cid:27)(cid:21) E (cid:13)(XTX)−1XTe(cid:13) =E Tr (XTX)−1XTe eTX(XTX)−1 e (cid:13) (cid:13) e (cid:26) (cid:20) (cid:21) (cid:27) =Tr (XTX)−1XTE eeT X(XTX)−1 e (cid:26) (cid:27) =Tr (XTX)−1XT · σ2I · X(XTX)−1 (cid:26) (cid:27) (cid:26) (cid:27) =σ2Tr (XTX)−1XT · X(XTX)−1 =σ2Tr (XTX)−1 . Analysis of the testing error Similarlytothetrainingerror,wecananalyzethetestingerror.Thetestingerrorisdefined as (cid:20) (cid:21) 1 E =MSE(y,y(cid:48))d =efE (cid:107)y−y(cid:48)(cid:107)2 , (7.18) test (cid:98) e,e(cid:48) M (cid:98) wherey =[y ,...,y ]T isavectorofM predictedvaluesandy(cid:48) =[y(cid:48),...,y(cid:48) ]T isavector (cid:98) (cid:98)1 (cid:98)M 1 M of M true values in the testing data.4 We would like to derive something concrete. To make our analysis simple, we consider a special case in which the testing set contains (x ,y(cid:48)),...,(x ,y(cid:48) ). That is, the inputs 1 1 N N x ,...,x areidenticalforbothtrainingandtesting(forexample,supposethatyoumeasure 1 N the temperature on two different days but at the same time stamps.) In this case, we have M =N, and we have X =X =X. However, the noise added to the testing data is test train still different from the noise added to the training data. With these simplifications, we can derive the testing error as follows. Theorem 7.4. Let θ∗ ∈ Rd be the ground truth linear model parameter, and X ∈ RN×d be a matrix such that N ≥d and XTX is invertible. Assume that the training data follows the linear model y = Xθ∗ +e, where e ∼ Gaussian(0,σ2I). Consider the linear regression problem θ(cid:98)= (XTX)−1XTy, and let y (cid:98) = Xθ(cid:98). Let X test = X be the testing input data matrix, and define y(cid:48) = X θ∗ + e(cid:48) ∈ RN, with e(cid:48) ∼ test Gaussian(0,σ2I), be the testing output. Then, the mean squared testing error of this linear model is (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ . (7.19) test (cid:98) e,e(cid:48) N (cid:98) N In this definition, the expectation is taken with respect to a joint distribution of (e,e(cid:48)). This is because, in testing, the trained model is based on y of which the randomness is e. However, the testing data is based on y(cid:48), where the randomness comes from e(cid:48). We assume that e and e(cid:48) are independent i.i.d. Gaussian vectors. 4Inpractice,thenumberoftestingsamplesMcanbemuchlargerthanthenumberoftrainingsamplesN. Thisprobablydoesnotagreewithyourexperience,inwhichthetestingdatasetisoftenmuchsmallerthan thetrainingdataset.Thereasonforthisparadoxisthatthepracticaltestingdatasetisonlyafinitesubset ofallthepossibletestingsamplesavailable.Sothe“testingerror”wecomputeinpracticeapproximatesthe truetestingerror.Ifyouwanttocomputethetruetestingerror,youneedaverylargetestingdataset. 424
7.2. OVERFITTING As with the previous proof, we recommend you study this proof later. Proof. The MSE can be derived from the definition: (cid:20) (cid:21) 1 MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 (cid:98) e,e(cid:48) N (cid:98) (cid:20) (cid:21) 1 = E (cid:107)Xθ∗+He−Xθ∗−e(cid:48)(cid:107)2 N e,e(cid:48) (cid:20) (cid:21) 1 = E (cid:107)He−e(cid:48)(cid:107)2 . N e,e(cid:48) Since each noise term e and e(cid:48) is an i.i.d. copy of the same Gaussian random variable, by n n using the fact that Tr(H)=Tr(X(XTX)−1XT) =Tr((XTX)−1XTX)=Tr(I)=d, we have that E (cid:104) (cid:107)He−e(cid:48)(cid:107)2(cid:105) =E (cid:2) (cid:107)He(cid:107)2(cid:3) −E (cid:104) 2eTHTe(cid:48)(cid:105) +E (cid:2) (cid:107)e(cid:48)(cid:107)2(cid:3) e,e(cid:48) e e,e(cid:48) e(cid:48) (cid:124) (cid:123)(cid:122) (cid:125) =0 =E (cid:104) Tr(cid:110) HeeTHT(cid:111)(cid:105) +E (cid:2) Tr(cid:8) e(cid:48)e(cid:48)T(cid:9)(cid:3) e e(cid:48) =Tr(cid:110) HE (cid:2) eeT(cid:3) HT(cid:111) +Tr{E (cid:2) e(cid:48)e(cid:48)T(cid:3) } e e(cid:48) =Tr(cid:110) H ·σ2I ·HT(cid:111) +Tr(cid:8) σ2I (cid:9) N×N N×N =σ2Tr(cid:110) HHT(cid:111) +Tr(cid:8) σ2I (cid:9) N×N =σ2Tr(I )+σ2Tr{I }=σ2(d+N). d×d N×N Combining all the terms, (cid:20) (cid:21) (cid:18) (cid:19) 1 d MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ , (cid:98) e,e(cid:48) N (cid:98) N which completes the proof. (cid:3) The end of the proof. 7.2.3 Interpreting the linear analysis results Let us summarize the two main theorems. They state that, for N ≥d, (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y)=E (cid:107)y−y(cid:107)2 =σ2 1− , (7.20) train (cid:98) e N (cid:98) N (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ . (7.21) test (cid:98) e,e(cid:48) N (cid:98) N This pair of equations tells us everything about the overfitting issue. 425
CHAPTER 7. REGRESSION How do E and E change w.r.t. σ2? train test • E ↑ as σ2 ↑. Thus noisier data are harder to fit. train • E ↑ as σ2 ↑. Thus a noiser model is more difficult to generalize. test The reasons for these results should be clear from the following equations: (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . test N As σ2 increases, the training error E grows linearly w.r.t. σ2. Since the training error train measures how good your model is compared with the training data, a larger E means it train is more difficult to fit. For the testing case, E also grows linearly w.r.t. σ2. This implies test that the model would be more difficult to generalize if the model were trained using noisier data. How do E and E change w.r.t. N? train test • E ↑ as N ↑. Thus more training samples make fitting harder. train • E ↓ as N ↑. Thus more training samples improve generalization. test The reason for this should also be clear from the following equations: (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . test N AsN increases,themodelseesmoretrainingsamples.Thegoalofthemodelistominimize theerrorwithallthetrainingsamples.Thusthemoretrainingsampleswehave,theharder itwillbetomakeeveryonehappy,sothetrainingerrorgrowsasN grows.Fortesting,ifthe model is trained with more samples it is more resilient to noise. Hence the generalization improves. How do E and E change w.r.t. d? train test • E ↓ as d ↑. Thus a more complex model makes fitting easier. train • E ↑ as d ↑. Thus a more complex model makes generalization harder. test These results are perhaps less obvious than the others. The following equations tell us that (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . (7.22) test N 426
7.2. OVERFITTING For this linear regression model to work, d has to be less than N; otherwise, the matrix inversion (XTX)−1 is invalid. However, as d grows while N remains fixed, we ask the linearregressiontofitalargerandlargermodelwhilenotprovidinganyadditionaltraining samples. Equation (7.22) says that E will drop as d increases but E will increase as d train test increases. Therefore, a larger model will not generalize as well if N is fixed. If d>N, then the optimization θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2 θ∈Rd willhavemanyglobalminimizers(seeFigure 7.10),implyingthatthetrainingerrorcango to zero. Our analysis of E and E does not cover this case because our proofs require train test (XTX)−1 to exist. However, we can still extrapolate what will happen. When the training error is zero, it only means that we fit perfectly into the training data. Since the testing error grows as d grows (though not in the particular form shown in Equation (7.22)), we should expect the testing error to become worse. Learning curve TheresultswederivedabovecanbesummarizedinthelearningcurveshowninFigure7.14. In this figure we consider a simple problem where y =θ +θ x +e , n 0 1 n n for e ∼Gaussian(0,1). Therefore, according to our theoretical derivations, we have σ =1 n andd=2.ForeveryN,wecomputetheaveragetrainingerrorE andtheaveragetesting train error E , and then mark them on the figure. These are our empirical training and testing test errors. On the same figure, we calculate the theoretical training and testing error according to Equation (7.22). TheMATLABandPythoncodesusedtogeneratethislearningcurveareshownbelow. Nset = round(logspace(1,3,20)); E_train = zeros(1,length(Nset)); E_test = zeros(1,length(Nset)); a = [1.3, 2.5]; for j = 1:length(Nset) N = Nset(j); x = linspace(-1,1,N)’; E_train_temp = zeros(1,1000); E_test_temp = zeros(1,1000); X = [ones(N,1), x(:)]; for i = 1:1000 y = a(1) + a(2)*x + randn(size(x)); y1 = a(1) + a(2)*x + randn(size(x)); theta = X\y(:); yhat = theta(1) + theta(2)*x; E_train_temp(i) = mean((yhat(:)-y(:)).^2); E_test_temp(i) = mean((yhat(:)-y1(:)).^2); end E_train(j) = mean(E_train_temp); 427
CHAPTER 7. REGRESSION E_test(j) = mean(E_test_temp); end semilogx(Nset, E_train, ’kx’, ’LineWidth’, 2, ’MarkerSize’, 16); hold on; semilogx(Nset, E_test, ’ro’, ’LineWidth’, 2, ’MarkerSize’, 8); semilogx(Nset, 1-2./Nset, ’k’, ’LineWidth’, 4); semilogx(Nset, 1+2./Nset, ’r’, ’LineWidth’, 4); import numpy as np import matplotlib.pyplot as plt Nset = np.logspace(1,3,20) Nset = Nset.astype(int) E_train = np.zeros(len(Nset)) E_test = np.zeros(len(Nset)) for j in range(len(Nset)): N = Nset[j] x = np.linspace(-1,1,N) a = np.array([1, 2]) E_train_tmp = np.zeros(1000) E_test_tmp = np.zeros(1000) for i in range(1000): y = a[0] + a[1]*x + np.random.randn(N) X = np.column_stack((np.ones(N), x)) theta = np.linalg.lstsq(X, y, rcond=None)[0] yhat = theta[0] + theta[1]*x E_train_tmp[i] = np.mean((yhat-y)**2) y1 = a[0] + a[1]*x + np.random.randn(N) E_test_tmp[i] = np.mean((yhat-y1)**2) E_train[j] = np.mean(E_train_tmp) E_test[j] = np.mean(E_test_tmp) plt.semilogx(Nset, E_train, ’kx’) plt.semilogx(Nset, E_test, ’ro’) plt.semilogx(Nset, (1-2/Nset), linewidth=4, c=’k’) plt.semilogx(Nset, (1+2/Nset), linewidth=4, c=’r’) The training error curve and the testing error curve behave in opposite ways as N increases. The training error E increases as N increases, because when we have more train trainingsamplesitbecomesharderforthemodeltofitallthedata.Bycontrast,thetesting errorE decreasesasN increases,becausewhenwehavemoretrainingsamplesthemodel test becomes more robust to noise and unseen data. Therefore, the testing error improves. As N goes to infinity, both the training error and the testing error converge. This is due to the law of large numbers, which says that the empirical training and testing errors shouldconvergetotheirrespectiveexpectedvalues.Ifthetrainingerrorandthetestingerror convergetothesamevalue,thetrainingcangeneralizetotesting.Iftheydonotconvergeto the same value, there is a mismatch between the training samples and the testing samples. It is important to pay attention to the gap between the converged values. We often assume that the training samples and the testing samples are drawn from the same distri- bution, and therefore the training samples are good representatives of the testing samples. 428
7.3. BIAS AND VARIANCE TRADE-OFF 1.2 1.15 1.1 1.05 1 0.95 0.9 0.85 0.8 101 102 103 Number of training samples, N rorrE Training Error Testing Error Figure 7.14: The learning curve is a pair of functions representing the training error and the testing error.AsN increasesweexpectthetrainingerrortoincreaseandthetestingerrortodecrease.Thetwo functions will converge to the same value as N goes to infinity. If they do not converge to the same value, there is an intrinsic mismatch between the training samples and the testing samples, e.g., the training samples are not representative enough for the dataset. If the assumption is not true, there will be a gap between the converged training error and the testing error. Thus, what you claim in training cannot be transferred to the testing. Consequently, the learning curve provides you with a useful debugging tool to check how well your training compares with your testing. Closing remark. In this section we have studied a very important concept in regression, overfitting.Weemphasizethatoverfittingisnotonlycausedbythecomplexityofthemodel but a combination of the three factors σ2, N, and d. We close this section by summarizing the causes of overfitting: What is the source of overfitting? • Overfitting occurs because you have an imbalance between σ2, N and d. • Selecting the correct complexity for your model is the key to avoid overfitting. 7.3 Bias and Variance Trade-Off Our linear analysis has provided you with a rough understanding of what we experience in overfitting. However, for general regression problems where the models are not necessarily linear, we need to go deeper. The goal of this section is to explain the trade-off between bias and variance. This analysis requires some patience as it involves many equations. We recommend skipping this section on a first reading and then returning to it later. 429
CHAPTER 7. REGRESSION If it is your first time reading it, we recommend you go through it slowly. 7.3.1 Decomposing the testing error Notations As we did at the beginning of Section 7.2, we consider a ground truth model that relates an input x and an output y: y =f(x)+e, where e∼Gaussian(0,σ2) is the noise. For example, if we use a linear model, then f could be f(x)=θTx, for some regression coefficients θ. During training, we pick a prediction model g (·) and try to predict the output when θ given a training sample x: y =g (x). (cid:98) θ Forexample,wemaychooseg (x)=θTx,whichisalsoalinearmodel.Wemayalsochoose θ a linear model in another basis, e.g., g (x)=θTφ(x) for some transformations φ(·). In any θ case, the goal of training is to minimize the training error: N θ(cid:98)=argmin N1 (cid:88) (g θ(x n)−y n)2, θ n=1 wherethesumistakenoverthetrainingsamplesD ={(x ,y ),...,(x ,y )}.Because train 1 1 N N the model parameter θ(cid:98) is learned from the training dataset D train, the prediction model depends on D . To emphasize this dependency, we write train (cid:26) (cid:27) g(Dtrain) =the model trained from (x ,y ),...,(x ,y ) . 1 1 N N Duringtesting,weconsideratestingdatasetD ={(x(cid:48),y(cid:48)),...,(x(cid:48) ,y(cid:48) )}.Weput test 1 1 M M these testing samples into the trained model to predict an output: y(cid:48) =g(Dtrain)(x(cid:48) ), m=1,...,M. (predicted value) (cid:98)m m Since the goal of regression is to make g(Dtrain) as close to f as possible, it is natural to expect y(cid:48) to be close to y(cid:48) . (cid:98)m m Testing error decomposition (noise-free) So we can now compute the testing error — the error that we ultimately care about. In the noise-free condition, i.e., e=0, the testing error is defined as E t( eD sttrain) =E x(cid:48)(cid:104)(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2(cid:105) (7.23) 1 (cid:88)M (cid:18) (cid:19)2 ≈ g(Dtraing)(x(cid:48) )−f(x(cid:48) ) . M m m m=1 There are several components in this equation. First, x(cid:48) is a testing sample drawn from a certain distribution. You can think of D as a finite subset drawn from this distribution. test 430
7.3. BIAS AND VARIANCE TRADE-OFF Second,theerror(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2 measuresthedeviationbetweenourpredictedvalue and the true value. Note that this error term is specific to one testing sample x(cid:48). Therefore, we take expectation E to find the average of the error for the distribution of x(cid:48). x(cid:48) The testing error E(Dtrain) is a function that is dependent on the training set D , test train because the model g(Dtrain) is trained from D train. Therefore, as we change the training set, we will have a different model g and hence a different testing error. To eliminate the randomness of the training set, we define the overall testing error as (cid:20) (cid:21) E =E E(Dtrain) test Dtrain test =E Dtrain(cid:20) E x(cid:48)(cid:104)(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2(cid:105)(cid:21) . (7.24) Note that this definition of the testing error is consistent with the special case in Equa- tion (7.18), in which the testing error involves a joint expectation over e and e(cid:48). The ex- pectationovereaccountsforthetrainingsamples,andtheexpectationovere(cid:48) accountsfor the testing samples. Let us try to extract some meaning from the testing error. Our method will be to decompose the testing error into bias and variance. Theorem 7.5. Assume a noise-free condition. The testing error of a regression prob- lem is given by (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2+E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] , (7.25) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) where g(x(cid:48))d =efE Dtrain[g(Dtrain)(x(cid:48))]. Proof. To simplify our notation, we will drop the subscript “train” in D when the train context is clear. We have that (cid:104) (cid:104) (cid:105)(cid:105) E =E E (g(D)(x(cid:48))−f(x(cid:48)))2 test D x(cid:48) (cid:104) (cid:104) (cid:105)(cid:105) =E E (g(D)(x(cid:48))−f(x(cid:48)))2 . x(cid:48) D Continuing the calculation, (cid:104) (cid:104) (cid:105)(cid:105) E =E E (g(D)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48)))2 test x(cid:48) D (cid:20) (cid:104) (cid:105) (cid:104) (cid:105) =E E (g(D)(x(cid:48))−g(x(cid:48)))2 +2E (g(D)(x(cid:48))−g(x(cid:48)))(g(x(cid:48))−f(x(cid:48))) x(cid:48) D D (cid:104) (cid:105)(cid:21) +E (g(x(cid:48))−f(x(cid:48)))2 . D Since g(x(cid:48))d =efE [g(D)(x(cid:48))], it follows that D (cid:104) (cid:105) 2E (g(D)(x(cid:48))−g(x(cid:48)))(g(x(cid:48))−f(x(cid:48))) =0 D 431
CHAPTER 7. REGRESSION because g(x(cid:48))−f(x(cid:48)) is independent of D, and (cid:104) (cid:105) E (g(x(cid:48))−f(x(cid:48)))2 =(g(x(cid:48))−f(x(cid:48)))2. D Therefore, (cid:20) (cid:104) (cid:105) (cid:104) (cid:105)(cid:21) E =E E (g(D)(x(cid:48))−g(x(cid:48)))2 + (g(x(cid:48))−f(x(cid:48)))2 . test x(cid:48) D Thus, by defining two following terms we have proved the theorem. bias(x(cid:48))d =ef (g(x(cid:48))−f(x(cid:48)))2, var(x(cid:48))d =efE [(g(D)(x(cid:48))−g(x(cid:48)))2]. D (cid:3) Let’s consider what this theorem implies. This result is a decomposition of the testing error into bias and variance. It is a universal result that applies to all regression models, not only linear cases. To summarize the meanings of bias and variance: What are bias and variance? • Bias = how far your average is from the truth. • Variance = how much fluctuation you have around the average. Figure 7.15 gives a pictorial representation of bias and variance. In this figure, we construct four scenarios of bias and variance. Each cross represents the predictor g(Dtrain), with the true predictor f at the origin. Figure 7.15(a) shows the case with a low bias and a low variance. All these predictors g(Dtrain) are very close to the ground truth, and they have small fluctuations around their average. Figure 7.15(b) shows the case of a high bias and a low variance. It has a high bias because the entire group of g(Dtrain) is shifted to the corner.Thebias,whichisthedistancefromthetruthtotheaverage,isthereforelarge.The variance remains small because the fluctuation around the average is small. Figure 7.15(c) shows the case of a low bias but high variance. In this case, the fluctuation around the average is large. Figure 7.15 shows the case of high bias and high variance. We want to avoid this case. Bias low Bias high Bias low Bias high Var low Var low Var high Var high (a) (b) (c) (d) Figure7.15:Imaginethatyouarethrowingadartwithatargetatthecenter.Thefoursubfiguresshow the levels of bias and variance. 432
7.3. BIAS AND VARIANCE TRADE-OFF Testing error decomposition (noisy case) Let us consider a situation when there is noise. In the presence of noise, the training and testing samples will follow the relationship y =f(x)+e, where e∼Gaussian(0,σ2). We assume that the noise is Gaussian to make the proof easier. We can consider other types of noise in theory, but the theoretical results will need to be modified. In the presence of noise, the testing error is (cid:20)(cid:16) (cid:17)2(cid:21) E (x(cid:48))d =efE g(Dtrain)(x(cid:48))−f(x(cid:48))+e test Dtrain,e (cid:20)(cid:16) (cid:17)2(cid:21) =E g(Dtrain)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48))+e , Dtrain,e where we take the joint expectation over the training dataset D and the error e. Con- train tinuingthecalculation,andusingthefactthatD andeareindependent(andE[e]=0), train it follows that (cid:20)(cid:16) (cid:17)2(cid:21) E (x(cid:48))=E g(Dtrain)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48))+e test Dtrain,e (cid:20)(cid:16) (cid:17)2 (cid:16) (cid:17)2 (cid:21) =E g(D)(x(cid:48))−g(x(cid:48)) + g(x(cid:48))−f(x(cid:48)) +e2 Dtrain,e (cid:20)(cid:16) (cid:17)2(cid:21) (cid:16) (cid:17)2 (cid:104) (cid:105) =E g(Dtrain)(x(cid:48))−g(x(cid:48)) + g(x(cid:48))−f(x(cid:48)) +E e2 . Dtrain e (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =var(x(cid:48)) =bias(x(cid:48)) =noise Taking the expectation of x(cid:48) over the entire testing distribution gives us E =E [E (x(cid:48))]=E [var(x(cid:48))]+E [bias(x(cid:48))]+σ2. test x(cid:48) test x(cid:48) x(cid:48) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) var bias The theorem below summarizes the results: Theorem 7.6. Assume a noisy condition where y =f(x)+e for some i.i.d. Gaussian noise e∼Gaussian(0,σ2). The testing error of a regression problem is given by (cid:20) (cid:21) (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2 +E x(cid:48) E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] +σ2, (7.26) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) where g(x(cid:48))d =efE Dtrain[g(Dtrain)(x(cid:48))]. 7.3.2 Analysis of the bias Letusexaminethebiasandvarianceinmoredetail.Todiscussbiaswemustfirstunderstand the quantity g(x(cid:48))d =efE [g(Dtrain)(x(cid:48))], (7.27) Dtrain 433
CHAPTER 7. REGRESSION whichisknownastheaverage predictor.Theaveragepredictor,astheequationsuggests,is theexpectationofthepredictorg(Dtrain).Rememberthatg(Dtrain) isapredictorconstructed from a specific training set D . If tomorrow our training set D contains other data train train (that come from the same underlying distribution), g(Dtrain) will be different. The average predictor g is the average across these random fluctuations of the dataset D . Here is an train example: Suppose we use a linear model with the ordinary polynomials as the bases. The data points are generated according to d−1 (cid:88) y = θ xp +e . (7.28) n p n n p=0 (cid:124) (cid:123)(cid:122) (cid:125) d=eff(xn)=θTxn IfweuseaparticulartrainingsetD andruntheregression,wewillbeabletoobtain train one of the regression lines, as shown in Figure 7.16. Let us call this line g(1). We repeat the experimentbydrawinganotherdataset,andcallitg(2).Wecontinueandeventuallywewill findasetofregressionlinesg(1),g(2),...,g(K),whereK denotesthenumberoftrainingsets you are using to generate all the gray curves. The average predictor g is defined as K 1 (cid:88) g(x(cid:48))=E [g(Dtrain)]≈ g(k)(x(cid:48)). Dtrain K k=1 Thus if we take the average of all these gray curves we will obtain the average predictor, which is the red curve shown in Figure 7.16. 3 2 1 0 -1 -1 -0.5 0 0.5 1 Figure 7.16: We run linear regression many times for different training datasets. Each one consists of different random realizations of noise. The gray curves are the regression lines returned by each of the training datasets. We then take the average of these gray curves to obtain the red curve, which is the average predictor. Ifyouarecuriousabouthowthisplotwasgenerated,theMATLABandPythoncodes are given below. % MATLAB code to visualize the average predictor N = 20; 434
7.3. BIAS AND VARIANCE TRADE-OFF a = [5.7, 3.7, -3.6, -2.3, 0.05]; x = linspace(-1,1,N); yhat = zeros(100,50); for i=1:100 X = [x(:).^0, x(:).^1, x(:).^2, x(:).^3, x(:).^4]; y = X*a(:) + 0.5*randn(N,1); theta = X\y(:); t = linspace(-1, 1, 50); yhat(i,:) = theta(1) + theta(2)*t(:) + theta(3)*t(:).^2 ... + theta(4)*t(:)^3 + theta(5)*t(:).^4; end figure; plot(t, yhat, ’color’, [0.6 0.6 0.6]); hold on; plot(t, mean(yhat), ’LineWidth’, 4, ’color’, [0.8 0 0]); axis([-1 1 -2 2]); import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre np.set_printoptions(precision=2, suppress=True) N = 20 x = np.linspace(-1,1,N) a = np.array([0.5, -2, -3, 4, 6]) yhat = np.zeros((50,100)) for i in range(100): y = a[0] + a[1]*x + a[2]*x**2 + \ a[3]*x**3 + a[4]*x**4 + 0.5*np.random.randn(N) X = np.column_stack((np.ones(N), x, x**2, x**3, x**4)) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(-1,1,50) Xhat = np.column_stack((np.ones(50), t, t**2, t**3, t**4)) yhat[:,i] = np.dot(Xhat, theta) plt.plot(t, yhat[:,i], c=’gray’) plt.plot(t, np.mean(yhat, axis=1), c=’r’, linewidth=4) We now show an analytic calculation to verify Figure 7.16. Example 7.4. Consider a linear model such that y =xTθ+e. (7.29) What is the predictor g(Dtrain)(x(cid:48))? What is the average predictor g(x(cid:48))? Solution. First, consider a training dataset D = {(x ,y ),...,(x ,y )}. We train 1 1 N N assume thatthe x ’sare deterministicand fixed.Therefore, thesource ofrandomness n in the training set is caused by the noise e ∼ Gaussian(0,σ2) and hence by the noisy 435
CHAPTER 7. REGRESSION observation y. The training set gives us the equation y = Xθ + e, where X is the matrix constructed from x ’s. The regression solution to this dataset is n θ(cid:98)=(XTX)−1XTy, which should actually be θ(cid:98)(Dtrain) because y is a dataset-dependent vector. Consequently, g(Dtrain)(x(cid:48))=θ(cid:98)T x(cid:48) =(x(cid:48))T(XTX)−1XTy =(x(cid:48))T(XTX)−1XT(Xθ+e) =(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe. Since the randomness of D is caused by the noise, it follows that train g(x(cid:48))=E [g(Dtrain)(x(cid:48))]=E [(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe] Dtrain e =(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTE [e] e =(x(cid:48))Tθ+0=f(x(cid:48)). So the average predictor will return the ground truth. However, note that not all predictors will return the ground truth. In the above example, we obtained an interesting result, namely that g(x(cid:48)) = f(x(cid:48)). That is, the average predictor equals the true predictor. However, in general, g(x(cid:48)) does not necessarily equal f(x(cid:48)). If this occurs, we have a deviation (g(x(cid:48))−f(x(cid:48)))2 > 0. This deviation is called the bias. Bias is independent of the number of training samples because we have taken the average of the predictors. Therefore, bias is more of an intrinsic (or systematic) error due to the choice of the model. What is bias? • Bias is defined as bias=E [(g(x(cid:48))−f(x(cid:48)))2], where x(cid:48) is a testing sample. x(cid:48) • It is the deviation from the average predictor to the true predictor. • Bias is not necessarily a bad thing. A good predictor can have some bias as long as it helps to reduce the variance. 7.3.3 Variance The other quantity in the game is the variance. Variance at a testing sample x(cid:48) is defined as var(x(cid:48))d =efE [(g(Dtrain)(x(cid:48))−g(x(cid:48)))2]. (7.30) Dtrain As the equation suggests, the variance measures the fluctuation between the predictor g(Dtrain) and the average predictor g. Figure 7.17 illustrates the polynomial-fitting prob- lem we discussed above. In this figure we consider two levels of variance by varying the 436
7.3. BIAS AND VARIANCE TRADE-OFF noisestrengthofe .Thefigureshowsthatastheobservationbecomesnoisier,thepredictor n g(Dtrain) will have a larger fluctuation for the average predictor. 2 2 1 1 0 0 -1 -1 -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) small variance (b) large variance Figure 7.17: Variance measures the magnitude of fluctuation between the particular predictor g(Dtrain) and the average predictor g. Example 7.5. Continuing with Example 7.4, we ask: What is the variance? Solution. We first determine the predictor and its average: g(Dtrain) =(XTX)−1XTy =θ+(XTX)−1XTe g =E[g(Dtrain)]=E [θ+(XTX)−1XTe]=θ, e so the prediction at a testing sample x(cid:48) is g(Dtrain)(x(cid:48))=(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe g(x(cid:48))=(x(cid:48))Tθ, Consequently, the variance is (cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21) E g(Dtrain)(x(cid:48))−g(x(cid:48)) =E (x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe−(x(cid:48))Tθ Dtrain e (cid:20)(cid:16) (cid:17)2(cid:21) =E (x(cid:48))T(XTX)−1XTe . e Continuing the calculation, (cid:20)(cid:16) (cid:17)2(cid:21) E g(Dtrain)(x(cid:48))−g(x(cid:48)) =(x(cid:48))T(XTX)−1XTE [eeT]X(XTX)−1x(cid:48) Dtrain e =(x(cid:48))T(XTX)−1XTσ2IX(XTX)−1x(cid:48) =σ2(x(cid:48))T(XTX)−1x(cid:48) (cid:110) (cid:111) =σ2Tr (XTX)−1(x(cid:48))(x(cid:48))T . 437
CHAPTER 7. REGRESSION What will happen if we use more samples so that N grows? As N grows, the matrix X will havemorerows.Assumingthatthemagnitudeoftheentriesremainsunchanged,morerows in X will increase the magnitude of XTX because we are summing more terms. Consider a 2×2 ordinary polynomial system where (cid:80)N x2 (cid:80)N x  n=1 n n=1 n XTX = . (cid:80)N x N n=1 n As N grows, all the entries in the matrix grow. As a result, (XTX)−1 will shrink in mag- (cid:110) (cid:111) nitude and thus drive the variance σ2Tr (XTX)−1(x(cid:48))(x(cid:48))T to zero. What is variance? • Variance is the deviation between the predictor g(Dtrain) and its average g. • It can be reduced by using more training samples. 7.3.4 Bias and variance on the learning curve The decomposition of the testing error into bias and variance is portrayed visually by the learning curve shown in Figure 7.18. This figure shows the testing error and the training error as functions of the number of training samples. As N increases, we observe that both testing and training errors converge to the same value. At any fixed N, the testing error is composed of bias and variance: • The bias is the distance from the ground to the steady-state level. This value is fixed and is a constant w.r.t. N. In other words, regardless of how many training samples you have, the bias is always there. It is the best outcome you can achieve. • The variance is the fluctuation from the steady-state level to the instantaneous state. It drops as N increases. Figure7.18:Thelearningcurvecanbedecomposedintothesumofthebiasandthevariance.Thebias is the testing error when N =∞. For finite N, the difference between the testing error and the bias is the variance. 438
7.3. BIAS AND VARIANCE TRADE-OFF Figure 7.19 compares the learning curve of two models. The first case requires us to fit the data using a simple model (marked in purple). The training error and the testing error have small fluctuations around the steady-state because, for simple models, you need only a small number of samples to make the model happy. The second case requires us to fit the data using a complex model (marked in green). This set of curves has a much wider fluctuation because it is harder to train and harder to generalize. However, when we have enough training samples, the training error and the testing error will converge to a lower steady-state value. Therefore, you need to pay the price of using a complex model, but if you do, you will enjoy a lower testing error. 2.5 2 1.5 1 0.5 0 101 102 103 Number of training samples, N rorrE Simple Model - Training Error Simple Model - Testing Error Complex Model - Training Error Complex Model - Testing Error Figure 7.19: The generalization capability of a model is summarized by the training and testing errors ofthemodel.Ifweuseasimplemodelwewillhaveaneasiertimewiththetrainingbutthesteady-state testing error will be high. In contrast, if we use a complex model we need to have a sufficient number of training samples to train the model well. However, when the complex model is well trained, the steady-state error will be lower. The implication of all this is that you should choose the model by considering the number of data points. Never buy an expensive toy when you do not have the money! If you insist on using a complex model while you do not have enough training data, you will suffer from a poor testing error even if you feel good about it. Closing remark. We close this section by revisiting the bias-variance trade-off: (cid:20) (cid:21) (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2 +E x(cid:48) E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] +σ2. (7.31) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) The relationship among the three terms is summarized below: What is the trade-off offered by the bias-variance analysis? • Overfitting improves if N ↑: Variance drops as N grows. Bias is unchanged. • Overfitting worsens if σ2 ↑. If training noise grows, g(Dtrain) will have more fluc- tuations, so variance will grow. If testing noise grows, e2 grows. 439
CHAPTER 7. REGRESSION • Overfittingworsensifthetargetfunctionf istoocomplicatedtobeapproximated by g. End of the section. Please join us again. 7.4 Regularization Having discussed the source of the overfitting problem, we now discuss methods to allevi- ate overfitting. The method we focus on here is regularization. Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term to force the parameters to“behave better”. As a preview of the technique, we change the original training loss N (cid:18) d−1 (cid:19)2 (cid:88) (cid:88) E (θ)= y − θ φ (x ) , (7.32) train n p p n n=1 p=0 (cid:124) (cid:123)(cid:122) (cid:125) datafidelity which consists of only the data fidelity term, to a modified training loss N (cid:18) d−1 (cid:19)2 d−1 (cid:88) (cid:88) (cid:88) E (θ)= y − θ φ (x ) + λ· θ2 . (7.33) train n p p n p n=1 p=0 p=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) F(θ),datafidelity λ·R(θ),regularization Putting this into the matrix form, we define the data fidelity term as F(θ)=(cid:107)Xθ−y(cid:107)2. (7.34) The newly added term R(θ) is called the regularization function or the penalty function. It can take a variety of forms, e.g., • Ridge regression: R(θ)=(cid:80)d−1θ2 =(cid:107)θ(cid:107)2. p=0 p • LASSO regression: R(θ)=(cid:80)d−1|θ |=(cid:107)θ(cid:107) . p=0 p 1 In this section we aim to understand the role of the regularization functions by studying these two examples of R(θ). 7.4.1 Ridge regularization To explain the meaning of Equation (7.33) we write it in terms of matrices and vectors: minimize (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2, (7.35) θ∈Rd where λ is called the regularization parameter.It needs to be tuned by the user. We refer to Equation (7.35) as the ridge regression.5 5Insignalprocessingandoptimization,Equation(7.35)iscalledtheTikhonovregularization.Wefollow thestatisticscommunityincallingittheridgeregression. 440
7.4. REGULARIZATION How can the regularization function help to mitigate the overfitting problem? First let’s find the solution to this problem. Practice Exercise 1. Prove that the solution to Equation (7.35) is θ(cid:98)=(XTX+λI)−1XTy. (7.36) Solution. Take the derivative with respect to θ.a This yields (cid:26) (cid:27) ∇ (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 =2XT(Xθ−y)+2λθ =0. θ Rearranging the terms gives (XTX+λI)θ =XTy. Taking the inverse of the matrix on both sides yields the solution. aThesolutionhererequiressomebasicmatrixcalculus.YoumayrefertotheUniversityofWater- loo’sMatrixCookbookhttps://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf. Let us compare the ridge regression solution with the vanilla regression solutions: θ(cid:98)vanilla =(XTX)−1XTy, θ(cid:98)ridge(λ)=(XTX+λI)−1XTy. Clearly, the only difference is the presence of the parameter λ: • If λ→0, then θ(cid:98)ridge(0)=θ(cid:98)vanilla. This is because E (θ)=(cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. train (cid:124) (cid:123)(cid:122) (cid:125) =0 Hence, when λ → 0, the regression problem goes back to the vanilla version, and so does the solution. • λ→∞, then θ(cid:98)ridge(∞)=0. This happens because 1 E (θ)= (cid:107)Xθ−y(cid:107)2+(cid:107)θ(cid:107)2. train λ (cid:124) (cid:123)(cid:122) (cid:125) =0 Since we are now minimizing (cid:107)θ(cid:107)2, the solution will be θ = 0 because zero is the smallest value a squared function can achieve. For any 0 < λ < ∞, the net effect of (XTX +λI) is the constant λ added to all the eigenvalues of XTX. By taking the eigendecomposition of XTX, [U,S]=eig(XTX), we have that XTX+λI =USUT +λI =USUT +λUUT =U(S+λI)UT. 441
CHAPTER 7. REGRESSION Therefore, if the eigenvalue matrix S has a zero eigenvalue it will be offset by λ:     ♣ ♣+λ  ♥   ♥+λ  S =  −→ S+λI =   ♠   ♠+λ  0 λ As a result, even if XTX is not invertible (or close to not invertible), the new matrix XTX+λI is guaranteed to be invertible. Practice Exercise 2. You may be wondering what happens if XTX has a negative eigenvalue so that when we add a positive λ, the resulting matrix may have a zero eigenvalue. Prove that XTX will never have a negative eigenvalue, and XTX+λI always has positive eigenvalues. Solution. Eigenvalues of a matrix A are nonnegative if and only if vTAv ≥ 0 for any v. Thus we need to check whether vTXTXv ≥0 for all v. However, this is easy: vTXTXv =(cid:107)Xv(cid:107)2, which must be nonnegative for any v. Matrices satisfying this property are called positive semidefinite. Therefore, XTX is positive semidefinite. Implementation Solvingtheridgeregressioniseasy.First,weobservethattheregularizationfunctionR(θ)= (cid:107)θ(cid:107)2 is a quadratic function. Therefore, it can be combined with the data fidelity term as θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 θ∈Rd √ =argmin (cid:107)Xθ−y(cid:107)2+(cid:107) λIθ−0(cid:107)2 θ∈Rd (cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)2 =argmin (cid:13) (cid:13) √X θ− y (cid:13) (cid:13) . θ∈Rd (cid:13) λI 0 (cid:13) Therefore, all we need to do is to concatenate the matrix X with a d×d identity operator √ λI, and concatenate y with a d×1 all-zero vector. InMATLABandPython,theimplementationoftheridgeregressionisdonebydefining a new matrix A and a new vector b, as shown below: % MATLAB command for ridge regression A = [X; sqrt(lambda)*eye(d)]; b = [y(:); zeros(d,1)]; theta = A\b; % MATLAB command for ridge regression A = np.vstack((X, np.sqrt(lambd)*np.eye(d))) b = np.hstack((y, np.zeros(d))) theta = np.linalg.lstsq(A, b, rcond=None)[0] 442
7.4. REGULARIZATION Example 7.6. Consider a dataset of N = 20 data points. These data points are constructed from the model y =0.5−2x −3x2 +4x3 +6x4 +e , n=1,...,N, n n n n N n where e ∼Gaussian(0,0.252) is the noise. Fit the data using n (a) Vanilla linear regression with a 4th-order polynomial. (b) Vanilla linear regression with a 20th-order polynomial. (c) Ridgeregressionwitha20th-orderpolynomial,byconsideringthreechoicesofλ: λ=10−6, λ=10−3, and λ=10. Solution. (a) We first fit the data using a 4th-order polynomial. This fitting is relatively straightforward. In the MATLAB / Python programs below, set d = 4 and λ=0. The result is shown in Figure 7.20(a). 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Vanilla, 4th-order polynomial (b) Vanilla, 20th-order polynomial Figure7.20:Overfittingoccurswhenthemodelistoocomplexforthenumberoftrainingsamples. When using a vanilla regression with a 20th-order polynomial, the curve overfits the data and causes a catastrophic fitting error. (b) Suppose we use a 20th-order polynomial g(x) = (cid:80)20 θ xp to fit the data. We p=0 p plot the result in Figure 7.20(b). Since the order of the polynomial is very high relativetothenumberoftrainingsamples,itcomesasnosurprisethatthefitting is poor. This is overfitting, and we know the reason. (c) Next,weconsideraridgeregressionusingthreechoicesofλ.Theresultisshown in Figure 7.21. If λ is too small, we observe that some overfitting still occurs. If λ is too large, then the curve underfits the data. For an appropriately chosen λ, it can be seen that the fitting is reasonably good. 443
CHAPTER 7. REGRESSION 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 data data data fitted curve fitted curve fitted curve -1 -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ridge, λ=10−6 (b) Ridge, λ=10−3 (c) Ridge, λ=10 Figure 7.21: Ridge regression addresses the overfitting problem by adding a regularization term tothetrainingloss.Dependingonthestrengthoftheparameterλ,thefittedcurvecanvaryfrom overfitting to underfitting. The MATLAB and Python codes used to generate the above plots are shown below. % MATLAB code to demonstrate a ridge regression example % Generate data N = 20; x = linspace(-1,1,N); a = [0.5, -2, -3, 4, 6]; y = a(1)+a(2)*x(:)+a(3)*x(:).^2+a(4)*x(:).^3+a(5)*x(:).^4+0.25*randn(N,1); % Ridge regression lambda = 0.1; d = 20; X = zeros(N, d); for p=0:d-1 X(:,p+1) = x(:).^p; end A = [X; sqrt(lambda)*eye(d)]; b = [y(:); zeros(d,1)]; theta = A\b; % Interpolate and display results t = linspace(-1, 1, 500); Xhat = zeros(length(t), d); for p=0:d-1 Xhat(:,p+1) = t(:).^p; end yhat = Xhat*theta; plot(x,y, ’ko’,’LineWidth’,2, ’MarkerSize’, 10); hold on; plot(t,yhat,’LineWidth’,4,’Color’,[0.2 0.2 0.9]); # Python code to demonstrate a ridge regression example import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre np.set_printoptions(precision=2, suppress=True) 444
7.4. REGULARIZATION N = 20 x = np.linspace(-1,1,N) a = np.array([0.5, -2, -3, 4, 6]) y = a[0] + a[1]*x + a[2]*x**2 + \ a[3]*x**3 + a[4]*x**4 + 0.25*np.random.randn(N) d = 20 X = np.zeros((N, d)) for p in range(d): X[:,p] = x**p lambd = 0.1 A = np.vstack((X, np.sqrt(lambd)*np.eye(d))) b = np.hstack((y, np.zeros(d))) theta = np.linalg.lstsq(A, b, rcond=None)[0] t = np.linspace(-1, 1, 500) Xhat = np.zeros((500,d)) for p in range(d): Xhat[:,p] = t**p yhat = np.dot(Xhat, theta) plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=4) plt.show() Why does ridge regression work? • The penalty term (cid:107)θ(cid:107)2 in θ(cid:98)ridge =argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 θ∈Rd does not allow solutions with very (cid:107)θ(cid:107)2. • The penalty term adds a positive offset to the eigenvalues of XTX. • Since the denominator in (XTX + λI)−1XTy becomes larger than that of (XTX)−1XTy, noise in y is less amplified. Choosing the parameter How should we choose the parameter λ? The honest answer is that there is no answer because the optimal λ can only be found if we have access to the testing samples. If we do, we can plot the MSE (the testing error) with respect to λ, as shown in Figure 7.22(a). Ofcourseinrealitywedonothaveaccesstothetestingdata.However,wecanreserve a small portion of the training samples and treat them as validation samples. Then we run the ridge regression for different choices of λ. The λ that minimizes the error on these validation samples is the one that you should deploy. If the training set is small, we can 445
CHAPTER 7. REGRESSION 10-1 0.3 0.25 0.2 10-2 0.15 0.1 0.05 10-3 0 10-10 10-5 100 100 105 (a) Testing error vs λ (b) F(θ(cid:98)λ) vs R(θ(cid:98)λ) Figure 7.22: (a) Determining the optimal λ requires knowledge of the testing samples. In practice, we canreplacethetestingsampleswiththevalidationsamples,whicharesubsetsofthetrainingdata.Then by plotting the validation error as a function of λ we can determine the optimal λ. (b) The alternative is to plot F(θ(cid:98)λ) versus R(θ(cid:98)λ). The optimal λ can be found by locating the elbow point. shuffle the validation samples randomly and compute the average. This scheme is known as cross-validation. For some problems, there are “tactics” you may be able to employ for determining the optimal λ. The first approach is to ask yourself what would be the reasonable range of (cid:107)θ(cid:107)2 or (cid:107)Xθ −y(cid:107)2? Are you expecting them to be large or small? Approximately in whatorderofmagnitude?Ifyouhavesomecluesaboutthis,thenyoucanplotthefunction F(θ(cid:98)λ) = (cid:107)Xθ(cid:98)λ −y(cid:107)2 as a function of R(θ(cid:98)λ) = (cid:107)θ(cid:98)λ(cid:107)2, where θ(cid:98)λ is a shorthand notation for θ(cid:98)ridge(λ), which is the estimated parameter using a specific value of λ. Figure 7.22(b) shows an example of such a plot. As you can see, by varying λ we have different values of F(θ(cid:98)λ) and R(θ(cid:98)λ). Ifyouhavesomeideasaboutwhat(cid:107)θ(cid:107)2 shouldbe,sayyouwant(cid:107)θ(cid:107)2 ≤τ,youcango to the F(θ(cid:98)λ) versus R(θ(cid:98)λ) curve and find a point such that R(θ(cid:98)λ)≤τ. On the other hand, if you want (cid:107)Xθ−y(cid:107)2 ≤ (cid:15), you can also go to the F(θ(cid:98)λ) versus R(θ(cid:98)λ) curve and find a point such that (cid:107)Xθ−y(cid:107)2 ≤(cid:15). In either case, you have the freedom to shift the difficulty offindingλtothatoffindingτ or(cid:15).Notethatτ and(cid:15)havebetterphysicalinterpretations. The quantity (cid:15) tells us the upper bound of the prediction error, and τ tells us the upper bound of the parameter magnitude. If you have been working on your dataset long enough, the historical data (and your experience) will help you determine these values. Another feasible option suggested in the literature is finding the anchor point of the F(θ(cid:98)λ) and R(θ(cid:98)λ). The idea is that if the curve has a sharp elbow, the turning point would indicate a rapid increase/decrease in F(θ(cid:98)λ) (or R(θ(cid:98)λ)). How to determine λ • Cross-validation: Reserve a few training samples as validation samples. Check the prediction error w.r.t. these validation samples. The λ that minimizes the validation error is the one you deploy. • (cid:107)θ(cid:107)2 ≤ τ: Plot the F(θ(cid:98)λ) and R(θ(cid:98)λ). Then go along the R-axis to find the 446
7.4. REGULARIZATION position where R(θ(cid:98)λ)≤τ. • (cid:107)Xθ−y(cid:107)2 ≤ τ: Plot the F(θ(cid:98)λ) and R(θ(cid:98)λ). Then go along the F-axis to find the position where F(θ(cid:98)λ)≤(cid:15). • Find the elbow point of F(θ(cid:98)λ) and R(θ(cid:98)λ). Bias and variance trade-off for ridge regression We now discuss the bias and variance trade-off of the ridge regression. Theorem 7.7. Let y =Xθ+e be the training data, where e is zero-mean and has a covariance σ2I. Consider the ridge regression θ(cid:98)λ =argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. (7.37) θ∈Rd Then the estimate has the properties that θ(cid:98)λ =(XTX+λI)−1XTXθ+(XTX+λI)−1XTe, E[θ(cid:98)λ]=(XTX+λI)−1XTXθ =W λθ, Cov[θ(cid:98)λ]=σ2(XTX+λI)−1XTX(XTX+λI)−1, (cid:26) (cid:27) MSE(θ(cid:98)λ,θ)=σ2Tr W λ(XTX)−1WT λ +θT(W λ−I)T(W λ−I)θ, where W =(XTX+λI)−1XTX. λ Proof. The proof of this theorem involves some tedious matrix operations that will be omitted here. If you are interested in the proof you can consult van Wieringen’s “Lecture notes on ridge regression”, https://arxiv.org/pdf/1509.09169.pdf. (cid:3) The results of this theorem provide a way to assess the bias and variance. Specifically, from the MSE we know that (cid:104) (cid:105) MSE(θ(cid:98)λ,θ)=E e (cid:107)θ(cid:98)λ−θ(cid:107)2 (cid:110) (cid:111) =(cid:107)E e[θ(cid:98)λ]−θ(cid:107)2+Tr Cov[θ(cid:98)λ] (cid:26) (cid:27) =θT(W −I)T(W −I)θ+σ2Tr W (XTX)−1WT . λ λ λ λ (cid:124) (cid:123)(cid:122) (cid:125) bias (cid:124) (cid:123)(cid:122) (cid:125) variance The bias and variance are defined respectively as Bias(θ(cid:98)λ,θ)=θT(W λ−I)T(W λ−I)θ, (cid:26) (cid:27) Var(θ(cid:98)λ,θ)=σ2Tr W λ(XTX)−1WT λ . We can then plot the bias and variance as a function of λ. An example is shown in Fig- ure 7.23. 447
CHAPTER 7. REGRESSION 103 102 101 100 MSE Bias Variance 10-1 10-4 10-3 10-2 10-1 Figure 7.23: Thebiasandvarianceoftheridgeregressionbehaveinoppositewaysasλincreases.The MSE is the sum of bias and variance. The result in Figure 7.23 can be summarized in three points: • Bias ↑ as λ↑. This is because a large λ pushes the solution towards θ =0. Therefore, the bias with respect to the ground truth θ will increase. • Variance ↓ as λ ↑. Since variance is caused by noise, increasing λ forces the solution θ to be small. Hence, it becomes less sensitive to noise. • MSE reaches a minimum pointsomewherein the middle. The MSE is thesum of bias and variance. Therefore, it drops to the minimum and then rises again as λ increases. With appropriate choice of λ, we can show that the ridge regression can have a lower mean squared error than the vanilla regression. The following result is due to C. M. Theobald:6 Theorem 7.8. For λ<2σ2(cid:107)θ(cid:107)−2, (cid:16) (cid:17) (cid:16) (cid:17) MSE θ(cid:98)ridge(λ),θ <MSE θ(cid:98)vanilla,θ . (7.38) This theorem says that as long as λ is small enough, the ridge regression will have a lower MSE than the vanilla regression. Thus ridge regression is almost always helpful. Of course, the optimal λ is not provided by the theorem, which only tells us where to search for a good λ. Why does ridge regression reduce the testing error? • The regularization reduces the variance (see Figure 7.23 when λ>0) • It pays the price of increasing the bias. 6Theobald, C. M. (1974). Generalizations of mean square error applied to ridge regression. Journal of the Royal Statistical Society.SeriesB(Methodological),36(1),103-106. 448
7.4. REGULARIZATION • Usually, the drop in variance outweighs the increase in bias. So the overall MSE drops. • Bias is not always a bad thing. 7.4.2 LASSO regularization Theridgeregressionwediscussedintheprevioussubsectionisjustoneofthemanypossible ways of doing regularization. One alternative is to replace (cid:107)θ(cid:107)2 by (cid:107)θ(cid:107) , where 1 d−1 (cid:88) (cid:107)θ(cid:107) = |θ |. (7.39) 1 p p=0 This change from the sum-squares to sum-absolute-values has been main driving force in data science, machine learning, and signal processing for at least the past two decades. The optimization associated with (cid:107)θ(cid:107) is 1 minimize (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107) , (7.40) 1 θ∈Rd or N (cid:18) d−1 (cid:19)2 d−1 (cid:88) (cid:88) (cid:88) E (θ)= y − θ φ (x ) + λ· |θ | . (7.41) train n p p n p n=1 p=0 p=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) F(θ),datafidelity λ·R(θ),regularization Seeking a sparse solution To understand the choice of (cid:107)·(cid:107) , we need to introduce the concept of sparsity. 1 Definition 7.1. A vector θ is called sparse if it has only a few non-zero elements. As illustrated in Figure 7.24, a sparse θ ensures that only a very few columns of the data matrix X are active. This is an attractive property because, in some of the regression problems, it is indeed possible to have just a few dominant factors. The LASSO regression says that if our problem possesses this sparse solution, then the (cid:107)·(cid:107) can help us find the 1 sparse solution. Figure 7.24: A vector θ is sparse if it only contains a few non-zero elements. If θ is sparse, then the observation y is determined by a few active components. 449
CHAPTER 7. REGRESSION How can (cid:107)θ(cid:107) promote sparsity? If we consider the sets 1 Ω ={θ |(cid:107)θ(cid:107) ≤τ}={(θ ,θ )||θ |+|θ |≤τ}, 1 1 1 2 1 2 Ω ={θ |(cid:107)θ(cid:107)2 ≤τ}={(θ ,θ )|θ2+θ2 ≤τ}, 2 1 2 1 2 we note that Ω has a diamond shape whereas Ω has a circular shape. Since the data 1 2 fidelity term (cid:107)Xθ −y(cid:107)2 is an ellipsoid, seeking the optimal value in the presence of the regularization term can be viewed as moving the ellipsoid until it touches the set defined by the regularization. As illustrated in Figure 7.25, since {θ | (cid:107)θ(cid:107)2 ≤ τ} is a circle, the solution will be somewhere in the middle. On the other hand, since {θ | (cid:107)θ(cid:107) ≤ τ} is a 1 diamond, the solution will be one of the vertices. The difference between “somewhere in the middle” and “a vertex” is that the vertex is a sparse solution, since by the definition of a vertex one coordinate must be zero and the other coordinate must be non-zero. We can easily extrapolate this idea to the higher-dimensional spaces. In this case, we will see that the solution for the (cid:107)·(cid:107) problem has only a few non-zero entries. 1 Figure 7.25: A vector θ is sparse if it contains only a few non-zero elements. If θ is sparse, then the observation y is determined by a few active components. TheoptimizationformulatedinEquation(7.41)isknownastheleast absolute shrink- age and selection operator (LASSO). LASSO problems are difficult, but over the past two decades we have increased our understanding of the problem. The most significant break- through is that we now have algorithms to solve the LASSO problem efficiently. This is important because, unlike the ridge regression, where we have a (very simple) closed-form solution, the LASSO problem can only be solved using iterative algorithms. What is so special about LASSO? • LASSO regularization promotes a sparse solution. • If the underlying model has a sparse solution, e.g., you choose a 50th-order polynomial, but the underlying model is a third-order polynomial, then there should only be three non-zero regression coefficients in your 50th-order polyno- mial. LASSO will help in this case. 450
7.4. REGULARIZATION • If the underlying model has a dense solution, then LASSO is of limited value. A ridge regression could be better. • While (cid:107)θ(cid:107) is not differentiable (at 0), there exist polynomial-time convex algo- 1 rithms to solve the problem, e.g., interior-point methods. Solving the LASSO problem Today, there are many open-source packages to solve the LASSO problem. They are mostly developed in the convex optimization literature. One of the most user-friendly packages is the CVX package developed by S. Boyd and colleagues at Stanford University.7 Once you have downloaded and installed the package, solving the optimization can be done literally by typing in the data fidelity term and the regularization term. An example is given below. cvx_begin variable theta(d) minimize(sum_square(X*theta-y) + lambda*norm(theta,1)) cvx_end As you can see, the program is extremely simple. You start by calling cvx_begin and end it with cvx_end. Inside the box we create a variable beta(d), where d denotes the dimension of the vector theta. The main command is minimize. However, this line is almost self-explanatory. As long as you follow the syntax given by the user guidelines, you will be able to set it up properly. In Python, we can call the cvxpy library. import cvxpy as cvx theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) prob = cvx.Problem(objective) prob.solve() To see a concrete example, we use the crime rate data obtained from https://web. stanford.edu/~hastie/StatLearnSparsity/data.html. A snapshot of the data is shown in the table below. In this dataset, the vector y is the crime rate, which is the last column of the table. The feature/basis vectors are funding, hs, not-hs, college. city crime rate funding hs no-hs college 1 478 40 74 11 31 2 494 32 72 11 43 3 643 57 71 18 16 4 341 31 71 11 25 . . . . . . . . . . . . . . . . . . 50 940 66 67 26 18 7The MATLAB version is here: http://cvxr.com/cvx/. The Python version is here: https://cvxopt. org/.Followtheinstructionstoinstallthepackage. 451
CHAPTER 7. REGRESSION We consider two optimizations: θ(cid:98)1(λ)=argmin E 1(θ)d =ef (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107) 1, θ θ(cid:98)2(λ)=argmin E 2(θ)d =ef (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. θ Aswehavediscussed,thefirstoptimizationusesthe(cid:107)·(cid:107) regularizedleastsquares,whichis 1 theLASSOproblem.Thesecondoptimizationisthestandard(cid:107)·(cid:107)2 regularizedleastsquares. Since both solutions depend on the parameter λ, we parameterize the solutions in terms of λ. Note that the optimal λ for θ(cid:98)1 is not necessarily the optimal λ for θ(cid:98)2. One thing we would like to demonstrate in this example is visualizing the linear re- gression coefficients θ(cid:98)1(λ) and θ(cid:98)2(λ) as λ changes. To solve the optimization, we use CVX with the MATLAB and Python implementation is shown below. data = load(’./dataset/data_crime.txt’); y = data(:,1); % The observed crime rate X = data(:,3:end); % Feature vectors [N,d]= size(X); lambdaset = logspace(-1,8,50); theta_store = zeros(d,50); for i=1:length(lambdaset) lambda = lambdaset(i); cvx_begin variable theta(d) minimize( sum_square(X*theta-y) + lambda*norm(theta,1) ) % minimize( sum_square(X*theta-y) + lambda*sum_square(theta) ) cvx_end theta_store(:,i) = theta(:); end figure(1); semilogx(lambdaset, theta_store, ’LineWidth’, 4); legend(’funding’,’% high’, ’% no high’, ’% college’, ... ’% graduate’, ’Location’,’NW’); xlabel(’lambda’); ylabel(’feature attribute’); import cvxpy as cvx import numpy as np import matplotlib.pyplot as plt data = np.loadtxt("/content/data_crime.txt") y = data[:,0] X = data[:,2:7] N,d = X.shape lambd_set = np.logspace(-1,8,50) 452
7.4. REGULARIZATION theta_store = np.zeros((d,50)) for i in range(50): lambd = lambd_set[i] theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) # objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.sum_squares(theta) ) prob = cvx.Problem(objective) prob.solve() theta_store[:,i] = theta.value for i in range(d): plt.semilogx(lambd_set, theta_store[i,:]) 14 12 10 8 6 4 2 0 -2 10-2 100 102 104 106 108 lambda etubirtta erutaef 14 funding % high 12 % no high % college 10 % graduate 8 6 4 2 0 -2 10-2 100 102 104 106 108 lambda etubirtta erutaef funding % high % no high % college % graduate (a) LASSO (b) Ridge Figure7.26:RidgeandLASSOregressiononthecrime-ratedataset.(a)TheLASSOregressionsuggests that there are only a few active components as we change λ. (b) The ridge regression returns a set of dense solutions for all choices of λ. Figure 7.26 shows some interesting differences between the two regression models. • Trajectory. For the (cid:107)·(cid:107)2 estimate θ(cid:98)2(λ), the trajectory of the regression coefficients is smooth. This is attributable to the fact that the training loss E (θ) is continuously 2 differentiable in θ, and so the solution trajectory is smooth. By contrast, the (cid:107)·(cid:107) 1 estimate θ(cid:98)1(λ) has a more disruptive trajectory. • Active members. For the LASSO problem, θ(cid:98)1(λ) switches the active member as λ changes. For example, the feature high-school is the first one being activated when λ ↓. This implies that if we limit ourselves to only one feature, then high-school is the feature we should select. The ridge regression does not have this feature-selection property.Howaboutwhenλ=106?Inthiscase,theLASSOhastwoactivemembers: funding and high-school. This suggests that if there are two contributing factors, funding and high-school are the two. As λ=104, we see that in LASSO, the green curve goes to zero but then the red curve rises. This means a correlation between 453
CHAPTER 7. REGRESSION high school and no high school, which should not be a surprise because they are complementary to each other. • Magnitude of solutions. The magnitude of the solutions does not necessarily convey a clear conclusion because the feature vectors (e.g., high school) and the observable crime rate have different units. • Limiting solutions. As λ→0, both θ(cid:98)1(λ) and θ(cid:98)2(λ) reach the same solution, because the training losses are identical when λ=0. LASSO for overfitting Does LASSO help to mitigate the overfitting problem? Not always, but it often does. In Figure 7.27 we consider fitting a dataset of N = 20 data points. The ground truth model we use is y =L (x )+0.5L (x )+0.5L (x )+1.5L (x )+L (x )+e , n 0 n 1 n 2 n 3 n 4 n n where e ∼ Gaussian(0,σ2) for σ = 0.25. When fitting the data, we purposely choose a n 20th-orderLegendrepolynomialastheregressionmodel.WithonlyN =20datapoints,we can be almost certain that there is overfitting. The MATLAB and Python codes for solving this LASSO problem are shown below. % MATLAB code to demonstrate overfitting and LASSO % Generate data N = 20; x = linspace(-1,1,N)’; a = [1, 0.5, 0.5, 1.5, 1]; y = a(1)*legendreP(0,x)+a(2)*legendreP(1,x)+a(3)*legendreP(2,x)+ ... a(4)*legendreP(3,x)+a(5)*legendreP(4,x)+0.25*randn(N,1); % Solve LASSO using CVX d = 20; X = zeros(N, d); for p=0:d-1 X(:,p+1) = reshape(legendreP(p,x),N,1); end lambda = 2; cvx_begin variable theta(d) minimize( sum_square( X*theta - y ) + lambda * norm(theta , 1) ) cvx_end % Plot results t = linspace(-1, 1, 200); Xhat = zeros(length(t), d); for p=0:d-1 Xhat(:,p+1) = reshape(legendreP(p,t),200,1); end yhat = Xhat*theta; 454
7.4. REGULARIZATION plot(x,y, ’ko’,’LineWidth’,2, ’MarkerSize’, 10); hold on; plot(t,yhat,’LineWidth’,6,’Color’,[0.2 0.5 0.2]); # Python code to demonstrate overfitting and LASSO import cvxpy as cvx import numpy as np import matplotlib.pyplot as plt # Setup the problem N = 20 x = np.linspace(-1,1,N) a = np.array([1, 0.5, 0.5, 1.5, 1]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.25*np.random.randn(N) # Solve LASSO using CVX d = 20 lambd = 1 X = np.zeros((N, d)) for p in range(d): X[:,p] = eval_legendre(p,x) theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) prob = cvx.Problem(objective) prob.solve() thetahat = theta.value # Plot the curves t = np.linspace(-1, 1, 500) Xhat = np.zeros((500,d)) for p in range(P): Xhat[:,p] = eval_legendre(p,t) yhat = np.dot(Xhat, thetahat) plt.plot(x, y, ’o’) plt.plot(t, yhat, linewidth=4) Let us compare the various regression results. Figure 7.27(b) shows the vanilla regres- sion, which as you can see fits the N = 20 data points very well. However, no one would believe that such a fitting curve can generalize to unseen data. Figure 7.27(c) shows the ridge regression result. When performing the analysis, we sweep a range of λ and pick the value λ=0.5 so that the fitted curve is neither too “wild” nor too “flat”. We can see that the fitting is improved. However, since the ridge regression only penalizes large-magnitude coefficients, the fitting is still not ideal. Figure 7.27(d) shows the LASSO regression result. Sincethetruemodelisa4th-orderpolynomialandweusea20th-orderpolynomial,thetrue solution is sparse. Therefore, LASSO is helpful, and hence we can pick a sparse solution. The significance of LASSO is often not about the fitting of the data points but the 455
CHAPTER 7. REGRESSION 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ground truth model (b) Vanilla regression 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (c) Ridge (d) LASSO Figure 7.27: We fit a dataset of N = 20 data points. (a) The ground truth model that generates the data. The model is a 4th-order ordinary polynomial. (b) Vanilla regression result, without any regularization.Notethatthereissevereoverfittingbecausethemodelcomplexityistoohigh.(c)Ridge regression result, by setting λ=0.5. (d) LASSO regression result, by setting λ=2. number of active coefficients. In Figure 7.28 we show a comparison between the ground truthcoefficients,thevanillaregressioncoefficients,theridgeregressioncoefficients,andthe LASSOregressioncoefficients.ItisevidentthattheLASSOsolutioncontainsamuchsmaller number of non-zeros compared to the ridge regression. Most of the high-order coefficients are zero. By contrast, the vanilla regression coefficients are wild. The ridge regression is better, but there are many non-zero high-order coefficients. Closing remark. In this section, we discussed two regularization techniques: ridge regres- sionandLASSOregression.Bothtechniquesareaboutaddingapenaltytermtothetraining loss to constrain the regression coefficients. In the optimization literature, writings on ridge and LASSO regression are abundant, covering both algorithms and theoretical properties. An example of a theoretical question addressed in the literature is: Under what conditions isLASSOguaranteedtorecoverthecorrectsupportofthesolution,i.e.,locatingthecorrect positions of the non-zeros? Problems like these are beyond the scope of this book. 456
7.5. SUMMARY 2 15 10 1 5 0 0 -5 -1 -10 -2 -15 5 10 15 20 5 10 15 20 (a) Ground truth model (b) Vanilla regression 2 2 1 1 0 0 -1 -1 -2 -2 5 10 15 20 5 10 15 20 (c) Ridge (d) LASSO Figure 7.28: Coefficients of the regression models. (a) The ground truth model, which is a 4th-order polynomial. There are only 5 non-zero coefficients. (b) The vanilla regression coefficients. Note that the values are wild and large, although the curve fits the training data points very well. (c) The ridge regression coefficients. While the overall magnitudes are significantly improved from the vanilla, some high-order coefficients are still non-zero. (d) The LASSO regression coefficients. There are very few non-zeros, and the non-zeros match well with the ground truth. 7.5 Summary Regressionisoneofthemostwidelyusedtechniquesindatascience.Theformulationofthe regression problem is as simple as setting up a system of linear equations: minimize (cid:107)Xθ−y(cid:107)2, (7.42) θ∈Rd which has a closed-form solution. The biggest problems in practice are outliers, lack of training samples, and poor choice of the regression model. • Outliers: We always recommend plotting the data whenever possible to check if there are obvious outliers. There are also statistical tests in which you can evaluate the validity of your samples. One simple way to debug outliers is to run the regression 457
CHAPTER 7. REGRESSION and check the prediction error against each training sample. If you have an outlier, and if your model is of reasonably low complexity, then a sample with an excessively large prediction error is an outlier. For example, if most of the training samples are within one standard deviation from your prediction but a few are substantially off, youwillknowwhichonesaretheoutliers.Robustlinearregressionisonetechniquefor countering outliers, but an experienced data scientist can often reject outliers before runninganyregressionalgorithms.Domainknowledgeisofgreatvalueforthispurpose. • Lack of training samples: As we have discussed in the overfitting section, it is ex- tremelyimportanttoensurethatyourmodelcomplexityisappropriateforthenumber of training samples. If the training set is small, do not use a complex model. Regu- larization techniques are valuable tools to mitigate overfitting. However, choosing a good regularization requires domain knowledge. For example, if you know that some featuresarenotimportant,youneedtoscalethemproperlysoasnottoover-influence the regression solution. • Wrongmodel:Wehavementionedseveraltimesthatregressioncanalwaysreturnyou a result because regression is an optimization problem. However, whether that result is meaningful depends on how meaningful your regression problem is. For example, if the noise is i.i.d. Gaussian, a data fidelity term with (cid:107)·(cid:107)2 would be a good choice; however, if the noise is i.i.d. Poisson, (cid:107)·(cid:107)2 would become a very bad model. We need a tighter connection with the statistics of the underlying data-generation model for problems like these. This is the subject of our next chapter, on parameter estimation. 7.6 References Linear regression Treatmentofstandardlinearregressionisabundant.Inthecontextofmachinelearningand data science, the following references are useful. 7-1 GarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani,AnIntroduction to Statistical Learning with Applications in R, Springer 2013, Chapter 3. 7-2 StephenBoydandLievenVandenberghe,Convex Optimization,CambridgeUniversity Press, 2004. Chapter 6. 7-3 Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning, Springer, 2001. Chapter 3. 7-4 ChristopherBishop,PatternRecognitionandMachineLearning,Springer2006.Chap- ter 3.1. 7-5 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data, AML Book, 2012. Chapter 3.2 Overfitting and Bias/Variance Thetheoryofoverfittingandthetrade-offbetweenbiasandvariancecanbefoundinmultiple references. The following are basic treatments of the subject. 458
7.7. PROBLEMS 7-6 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data, AML Book, 2012. Chapter 4. 7-7 ChristopherBishop,PatternRecognitionandMachineLearning,Springer2006.Chap- ter 3.2. Ridge and LASSO regression RidgeandLASSOregressionareimportanttoolsinstatisticallearningtoday.Thefollowing two textbooks cover some of the perspectives of the statistical community and the signal processing community. 7-8 Trevor Hastie, Robert Tibshirani, and Martin Wainwright, Statistical Learning with Sparsity: The LASSO and Generalizations, CRC Press, 2015. 7-9 Michael Elad, Sparse and Redundant Representations, Springer, 2010. Chapters 1 and 3. 7.7 Problems Exercise 1. (a) Construct a dataset with N =20 samples, following the model d−1 (cid:88) y = θ L (x )+e , (7.43) n p p n n p=0 where θ =1, θ =0.5, θ =0.5, θ =1.5, θ =1, for −1<x<1. Here, L (x) is the 0 1 2 3 4 p Legendrepolynomialofthepthorder.TheN =20samplesarerandomuniformlysam- pled from the interval [−1,1]. The noise samples e are i.i.d. Gaussian with variance n σ2 =0.252. Plot the dataset using the MATLAB or Python command scatter. (b) Run the regression using the same model where d = 5, without any regularization. Plot the predicted curve and overlay with the training samples. (c) Repeat (b) by running the regression with d=20. Explain your observations. (d) Increase the number of training samples N to N =50, N =500, and N =5000, and repeat (c). Explain your observations. (e) ConstructatestingdatasetwithM =1000testingsamples.Foreachoftheregression models trained in (b)-(d), compute the testing error. Exercise 2. Consider a data generation model N−1 x n = (cid:88) c ke−j2π Nkn,n=0,...,N −1. k=0 459
CHAPTER 7. REGRESSION (a) Write the above equation in matrix-vector form x=Wc. What are the vectors c and x, and what is the matrix W? (b) Show that W is orthogonal, i.e.,, WHW =I, where WH is the conjugate transpose of W. (c) Using (b), derive the least squares regression solution. Exercise 3. Consider a simplified LASSO regression problem: θ(cid:98)=argmin (cid:107)y−θ(cid:107)2+λ(cid:107)θ(cid:107) 1. (7.44) θ∈Rd Show that the solution is given by θ(cid:98)=sign(y)·max(|y|−λ,0), (7.45) where · is the elementwise multiplication. Exercise 4. A one-dimensional signal is corrupted by blur and noise: L−1 (cid:88) y = h x +e . n (cid:96) n−(cid:96) n (cid:96)=0 (a) Formulate the least squares regression problem in matrix-vector form y = Hx+e. Find x, y and H. (b) Consider a regularization function N (cid:88) R(x)= (x −x )2. n n−1 n=2 Show that this regularization is equivalent to R(x)=(cid:107)Dx(cid:107)2 for some D. Find D. (c) Using the regularization in (b), derive the regularized least squares regression result: minimize (cid:107)y−Hx(cid:107)2+λ(cid:107)Dx(cid:107)2. x Exercise 5. Let σ(·) be the sigmoid function 1 σ(a)= . 1+ea We want to use σ(a) as a basis function. 460
7.7. PROBLEMS (a) Show that the tanh function and the sigmoid function are related by tanh(a)=2σ(2a)−1. (b) Show that a linear combination of sigmoid functions d−1 (cid:18) (cid:19) y =θ +(cid:88) θ σ x n−µ j n 0 p s p=1 is equivalent to a linear combination of tanh functions d−1 (cid:18) (cid:19) y =α +(cid:88) α tanh x n−µ j . n 0 p 2s p=1 (c) Find the relationship between θ and α . p p Exercise 6. (NHANES Part 1)(Data download) The National Health and Nutrition Examination Survey (NHANES) is a program to assess thehealthandnutritionalstatusofadultsandchildrenintheUnitedStates8.Thecomplete survey result contains over 4,000 samples of health-related data of individuals who partici- pated in the survey between 2011 and 2014. In the following exercises, we will focus on two categories of the data for each individual: height (in mm) and body mass index (BMI). The data is divided into two classes based on gender. Table 1 contains snippets of the data. index female bmi female stature mm index male bmi male stature mm 0 28.2 1563 0 30 1679 1 22.2 1716 1 25.6 1586 2 27.1 1484 2 24.2 1773 3 28.1 1651 3 27.4 1816 Table 7.2: Male and Female Data Snippets Use csv.reader to read the training data files for the two data classes. Important! Before proceeding to the problems, • normalizethenumberinmale_stature_mmandfemale_stature_mmbydividingthem by 1000, and • normalize that of male_bmi and female_bmi by dividing them by 10. This will significantly reduce the numerical error. Consider a linear model: g =θTx, (7.46) θ 8https://www.cdc.gov/nchs/nhanes/index.htm 461
CHAPTER 7. REGRESSION The regression problem we want to solve is N θ(cid:98)=argmin (cid:88) (y n−g θ(x n))2, θ∈Rd n=1 whereD ={(x ,y )}N isthetrainingdataset.Puttingtheequationintothematrixform, n n n=1 we know that the optimization is equivalent to θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. θ∈Rd (cid:124) (cid:123)(cid:122) (cid:125) Etrain(θ) (a) Derive the solution θ(cid:98). State the conditions under which the solution is the unique global minimum in terms of the rank of X. Suggest two techniques that can be used when XTX is not invertible. (b) For the NHANES dataset, assign y = +1 if the nth sample is a male and y = −1 n n if the nth sample is a female. Implement your answer in (a) with Python to solve the problem. Report your answer. (c) Repeat (b), but this time use CVXPY. Report your answer, and compare with (b). Exercise 7. (NHANES Part 2)(Data download) We want to do a classification based on the linear model we found in the previous exercise. The classifier we will use is predicted label=sign(g (x)), (7.47) θ where x∈Rd is the a test sample. Here, we label +1 for male and −1 for female. Because the dataset we consider in this exercise has only two columns, the linear model is g (x)=θ +θ x +θ x , θ 0 1 1 2 2 where x=[1,x ,x ]T is the input data and θ =[θ ,θ ,θ ]T is the parameter vector. 1 2 0 1 2 (a) First, we want to visualize the classifier. (i) Plotthetrainingdatapointsofthemaleandfemaleclasses.Markthemaleclass with blue circles and the female class with red dots. (ii) Plot the decision boundary g (·) and overlay it with the data plotted in (a). θ Hint: g (·) is a straight line in 2D. You can express x in terms of x and other θ 2 1 parameters. (b) (This problem requires knowledge of the content of Chapter 9). Report the classifica- tion accuracy. To do so, take testing data x and compute the prediction according to Equation (7.47). (i) WhatistheType1error(FalseAlarm)ofclassifyingmales?Thatis,whatisthe percentage of testing samples that should be female but a male was predicted? (ii) What is the Type 2 error (Miss) of classifying males? That is, what is the per- centage of testing samples that should be male but a female was predicted? 462
7.7. PROBLEMS (iii) Whatistheprecisionandrecallforthisclassifier?Forthedefinitionsofprecision and recall, refer to Chapter 9.5.4. Exercise 8. (NHANES Part 3)(Data download) Thisexerciserequiressomebackgroundinoptimization.PleaserefertoReference[7.2,Chap- ter 9 and 10]. Consider the following three optimization problems: θ(cid:98)λ =argmin (cid:107)Xθ−y(cid:107)2 2+λ(cid:107)θ(cid:107)2 2, (7.48) θ∈Rd θ(cid:98)α =argmin (cid:107)Xθ−y(cid:107)2 2 subjectto (cid:107)θ(cid:107)2 2 ≤α, (7.49) θ∈Rd θ(cid:98)(cid:15) =argmin (cid:107)θ(cid:107)2 2 subjectto (cid:107)Xθ−y(cid:107)2 2 ≤(cid:15). (7.50) θ∈Rd (a) Set lambd = np.arange(0.1,10,0.1). Plot • (cid:107)Xθ(cid:98)λ−y(cid:107)2 2 as a function of (cid:107)θ(cid:98)λ(cid:107)2 2. • (cid:107)Xθ(cid:98)λ−y(cid:107)2 2 as a function of λ. • (cid:107)θ(cid:98)λ(cid:107)2 2 as a function of λ. (b) (i) Write down the Lagrangian for each of the three problems. Note that the first problem does not have any Lagrange multiplier. For the second and third prob- lems you may use the following notations: • γ = the Lagrange multiplier of Equation (7.49), and α • γ = the Lagrange multiplier of Equation (7.50). (cid:15) (ii) State the first-order optimality conditions (the Karush-Kuhn-Tucker or KKT conditions) for each of the three problems. Express your answers in terms of X, θ, y, λ, α, (cid:15), and the two Lagrange multipliers γ , γ . α (cid:15) (iii) Fix λ>0. We can solve Equation (7.48) to obtain θ(cid:98)λ. Find α and the Lagrange multiplier γ α in Equation (7.49) such that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.49). (iv) Fix λ>0. We can solve Equation (7.48) to obtain θ(cid:98)λ. Find (cid:15) and the Lagrange multiplier γ (cid:15) in Equation (7.50) such that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.50). (v) Fix λ>0. By using the α and γ α you found in (iii), you can show that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.49). Is it enough to claim that θ(cid:98)λ is thesolutionofEquation(7.49)?Ifyes,why?Ifno,whatelsedoweneedtoshow? Please elaborate through a proof, if needed. Exercise 9. ConsideratrainingdatasetD ={(x ,y ),...,(x ,y )}andaweightw =[w ,...,w ]T. train 1 1 N N 1 N Findtheregressionsolutiontothefollowingproblemanddiscusshowyouwouldchoosethe weight. N θ(cid:98)=argmin (cid:88) w n(cid:0) y n−xT nθ(cid:1)2 . (7.51) θ∈Rd n=1 463
CHAPTER 7. REGRESSION Exercise 10. Consider a training dataset D = {(x ,y ),...,(x ,y )}. Suppose that the input data train 1 1 N N x is corrupted by i.i.d. Gaussian noise e ∼ Gaussian(0,σ2I ) so that the training set n n d becomes D ={(x +e ,y ),...,(x +e ,y )}. Show that the (vanilla) least squares train 1 1 1 N N N linear regression by taking the expectation over e , n N θ(cid:98)=argmin (cid:88) E en(cid:104)(cid:0) y n−(x n+e n)Tθ(cid:1)2(cid:105) , (7.52) θ∈Rd n=1 is equivalent to a ridge regression. 464
Chapter 8 Estimation Inthischapter,wediscussanothersetofimportantcombatskillsindatascience,namelyes- timation.Estimationhasacloserelationshipwithregression.Regressionprimarilytakesthe optimization route, while estimation takes the probabilistic route. As we will see, at a cer- tainpointthetwowillmerge.Thatis,undersomespecificstatisticalconditions,estimation processes will coincide with the regression. EstimationissummarizedpictoriallyinFigure8.1.Imaginethatwehavesomerandom samples X ,...,X . These samples are drawn from a distribution f (x;θ), where θ is a 1 N X parameter that characterizes the distribution. The parameter θ is not known to us. The goal of estimation is to solve an inverse problem to recover the parameter based on the observations X ,...,X . 1 N Figure 8.1: Estimation is an inverse problem of recovering the unknown parameters that were used by the distribution. In this figure, the PDF of X using a parameter θ is denoted as f (x;θ). The forward X data-generationprocesstakestheparameterθandcreatestherandomsamplesX ,...,X .Estimation 1 N takes these observed random samples and recovers the underlying model parameter θ. What is estimation? Estimation is an inverse problem with the goal of recovering the underlying pa- rameter θ of a distribution f (x;θ) based on the observed samples X ,...,X . X 1 N 465
CHAPTER 8. ESTIMATION What are parameters? Before we discuss the methods of estimation, let us clarify the meaning of the parameter θ. Allprobabilitydensityfunctions(PDFs)haveparameters.Forexample,aBernoullirandom variableischaracterizedbyaparameterpthatdefinestheprobabilityofgettinga“head”.A Gaussian random variable is characterized by two parameters: the mean µ and variance σ2. Example 8.1. (Parameter of a Bernoulli) If X is a Bernoulli random variable, then n the PMF has a parameter θ: p (x ; θ)=θxn(1−θ)1−xn. Xn n Remark. The PMF is expressed in this form because x is either 1 or 0: n (cid:40) θ1(1−θ)1−1 =θ, if x =1, p (x ; θ)= n Xn n θ0(1−θ)1−0 =1−θ, if x =0. n Example 8.2. (Parameter of a Gaussian) If X is a Gaussian random variable, the n PDF is 1 (cid:26) (x −µ)2(cid:27) f (x ; θ )= √ exp − n , Xn n (cid:124)(cid:123)(cid:122)(cid:125) 2πσ2 2σ2 =(µ,σ) where θ = [µ,σ] consists of both the mean and the variance. We can also designate the parameter θ to be the mean only. For example, if we know that σ = 1, then the PDF is 1 (cid:26) (x −µ)2(cid:27) f (x ; θ )= √ exp − n , Xn n (cid:124)(cid:123)(cid:122)(cid:125) 2π 2 =µ where θ is the mean. Since all probability density functions have parameters, estimating them from the observed random variables is a well-defined inverse problem. Of course, there are better estimates and there are worse estimates. Let us look at the following example to develop our intuitions about estimation. Figure 8.2showsadatasetcontaining1000datapointsgeneratedfroma2DGaussian distribution with an unknown mean vector µ and an unknown covariance matrix Σ. We duplicate this dataset in the four subfigures. The estimation problem is to recover the unknown mean vector µ and the covariance matrix Σ. In the subfigures we propose four candidates, each with a different mean vector and a different covariance matrix. We draw thecontourlinesofthecorrespondingGaussians.ItcanbeseenthatsomeGaussiansfitthe data better than others. The goal of this chapter is to develop a systematic way of finding the best fit for the data. Plan for this chapter The discussions in this chapter concern the three elementary distributions: 466
5 5 5 5 4 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1 0 0 0 0 -1 -1 -1 -1 -2 -2 -2 -2 -3 -3 -3 -3 -4 -4 -4 -4 -5 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 Bad estimate Bad estimate Bad estimate Good estimate (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 2 0 −0.5 0 µ= µ= µ= µ= −0.5 −1.5 −0.7 0 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.25 0.2 1 −0.2 1 0 0.25 0.3 Σ= Σ= Σ= Σ= 0.2 1 −0.2 0.1 0 1 0.3 1 Figure8.2:Anestimationproblem.Givenasetof1000datapointsdrawnfromaGaussiandistribution with unknown mean µ and covariance Σ, we propose several candidate Gaussians and see which one would be the best fit to the data. Visually, we observe that the right-most Gaussian has the best fit. The goal of this chapter is to develop a systematic way of solving estimation problems of this type. • Likelihood: f (x|θ), which is the conditional PDF of X given that the parameter X|Θ is Θ. • Prior: f (θ), which is the PDF of Θ. Θ • Posterior: f (θ|x), which is the conditional PDF of Θ given the data X. Θ|X Eachofthesedensityfunctionshasitsrespectivemeaning,andconsequentlyasetofdifferent estimationtechniques.InSection8.1weintroducetheconceptof maximum-likelihood(ML) estimation. As the name suggests, the estimate is constructed by maximizing the likelihood function. We will discuss a few examples of ML estimation and draw connections between ML estimation and regression. In Section 8.2 we will discuss several basic properties of an ML estimate. Specifically, we will introduce the ideas of unbiasedness, consistency, and the invariance principle. The second topic discussed in this chapter is the maximum-a-posteriori (MAP) esti- mation,detailedinSection8.3.InMAP,theparameterΘisarandomvariable.SinceΘisa random variable, it has its own probability density function f (θ), which we call the prior. Θ Given the likelihood and the prior, we can define the posterior. The MAP estimation finds the peak of the posterior distribution as a way to “explain” the data. Several important topics will be covered in Section 8.3. For example, we will discuss the choice of the prior via the concept of conjugate prior. We will also discuss how MAP is related to regularized regressions such as the ridge and LASSO regressions. The third topic is the minimum mean-square estimation (MMSE), outlined in Sec- tion8.4.TheMMSEisaBayesianapproach.Animportantresultthatwillbedemonstrated is that the MMSE estimate is the conditional expectation of the posterior distribution. In otherwords,itisthemeanoftheposterior.AnMMSEestimatehasanimportantdifference compared to a MAP estimate, namely that while an MMSE estimate is the mean of the posterior, a MAP estimate is the mode of the posterior. We discuss the formulation of the estimation problem and ways of solving the problem. We also discuss how the MMSE can be performed for multidimensional Gaussian distributions. 467
CHAPTER 8. ESTIMATION 8.1 Maximum-Likelihood Estimation Maximum-likelihood (ML) estimation, as the name suggests, is an estimation method that “maximizes”the“likelihood”.Therefore,tounderstandtheMLestimation,wefirstneedto understand the meaning of likelihood, and why maximizing the likelihood would be useful. 8.1.1 Likelihood function ConsiderasetofN datapointsD ={x ,x ,...,x }.Wewanttodescribethesedatapoints 1 2 N using a probability distribution. What would be the most general way of defining such a distribution? SincewehaveN datapoints,andwedonotknowanythingaboutthem,themostgen- eralwaytodefineadistributionisasahigh-dimensionalprobabilitydensityfunction(PDF) f (x). This is a PDF of a random vector X = [X ,...,X ]T. A particular realization of X 1 N this random vector is x=[x ,...,x ]T. 1 N f (x) is the most general description for the N data points because f (x) is the X X joint PDF of all variables. It provides the complete statistical description of the vector X. For example, we can compute the mean vector E[X], the covariance matrix Cov(X), the marginal distributions, the conditional distribution, the conditional expectations, etc. In short, if we know f (x), we know everything about X. X ThejointPDFf (x)isalwaysparameterizedbyacertainparameterθ.Forexample,if X weassumethatX isdrawnfromajointGaussiandistribution,thenf (x)isparameterized X by the mean vector µ and the covariance matrix Σ. So we say that the parameter θ is θ =(µ,Σ). To state the dependency on the parameter explicitly, we write f (x; θ)=PDF of the random vector X with a parameter θ. X When you express the joint PDF as a function of x and θ, you have two variables to play with. The first variable is the observation x, which is given by the measured data. We usually think about the probability density function f (x) in terms of x, because the PDF X is evaluated at X = x. In estimation, however, x is something that you cannot control. Whenyourbosshandsadatasettoyou,xisalreadyfixed.Youcanconsidertheprobability of getting this particular x, but you cannot change x. The second variable stated in f (x; θ) is the parameter θ. This parameter is what X we want to find out, and it is the subject of interest in an estimation problem. Our goal is to find the optimal θ that can offer the “best explanation” to data x, in the sense that it can maximize f (x; θ). X The likelihood function is the PDF that shifts the emphasis to θ: Definition 8.1. Let X =[X ,...,X ]T be a random vector drawn from a joint PDF 1 N f (x;θ), and let x = [x ,...,x ]T be the realizations. The likelihood function is a X 1 N function of the parameter θ given the realizations x: def L(θ|x) = f (x; θ). (8.1) X 468
8.1. MAXIMUM-LIKELIHOOD ESTIMATION A word of caution: L(θ|x) is not a conditional PDF because θ is not a random variable. The correct way to interpret L(θ|x) is to view it as a function of θ. This function changes its shape according the observed data x. We will return to this point shortly. Independent observations Whilef (x)providesuswithacompletepictureoftheX,usingf (x)istedious.Weneed X X to describe how each X is generated and describe how X is related to X for all pairs of n n m n and m. If the vector X contains N entries, then there are N2/2 pairs of correlations we need to compute. When N is large, finding f (x) would be very difficult if not impossible. X In practice, f (x) may sometimes be overkill. For example, if we measure the inter- X arrival time of a bus for several days, it is quite likely that the measurements will not be correlated. In this case, instead of using the full f (x), we can make assumptions about X the data points. The assumption we will make is that all the data points are independent and that they are drawn from an identical distribution f (x). The assumption that the X datapointsareindependently and identically distributed(i.i.d.)significantlysimplifiesthe problem so that the joint PDF f can be written as a product of single PDFs f : X Xn N (cid:89) f (x)=f (x ,...,x )= f (x ). X X1,...,XN 1 N Xn n n=1 If you prefer a visualization, we can take a look at the covariance matrix, which goes from a full covariance matrix to a diagonal matrix and then to an identity matrix:     Var[X1] Cov(X1,X2) ··· Cov(X1,XN) Var[X1] 0 ··· 0 Cov[X2,X1] Var[X2] ··· Cov(X2,XN) 0 Var[X2] ··· 0   . . . . . . ... . . .   inde= pe⇒ ndent  . . . . . . ... . . .   Cov(XN,X1) Cov(XN,X2) ··· Var[XN] 0 0 ··· Var[XN]  σ2 0 ··· 0 0 σ2 ··· 0 ide= n⇒ tical  . . . . . . ... . . .  . 0 0 ··· σ2 The assumption of i.i.d. is strong. Not all data can be modeled as i.i.d. (For example, photons passing through a scattering medium have correlated statistics.) However, if the i.i.d. assumption is valid, we can simplify the model significantly. If the data points are i.i.d., then we can write the joint PDF as N (cid:89) f (x; θ)= f (x ; θ). X Xn n n=1 This simplifies the likelihood function as a product of the individual PDFs. Definition8.2. Giveni.i.d.randomvariablesX ,...,X thatallhavethesamePDF 1 N f (x ), the likelihood function is Xn n N def (cid:89) L(θ|x) = f (x ; θ). (8.2) Xn n n=1 Incomputationweoftentakethelogofthelikelihoodfunction.Wecalltheresultingfunction the log-likelihood. 469
CHAPTER 8. ESTIMATION Definition8.3. Givenasetofi.i.d.randomvariablesX ,...,X withPDFf (x;;θ), 1 N Xn the log-likelihood is defined as N (cid:88) logL(θ|x)=logf (x; θ)= logf (x ; θ). (8.3) X Xn n n=1 Example8.3.Findthelog-likelihoodofasequenceofi.i.d.Gaussianrandomvariables X ,...,X with mean µ and variance σ2. 1 N Solution. Since the random variables X ,...,X are i.i.d. Gaussian, the PDF is 1 N N (cid:26) (cid:27) f X(x; µ,σ2)= (cid:89) √ 1 e−(xn 2σ− 2µ)2 . (8.4) 2πσ2 n=1 Taking the log on both sides yields the log-likelihood function: logL(µ,σ2|x)=logf (x; µ,σ2) X (cid:40) N (cid:26) (cid:27)(cid:41) =log (cid:89) √ 1 e−(xn 2σ− 2µ)2 2πσ2 n=1 N (cid:26) (cid:27) = (cid:88) log √ 1 e−(xn 2σ− 2µ)2 2πσ2 n=1 = (cid:88)N (cid:26) −1 log(2πσ2)− (x n−µ)2(cid:27) 2 2σ2 n=1 N N 1 (cid:88) =− log(2πσ2)− (x −µ)2. 2 2σ2 n n=1 PracticeExercise8.1.Findthelog-likelihoodofasequenceofi.i.d.Bernoullirandom variables X ,...,X with parameter θ. 1 N Solution. If X ,...,X are i.i.d. Bernoulli random variables, we have 1 N N (cid:26) (cid:27) (cid:89) f (x; θ)= θxn(1−θ)1−xn . X n=1 Taking the log on both sides of the equation yields the log-likelihood function: (cid:40) N (cid:26) (cid:27)(cid:41) (cid:89) logL(θ|x)=log θxn(1−θ)1−xn . n=1 470
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Hence, N (cid:26) (cid:27) (cid:88) logL(θ|x)= log θxn(1−θ)1−xn n=1 N (cid:88) = x logθ+(1−x )log(1−θ) n n n=1 (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) = x ·logθ+ N − x ·log(1−θ). n n n=1 n=1 Visualizing the likelihood function The likelihood function L(θ|x) is a function of θ, but its value also depends on the under- lying measurements x. It is extremely important to keep in mind the presence of both. Tohelpyouvisualizetheeffectofθ andx,weconsiderasetofi.i.d.Bernoullirandom variables. As we have just shown in the practice exercise, the likelihood function of these i.i.d. random variables is (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) logL(θ|x)= x ·logθ+ N − x ·log(1−θ), (8.5) n n n=1 n=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) S N−S where we define S =(cid:80)N x as the sum of the (binary) measurements. n=1 n To make the dependency on S and θ explicit, we write L(θ|x) as logL(θ |S)=Slogθ+(N −S)log(1−θ), (8.6) which emphasizes the role of S in defining the log-likelihood function. We plot the surface of L(θ|S) as a function of S and θ, assuming that N =50. As shown on the left-hand side of Figure 8.3, the surface L(θ|S) has a saddle shape. Along one direction the function goes up, whereas along another direction the function goes down. In the middle of Figure 8.3, we show a bird’s-eye view of the surface, with the color-coding matched with the surface plot. As you can see, when plotted as a function of θ and x (in our case, we use a summary statistic S = (cid:80)N x ), the two-dimensional plot tells us how the log-likelihood function n=1 n changes when S changes. On the right-hand side of Figure 8.3, we show two particular cross sections of the two-dimensional plot. One cross section is taken from S = 25 and the other cross section is taken from S =12. Since the total number of heads in this numerical experiment is assumed to be N = 50, the first cross section at S = 25 is obtained when half of the Bernoulli measurements are “1”, whereas the second cross section at S = 12 is obtained when a quarter of the Bernoulli measurements are “1”. The cross sections tell us the log-likelihood function logL(θ|S) is a function defined specifically for a given measurement x. As you can see from Figure 8.3, the log-likelihood function changes when S changes. Therefore, if our goal is to “find a θ that maximizes the log-likelihood function”, then for a different x we will have a different answer. For example, according to Figure 8.3, the maximum for logL(θ|S = 25) occurs when θ ≈ 0.5, and the maximum for logL(θ|S = 12) occurs when θ ≈ 0.24. These are the maximum-likelihood estimates for the respective measurements. 471
CHAPTER 8. ESTIMATION Figure 8.3: We plot the log-likelihood function as a function of S =(cid:80)N x and θ. [Left] We show n=1 n the surface plot of L(θ|S) = Slogθ+(N −S)log(1−θ). Note that the surface has a saddle shape. [Middle] By taking a bird’s-eye view of the surface plot, we obtain a 2-dimensional contour plot of the surface, where the color code matches the height of the log-likelihood function. [Right] We take two cross sections along S =25 and S =12. Observe how the shape changes. We use the following MATLAB code to generate the surface plot: % MATLAB code to generate the surface plot N = 50; S = 1:N; theta = linspace(0.1,0.9,100); [S_grid, theta_grid] = meshgrid(S, theta); L = S_grid.*log(theta_grid) + (N-S_grid).*log(1-theta_grid); s = surf(S,theta,L); s.LineStyle = ’-’; colormap jet view(65,15) For the bird’s-eye view plot, we replace surf with imagesc(S,theta,L). For the cross section plots, we call the commands plot(theta, L(:,12)) and plot(theta, L(:,25)). 8.1.2 Maximum-likelihood estimate The likelihood is the PDF of X but viewed as a function of θ. The optimization problem of maximizing L(θ|x) is called the maximum-likelihood (ML) estimation: Definition 8.4. Let L(θ) be the likelihood function of the parameter θ given the measurements x=[x ,...,x ]T. The maximum-likelihood estimate of the parameter 1 N θ is a parameter that maximizes the likelihood: def θ(cid:98)ML = argmax L(θ|x). (8.7) θ 472
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Example 8.4. Find the ML estimate for a set of i.i.d. Bernoulli random variables {X ,...,X } with X ∼Bernoulli(θ) for n=1,...,N. 1 N n Solution. We know that the log-likelihood function of a set of i.i.d. Bernoulli random variables is given by (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) logL(θ|x)= x ·logθ+ N − x ·log(1−θ). (8.8) n n n=1 n=1 Thus, to find the ML estimate, we need to solve the optimization problem (cid:40)(cid:32) N (cid:33) (cid:32) N (cid:33) (cid:41) (cid:88) (cid:88) θ(cid:98)ML =argmax x n ·logθ+ N − x n ·log(1−θ) . θ n=1 n=1 Taking the derivative with respect to θ and setting it to zero, we obtain (cid:40)(cid:32) N (cid:33) (cid:32) N (cid:33) (cid:41) d (cid:88) (cid:88) x ·logθ+ N − x ·log(1−θ) =0. dθ n n n=1 n=1 This gives us (cid:16) (cid:17) (cid:80)N n=1x n − N −(cid:80)N n=1x n =0. θ 1−θ Rearranging the terms yields N 1 (cid:88) θ(cid:98)ML = N x n. n=1 Let’s do a sanity check to see if this result makes sense. The solution to this problem says that θ(cid:98)ML is the empirical average of the measurements. Assume that N = 50. Let us consider two particular scenarios as illustrated in Figure 8.4. • Scenario 1: x is a vector of measurements such that S d =ef (cid:80)N x = 25. Since n=1 n N =50, the formula tells us that θ(cid:98)ML = 52 05 =0.5. This is the best guess based on the 50 measurements where 25 are heads. If you look at Figure 8.3 and Figure 8.4, when S = 25, we are looking at a particular cross section in the 2D plot. The likelihood function we are inspecting is L(θ|S =25). For this likelihood function, the maximum occurs at θ =0.5. • Scenario 2:xisavectorofmeasurementssuchthatS d =ef(cid:80)N x =12.Theformula n=1 n tells us that θ(cid:98)ML = 1 52 0 = 0.24. This is again the best guess based on the 50 measure- ments where 12 are heads. Referring to Figure 8.3 and Figure 8.4, we can see that the likelihood function corresponds to another cross section L(θ|S = 12) where the maximum occurs at θ =0.24. Atthispoint,youmaywonderwhytheshapeofthelikelihoodfunctionL(θ|x)changes so radically as x changes? The answer can be found in Figure 8.5. Imagine that we have N = 50 measurements of which S = 40 give us heads. If these i.i.d. Bernoulli random 473
CHAPTER 8. ESTIMATION Figure 8.4: Illustration of how the maximum-likelihood estimate of a set of i.i.d. Bernoulli random variables is determined. The subfigures above show two particular scenarios at S = 25 and S = 12, assuming that N = 50. When S = 25, the likelihood function has a quadratic shape centered at θ=0.5.ThispointisalsothepeakofthelikelihoodfunctionwhenS =25.Therefore,theMLestimate is θ(cid:98)ML =0.5. The second case is when S =12. The quadratic likelihood is shifted toward the left. The ML estimate is θ(cid:98)ML =0.24. variables have a parameter θ = 0.5, it is quite unlikely that we will get 40 out of 50 measurements to be heads. (If it were θ = 0.5, we should get more or less 25 out of 50 heads.) When S = 40, and without any additional information about the experiment, the most logical guess is that the Bernoulli random variables have a parameter θ = 0.8. Since the measurement S can be as extreme as 0 out of 50 or 50 out of 50, the likelihood function L(θ|x) has to reflect these extreme cases. Therefore, as we change x, we observe a big change in the shape of the likelihood function. As you can see from Figure 8.5, S = 40 corresponds to the marked vertical cross section. As we determine the maximum-likelihood estimate, we search among all the possi- bilities,suchasθ =0.2,θ =0.5,θ =0.8,etc.Thesepossibilitiescorrespondtothehorizontal lines we drew in the figure. Among those horizontal lines, it is clear that the best estimate occurs when θ =0.8, which is also the ML estimate. Visualizing ML estimation as N grows Maximum-likelihood estimation can also be understood directly from the PDF instead of the likelihood function. To explain this perspective, let’s do a quick exercise. Practice Exercise 8.2. Suppose that X is a Gaussian random variable. Assume n that σ =1 is known but the mean θ is unknown. Find the ML estimate of the mean. Solution. The ML estimate θ(cid:98)ML is θ(cid:98)ML =argmax logL(θ|x). θ 474
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Figure 8.5: Suppose that we have a set of measurements such that S = 40. To determine the ML estimate, we look at the vertical cross section at S = 40. Among the different candidate parameters, e.g., θ =0.2, θ =0.5 and θ =0.8, we pick the one that has the maximum response to the likelihood function.ForS =40,itismorelikelythattheunderlyingparameterisθ=0.8thanθ=0.2orθ=0.5. With some calculation, we can show that θ(cid:98)ML =argm θ ax log(cid:40) (cid:89)N √1 2π exp(cid:26) −(x n− 2 θ)2(cid:27)(cid:41) n=1 N N 1 (cid:88) =argmax − log(2π)− (x −θ)2. 2 2 n θ n=1 Taking the derivative with respect to θ, we obtain (cid:40) N (cid:41) d N 1 (cid:88) − log(2π)− (x −θ)2 =0. dθ 2 2 n n=1 This gives us (cid:80)N (x −θ)=0. Therefore, the ML estimate is n=1 n N 1 (cid:88) θ(cid:98)ML = N x n. n=1 Now we will draw the PDF and compare it with the measured data points. Our focus is to analyze how the ML estimate changes as N grows. WhenN =1.Thereisonlyoneobservationx .ThebestGaussianthatfitsthissample 1 must be the one that is centered at x . In fact, the optimization is1 1 (cid:26) 1 (cid:26) (x −θ)2(cid:27)(cid:27) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 =argm θ ax −(x 1−θ)2 =x 1. 1Weskipthestepofcheckingwhetherthestationarypointisamaximumoraminimum,whichcanbe done by evaluating the second-order derivative. In fact, since the function −(x1−θ)2 is concave in θ, a stationarypointmustbeamaximum. 475
CHAPTER 8. ESTIMATION Therefore, the ML estimate is θ(cid:98)ML =x 1. Figure 8.6 illustrates this case. As we conduct the ML estimation, we imagine that there are a few candidate PDFs. The ML estimation says thatamongallthesecandidatePDFsweneedtofindonethatcanmaximizetheprobability of obtaining the observation x . Since we only have one observation, we have no choice but 1 to pick a Gaussian centered at x . Certainly the sample X =x could be bad, and we may 1 1 1 findawrongGaussian.However,withonlyonesamplethereisnowayforustomakebetter decisions. 0.5 0.45 Data Point 0.4 Candidate PDF 0.35 Estimated PDF 0.3 0.25 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 x Figure 8.6: N =1. Suppose that we are given one observed data point located around x=−2.1. To conducttheMLestimationweproposeafewcandidatePDFs,eachbeingaGaussianwithunitvariance but a different mean θ. The ML estimate is a parameter θ such that the corresponding PDF matches thebestwiththeobserveddata.InthisexamplethebestmatchhappenswhentheestimatedGaussian PDF is centered at x . 1 When N = 2. In this case we need to find a Gaussian that fits both x and x . The 1 2 probability of simultaneously observing x and x is determined by the joint distribution. 1 2 By independence we then have (cid:40)(cid:18) 1 (cid:19)2 (cid:26) (x −θ)2+(x −θ)2)(cid:27)(cid:41) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 2 (cid:26) (x −θ)2+(x −θ)2(cid:27) x +x =argmax − 1 2 = 1 2, 2σ2 2 θ where the last step is obtained by taking the derivative: d (cid:8) (x −θ)2+(x −θ)2(cid:9) =2(x −θ)+2(x −θ). dθ 1 2 1 2 Equating this with zero yields the solution θ = x1+x2. Therefore, the best Gaussian that 2 fits the observations is Gaussian(x1+x2,σ2). 2 Does this result make sense? When you have two data points x and x , the ML 1 2 estimation is trying to find a Gaussian that can best fit both of these two data points. Your best bet here is θ(cid:98)ML =(x 1+x 2)/2, because there are no other choices. If you choose θ(cid:98)ML = x 1 or θ(cid:98)ML = x 2, it cannot be a good estimate because you are not using both data points. As shown in Figure 8.7, for these two observed data points x and x , the PDF 1 2 marked in red (which is a Gaussian centered at (x +x )/2) is indeed the best fit. 1 2 476
8.1. MAXIMUM-LIKELIHOOD ESTIMATION 0.5 0.45 Data Point 0.4 Candidate PDF 0.35 Estimated PDF 0.3 0.25 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 x Figure 8.7: N = 2. Suppose that we are given two observed data points located around x = −0.98 1 and x = −1.15. To conduct the ML estimation we propose a few candidate PDFs, each being a 2 Gaussian with unit variance but a different mean θ. The ML estimate is a parameter θ such that the correspondingPDFbestmatchestheobserveddata.Inthisexamplethebestmatchhappenswhenthe estimated Gaussian PDF is centered at (x +x )/2≈−1.07. 1 2 When N = 10 and N = 100. We can continue the above calculation for N = 10 and N =100. In this case the MLE is (cid:40)(cid:18) 1 (cid:19)N (cid:26) (x −θ)2+···+(x −θ)2(cid:27)(cid:41) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 N =argmax −(cid:88)N (x n−θ)2 = 1 (cid:88)N x . 2σ2 N n θ n=1 n=1 where the optimization is solved by taking the derivative: N N d (cid:88) (cid:88) (x −θ)2 =−2 (x −θ) dθ n n n=1 n=1 Equating this with zero yields the solution θ = 1 (cid:80)N x . N n=1 n The result suggests that for an arbitrary number of training samples the ML estimate is the sample average. These cases are illustrated in Figure 8.8. As you can see, the red curves (the estimated PDF) are always trying to fit as many data points as possible. The above experiment tells us something about the ML estimation: How does ML estimation work, intuitively? • The likelihood function L(θ|x) measures how “likely” it is that we will get x if the underlying parameter is θ. • InthecaseofaGaussianwithanunknownmean,youmovearoundtheGaussian until you find a good fit. 477
CHAPTER 8. ESTIMATION 0.5 0.5 0.45 Data Point 0.45 Data Point 0.4 Candidate PDF 0.4 Candidate PDF 0.35 Estimated PDF 0.35 Estimated PDF 0.3 0.3 0.25 0.25 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (c) N =10 (d) N =100 Figure 8.8: When N = 10 and N = 100, the ML estimation continues to evaluate the different candidate PDFs. For a given set of data points, the ML estimation picks the best PDF to fit the data points. In this Gaussian example it was shown that the optimal parameter is θ(cid:98)ML = (1/N)(cid:80)N n=1x n, which is the sample average. 8.1.3 Application 1: Social network analysis MLestimationhasextremelybroadapplicability.Inthissubsectionandthenextwediscuss two real examples. We start with an example in social network analysis. In Chapter 3, when we discussed the Bernoulli random variables, we introduced the Erd˝os-R´enyigraph—oneofthesimplestmodelsforsocialnetworks.TheErd˝os-R´enyigraph isasingle-membershipnetworkthatassumesthatallusersbelongtothesamecluster.Thus theconnectivitybetweenusersisspecifiedbyasingleparameter,whichisalsotheprobability of the Bernoulli random variable. In our discussions in Chapter 3 we defined an adjacency matrix to represent a graph. Theadjacencymatrixisabinarymatrix,withthe(i,j)thentryindicatinganedgeconnect- ing nodes i and j. Since the presence and absence of an edge is binary and random, we may model each element of the adjacency matrix as a Bernoulli random variable X ∼Bernoulli(p). ij In other words, the edge X linking user i and user j in the network is either X =1 with ij ij probability p, or X =0 with probability 1−p. In terms of notation, we define the matrix ij X ∈RN×N as the adjacency matrix, with the (i,j)th element being X . ij Afewexamplesofasingle-membershipErd˝os-R´enyigraphareshowninFigure8.9.As the figure shows, the network connectivity increases as the Bernoulli parameter p increases. This happens because p defines the “density” of the edges. If p is large, we have a greater chance of getting X = 1, and so there is a higher probability that an edge is present ij between node i and node j. If p is small, the probability is lower. Supposethatwearegivenone snapshotofthenetwork,i.e.,onerealizationx∈RN×N of the adjacency matrix X ∈RN×N. The problem of recovering the latent parameter p can be formulated as an ML estimation. 478
8.1. MAXIMUM-LIKELIHOOD ESTIMATION p = 0.3 p = 0.5 p = 0.7 p = 0.9 4 --012 21 2 5 32 4 8 3 1 96 1 1 1 337 6 8 0 1 4 612 8 307 7 2 1 3 4 8 23 73 9 3 1 45 2 2 0 0 61 3 1 29 13 2 23 3 2 113 5 29 5 2 4 21 ---0123 321 114 33 54 3 256 2 6 1 1 9 2 9 34 03 3 2 4 1 3 93 7 20 3 1 4 28 3 6 1 2 2 1 5 1 88 1 2 1 7 23 79 2 1 3 1 262 5 8 0 10 3 3 72 ---0123 321 3 2 2 1 1 41 1 2 3 4 5 1 0 56 2 1 39 34 8 1 3 4 11 2 5 2 7 5 3 8 73 0 33 32 9 1 2 1 2 91 2 2 8 73 236 8 932 6 2 11 0 60 3 47 -02 2 7 32 220 6 2 6 3 4 15 9 1 2 5 2 2 1 3 3 6 6 1 7 1 91 345 3 1 1 3 23 1 1 2 2 338 9 74 38 9 11 3 18 38 4 2 31 0 0 2 5 4 02 72 -3 -4 -4 -4 -2 0 2 -2 0 2 -4 -2 0 2 4 -4 -2 0 2 4 (a) Graph representations of Erd˝os-R´enyi graphs at different p. (b) Adjacent matrices of the corresponding graphs. Figure 8.9: A single-membership Erdo˝s-R´enyi graph is a graph structure in which the edge between nodeiandnodej isdefinedasaBernoullirandomvariablewithparameterp.Aspincreases,thegraph has a higher probability of having more edges. The adjacent matrices shown in the bottom row are the mathematical representations of the graphs. Example8.5.Writedownthelog-likelihoodfunctionofthesingle-membershipErd˝os- R´enyi graph ML estimation problem. Solution. Based on the definition of the graph model, we know that X ∼Bernoulli(p). ij Therefore, the probability mass function of X is ij P[X =1]=p and P[X =0]=1−p. ij ij This can be compactly expressed as N N (cid:89)(cid:89) f (x; p)= pxij(1−p)1−xij. X i=1j=1 Hence, the log-likelihood is N N (cid:88)(cid:88) logL(p|x)= {x logp+(1−x )log(1−p)}. ij ij i=1j=1 Now that we have the log-likelihood function, we can proceed to estimate the param- eter p. The solution to this is the ML estimate. 479
CHAPTER 8. ESTIMATION Practice Exercise 8.3. Solve the ML estimation problem: p =argmax logL(p|x). (cid:98)ML p Solution. Using the log-likelihood we just derived, we have that N N (cid:88)(cid:88) p = {x logp+(1−x )log(1−p)}. (cid:98)ML ij ij i=1j=1 Taking the derivative and setting it to zero,   d d (cid:88)N (cid:88)N  logL(p|x)= {x logp+(1−x )log(1−p)} dp dp ij ij   i=1j=1 N N (cid:26) (cid:27) =(cid:88)(cid:88) x ij − 1−x ij =0. p 1−p i=1j=1 Let S =(cid:80)N (cid:80)N x . The equation above then becomes i=1 j=1 ij S N2−S − =0. p 1−p Rearranging the terms yields (1−p)S =p(N2−S), which gives us N N S 1 (cid:88)(cid:88) p = = x . (8.9) (cid:98)ML N2 N2 ij i=1j=1 Oncomputers,visualizingthegraphsandcomputingtheMLestimatesarereasonably straightforward. In MATLAB, you can call the command graph to build a graph from the adjacencymatrixA.Thiswillallowyoutoplotthegraph.Thecomputation,however,isdone directlybytheadjacencymatrix.Inthecodebelow,youcanseethatwecallrandtogenerate the Bernoulli random variables. The command triu extracts the upper triangular matrix from the matrix A. This ensures that we do not pick the diagonals. The symmetrization of A+A’ ensures that the graph is indirectional, meaning that i to j is the same as j to i. % MATLAB code to visualize a graph n = 40; # Number of nodes p = 0.3 # probability A = rand(n,n)<p; A = triu(A,1); A = A+A’; # Adj matrix G = graph(A); # Graph plot(G); # Drawing p_ML = mean(A(:)); # ML estimate 480
8.1. MAXIMUM-LIKELIHOOD ESTIMATION In Python, the computation is done similarly with the help of the networkx library. Thenumberofedgesmisdefinedasm=pn2.Thisisbecauseforagraphwithnnodes,there 2 are at most n2 unique pairs ofindirected edges. Multiplying this number by the probability 2 p will give us the number of edges m. # Python code to visualize a graph import networkx as nx import numpy as np n = 40 # Number of nodes p = 0.3 # probability m = np.round(((n ** 2)/2)*p) # Number of edges G = nx.gnm_random_graph(n,m) # Graph A = nx.adjacency_matrix(G) # Adj matrix nx.draw(G) # Drawing p_ML = np.mean(A) # ML estimate AsyoucanseeinboththeMATLABandthePythoncode,theMLestimatep isde- (cid:98)ML terminedbytakingthesampleaverage.ThustheMLestimate,accordingtoourcalculation, is p = 1 (cid:80)N (cid:80)N x . (cid:98)ML N2 i=1 j=1 ij 8.1.4 Application 2: Reconstructing images Beingabletoseeinthedarkistheholygrailofimaging.Manyadvancedsensingtechnologies havebeendevelopedoverthepastdecade.Inthisexample,weconsiderasingle-photonimage sensor. This is a counting device that counts the number of photons arriving at the sensor. Physicists have shown that a Poisson process can model the arrival of the photons. For simplicity we assume a homogeneous pattern of N pixels. The underlying intensity of the homogeneous pattern is a constant λ. Suppose that we have a sensor with N pixels X ,...,X . According to the Poisson 1 N statistics,theprobabilityofobservingapixelvalueisdeterminedbythePoissonprobability: X ∼Poisson(λ), n=1,...,N, n or more explicitly, λxn P[X =x ]= e−λ, n n x ! n where x is the nth observed pixel value, and is an integer. n A single-photon image sensor is slightly more complicated in the sense that it does not report X but instead reports a truncated version of X . Depending on the number of n n incoming photons, the sensor reports (cid:40) 1, X ≥1, Y = n (8.10) n 0, X =0. n We call this type of sensors a one-bit single-photon image sensor (see Figure 8.10). Our question is: If we are given the measurements X ,...,X , can we estimate the underlying 1 N parameter λ? 481
CHAPTER 8. ESTIMATION Figure 8.10: A one-bit single-photon image sensor captures an image with binary bits: It reports a “1” when the number of photons exceeds certain threshold, and “0” otherwise. The recovery problem here is to estimate the underlying image from the measurements. Example 8.6. Derive the log-likelihood function of the estimation problem for the single-photon image sensors. Solution.SinceY isabinaryrandomvariable,itsprobabilityiscompletelyspecified n by the two states it takes: P[Y =0]=P[X =0]=e−λ n n P[Y =1]=P[X (cid:54)=0]=1−e−λ. n n Thus, Y is a Bernoulli random variable with probability 1−e−λ of getting a value n of 1, and probability e−λ of getting a value of 0. By defining y as a binary number n taking values of either 0 or 1, it follows that the log-likelihood is (cid:26) N (cid:27) logL(λ|y)=log (cid:89)(cid:0) 1−e−λ(cid:1)yn(cid:0) e−λ(cid:1)1−yn n=1 N (cid:26) (cid:27) (cid:88) = y log(1−e−λ)−λ(1−y ) . n n n=1 Practice Exercise 8.4. Solve the ML estimation problem λ(cid:98)ML =argmax logL(λ|y). (8.11) λ Solution. First, we define S =(cid:80)N y . This simplifies the log-likelihood function to n=1 n N (cid:26) (cid:27) (cid:88) logL(λ|y)= y log(1−e−λ)−λ(1−y ) n n n=1 =Slog(1−e−λ)−λ(N −S). 482
8.1. MAXIMUM-LIKELIHOOD ESTIMATION The ML estimation is λ(cid:98)ML =argmax Slog(1−e−λ)−λ(N −S). λ Taking the derivative w.r.t. λ yields (cid:26) (cid:27) d S Slog(1−e−λ)−λ(N −S) = e−λ−(N −S). dλ 1−e−λ Moving around the terms, it follows that (cid:18) (cid:19) S S e−λ−(N −S)=0 =⇒ λ=−log 1− . 1−e−λ N Therefore, the ML estimate is (cid:32) N (cid:33) 1 (cid:88) λ(cid:98)ML =−log 1− N y n . (8.12) n=1 Forrealimages,youcanextrapolatetheideafromy toy ,whichdenotesthe(i,j)th n i,j,t pixel located at time t. Defining y ∈RN×N as the tth frame of the observed data, we can t use T frames to recover one image λ(cid:98)ML ∈RN×N. It follows from the above derivation that the ML estimate is (cid:32) T (cid:33) 1 (cid:88) λ(cid:98)ML =−log 1− T y t . (8.13) t=1 Figure 8.11 shows a pair of input-output images of a 256×256 image. (a) Observed data (1-frame) (b) ML estimate (using 100 frames) Figure 8.11: ML estimation for a single-photon image sensor problem. The observed data consists of 100 frames of binary measurements y ,...,y , where T = 100. The ML estimate is constructed by 1 T λ=−log(1− 1 (cid:80)T y ). T t=1 t On a computer the ML estimation can be done in a few lines of MATLAB code. The code in Python requires more work, as it needs to read images using the openCV library. 483
CHAPTER 8. ESTIMATION % MATLAB code to recover an image from binary measurements lambda = im2double(imread(’cameraman.tif’)); T = 100; % 100 frames x = poissrnd( repmat(lambda, [1,1,T]) ); % generate Poisson r.v. y = (x>=1); % binary truncation lambdahat = -log(1-mean(y,3)); % ML estimation figure(1); imshow(x(:,:,1)); figure(2); imshow(lambdahat); # Python code to recover an image from binary measurements import cv2 import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt lambd = cv2.imread(’./cameraman.tif’) # read image lambd = cv2.cvtColor(lambd, cv2.COLOR_BGR2GRAY)/255 # gray scale T = 100 lambdT = np.repeat(lambd[:, :, np.newaxis], T, axis=2) # repeat image x = stats.poisson.rvs(lambdT) # Poisson statistics y = (x>=1).astype(float) # binary truncation lambdhat = -np.log(1-np.mean(y,axis=2)) # ML estimation plt.imshow(lambdhat,cmap=’gray’) 8.1.5 More examples of ML estimation By now you should be familiar with the procedure for solving the ML estimation problem. We summarize the two steps as follows. How to solve an ML estimation problem • Write down the likelihood L(θ|x). • Maximize the likelihood by solving θ(cid:98)ML =argmax logL(θ|x). θ Practice Exercise 8.5 (Gaussian). Suppose that we are given a set of i.i.d. Gaus- sian random variables X ,...,X , where both the mean µ and the variance σ2 are 1 N unknown. Let θ =[µ,σ2]T be the parameter. Find the ML estimate of θ. Solution. We first write down the likelihood. The likelihood of these i.i.d. Gaussian random variables is (cid:18) 1 (cid:19)N (cid:40) 1 (cid:88)N (cid:41) L(θ|x)= √ exp − (x −µ)2 . 2πσ2 2σ2 n n=1 484
8.1. MAXIMUM-LIKELIHOOD ESTIMATION To solve the ML estimation problem, we maximize the log-likelihood: def θ(cid:98)ML = argmax L(θ|x) θ (cid:40) N (cid:41) N 1 (cid:88) =argmax − log(2πσ2)− (x −µ)2 . 2 2σ2 n µ,σ2 n=1 Since we have two parameters, we need to take the derivatives for both. (cid:40) N (cid:41) d N 1 (cid:88) − log(2πσ2)− (x −µ)2 =0, dµ 2 2σ2 n n=1 (cid:40) N (cid:41) d N 1 (cid:88) − log(2πσ2)− (x −µ)2 =0. dσ2 2 2σ2 n n=1 (Note that the derivative of the second equation is taken w.r.t. to σ2 and not σ.) This pair of equations gives us N N 1 (cid:88) N 1 1 (cid:88) (x −µ)=0, and − · ·(2π)+ (x −µ)2 =0. σ2 n 2 2πσ2 2σ4 n n=1 n=1 Rearranging the equations, we find that N N 1 (cid:88) 1 (cid:88) µ = x and σ2 = (x −µ )2. (8.14) (cid:98)ML N n (cid:98)ML N n (cid:98)ML n=1 n=1 Practice Exercise 8.6. (Poisson) Given a set of i.i.d. Poisson random variables X ,...,X with an unknown parameter λ, find the ML estimate of λ. 1 N Solution. For a Poisson random variable, the likelihood function is (cid:89)N (cid:26) λxn (cid:27) L(λ|x)= e−λ . (8.15) x ! n n=1 To solve the ML estimation problem, we note that (cid:40) (cid:89)N λxn (cid:41) λ(cid:98)ML =argmax L(λ|x)=argmax log x !e−λ λ λ n n=1 =argmax log(cid:26) λ(cid:80) nxn e−Nλ(cid:27) . (cid:81) x ! λ n n (cid:81) Since x !isindependentofλ,itspresenceorabsencewillnotaffecttheoptimization n n 485
CHAPTER 8. ESTIMATION problem. Consequently we can drop the term. It follows that λ(cid:98)ML =argmax log(cid:110) λ(cid:80) nxne−Nλ(cid:111) λ (cid:32) (cid:33) (cid:88) =argmax x logλ−Nλ. n λ n Taking the derivative and setting it to zero yields (cid:40)(cid:32) (cid:33) (cid:41) (cid:80) d (cid:88) x logλ−Nλ = nx n −N =0. dλ n λ n Rearranging the terms yields N 1 (cid:88) λ(cid:98)ML = N x n. (8.16) n=1 The idea of ML estimation can also be extended to vector observations. Example 8.7. (High-dimensional Gaussian) Suppose that we are given a set of i.i.d. d-dimensional Gaussian random vectors X ,...,X such that 1 N X ∼Gaussian(µ,Σ). n We assume that Σ is fixed and known, but µ is unknown. Find the ML estimate of µ. Solution. The likelihood function is N (cid:89) L(µ|{x }N )= f (x ; µ) n n=1 Xn n n=1 N (cid:40) (cid:26) (cid:27)(cid:41) (cid:89) 1 1 = exp − (x −µ)TΣ−1(x −µ) (cid:112) (2π)d|Σ| 2 n n n=1 (cid:32) (cid:33)N (cid:40) N (cid:41) 1 1 (cid:88) = exp − (x −µ)TΣ−1(x −µ) . (cid:112) (2π)d|Σ| 2 n n n=1 Thus the log-likelihood function is N (cid:26) (cid:27) N N (cid:88) 1 logL(µ|{x }N )= log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) . n n=1 2 2 2 n n n=1 The ML estimate is found by maximizing this log-likelihood function: µ =argmax logL(µ|{x }N ). (cid:98)ML n n=1 µ 486
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Taking the gradient of the function and setting it to zero, we have that (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0. dµ 2 2 2 n n n=1 Thederivativesofthefirsttwotermsarezerobecausetheydonotdependonµ).Thus we have that: N (cid:26) (cid:27) (cid:88) Σ−1(x −µ) =0. n n=1 Rearranging the terms yields the ML estimate N 1 (cid:88) µ = x . (cid:98)ML N n n=1 Example 8.8. (High-dimensional Gaussian) Assume the same problem setting as in Example 8.7, except that this time we assume that both the mean vector µ and the covariance matrix Σ are unknown. Find the ML estimate for θ =(µ,Σ). Solution. The log-likelihood follows from Example 8.7: N N logL(µ|{x }N )= log|Σ|+ log(2π)d n n=1 2 2 N (cid:26) (cid:27) (cid:88) 1 + (x −µ)TΣ−1(x −µ) . 2 n n n=1 FindingtheMLestimaterequirestakingthederivativewithrespecttobothµandΣ: (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0, dµ 2 2 2 n n n=1 (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0. dΣ 2 2 2 n n n=1 After some tedious algebraic steps (see Duda et al., Pattern Classification, Problem 3.14), we have that N 1 (cid:88) µ = x , (8.17) (cid:98)ML N n n=1 N 1 (cid:88) Σ(cid:98)ML = N (x n−µ (cid:98)ML)(x n−µ (cid:98)ML)T. (8.18) n=1 487
CHAPTER 8. ESTIMATION 8.1.6 Regression versus ML estimation ML estimation is closely related to regression. To understand the connection, we consider a linear model that we studied in Chapter 7. This model describes the relationship between the inputs x ,...,x and the observed outputs y ,...,y , via the equation 1 N 1 N d−1 (cid:88) y = θ φ (x )+e , n=1,...,N. (8.19) n p p n n p=0 In this expression, φ (·) is a transformation that extracts the “features” of the input vector p x to produce a scalar. The coefficient θ defines the relative weight of the feature φ (x ) in p p n constructing the observed variable y . The error e defines the modeling error between the n n observation y and the prediction (cid:80)d−1θ φ (x ). We call this equation a linear model. n p=0 p p n Expressed in matrix form, the linear model is        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . ,  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X =θ =e or more compactly as y =Xθ+e. Rearranging the terms, it is easy to show that N N (cid:32) d−1 (cid:33)2 (cid:88) (cid:88) (cid:88) e2 = y − θ φ (x ) n n p p n n=1 n=1 p=0 N = (cid:88) (y −[Xθ] )2 =(cid:107)y−Xθ(cid:107)2. n n n=1 Nowwemakeanassumption:thateachnoisee isani.i.d.copyofaGaussianrandom n variable with zero mean and variance σ2. In other words, the error vector e is distributed according to e ∼ Gaussian(0,σ2I). This assumption is not always true because there are many situations in which the error is not Gaussian. However, this assumption is necessary for us to make the connection between ML estimation and regression. With this assumption, we ask, given the observations y ,...,y , what would be the 1 N ML estimate of the unknown parameter θ? We answer this question in two steps. Example 8.9. Find the likelihood function of θ, given y =[y ,...,y ]T. 1 N Solution. The PDF of y is given by a Gaussian: f (y; θ)= (cid:89)N (cid:26) √ 1 exp(cid:26) −(y n−[Xθ] n)2(cid:27)(cid:27) Y 2πσ2 2σ2 n=1 (cid:40) N (cid:41) 1 1 (cid:88) = exp − (y −[Xθ] )2 (cid:112) (2πσ2)N 2σ2 n n n=1 (cid:26) (cid:27) 1 1 = exp − (cid:107)y−Xθ(cid:107)2 . (8.20) (cid:112) (2πσ2)N 2σ2 488
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Therefore, the log-likelihood function is (cid:40) (cid:26) (cid:27)(cid:41) 1 1 logL(θ|y)=log exp − (cid:107)y−Xθ(cid:107)2 (cid:112) (2πσ2)N 2σ2 N 1 =− log(2πσ2)− (cid:107)y−Xθ(cid:107)2. 2 2σ2 The next step is to solve the ML estimation by maximizing the log-likelihood. Example 8.10.SolvetheMLestimationproblemstatedinExample8.9.Assumethat XTX is invertible. Solution. θ(cid:98)ML =argmax logL(θ|y) θ (cid:26) (cid:27) N 1 =argmax − log(2πσ2)− (cid:107)y−Xθ(cid:107)2 . 2 2σ2 θ Taking the derivative w.r.t. θ yields (cid:26) (cid:27) d N 1 − log(2πσ2)− (cid:107)y−Xθ(cid:107)2 =0. dθ 2 2σ2 Since d θTAθ =A+AT, it follows from the chain rule that dθ (cid:26) (cid:27) (cid:26) (cid:27) d 1 d 1 − (cid:107)y−Xθ(cid:107)2 = − (y−Xθ)T(y−Xθ) dθ 2σ2 dθ 2σ2 1 = XT(Xθ−y). σ2 Substituting this result into the equation, 1 XT(Xθ−y)=0. σ2 Rearranging terms we obtain XTXθ =XTy, of which the solution is θ(cid:98)ML =(XTX)−1XTy. (8.21) Since the ML estimate in Equation (8.21) is the same as the regression solution (see Chapter7),weconcludethattheregressionproblemofalinearmodelisequivalenttosolving an ML estimation problem. ThemaindifferencebetweenalinearregressionproblemandanMLestimationproblem is the underlying statistical model, as illustrated in Figure 8.12. In linear regression, you do not care about the statistics of the noise term e . We choose (·)2 as the error because it n is differentiable and convenient. In ML estimation, we choose (·)2 as the error because the noise is Gaussian. If the noise is not Gaussian, e.g., the noise follows a Laplace distribution, we need to choose |·| as the error. Therefore, you can always get a result by solving the linearregression.However,thisresultwillonlybecomemeaningfulifyouprovideadditional 489
CHAPTER 8. ESTIMATION Figure 8.12: ML estimation is equivalent to a linear regression when the underlying statistical model forMLestimationisaGaussian.Specifically,iftheerrorterme=y−Xθ isanindependentGaussian vectorwithzeromeanandcovariancematrixσ2I,thentheresultingMLestimationisthesameaslinear regression. If the underlying statistical model is not Gaussian, then solving the regression is equivalent to applying a Gaussian ML estimation to a non-Gaussian problem. This will still give us a result, but that result will not maximize the likelihood, and thus it will not have any statistical guarantee. information about the problem. For example, if you know that the noise is Gaussian, then the regression solution is also the ML solution. This is a statistical guarantee. In practice, of course, we do not know whether the noise is Gaussian or not. At this point we have two courses of action: (i) Use your prior knowledge/domain expertise to determine whether a Gaussian assumption makes sense, or (ii) select an alternative model and see if the alternative model fits the data better. In practice, we should also question whether maximizing the likelihood is what we want. We may have some knowledge and therefore prefer the parameter θ, e.g., we want a sparse solution so that θ only contains a few non-zeros. In that case, maximizing the likelihood without any constraint may not be the solution we want. ML estimation versus regression • ML estimation requires a statistical assumption, whereas regression does not. • Suppose that you use a linear model y = (cid:80)d−1θ φ (x ) + e where e ∼ n p=0 p p n n n Gaussian(0,σ2), for n=1,...,N. • Then the likelihood function in the ML estimation is (cid:26) (cid:27) 1 1 L(θ|y)= exp − (cid:107)y−Xθ(cid:107)2 , (cid:112) (2πσ2)N 2σ2 • The ML estimate θ(cid:98)ML is θ(cid:98)ML = (XTX)−1XTy, which is exactly the same as theregressionsolution.Iftheabovestatisticalassumptionsdonothold,thenthe regression solution will not maximize the likelihood. 490
8.2. PROPERTIES OF ML ESTIMATES 8.2 Properties of ML Estimates MLestimationisaveryspecialtypeofestimation.NotallestimationsareML.Ifanestimate is ML, are there any theoretical properties we can analyze? For example, will ML estimates guarantee the recovery of the true parameter? If so, when will this happen? In this section we investigate these theoretical questions so that you will acquire a better understanding of the statistical nature of ML estimates.2 8.2.1 Estimators We know that an ML estimate is defined as θ(cid:98)ML(x)=argmax L(θ|x). (8.22) θ We write θ(cid:98)ML(x) to emphasize that θ(cid:98)ML is a function of x. The dependency of θ(cid:98)ML(x) on x should not be a surprise. For example, if the ML estimate is the sample average, we have that N 1 (cid:88) θ(cid:98)ML(x 1,...,x N)= N x n, n=1 where x=[x ,...,x ]T. 1 N However, in this setting we should always remember that x ,...,x are realizations 1 N of the i.i.d. random variables X ,...,X . Therefore, if we want to analzye the randomness 1 N ofthevariables,itismorereasonabletowriteθ(cid:98)ML asarandomvariableΘ(cid:98)ML.Forexample, in the case of sample average, we have that N 1 (cid:88) Θ(cid:98)ML(X 1,...,X N)= N X n. (8.23) n=1 We call Θ(cid:98)ML the ML estimator of the true parameter θ. Estimate versus estimator N 1 (cid:88) • An estimate is a number, e.g., θ(cid:98)ML = N x n. It is the random realization of n=1 a random variable. N 1 (cid:88) • An estimator is a random variable, e.g., Θ(cid:98)ML = N X n. It takes a set of n=1 random variables and generates another random variable. 2For notational simplicity, in this section we will focus on a scalar parameter θ instead of a vector parameterθ. 491
CHAPTER 8. ESTIMATION The ML estimators are one type of estimator, namely those that maximize the likeli- hoodfunctions.Ifwedonotwanttomaximizethelikelihoodwecanstilldefineanestimator. An estimator is any function that takes the data points X ,...,X and maps them to a 1 N number (or a vector of numbers). That is, an estimator is Θ(cid:98)(X 1,...,X N). We call Θ(cid:98) the estimator of the true parameter θ. Example 8.11. Let X ,...,X be Gaussian i.i.d. random variables with unknown 1 N mean θ and known variance σ2. Construct two possible estimators. Solution. We define two estimators: N 1 (cid:88) Θ(cid:98)1(X 1,...,X N)= N X n, n=1 Θ(cid:98)2(X 1,...,X N)=X 1, Inthefirstcase,theestimatortakesallthesamplesandconstructsthesampleaverage. The second estimator takes all the samples and returns on the first element. Both are legitimate estimators. However, Θ(cid:98)1 is the ML estimator, whereas Θ(cid:98)2 is not. 8.2.2 Unbiased estimators Whileyoucandefineestimatorsinanywayyoulike,certainestimatorsaregoodandothers arebad.By“good”wemeanthattheestimatorcanprovideyouwiththeinformationabout thetrueparameterθ;otherwise,whywouldyouevenconstructsuchanestimator?However, thedifficultyhereisthatΘ(cid:98) isarandom variablebecauseitisconstructedfromX 1,...,X N. Therefore, we need to define different metrics to quantify the usefulness of the estimators. Definition 8.5. An estimator Θ(cid:98) is unbiased if E[Θ(cid:98)]=θ. (8.24) Unbiasedness means that the average of the random variable Θ(cid:98) matches the true parameter θ. In other words, while we allow Θ(cid:98) to fluctuate, we expect the average to match the true θ. If this is not the case, using more measurements will not help us get closer to θ. Example 8.12. Let X ,...,X be i.i.d. Gaussian random variables with a unknown 1 N mean θ. It has been shown that the ML estimator is N 1 (cid:88) Θ(cid:98)ML = N X n. (8.25) n=1 Is the ML estimator Θ(cid:98)ML unbiased? 492
8.2. PROPERTIES OF ML ESTIMATES Solution: To check the unbiasedness, we look at the expectation: N N 1 (cid:88) 1 (cid:88) E[Θ(cid:98)ML]= N E[X n]= N θ =θ. n=1 n=1 Thus, Θ(cid:98)ML = N1 (cid:80)N n=1X n is an unbiased estimator of θ. Example 8.13. Same as the example before, but this time we consider an estimator Θ(cid:98) =X 1+X 2+5. (8.26) Is this estimator unbiased? Solution: In this case, E[Θ(cid:98)]=E[X 1+X 2+5]=E[X 1]+E[X 2]+5=2θ+5(cid:54)=θ. Therefore, the estimator is biased. Example 8.14. Let X ,...,X be i.i.d. Gaussian random variables with unknown 1 N mean µ and unknown variance σ2. We have shown that the ML estimators are N N 1 (cid:88) 1 (cid:88) µ = X and σ2 = (X −µ )2. (cid:98)ML N n (cid:98)ML N n (cid:98)ML n=1 n=1 It is easy to show that E[µ ]=µ. How about σ2 ? Is it an unbiased estimator? (cid:98)ML (cid:98)ML Solution: For simplicity we assume µ=0 so that E[X2]=E[(X −0)2]=σ2. n n Note that N (cid:26) (cid:27) 1 (cid:88) E[σ2 ]= E[X2]−2E[µ X ]+E[µ2 ] (cid:98)ML N n (cid:98)ML n (cid:98)ML n=1 1 (cid:88)N    1 (cid:88)N  (cid:32) 1 (cid:88)N (cid:33)2  = N σ2−2E  N X jX n+E  N X n  .   n=1 j=1 n=1 By independence, we observe that E[X X ]=E[X ]E[X ]=0, for any j (cid:54)=n. There- j n j n fore,   N (cid:20) (cid:21) 1 (cid:88) 1 E  N X jX n= NE X 1X n+···+X NX n j=1 1 σ2 = (0+···+σ2+···+0)= . N N 493
CHAPTER 8. ESTIMATION Similarly, we have that (cid:32) 1 (cid:88)N (cid:33)2 1 (cid:88)N   (cid:88)   E  N X n = N2 E[X n2]+ E[X jX n]   n=1 n=1 j(cid:54)=n 1 (cid:88)N (cid:110) (cid:111) σ2 = σ2+0 = . N2 N n=1 Combining everything, we arrive at the result: 1 (cid:88)N    1 (cid:88)N  (cid:32) 1 (cid:88)N (cid:33)2  E[σ (cid:98)M2 L]= N σ2−2E  N X jX n+E  N X n    n=1 j=1 n=1 1 (cid:88)N (cid:26) 2σ2 σ2(cid:27) = σ2− + N N N n=1 N −1 = σ2, N which is not equal to σ2. Therefore, σ2 is a biased estimator of σ2. (cid:98)ML In the previous example, it is possible to construct an unbiased estimator for the variance. To do so, we can use N 1 (cid:88) σ2 = (X −µ )2, (8.27) (cid:98)unbias N −1 n (cid:98)ML n=1 sothatE[σ2 ]=σ2.However,notethatσ2 doesnotmaximizethelikelihood,sowhile (cid:98)unbias (cid:98)unbias youcangetunbiasedness,youcannotmaximizethelikelihood.Ifyouwanttomaximizethe likelihood, you cannot get unbiasedness. What is an unbiased estimator? • An estimator Θ(cid:98) is unbiased if E[Θ(cid:98)]=θ. • Unbiased means that the statistical average of Θ(cid:98) is the true parameter θ. • If X n ∼ Gaussian(θ,σ2), then Θ(cid:98) = (1/N)(cid:80)N n=1X n is unbiased, but Θ(cid:98) = X 1 is biased. 8.2.3 Consistent estimators Bydefinition,anestimatorΘ(cid:98)(X 1,...,X N)isafunctionofN randomvariablesX 1,...,X N. Therefore,Θ(cid:98)(X 1,...,X N)changesasN grows.InthissubsectionweanalyzehowΘ(cid:98) behaves when N changes. For notational simplicity we use the following notation: Θ(cid:98)N =Θ(cid:98)(X 1,...,X N). (8.28) Thus, as N increases, we use more random variables in defining Θ(cid:98)(X 1,...,X N). 494
8.2. PROPERTIES OF ML ESTIMATES p Definition 8.6. An estimator Θ(cid:98)N is consistent if Θ(cid:98)N −→θ, i.e., (cid:20)(cid:12) (cid:12) (cid:21) lim P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) =0. (8.29) N→∞ The definition here follows from our discussions of the law of large numbers in Chapter 6. The specific type of convergence is known as the convergence in probability. It says that as N grows, the estimator Θ(cid:98) will be close enough to θ so that the probability of getting a large deviation will diminish, as illustrated in Figure 8.13. 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) N =1 (b) N =2 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (c) N =4 (d) N =8 Figure 8.13: The four subfigures here illustrate the probability of error P(cid:2) |Θ(cid:98)N −θ| ≥ (cid:15)(cid:3) , which is represented by the areas shaded in blue. We assume that the estimator Θ(cid:98)N is a Gaussian random variable following a distribution Gaussian(0,σ2), where we set σ = 1. The threshold we use in this N figure is (cid:15) = 1. As N grows, we see that the probability of error diminishes. If the probability of error goes to zero, we say that the estimator is consistent. The examples in Figure 8.13 are typical situations for an estimator based on the sample average. For example, if we assume that X ,...,X are i.i.d. Gaussian copies of 1 N Gaussian(0,σ2), then the estimator N 1 (cid:88) Θ(cid:98)(X 1,...,X N)= N X n n=1 willfollowaGaussiandistributionGaussian(0,σ2).(PleaserefertoChapter6forthederiva- N tion.) Then, as N grows, the PDF of Θ(cid:98)N becomes narrower and narrower. For a fixed (cid:15), it followsthattheprobabilityoferrorwilldiminishtozero.Infact,wecanprovethat,forthis 495
CHAPTER 8. ESTIMATION example, (cid:20)(cid:12) (cid:12) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) =P Θ(cid:98)N −θ ≥(cid:15) +P Θ(cid:98)N −θ ≤−(cid:15) (cid:90) ∞ (cid:18) (cid:12) σ2(cid:19) (cid:90) θ−(cid:15) (cid:18) (cid:12) σ2(cid:19) = Gaussian z(cid:12)θ, dz+ Gaussian z(cid:12)θ, dz N N θ+(cid:15) −∞ (cid:90) ∞ 1 −(z−θ)2 (cid:90) θ−(cid:15) 1 −(z−θ)2 = (cid:112) e 2σ2/N dz+ (cid:112) e 2σ2/N dz 2πσ2/N 2πσ2/N θ+(cid:15) −∞ =(cid:90) ∞ √1 e−z 22 dz+(cid:90) − σ/√(cid:15) N √1 e−z 22 dz √(cid:15) 2π −∞ 2π σ/ N (cid:18) (cid:19) (cid:18) (cid:19) (cid:15) −(cid:15) =1−Φ √ +Φ √ σ/ N σ/ N (cid:18) (cid:19) −(cid:15) =2Φ √ . σ/ N Therefore, as N →∞, it holds that −√(cid:15) →−∞. Hence, σ/ N (cid:20)(cid:12) (cid:12) (cid:21) (cid:18) −(cid:15) (cid:19) lim P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) = lim 2Φ √ =0. N→∞ N→∞ σ/ N This explains why in Figure 8.13 the probability of error diminishes to zero as N grows. Therefore, we say that Θ(cid:98)N is consistent. In general, there are two ways to check whether an estimator is consistent: • Prove convergence in probability. This is based on the definition of a consistent estimator. If we can prove that lim P(cid:2) |Θ(cid:98)N −θ|≥(cid:15)(cid:3) =0, (8.30) N→∞ then we say that the estimator is consistent. • Prove convergence in mean squared error: lim E[(Θ(cid:98)N −θ)2]=0. (8.31) N→∞ Toseewhyconvergenceinthemeansquarederrorissufficienttoguaranteeconsistency, we recall Chebyshev’s inequality in Chapter 6, which says that P(cid:2) |Θ(cid:98)N −θ|≥(cid:15)(cid:3) ≤ E[(Θ(cid:98)N (cid:15)2−θ)2] . Thus, if lim N→∞E[(Θ(cid:98)N −θ)2] = 0, convergence in probability will also hold. How- ever,sincemeansquareconvergenceisstrongerthanconvergenceinprobability,being unable to show mean square convergence does not imply that an estimator is incon- sistent. Be careful not to confuse a consistent estimator and an unbiased estimator. The two are different concepts; one does not imply the other. 496
8.2. PROPERTIES OF ML ESTIMATES Consistent versus unbiased • Consistent = If you have enough samples, then the estimator Θ(cid:98) will converge to the true parameter. • Unbiasedness does not imply consistency. For example (Gaussian), if Θ(cid:98) =X 1, then E[X 1] = µ. But P[|Θ(cid:98) −µ| > (cid:15)] does not converge to 0 as N grows. So this estimator is inconsistent. (See Example 8.16 below.) • Consistency does not imply unbiasedness. For example (Gaussian), if N 1 (cid:88) Θ(cid:98) = N (X n−µ)2 n=1 is a biased estimate for variance, but it is consistent. (See Example 8.17 below.) Example 8.15.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and known variance σ2. We know that the ML estimator for the mean is µ =(1/N)(cid:80)N X . Is µ consistent? (cid:98)ML n=1 n (cid:98)ML Solution. We have shown that the ML estimator is N 1 (cid:88) µ = X . (cid:98)ML N n n=1 Since E[µ ]=µ, and E[(µ −µ)2]=Var[µ ]= σ2, it follows that (cid:98)ML (cid:98)ML (cid:98)ML N P(cid:2) |µ −µ|≥(cid:15)(cid:3) ≤ E[(µ (cid:98)ML−µ)2] = σ2 . (cid:98)ML (cid:15)2 N(cid:15)2 Thus, when N goes to infinity, the probability converges to zero, and hence the esti- mator is consistent. Example 8.16.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and known variance σ2. Define an estimator µ=X . Show that the estimator (cid:98) 1 is unbiased but inconsistent. Solution. We know that E[µ] = E[X ] = µ. So µ is an unbiased estimator. However, (cid:98) 1 (cid:98) we can show that E[(µ−µ)2]=E[(X −µ)2]=σ2. (cid:98) 1 SincethisvarianceE[(µ−µ)2]doesnotshrinkasN increases,itfollowsthatnomatter (cid:98) 497
CHAPTER 8. ESTIMATION how many samples we use we cannot make E[(µ−µ)2] go to zero. To be more precise, (cid:98) (cid:20) (cid:21) (cid:20) (cid:21) P |µ−µ|≥(cid:15) =P |X −µ|≥(cid:15) (cid:98) 1 (cid:20) (cid:21) (cid:20) (cid:21) =P X ≤µ−(cid:15) +P X ≥µ+(cid:15) 1 1 =(cid:90) µ−(cid:15) √ 1 e−(x 2− σµ 2)2 dx+(cid:90) ∞ √ 1 e−(x 2− σµ 2)2 dx 2πσ2 2πσ2 −∞ µ+(cid:15) (cid:18) (cid:19) −(cid:15) =2Φ , σ which does not converge to zero as N →∞. So the estimator is inconsistent. Example 8.17.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and an unknown variance σ2. Is the ML estimate of the variance, i.e., σ2 , (cid:98)ML consistent? Solution. We know that the ML estimator for the mean is N 1 (cid:88) µ = X , (cid:98)ML N n n=1 and we have shown that it is an unbiased and consistent estimator of the mean. For the variance, N N σ2 = 1 (cid:88) (X −µ )2 = 1 (cid:88)(cid:2) X2−2µ X +µ2 (cid:3) (cid:98)ML N n (cid:98)ML N n (cid:98)ML n (cid:98)ML n=1 n=1 N N 1 (cid:88) 1 (cid:88) = X2−2µ · X +µ2 N n (cid:98)ML N n (cid:98)ML n=1 n=1 N 1 (cid:88) = X2−µ2 . N n (cid:98)ML n=1 Note that 1 (cid:80)N X2 is the sample average of the second moment, and so by the N n=1 n weak law of large numbers it should converge in probability to E[X2]. Similarly, µ n (cid:98)ML will converge in probability to µ. Therefore, we have N σ2 = 1 (cid:88) X2−µ2 −p →(σ2+µ2)−µ2 =σ2. (cid:98)ML N n (cid:98)ML n=1 Thus, we have shown that the ML estimator of the variance is biased but consistent. 498
8.2. PROPERTIES OF ML ESTIMATES The following discussions about the consistency of ML estimators can be skipped. As we have said, there are many estimators. Some estimators are consistent and some arenot.TheMLestimatorsarespecial.Itturnsoutthatundercertainregularityconditions the ML estimators of i.i.d. observations are consistent. Without proving this result formally, we highlight a few steps to illustrate the idea. Suppose that we have a set of i.i.d. data points x ,...,x drawn from some distribution 1 N f(x,|θ ). To formulate the ML estimation, we consider the log-likelihood function (di- true vided by N): N 1 1 (cid:88) logL(θ|x)= logf(x ; θ). (8.32) N N n n=1 Here, the variable θ is unknown. We need to find it by maximizing the log-likelihood. By the weak law of large numbers, we can show that the log-likelihood based on the N samples will converge in probability to N 1 (cid:88) logf(x ; θ)−p →E[logf(x; θ)]. (8.33) N n n=1 (cid:124) (cid:123)(cid:122) (cid:125) gN(θ) The expectation can be evaluated by integrating over the true distribution: (cid:90) E[logf(x; θ)]= logf(x; θ)·f(x; θ )dx. true (cid:124) (cid:123)(cid:122) (cid:125) g(θ) where f(x; θ ) denotes the true distribution of the samples x ’s. From these two results true n we define two functions: def 1 (cid:88)N def(cid:90) g (θ) = logf(x ; θ), and g(θ) = logf(x; θ)·f(x; θ )dx, N N n true n=1 p and we know that g (θ)−→g(θ). N We also know that θ(cid:98)ML is the ML estimator, and so θ(cid:98)ML =argmax g N(θ). θ Let θ∗ be the maximizer of the limiting function, i.e., θ∗ =argmax g(θ). θ Because g N(θ) →p g(θ), we can (loosely3) argue that θ(cid:98)ML →p θ∗. If we can show that θ∗ =θ true, then we have shown that θ(cid:98)ML →p θ true, implying that θ(cid:98)ML is consistent. 3To rigorously prove this statement we need some kind of regularity conditions on gN and g. A more formalproofcanbefoundinH.VincentPoor,AnIntroductionSignalDetectionandEstimation,Springer, 1998,SectionIV.D. 499
CHAPTER 8. ESTIMATION To show that θ∗ =θ , we note that true d (cid:90) (cid:90) d logf(x; θ)·f(x; θ )dx= logf(x; θ)·f(x; θ )dx dθ true dθ true (cid:90) f(cid:48)(x; θ) = ·f(x; θ )dx. f(x; θ) true We ask whether this is equal to zero. Putting θ =θ , we have that true (cid:90) f(cid:48)(x; θ ) (cid:90) true ·f(x; θ )dx= f(cid:48)(x; θ )dx. f(x; θ ) true true true However, this integral can be simplified to (cid:90) f(cid:48)(x; θ true)dx= dd θ(cid:90) f(x; θ)dx(cid:12) (cid:12) (cid:12) (cid:12) =0. (cid:124) (cid:123)(cid:122) (cid:125) θ=θtrue =1 Therefore, θ is the maximizer for g(θ), and so θ =θ∗. true true End of the discussion. Please join us again. 8.2.4 Invariance principle Another useful property satisfied by the ML estimate is the invariance principle. The in- variance principle says that a monotonic transformation of the true parameter is preserved for the ML estimates. What is the invariance principle? • There is a monotonic function h. • There is an ML estimate θ(cid:98)ML for θ. • The monotonic function h maps the true parameter θ (cid:55)−→h(θ). • Then the same function will map the ML estimate θ(cid:98)ML (cid:55)−→h(θ(cid:98)ML). The formal statement of the invariance principle is given by the theorem below. Theorem 8.1. If θ(cid:98)ML is the ML estimate of θ, then for any one-to-one function h of θ, the ML estimate of h(θ) is h(θ(cid:98)ML). Proof. Define the likelihood function L(θ) (we have dropped x to simplify the notation). Then, for any monotonic function h, we have that L(θ)=L(h−1(h(θ))). 500
8.2. PROPERTIES OF ML ESTIMATES Let θ(cid:98)ML be the ML estimate: θ(cid:98)ML =argmax L(θ)=argmax L(h−1(h(θ))). θ θ By the definition of ML, θ(cid:98)ML must maximize the likelihood. Therefore, L(h−1(h(θ))) is maximizedwhenh−1(h(θ))=θ(cid:98)ML.Thisimpliesthath(θ)=h(θ(cid:98)ML)becausehismonotonic. Since h(θ) is the parameter we try to estimate, the equality h(θ) = h(θ(cid:98)ML) implies that h(θ(cid:98)ML) is the ML estimate of h(θ). (cid:3) Example 8.18. Consider the single-photon image sensor example we discussed in Section 8.1. We consider a set of i.i.d. Bernoulli random variables with PMF p (1)=1−e−η and p (0)=e−η. (8.34) Xn Xn Find the ML estimate through (a) direct calculation and (b) the invariance principle. Solution. (a) Following the example in Equation (8.12), the ML estimate of η is N (cid:89) η =argmax (1−e−η)xn(e−η)1−xn (cid:98)ML η n=1 (cid:32) N (cid:33) 1 (cid:88) =−log 1− x . N n n=1 (b) We can obtain the same result using the invariance principle. Since X is a n binary random variable, we assume that it is a Bernoulli with parameter θ. Then the ML estimate of θ is N (cid:89) θ(cid:98)ML =argmax θxn(1−θ)1−xn θ n=1 N 1 (cid:88) = x . N n n=1 The relationship between θ and η is that θ = 1−e−η, or η = −log(1−θ). So we let h(θ)=−log(1−θ). The invariance principle says that the ML estimate of h(θ) is def η = h(cid:100)(θ) (cid:98)ML ML (i) = h(θ(cid:98)ML) (cid:32) N (cid:33) 1 (cid:88) =−log 1− x , N n n=1 where (i) follows from the invariance principle. The invariance principle can be very convenient, especially when the transformation h is complicated, so that a direct evaluation of the ML estimate is difficult. 501
CHAPTER 8. ESTIMATION Figure 8.14: The invariance principle is a transformation of the ML estimate. In this example, we consideraBernoullilog-likelihoodfunctionshowninthelowermostplot.Forthislog-likelihood,theML estimate is θ(cid:98)ML = 0.4. On the left-hand side we show another log-likelihood, derived for a truncated Poisson random variable. Note that the ML estimate is η =0.5108. The invariance principle asserts (cid:98)ML that, instead of computing these ML estimates directly, we can first derive the relationship between η and θ for any θ. Since we know that θ = 1−e−η, it follows that η = −log(1−θ). We define this transformationasη=h(θ)=−log(1−θ).ThentheMLestimateisη (cid:98)ML =h(θ(cid:98)ML)=h(0.4)=0.5108. The invariance principle saves us the trouble of computing the maximization of the more truncated Poisson likelihood. The invariance principle is portrayed in Figure 8.14. We start with the Bernoulli log- likelihood logL(θ|S)=Slogθ+(1−S)log(1−θ). In this particular example we let S =20, where S denotes the sum of the N =50 Bernoulli random variables. The other log-likelihood is the truncated Poisson, which is given by logL(η|S)=Slog(1−e−η)+(1−S)log(e−η). The transformation between the two is the function η =h(θ)=−log(1−θ). Putting everything into the figure, we see that the ML estimate (θ = 0.4) is translated to η = 0.5108. The invariance principle asserts that this calculation can be done by η = (cid:98)ML h(θ(cid:98)ML)=h(0.4)=−0.5108. 502
8.3. MAXIMUM A POSTERIORI ESTIMATION 8.3 Maximum A Posteriori Estimation InMLestimation,theparameterθistreatedasadeterministicquantity.Thereare,however, many situations where we have some prior knowledge about θ. For example, we may not know exactly the speed of a car, but we may know that the speed is roughly 65 mph with a standard deviation of 5 mph. How do we incorporate such prior knowledge into the estimation problem? Inthissection,weintroducethesecondestimationtechnique,knownasthemaximum a posteriori(MAP)estimation.MAPestimationlinksthelikelihoodandtheprior.Thekey idea is to treat the parameter θ as a random variable (vector) Θ with a PDF f (θ). Θ 8.3.1 The trio of likelihood, prior, and posterior To understand how the MAP estimation works, it is important first to understand the role of the parameter θ, which changes from a deterministic quantity to a random quantity. Recall the likelihood function we defined in the ML estimation; it is L(θ|x)=f (x; θ), X ifweassumethatwehaveasetofi.i.d.observationsx=[x ,...,x ]T.BywritingthePDF 1 N of X as f (x; θ), we emphasize that θ is a deterministic but unknown parameter. There X is nothing random about θ. In MAP, we change the nature of θ from deterministic to random. We replace θ by Θ and write becomes f (x; θ) =⇒ f (x|θ). (8.35) X X|Θ The difference between the left-hand side and the right-hand side is subtle but important. On the left-hand side, f (x; θ) is the PDF of X. This PDF is parameterized by θ. On the X right-hand side, f (x|θ) is a conditional PDF of X given Θ. The values they provide X|Θ are exactly the same. However, in f (x|θ), θ is a realization of a random variable Θ. X|Θ Because Θ is now a random variable (vector), we can define its PDF (yes, the PDF of Θ), and denote it by f (θ), (8.36) Θ which is called the prior distribution. The prior distribution of Θ is unique in MAP estima- tion. There is nothing called a prior in ML estimation. Multiplying f (x|θ) with the prior PDF f (θ), and using Bayes’ Theorem, we X|Θ Θ obtain the posterior distribution: f (x|θ)f (θ) X|Θ Θ f (θ|x)= . (8.37) Θ|X f (x) X The posterior distribution is the PDF of Θ given the measurements X. Thelikelihood,theprior,andtheposteriorcanbeconfusing.Letusclarifytheirmean- ings. 503
CHAPTER 8. ESTIMATION • Likelihood f (x|θ): This is the conditional probability density of X given the pa- X|Θ rameter Θ. Do not confuse the likelihood f (x|θ) defined in the MAP context X|Θ and the likelihood f (x;|θ) defined in the ML context. The former assumes that Θ X is random whereas the latter assumes that θ is deterministic. They have the same values. • Prior f (θ): This is the prior distribution of Θ. It does not come from the data X Θ but from our prior knowledge. For example, if we see a bike on the road, even before we take any measurement we will have a rough idea of its speed. This is the prior distribution. • Posteriorf (θ|x):ThisistheposteriordensityofΘgiventhatwehaveobservedX. Θ|X Donotconfusef (θ|x)andL(θ|x).Theposteriordistributionf (θ|x)isaPDF Θ|X Θ|X of Θ given X = x. The likelihood L(θ|x) is not a PDF. If you integrate f (θ|x) Θ|X with respect to θ, you get 1, but if you integrate L(θ|x) with respect to θ, you do not get 1. What is the difference between ML and MAP? Likelihood ML f (x; θ) The parameter θ is deterministic. X MAP f (x|θ) The parameter Θ is random. X|Θ Prior ML There is no prior, because θ is deterministic. MAP f (θ) This is the PDF of Θ. Θ Optimization ML Find the peak of the likelihood f (x; θ). X MAP Find the peak of the posterior f (θ |x). Θ|X Maximum a posteriori (MAP) estimation is a form of Bayesian estimation. Bayesian methods emphasize our prior knowledge or beliefs about the parameters. As we will see shortly, the prior has something valuable to offer, especially when we have very few data points. 8.3.2 Understanding the priors Since the biggest difference between MAP and ML is the addition of the prior f (θ), we Θ need to take a closer look at what they mean. In Figure 8.15 below, we show a set of six different priors. We ask two questions: (1) What do they mean? (2) Which one should we use? What does the shape of a prior tell us? It tells us your belief as to how the underlying parameter Θ should be distributed. 504
8.3. MAXIMUM A POSTERIORI ESTIMATION Figure8.15:Thisfigureillustratessixdifferentexamplesofthepriordistributionf (θ),whentheprior Θ is a 1D parameter θ. The prior distribution f (θ) is the PDF of Θ. (a) f (θ)=δ(θ), which is a delta Θ Θ function. (b) f (θ) = 1 for a ≤ θ ≤ b. This is a uniform distribution. (c) This is also a uniform Θ b−a distribution, but the spread is very wide. (d) f (θ)=Gaussian(0,σ2), which is a zero-mean Gaussian. Θ (e)ThesameGaussian,butwithadifferentmean.(f)AGaussianwithzeromean,butalargevariance. The meaning of this statement can be best understood from the examples shown in Fig- ure 8.15: • Figure8.15(a).Thisisadeltapriorf (θ)=δ(θ)(orf (θ)=δ(θ−θ )).Ifyouusethis Θ Θ 0 prior, you are absolutely sure that the parameter Θ takes a specific value. There is no uncertainty about your belief. Since you are so confident about your prior knowledge, you will ignore the likelihood that is constructed from the data. No one will use a delta prior in practice. • Figure 8.15(b). f (θ) = 1 for a ≤ θ ≤ b, and is zero otherwise. This is a bounded Θ b−a uniform prior. You do not have any preference for the parameter Θ, but you do know from your prior experience that a≤Θ≤b. • Figure 8.15(c). This prior is the same as (b) but is short and very wide. If you use this prior, it means that you know nothing about the parameter. So you give up the prior and let the likelihood dominate the MAP estimate. • Figure8.15(d).f (θ)=Gaussian(0,σ2).Youusethispriorwhenyouknowsomething Θ about the parameter, e.g., that it is centered at certain location and you have some uncertainty. • Figure 8.15(e). Same as (d), but the parameter is centered at some other location. • Figure 8.15(f). Same as (d), but you have less confidence about the parameter. As you can see from these examples, the shape of the prior tells us how you want Θ to be distributed. The choice you make will directly influence the MAP optimization, and hence the MAP estimate. Since the prior is a subjective quantity in the MAP framework, you as the user have the freedom to choose whatever you like. For instance, if you have conducted a similar experiment before, you can use the results of the previous experiments as the current prior. Another strategy is to go with physics. For instance, we can argue that θ should be sparse so that it contains as few non-zeros as possible. In this case, a sparsity-driven prior, such as f (θ) = exp{−(cid:107)θ(cid:107) }, could be a choice. The third strategy is to choose a prior that is Θ 1 computationally “friendlier”, e.g., in quadratic form so that the MAP is differentiable. One such choice is the conjugate prior. We will discuss this later in Section 8.3.6. 505
CHAPTER 8. ESTIMATION Which prior should we choose? • Basedonyourpreference,e.g.,youknowfromhistoricaldatathattheparameter should behave in certain ways. • Based on physics, e.g., the parameter has a physical interpretation, so you need to abide by the physical laws. • Choose a prior that is computationally “friendlier”. This is the topic of the conjugate prior, which is a prior that does not change the form of the posterior distribution. (We will discuss this later in Section 8.3.6.) 8.3.3 MAP formulation and solution Our next task is to study how to formulate the MAP problem and how to solve it. Definition 8.7. Let X = [X ,...,X ]T be i.i.d. observations. Let Θ be a random 1 N parameter. The maximum-a-posteriori estimate of Θ is θ(cid:98)MAP =argmax f Θ|X(θ|x). (8.38) θ Philosophicallyspeaking,MLandMAPhavetwodifferentgoals.MLconsidersapara- metricmodelwithadeterministicparameter.Itsgoalistofindtheparameterthatmaximizes the likelihood for the data we have observed. MAP also considers a parametric model but the parameter Θ is random. Because Θ is random, we are finding one particular state θ of the parameter Θ that offers the best explanation conditioned on the data X we observe. In a sense, the two optimization problems are θ(cid:98)ML =argmax f X|Θ(x|θ), θ θ(cid:98)MAP =argmax f Θ|X(θ|x). θ This pair of equations is interesting, as the pair tells us that the difference between the ML estimation and the MAP estimation is the flipped order of X and Θ. There are two reasons we care about the posterior. First, in MAP the posterior allows us to incorporate the prior. ML does not allow a prior. A prior can be useful when the number of samples is small. Second, maximizing the posterior does have some physical interpretations. MAP asks for the probability of Θ=θ after observing N training samples X = x. ML asks for the probability of observing X = x given a parameter θ. Both are correct and legitimate criteria, but sometimes we might prefer one over the other. To solve the MAP problem, we notice that θ(cid:98)MAP =argmax f Θ|X(θ|x) θ f (x|θ)f (θ) X|Θ Θ =argmax f (x) θ X =argmax f (x|θ)f (θ), f (x) does not contain θ X|Θ Θ X θ =argmax logf (x|θ)+logf (θ). X|Θ Θ θ 506
8.3. MAXIMUM A POSTERIORI ESTIMATION Therefore, what MAP adds is the prior logf (θ). If you use an uninformative prior, e.g., a Θ prior with extremely wide support, then the MAP estimation will return more or less the same result as the ML estimation. When does MAP = ML? • The relation “=” does not make sense here, because θ is random in MAP but deterministic in ML. • Solution of MAP optimization = solution of ML optimization, when f (θ) is Θ uniform over the parameter space. • In this case, f (θ)=constant and so it can be dropped from the optimization. Θ Example 8.19. Let X ,...,X be i.i.d. random variables with a PDF f (x |θ) 1 N Xn|Θ n for all n, and Θ be a random parameter with PDF f (θ): Θ 1 (cid:26) (x −θ)2(cid:27) f (x |θ)= √ exp − n , Xn|Θ n 2πσ2 2σ2 1 (cid:26) (θ−µ )2(cid:27) f (θ)= exp − 0 . Θ (cid:112) 2πσ2 2σ2 0 0 Find the MAP estimate. Solution. The MAP estimate is θ(cid:98)MAP =argm θ ax (cid:34) n(cid:89)N =1√ 21 πσ2 exp(cid:26) −(x n 2− σ2θ)2(cid:27)(cid:35) ×(cid:34) (cid:112) 21 πσ 02 exp(cid:26) −(θ− 2σµ 020)2(cid:27)(cid:35) =argmax (cid:18) √ 1 (cid:19)N × 1 exp(cid:40) −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:41) . θ 2πσ2 (cid:112) 2πσ 02 n=1 2σ2 2σ 02 Since the maximizer is not changed by any monotonic function, we apply logarithm to the above equations. This yields (cid:26) θ(cid:98)MAP =argmax − N 2 log(cid:0) 2πσ2(cid:1) − 1 2log(2πσ 02) θ −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:27) . 2σ2 2σ2 n=1 0 Constants in the maximization do not matter. So by dropping the constant terms we obtain θ(cid:98)MAP =argmax (cid:40) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2(cid:41) . (8.39) θ n=1 0 It now remains to solve the maximization. To this end we take the derivative w.r.t. θ 507
CHAPTER 8. ESTIMATION and show that d (cid:40) −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:41) =0. dθ 2σ2 2σ2 n=1 0 This yields N (cid:88) (x n−θ) − θ−µ 0 =0. σ2 σ2 n=1 0 Rearranging the terms gives us the final result: (cid:16) (cid:17) σ2 1 (cid:80)N x + σ2µ 0 N n=1 n N 0 θ(cid:98)MAP = σ2+ σ2 . (8.40) 0 N PracticeExercise8.7.Provethatiff Θ(θ)=δ(θ−θ 0),theMAPestimateisθ(cid:98)MAP = θ . 0 Solution. If f (θ)=δ(θ−θ ), then Θ 0 θ(cid:98)MAP =argmax logf X|Θ(x|θ)+logf Θ(θ) θ =argmax logf (x|θ)+logδ(θ−θ ) X|Θ 0 θ  argmax logf (x|θ)−∞, θ (cid:54)=θ .  X|Θ 0 = θ argmax logf X|Θ(x|θ)+0, θ =θ 0. θ Thus, if θ(cid:98)MAP (cid:54)= θ 0, the first case says that there is no solution, so we must go with the second case θ(cid:98)MAP = θ 0. But if θ(cid:98)MAP = θ 0, there is no optimization because we have already chosen θ(cid:98)MAP =θ 0. This proves the result. 8.3.4 Analyzing the MAP solution As we said earlier, MAP offers something that ML does not. To see this, we will use the result of the Gaussian random variables as an example and analyze the MAP solution as we change the parameters N and σ . Recall that if X ,...,X are i.i.d. Gaussian random 0 1 N variables with unknown mean θ and known variance σ, the ML estimate is N 1 (cid:88) θ(cid:98)ML = N x n. n=1 508
8.3. MAXIMUM A POSTERIORI ESTIMATION Assuming that the parameter Θ is distributed according to a PDF Gaussian(µ ,σ2), we 0 0 have shown in the previous subsection that (cid:16) (cid:17) θ(cid:98)MAP = σ 02 N1 (cid:80) σN n 2= +1x σn 2 + σ N2µ 0 = σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 . 0 N 0 N In what follows, we will take a look at the behavior of the MAP estimate θ(cid:98)MAP as N and σ change. The results of our discussion are summarized in Figure 8.16. 0 (a) Effect of N (b) Effect of σ 0 Figure8.16:TheMAPestimateθ(cid:98)MAP swingsbetweentheMLestimateθ(cid:98)ML andthepriorµ 0.(a)When N increases, the likelihood is more reliable and so we lean towards the ML estimate. If N is small, we should trust the prior more than the ML estimate. (b) When σ decreases, we become more confident 0 about the prior and so we will use it. If σ is large, we use more information from the ML estimate. 0 First, let’s look at the effect of N. How does N change θ(cid:98)MAP? • As N → ∞, the MAP estimate θ(cid:98)MAP → θ(cid:98)ML: If we have enough samples, we trust the data. • As N → 0, the MAP estimate θ(cid:98)MAP → θ 0. If we do not have any samples, we trust the prior. Thesetworesultscanbedemonstratedbytakingthelimits.AsN →∞,theMAPestimate converges to Nl →im ∞θ(cid:98)MAP = Nl →im ∞σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 =θ(cid:98)ML. (8.41) 0 N This result is not surprising. When we have infinitely many samples, we will completely rely on the data and make our estimate. Thus, the MAP estimate is the same as the ML estimate. When N →0, the MAP estimate converges to Nli →m 0θ(cid:98)MAP = Nli →m 0σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 =µ 0. (8.42) 0 N Thismeansthat,whenwedonothaveanysamples,theMAPestimateθ(cid:98)MAP willcompletely use the prior distribution, which has a mean µ . 0 509
CHAPTER 8. ESTIMATION The implication of this result is that MAP offers a natural swing between θ(cid:98)ML and θ(cid:98)0, controlled by N. Where does this N come from? If we recall the derivation of the result, we note that the N affects the likelihood term through the number of samples: θ(cid:98)MAP =argmax (cid:26) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2(cid:27) . θ n=1 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) 1term N termshere Thus, as N increases, the influence of the data term grows, and so the result will gradually shift towards θ(cid:98)ML. Figure 8.17 illustrates a numerical experiment in which we draw N random samples x ,...,x according to a Gaussian distribution Gaussian(θ,σ2), with σ = 1. We assume 1 N thatthepriordistributionisGaussian(µ ,σ2),withµ =0andσ =0.25.TheMLestimate 0 0 0 0 ofthisproblemisθ(cid:98)ML = N1 (cid:80)N n=1x n,whereastheMAPestimateisgivenbyEquation(8.40). The figure shows the resulting PDFs. A helpful analogy is that the prior and the likelihood arepullingaropeintwooppositedirections.AsN grows,theforceofthelikelihoodincreases and so the influence becomes stronger. (a) N =1 (b) N =50 Figure 8.17: The subfigures show the prior distribution f (θ) and the likelihood function f (x|θ), Θ X|Θ given the observed data. (a) When N = 1, the estimated posterior distribution f (θ|x) is pulled Θ|X towards the prior. (b) When N =50, the posterior is pulled towards the ML estimate. The analogy for the situation is that each data point is acting as a small force against the big force of the prior. As N grows, the small forces of the data points accumulate and eventually dominate. We next look at the effect of σ . 0 How does σ 0 change θ(cid:98)MAP? • Asσ 0 →∞,theMAPestimateθ(cid:98)MAP →θ(cid:98)ML:Ifwehavedoubtsabouttheprior, we trust the data. • As σ 0 → 0, the MAP estimate θ(cid:98)MAP → θ 0. If we are absolutely sure about the prior, we ignore the data. 510
8.3. MAXIMUM A POSTERIORI ESTIMATION When σ 0 →∞, the limit of θ(cid:98)MAP is σ0li →m ∞θ(cid:98)MAP = σ0li →m ∞σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 =θ(cid:98)ML. (8.43) The reason why this happens is that σ is the uncertainty level of the prior. If σ is high, 0 0 we are not certain about the prior. In this case, MAP chooses to follow the ML estimate. When σ 0 →0, the limit of θ(cid:98)MAP is σl 0i →m 0θ(cid:98)MAP = σl 0i →m 0σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 =µ 0. (8.44) Note that when σ → 0, we are essentially saying that we are absolutely sure about the 0 prior. If we are so sure about the prior, there is no need to look at the data. In that case the MAP estimate is µ . 0 The way to understand the influence of σ is to inspect the equation: 0 θ(cid:98)MAP =argmax (cid:26) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2 (cid:27) . θ n=1 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fixedw.r.t.σ0 changeswithσ0 Since σ is purely a preference you decide, you can control how much trust to put onto the 0 prior. (a) σ =0.1 (b) σ =1 0 0 Figure 8.18: The subfigures show the prior distribution f (θ) and the likelihood function f (x|θ), Θ X|Θ given the observed data. (a) When σ = 0.1, the estimated posterior distribution f (θ|x) is pulled 0 Θ|X towards the prior. (b) When σ = 1, the posterior is pulled towards the ML estimate. An analogy for 0 the situation is that the strength of the prior depends on the magnitude of σ . If σ is small the prior 0 0 is strong, and so the influence is large. If σ is large the prior is weak, and so the ML estimate will 0 dominate. Figure 8.18 illustrates a numerical experiment in which we compare σ = 0.1 and 0 σ =1. If σ is small, the prior distribution f (θ) becomes similar to a delta function. We 0 0 Θ can interpret it as a very confident prior, so confident that we wish to align with the prior. Thesituationcanbeimaginedasagameoftug-of-warbetweenapowerfulbullandahorse, 511
CHAPTER 8. ESTIMATION which the bull will naturally win. If σ is large the prior distribution will become flat. It 0 means that we are not very confident about the prior so that we will trust the data. In this case the MAP estimate will shift towards the ML estimate. 8.3.5 Analysis of the posterior distribution Whenthelikelihoodismultipliedwiththepriortoformtheposterior,whatdoestheposte- rior distribution look like? To answer this question we continue our Gaussian example with a fixed variance σ and an unknown mean θ. The posterior distribution is proportional to f (x|θ)f (θ) X|Θ Θ f (θ|x)= ∝f (x|θ)f (θ) Θ|X f (x) X|Θ Θ X =(cid:34) (cid:89)N √ 1 exp(cid:26) −(x n−θ)2(cid:27)(cid:35) ·(cid:34) 1 exp(cid:26) −(θ−µ 0)2(cid:27)(cid:35) . (8.45) 2πσ2 2σ2 (cid:112) 2πσ2 2σ2 n=1 0 0 Performing the multiplication and completing the squares, (cid:88)N (x n−θ)2 + (θ−µ 0)2 = (θ−θ(cid:98)MAP)2 , 2σ2 2σ2 2σ2 n=1 0 MAP where θ(cid:98)MAP = σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 , and σ (cid:98)M21 AP = σ1 02 + σN 2. (8.46) In other words, the posterior distribution f (θ|x) is also a Gaussian with Θ|X f Θ|X(θ|x)=Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP). If f (x|θ) = Gaussian(x; θ,σ), and f (θ) = Gaussian(θ; µ ,σ2), what is the X|Θ Θ 0 0 posterior f (θ|x)? Θ|X The posterior f Θ|X(θ|x) is Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP), where θ(cid:98)MAP = σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 , and σ (cid:98)M21 AP = σ1 02 + σN 2. (8.47) The posterior tells us how N and σ will influence the MAP estimate. As N grows, 0 the posterior mean and variance becomes lim θ(cid:98)MAP =θ(cid:98)ML =θ, and lim σ (cid:98)MAP =0. N→∞ N→∞ As a result, the posterior distribution f (θ|x) will converge to a delta function centered Θ|X attheMLestimateθ(cid:98)ML.Therefore,aswetrytosolvetheMAPproblembymaximizingthe posterior, the MAP estimate has to improve because σ →0. (cid:98)MAP We can plot the posterior distribution Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP) as a function of the numberofsamplesN.Figure8.19illustratesthisexampleusingthefollowingconfigurations. 512
8.3. MAXIMUM A POSTERIORI ESTIMATION The likelihood is Gaussian with µ = 1, σ = 0.25. The prior is Gaussian with µ = 0 and 0 σ = 0.25. We construct the Gaussian according to Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP) by varying N. TheresultshowninFigure8.19confirmsourprediction:AsN grows,theposteriorbecomes more like a delta function whose mean is the true mean µ. The posterior estimator θ(cid:98)MAP, for each N, is the peak of the respective Gaussian. 8 N = 0 N = 1 6 N = 2 N = 5 N = 8 4 N = 12 N = 20 2 0 -1 -0.5 0 0.5 1 1.5 Figure 8.19: Posterior distribution f Θ|X(θ|x)=Gaussian(θ(cid:98)MAP, σ M2 AP) as N grows. When N is small, the posterior distribution is dominated by the prior. As N increases, the posterior distribution changes its mean and its variance. What is the pictorial interpretation of the MAP estimate? • For every N, MAP has a posterior distribution f (θ|x). Θ|X • As N grows, f Θ|X(θ|x) converges to a delta function centered at θ(cid:98)ML. • MAP tries to find the peak of f Θ|X(θ|x). For large N, it returns θ(cid:98)ML. 8.3.6 Conjugate prior Choosing the prior is an important topic in a MAP estimation. We have elaborated two “engineering” solutions: Use your prior experience or follow the physics. In this subsection, we discuss the third option: to choose something computationally friendly. To explain what we mean by “computationally friendly”, let us consider the following example, thanks to Avinash Kak.4 Consider a Bernoulli distribution with a PDF N (cid:89) f (x|θ)= θxn(1−θ)1−xn. (8.48) X|Θ n=1 TocomputetheMAPestimate,weassumethatwehaveapriorf (θ).Therefore,theMAP Θ 4Avinash Kak “ML, MAP, and Bayesian — The Holy Trinity of Parameter Estimation and Data Pre- diction”,https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf 513
CHAPTER 8. ESTIMATION estimate is given by θ(cid:98)MAP =argmax f X|Θ(x|θ)f Θ(θ) θ (cid:34) N (cid:35) (cid:89) =argmax θxn(1−θ)1−xn ·f (θ) Θ θ n=1 N (cid:88) =argmax x logθ+(1−x )log(1−θ)+logf (θ). n n Θ θ n=1 Let us consider three options for the prior. Which one would you use? (cid:110) (cid:111) • Candidate 1: f Θ(θ) = √ 21 πσ2 exp −(θ 2− σµ 2)2 , a Gaussian prior. If you choose this prior, the optimization problem will become (cid:88)N (cid:26) (cid:27) (θ−µ)2 θ(cid:98)MAP =argmax x nlogθ+(1−x n)log(1−θ) − 2σ2 . θ n=1 We can still take the derivative and set it to zero. This gives (cid:80)N x N −(cid:80)N x θ−µ n=1 n − n=1 n = . θ 1−θ σ2 Defining S =(cid:80)N x and moving the terms around, we have n=1 n (1−θ)σ2S−θσ2(N −S)=θ(1−θ)(θ−µ). Thisisacubicpolynomialproblemthathasaclosed-formsolutionandisalsosolvable by a computer. But it’s also tedious, at least to lazy engineers like ourselves. • Candidate2:f (θ)= λe−λ|θ|,aLaplaceprior.Inthiscase,theoptimizationproblem Θ 2 becomes N (cid:26) (cid:27) (cid:88) θ(cid:98)MAP =argmax x nlogθ+(1−x n)log(1−θ) −λ|θ|. θ n=1 Welcometoconvexoptimization!Thereisnoclosed-formsolution.Ifyouwanttosolve this problem, you need to call a convex solver. • Candidate 3: f (θ)= 1θα−1(1−θ)β−1, a beta prior. This prior looks very compli- Θ C cated, but let’s plug it into our optimization problem: N (cid:26) (cid:88) θ(cid:98)MAP =argmax x nlogθ θ n=1 (cid:27) +(1−x )log(1−θ) +(α−1)logθ+(β−1)log(1−θ) n =argmax (S+α−1)logθ+(N −S+β−1)log(1−θ), θ 514
8.3. MAXIMUM A POSTERIORI ESTIMATION where S =(cid:80)N x . Taking the derivative and setting it to zero, we have n=1 n S+α−1 N −S+β−1 = . θ 1−θ Rearranging the terms we obtain the final estimate: S+α−1 θ(cid:98)MAP = N +β+α−2. (8.49) There are a number of intuitions that we can draw from this beta prior, but most importantly, we have obtained a very simple solution. That is because the posterior distri- bution remains in the same form as the prior, after multiplying by the prior. Specifically, if we use the beta prior, the posterior distribution is f (θ|x)∝f (x|θ)f (θ) Θ|X X|Θ Θ (cid:34) N (cid:35) (cid:89) 1 = θxn(1−θ)1−xn · θα−1(1−θ)β−1 C n=1 =θS+α−1(1−θ)N−S+β−1. This is still in the form of θ(cid:70)−1(1 − θ)(cid:4)−1, which is the same as the prior. When this happens, we call the prior a conjugate prior. In this example, the beta prior is a conjugate before the Bernoulli likelihood. What is a conjugate prior? • It is a prior such that when multiplied by the likelihood to form the posterior, the posterior f (θ|x) takes the same form as the prior f (θ). Θ|X Θ • Every likelihood has its conjugate prior. • Conjugate priors are not necessarily good priors. They are just computationally friendly. Some of them have good physical interpretations. We can make a few interpretations of the beta prior, in the context of Bernoulli likeli- hood. First, the beta distribution takes the form 1 f (θ)= θα−1(1−θ)β−1, (8.50) Θ B(α,β) with B(α,β) is the beta function5. The shape of the beta distribution is shown in Fig- ure 8.20. For different choices of α and β, the distribution has a peak located towards either side of the interval [0,1]. For example, if α is large but β is small, the distribution f (θ) leans towards 1 (the yellow curve). Θ As a user, you have the freedom to pick f (θ). Even if you are restricted to the beta Θ distribution, you still have plenty of degrees of freedom in choosing α and β so that your choice matches your belief. For example, if you know ahead of time that the Bernoulli experiment is biased towards 1 (e.g., the coin is more likely to come up heads), you can choose a large α and a small β. By contrast, if you believe that the coin is fair, you choose α=β.Theparametersαandβ areknownasthehyperparametersofthepriordistribution. Hyperparameters are parameters for f (θ). Θ 5The beta function is defined as B(α,β) = Γ(α)Γ(β), where Γ is the gamma function. For integer n, Γ(α+β) Γ(n)=(n−1)! 515
CHAPTER 8. ESTIMATION 4 = 2, = 8 = 3, = 7 = 8, = 2 3 2 1 0 0 0.2 0.4 0.6 0.8 1 Figure 8.20: Beta distribution f (θ) for various choices of α and β. When (α,β) = (2,8), the beta Θ distribution favors small θ. When (α,β) = (8,2), the beta distribution favors large θ. By swinging between the (α,β) pairs, we obtain a prior that has a preference over θ. Example 8.20. (Prior for Gaussian mean) Consider a Gaussian likelihood for a fixed variance σ2 and unknown mean θ: f (x|θ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−θ)2(cid:41) . X|Θ 2πσ2 2σ2 n=1 Show that the conjugate prior is given by 1 (cid:26) (θ−µ )2(cid:27) f (θ)= exp − 0 . (8.51) Θ (cid:112) 2πσ2 2σ2 0 0 Solution.Wehaveshownthisresultpreviously.Bysome(tedious)completingsquares, we show that 1 (cid:26) (θ−µ )2(cid:27) f (θ|x)= exp − N , Θ|X (cid:112) 2πσ2 2σ2 N N where σ2 Nσ2 µ N = Nσ2+σ2µ 0+ Nσ2+0 σ2θ(cid:98)ML, 0 0 σ2σ2 σ2 = 0 . N σ2+Nσ2 0 Sincef (θ|x)isinthesameformasf (θ),weknowthatf (θ)isaconjugateprior. Θ|X Θ Θ 516
8.3. MAXIMUM A POSTERIORI ESTIMATION Example 8.21. (Prior for Gaussian variance) Consider a Gaussian likelihood for a mean µ and unknown variance σ2: f (x|σ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−µ)2(cid:41) . X|σ 2πσ2 2σ2 n=1 Find the conjugate prior. Solution. We first define the precision θ = 1 . The likelihood is σ2 f (x|θ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−µ)2(cid:41) X|Θ 2πσ2 2σ2 n=1 (cid:40) N (cid:41) 1 θ (cid:88) = θN/2exp − (x −µ)2 . (2π)N/2 2 n n=1 We propose to choose the prior f (θ) as Θ 1 f (θ)= baθa−1exp{−bθ}, Θ Γ(a) forsomeaandb.Thisf (θ)iscalledtheGammadistributionGamma(θ|a,b).Wecan Θ show that E[Θ] = a and Var[Θ] = a. With some (tedious) completing squares, we b b2 show that the posterior is (cid:40) (cid:32) N (cid:33) (cid:41) 1 (cid:88) f (θ|x)∝θ(a0+N/2)−1exp − b + (x −µ)2 θ , Θ|X 0 2 n n=1 which is in the same form as the prior. So we know that our proposed f (θ) is a Θ conjugate prior. Thestoryofconjugatepriorsisendlessbecauseeverylikelihoodhasitsconjugateprior. Table 8.1 summarizes a few commonly used conjugate priors, their likelihoods, and their posteriors. The list can be expanded further to distributions with multiple parameters. For example, if a Gaussian has both unknown mean and variance, then there exists a conjugate priorconsistingofaGaussianmultipliedbyaGamma.Conjugatepriorsalsoapplytomulti- dimensional distributions. For example, the prior for the mean vector of a high-dimensional Gaussian is another high-dimensional Gaussian. The prior for the covariance matrix of a high-dimensionalGaussianistheWishartprior.Thepriorforboththemeanvectorandthe covariance matrix is the normal Wishart. 8.3.7 Linking MAP with regression MLandregressionrepresentthestatisticsandtheoptimizationaspectsofthesameproblem. Withtheparallelargument,MAPislinkedtotheregularizedregression.Thereasonfollows 517
CHAPTER 8. ESTIMATION Table of Conjugate Priors Likelihood Conjugate Prior Posterior f (x|θ) f (θ) f (θ|x) X|Θ Θ Θ|X Bernoulli(θ) Beta(α,β) Beta(α+S,β+N −S) (cid:16) (cid:17) Poisson(θ) Gamma(α,β) Gamma α+S, β 1+N (cid:16) (cid:17) Exponential(θ) Gamma(α,β) Gamma α+N, β 1+βS Gaussian(θ,σ2) Gaussian(µ 0,σ 02) Gaussian(cid:16) µ 10 // σσ 202 ++ NS // σσ 22(cid:17) 0 (cid:16) (cid:17) Gaussian(µ,θ2) Inv. Gamma(α,β) Gamma α+ N,β+ 1(cid:80)N (x −µ)2 2 2 n=1 n Table 8.1: Commonly used conjugate priors. immediately from the definition of MAP: θ(cid:98)MAP =argmax logf X|Θ(x|θ)+ logf Θ(θ) . θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) datafidelity regularization To make this more explicit, we consider following linear regression problem:        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . .  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X =θ =e If we assume that e∼Gaussian(0,σ2I), the likelihood is defined as (cid:26) (cid:27) 1 1 f (y|θ)= exp − (cid:107)y−Xθ(cid:107)2 . (8.52) Y|Θ (cid:112) (2πσ2)N 2σ2 In the ML setting, the ML estimate is the maximizer of the likelihood: θ(cid:98)ML =argmax logf Y|Θ(y|θ) θ 1 =argmax − (cid:107)y−Xθ(cid:107)2. 2σ2 θ For MAP, we add a prior term so that the optimization becomes θ(cid:98)MAP =argmax logf Y|Θ(y|θ)+logf Θ(θ) θ 1 =argmin (cid:107)y−Xθ(cid:107)2−logf (θ). 2σ2 Θ θ 518
8.3. MAXIMUM A POSTERIORI ESTIMATION Therefore,theregularizationoftheregressionisexactly−logf (θ).Wecanperformreverse Θ engineeringtofindoutthecorrespondingpriorforourfavoritechoicesoftheregularization. Ridge regression. Suppose that (cid:26) (cid:107)θ(cid:107)2(cid:27) f (θ)=exp − . Θ 2σ2 0 Taking the negative log on both sides yields (cid:107)θ(cid:107)2 −logf (θ)= . Θ 2σ2 0 Putting this into the MAP estimate, 1 1 θ(cid:98)MAP =argmin 2σ2(cid:107)y−Xθ(cid:107)2+ 2σ2(cid:107)θ(cid:107)2 θ 0 σ2 =argmin (cid:107)y−Xθ(cid:107)2+ (cid:107)θ(cid:107)2, σ2 θ 0 (cid:124) (cid:123)(cid:122) (cid:125) =λ whereλisthecorrespondingridgeregularizationparameter.Therefore,theridgeregression is equivalent to a MAP estimation using a Gaussian prior. How is MAP related to ridge regression? • In MAP, define the prior as a Gaussian: (cid:26) (cid:107)θ(cid:107)2(cid:27) f (θ)=exp − . (8.53) Θ 2σ2 0 • ThepriorsaysthatthesolutionθisnaturallydistributedaccordingtoaGaussian with mean zero and variance σ2. 0 LASSO regression. Suppose that (cid:26) (cid:27) (cid:107)θ(cid:107) f (θ)=exp − 1 . Θ α Taking the negative log on both sides yields (cid:107)θ(cid:107) −logf (θ)= 1. Θ α Putting this into the MAP estimate we can show that 1 1 θ(cid:98)MAP =argmin 2σ2(cid:107)y−Xθ(cid:107)2+ α(cid:107)θ(cid:107) 1 θ 1 σ2 =argmin (cid:107)y−Xθ(cid:107)2+ (cid:107)θ(cid:107) . 2 α 1 θ (cid:124) (cid:123)(cid:122) (cid:125) =λ 519
CHAPTER 8. ESTIMATION To summarize: How is MAP related to LASSO regression? • LASSO is a MAP using the prior (cid:26) (cid:27) (cid:107)θ(cid:107) f (θ)=exp − 1 . (8.54) Θ α At this point, you may be wondering what MAP buys us when regularized regression canalreadydothejob.Theanswerisabouttheinterpretation.Whileregularizedregression canalwaysreturnusaresult,thatisjustaresult.However,ifyouknowthattheparameterθ isdistributedaccordingtosomedistributionsf (θ),MAPoffersastatisticalperspectiveof Θ thesolutioninthesensethatitreturnsthepeakoftheposteriorf (θ|x).Forexample,if Θ|X weknowthatthedataisgeneratedfromalinearmodelwithGaussiannoise,andifweknow that the true regression coefficients are drawn from a Gaussian, then the ridge regression is guaranteedtobeoptimalintheposteriorsense.Similarly,ifweknowthatthereareoutliers and have some ideas about the outlier statistics, perhaps the LASSO regression is a better choice. ItisalsoimportanttonotethedifferentoptimalitiesofferedbyMAPversusMLversus regression. The optimality offered by regression is the training loss, which can always give us a result even if the underlying statistics do not match the optimization formulation, e.g.,thereareoutliers,andyouuseunregularizedleast-squaresminimization.Youcangeta result, but the outliers will heavily influence your solution. On the other hand, if you know thedatastatisticsandchoosetofollowtheML,thentheMLsolutionisoptimalinthesense of optimizing the likelihood f (x|θ). If you further know the prior statistics, the MAP X|Θ solution will be optimal, but this time it is optimal w.r.t. to the posterior f (θ|x). Since Θ|X eachoftheseisoptimizingforadifferentgoal,theyareonlygoodfortheirchosenobjectives. For example, θ(cid:98)MAP can be a biased estimate if our goal is to maximize the likelihood. The θ(cid:98)ML is optimal for the likelihood but can be a bad choice for the posterior. Both θ(cid:98)MAP and θ(cid:98)ML can possibly achieve a reasonable mean-squared error, but their results may not make sense (e.g., if θ is an image then θ(cid:98)MAP may over-smooth the image whereas θ(cid:98)ML amplifies noise). So it’s incorrect to think that θ(cid:98)MAP is superior to θ(cid:98)ML because it is more general. Here are some rules of thumb for MAP, ML, and regression: When should I use regression, ML and MAP? • Regression: If you are lazy and you know nothing about the statistics, do the regression with whatever regularization you prefer. It will give you a result. See if it makes sense with your data. • MAP:Ifyouknowthestatisticsofthedata,andifyouhavesomepreferencefor the prior distribution, go with MAP. It will offer you the optimal solution w.r.t. finding the peak of the posterior. • ML: If you are interested in some simple-form solution, and you want those nice properties such as consistency and unbiasedness, then go with ML. It usually possessesthe“friendly”propertiessothatyoucanderivetheperformancelimit. 520
8.4. MINIMUM MEAN-SQUARE ESTIMATION 8.4 Minimum Mean-Square Estimation First-time readers are often tempted to think that the maximum-likelihood estimation or themaximumaposterioriestimationarethe bestmethodstoestimateparameters.Insome sense,thisistruebecausebothestimationproceduresoffersomeformofoptimalexplanation for the observed variables. However, as we said above, being optimal with respect to the likelihoodortheposterioronlymeansoptimalundertherespectivecriteria.AnMLestimate is not necessarily optimal for the posterior, whereas a MAP estimate is not necessarily optimal for the likelihood. Therefore, as we proceed to the third commonly used estimation strategy, we need to remind ourselves of the specific type of optimality we seek. 8.4.1 Positioning the minimum mean-square estimation Mean-square error estimation, as it is termed, uses the mean-square error as the optimality criterion. The corresponding estimation process is known as the minimum mean-square estimation (MMSE). MMSE is a Bayesian approach, meaning that it uses the prior f (θ) Θ as well as the likelihood f (x|θ). As we will show shortly, the MMSE estimate of a set X|Θ of i.i.d. observation X =[X ,...,X ]T is 1 N θ(cid:98)MMSE(x)( =a)E Θ|X[Θ|X =x] (a):We will discuss this. (cid:90) = θ·f (θ|x)dθ. (8.55) Θ|X You may find this equation very surprising, because it says that the MMSE estimate is the mean of the posterior distribution f (θ|x). Let’s compare this result with the ML Θ|X estimate and the MAP estimate: θ(cid:98)ML =peak of f X|Θ(x|θ), θ(cid:98)MAP =peak of f ΘX|(θ |x), θ(cid:98)MMSE =average of f Θ|X(θ |x). Therefore,anMMSEestimateisnotbyanymeansuniversallysuperiororinferiortoaMAP estimate or an ML estimate. It is just a different estimate with a different goal. Sohowexactlyaretheseestimatesdifferent?Figure 8.21illustratesatypicalsituation of asymmetric distribution. Here, we plot both the likelihood function f (x|θ) and the X|Θ posterior function f (θ |x). ΘX| Asshowninthefigure,theMLestimateisthepeakofthelikelihood,whereastheMAP estimate is the peak of the posterior. The third estimate is the MMSE estimate, which is the average of the posterior distribution. It is easy to see that if the posterior distribution is symmetric and has a single peak, the peak is always the mean. Therefore, for single-peak symmetric distributions, MMSE and MAP estimates are identical. 521
CHAPTER 8. ESTIMATION Figure 8.21: A typical example of an ML estimate, a MAP estimate and an MMSE estimate. What is so special about the MMSE estimate? • MMSE is a Bayesian estimation, so it requires a prior. • An MMSE estimate is the mean of the posterior distribution. • MMSE estimate = MAP estimate if the posterior distribution is symmetric and has a single peak. 8.4.2 Mean squared error The MMSE is based on minimizing the mean squared error (MSE). In this subsection we discuss the mean squared error in the Bayesian setting. In the deterministic setting, given an estimate θ(cid:98)and a ground truth θ, the MSE is defined as MSE( θ , θ(cid:98) )=(θ−θ(cid:98))2. (8.56) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) groundtruth estimate In any estimation problem, the estimate θ(cid:98)is always a function of the observed variables. Thus, we have θ(cid:98)(X)=g(X), where X =[X 1,...,X N]T, forsomefunctiong(·).SubstitutingthisintothedefinitionofMSE,andrecognizingthatX is drawn from a distribution f (x), we take the expectation to define the MSE as X MSE(θ,θ(cid:98))=(θ−θ(cid:98))2 ⇓replaceθ(cid:98)byg(X) MSE(θ,θ(cid:98))=(θ−g(X))2 ⇓takeexpectationoverX MSE(θ,θ(cid:98))=E X(cid:2) (θ−g(X))2(cid:3) . ThuswehavearrivedatthedefinitionofMSE.Wecallthisthefrequentistversion,because the parameter θ is deterministic. 522
8.4. MINIMUM MEAN-SQUARE ESTIMATION Definition 8.8 (Mean squared error, frequentist). The mean squared error of an estimate g(X) w.r.t. the true parameter θ is MSE (θ,g(·))=E (cid:2) (θ−g(X))2(cid:3) . (8.57) freq X If the parameter θ is high-dimensional, so is the estimate g(X), and the MSE is MSE (θ,g(·))=E (cid:2) (cid:107)θ−g(X)(cid:107)2(cid:3) . (8.58) freq X Note that in the above definition the MSE is measured between the true parameter θ and the estimator g(·). We use the function g(·) here because we have taken the expectation of all the possible inputs X. So we are not comparing θ with a value g(X) but with the function g(·). If we take a Bayesian approach such as the MAP, then θ itself is a random variable Θ. To compute the MSE, we then need to take the average across all the possible choices of ground truth Θ. This leads to MSE(θ,θ(cid:98))=E X(cid:2) (θ−g(X))2(cid:3) ⇓replaceθ byΘ MSE(θ,θ(cid:98))=E X(cid:2) (Θ−g(X))2(cid:3) ⇓takeexpectationoverΘ MSE(θ,θ(cid:98))=E X,Θ(cid:2) (Θ−g(X))2(cid:3) . Therefore, we have arrived at our definition of the MSE, in the Bayesian setting. Definition 8.9 (Mean squared error, Bayesian). The mean squared error of an es- timate g(X) w.r.t. the true parameter Θ is MSE (Θ,g(·))=E (cid:2) (Θ−g(X))2(cid:3) . (8.59) Bayes Θ,X If the parameter Θ is high-dimensional, so is the estimate g(X), and the MSE is MSE (Θ,g(·))=E (cid:2) (cid:107)Θ−g(X)(cid:107)2(cid:3) . (8.60) Bayes Θ,X ThedifferencebetweentheBayesianMSEandthefrequentistMSEistheexpectationoverΘ. Practicallyspeaking,thefrequentistMSEismoreofanevaluationmetricthananobjective function for solving an inverse problem. The reason is that in an inverse problem, we never have access to the true parameter θ. (If we knew θ, there would be no problem to solve.) Bayesian MSE is more meaningful. It says that we do not know the true parameter θ, but we know its statistics. We are trying to find the best g(·) that minimizes the error. Our solution will depend on the statistics of Θ but not on the unknown true parameter θ. When we say minimum mean squared error estimation, we typically refer to the Bayesian MMSE. In this case, the problem we solve is g(·)=argmin E (cid:2) (Θ−g(X))2(cid:3) . (8.61) Θ,X g(·) 523
CHAPTER 8. ESTIMATION As you can see from Definition 8.9, the goal of the Bayesian MMSE is to find a function g : RN → R such that the joint expectation E (cid:2) (Θ−g(X))2(cid:3) is minimized. In the case Θ,X where Θ is a vector, the problem becomes g(·)=argmin E (cid:2) (cid:107)Θ−g(X)(cid:107)2(cid:3) , (8.62) Θ,X g(·) where g(·):RN×d →Rd if Θ is a d-dimensional vector. The function g will take a sequence of N observed numbers and estimate the parameter Θ. What is the Bayesian MMSE estimate? The Bayesian MMSE estimate is obtained by minimizing the MSE: g(·)=argmin E (cid:2) (Θ−g(X))2(cid:3) . (8.63) Θ,X g(·) 8.4.3 MMSE estimate = conditional expectation Theorem 8.2. The Bayesian MMSE estimate is θ(cid:98)MMSE =argmin E Θ,X(cid:2) (Θ−g(X))2(cid:3) g(·) =E [Θ|X =x]. (8.64) Θ|X Proof. First of all, we decompose the joint expectation: (cid:90) E (cid:2) (Θ−g(X))2(cid:3) = E (cid:2) (Θ−g(X))2 |X =x(cid:3) f (x)dx. Θ,X Θ|X X Since f (x) ≥ 0 for all x, and E (cid:2) (Θ−g(X))2 |X =x(cid:3) ≥ 0 because it is a square, it X Θ|X follows that the integral is minimized when E (cid:2) (Θ−g(X))2 |X =x(cid:3) is minimized. Θ|X The conditional expectation can be evaluated as E [(Θ−g(X))2 |X =x] Θ|X (cid:20) (cid:12) (cid:21) =E Θ|X Θ2−2Θg(X)+g(X)2 (cid:12) (cid:12)X =x (cid:12) (cid:20) (cid:12) (cid:21) (cid:20) (cid:12) (cid:21) =E Θ2 (cid:12)X =x −2E Θ(cid:12)X =x g(x)+g(x)2 Θ|X (cid:12) Θ|X (cid:12) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) d=efV(x) d=efu(x) =V(x)−2u(x)g(x)+g(x)2+u(x)2−u(x)2 =V(x)−u(x)2+(u(x)−g(x))2 ≥V(x)−u(x)2, ∀g(·), where the last inequality holds because no matter what g(·) we choose, the square term (u(x)−g(x))2 is non-negative. Therefore, E [(Θ−g(X))2 |X =x] is lower-bounded by Θ|X 524
8.4. MINIMUM MEAN-SQUARE ESTIMATION V(x)−u(x)2, which is a bound that is independent of g(·). If we can find a g(·) such that this lower bound can be met, the corresponding g(·) is the minimizer. To this end we only need to make E [(Θ−g(X))2 | X = x] equal V(x)−u(x)2, Θ|X but this is easy: the equality holds if and only if (u(x)−g(x))2 = 0. In other words, if we choose g(·) such that g(x) = u(x), the corresponding g(·) is the minimizer. This g(·), by substituting the definition of u(x), is (cid:20) (cid:12) (cid:21) g(x)=E Θ(cid:12)X =x . (8.65) Θ|X (cid:12) This completes the proof. (cid:3) What is the MMSE estimate? The MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x]. (8.66) We emphasize that θ(cid:98)MMSE(x) is a function of x, because for a different set of observations x we will have a different estimated value. Since x is a random realization of the random vector X, we can also define the MMSE estimator as Θ(cid:98)MMSE(X)=E Θ|X[Θ|X]. (8.67) Inthisnotation,weemphasizethattheestimatorΘ(cid:98)MMSE returnsarandomparameter.The input to the estimator is the random vector X. Because we are not looking at a particular realization X =x but the general X, Θ(cid:98)MMSE is a function of X and not x. Conditional expectation of what? An MMSE estimator is the conditional expectation of Θ given X =x: (cid:20) (cid:12) (cid:21) (cid:90) E Θ(cid:12)X =x = θ f (θ|x)dθ. (8.68) Θ|X (cid:12) Θ|X This is the expectation using the posterior distribution f (θ|x). It should be compared Θ|X tothepeakoftheposterior,whichreturnsustheMAPestimate.Theposteriordistribution is constructed through Bayes’ theorem: f (x|θ)f (θ) X|Θ Θ f (θ|x)= . (8.69) Θ|X f (x) X Therefore, to evaluate the expectation of the condition distribution, we need to include the normalization constant f (x), which was omitted in MAP. X 525
CHAPTER 8. ESTIMATION The discussion about the mean squared error and the vector estimates can be skipped if this is your first time reading the book. What is the mean squared error when using the MMSE estimator? • The mean squared error conditioned on the observation is MSE(Θ,Θ(cid:98)MMSE(X))d =efE Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X] =Var [Θ|X], Θ|X which is the conditional variance. • The overall mean squared error, unconditioned, is MSE(Θ,Θ(cid:98)MMSE(·))=E X(cid:2) Var Θ|X[Θ|X](cid:3) =Var [Θ]. Θ Proof. Let us prove these two statements. The resulting MSE is obtained by substituting Θ(cid:98)MMSE(x)=E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3) into the MSE(Θ,Θ(cid:98)MMSE(X)). To this end, we have that E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X]=V(X)−u(X)2 + (u(X)−Θ(cid:98)MMSE(X))2 . (cid:124) (cid:123)(cid:122) (cid:125) =0,becauseΘ(cid:98)MMSE(X)=E Θ|X[Θ|X]=u(X) The variables V and u are defined as (cid:12) V(X)=E (cid:2) Θ2 (cid:12)X(cid:3) =2nd moment of Θ using f (θ|x), Θ|X (cid:12) Θ|X u(X)=E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3) =1st moment of Θ using f Θ|X(θ|x). Since Var[Z]=E[Z2]−E[Z]2 for any random variable Z, it follows that E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X]=V(X)−u(X)2 =E Θ|X(cid:2) Θ2 (cid:12) (cid:12) (cid:12)X(cid:3) −(cid:0)E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3)(cid:1)2 =variance of Θ using f (θ|x) Θ|X def = Var [Θ|X]. Θ|X Substituting this conditional variance into the MSE definition, (cid:90) MSE(Θ,Θ(cid:98)MMSE(·))= E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X =x]f X(x)dx (cid:90) = Var [Θ|X =x]f (x)dx Θ|X X =Var [Θ]. Θ (cid:3) 526
8.4. MINIMUM MEAN-SQUARE ESTIMATION What happens if the parameter is a vector? • The MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x]. • The MSE is (cid:26) (cid:110) (cid:111)(cid:27) MSE(Θ,Θ(cid:98)MMSE(·))=Tr E X Cov(Θ|X) . (8.70) Proof. The first statement, that the MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x], iseasytounderstandsinceitjustfollowsfromthescalarcase.TheestimatorisΘ(cid:98)MMSE(X)= E [Θ|X]. The corresponding MSE is Θ|X MSE(Θ,Θ(cid:98)MMSE(·))=E Θ,X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2] (cid:26) (cid:27) =E X E Θ|X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2 |X] , where we have used the law of total expectation to decompose the joint expectation. Using the matrix identity below, we have that (cid:26) (cid:27) E X E Θ|X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2 |X] (cid:26) (cid:104) (cid:110) (cid:111) (cid:105)(cid:27) =E X E Θ|X Tr (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X (cid:26) (cid:26) (cid:104) (cid:105)(cid:27)(cid:27) =Tr E X E Θ|X (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X . However, since the MMSE estimator is the condition expectation of the posterior, it follows that the inner expectation is the conditional covariance. Therefore, we arrive at the second statement: (cid:26) (cid:26) (cid:104) (cid:105)(cid:27)(cid:27) MSE(Θ,Θ(cid:98)MMSE(·))=Tr E X E Θ|X (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X (cid:26) (cid:110) (cid:111)(cid:27) =Tr E Cov(Θ|X) . X (cid:3) To prove the two statements above, we need some tools from linear algebra. The two specific matrix identities are given by the following lemma: Lemma 8.1. The following are matrix identities: • For any random vector Θ∈Rd, (cid:107)Θ(cid:107)2 =Tr(ΘTΘ)=Tr(ΘΘT). 527
CHAPTER 8. ESTIMATION • For any random vector Θ∈Rd, E [Tr(ΘΘT)]=Tr(E [ΘΘT]). Θ Θ The proof of these two results is straightforward. The first is due to the cyclic property of the trace operator. The second statement is true because the trace is a linear operator that sums the diagonal of a matrix. The end of the discussion. Please join us again. Example 8.22. Let (cid:40) (cid:40) θe−θx, x≥0, αe−αθ, θ ≥0, f (x|θ)= and f (θ)= X|Θ Θ 0, x<0, 0, θ <0. Find the ML, MAP, and MMSE estimates for a single observation X =x. Solution. We first find the posterior distribution: f (x|θ)f (θ) X|Θ Θ f (θ|x)= Θ|X f (x) X αθe−(α+x)θ = (cid:82)∞ αθe−(α+x)θ dθ 0 αθe−(α+x)θ = α (α+x)2 =(α+x)2θe−(α+x)θ. The MMSE estimate is the conditional expectation of the posterior: θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x] (cid:90) ∞ = θf (θ|x)dθ Θ|X 0 (cid:90) ∞ = θ(α+x)2θe−(α+x)θ dθ 0 (cid:90) ∞ =(α+x) θ2·(α+x)e−(α+x)θ dθ 0 (cid:124) (cid:123)(cid:122) (cid:125) 2ndmomentofexponentialdistribution 2 2 =(α+x)· = . (α+x)2 α+x 528
8.4. MINIMUM MEAN-SQUARE ESTIMATION The MAP estimate is the peak of the posterior: θ(cid:98)MAP(x)=argmax logf X|Θ(x|θ)+logf Θ(θ) θ =argmax −θx+logθ−αθ+logα. θ Taking the derivative and setting it to zero yields −x+ 1 −α=0. This implies that θ 1 θ(cid:98)MAP(x)= α+x. Finally, the ML estimate is 1 θ(cid:98)ML(x)=argmax logf X|Θ(x|θ)= x. θ Practice Exercise 8.8. Following the previous example, derive the estimates for multiple observations X =x. Solution. The posterior is f (x|θ)f (θ) X|Θ Θ f (θ|x)= Θ|X f (x) X ((cid:81)N f (x |θ))f (θ) = n=1 X|Θ n Θ f (x) X αθe−(α+(cid:80)N n=1xn)θ = (cid:82)∞ αθe−(α+(cid:80)N n=1xn)θ dθ 0 (cid:32) N (cid:33)2 = α+(cid:88) x n θe−(α+(cid:80)N n=1xn)θ. n=1 Therefore, we are only replacing x by the sum (cid:80)N x in the posterior. Hence, the n=1 n estimates are: 2 θ(cid:98)MMSE(x)= α+(cid:80)N x , n=1 n 1 θ(cid:98)MAP(x)= α+(cid:80)N x , n=1 n 1 θ(cid:98)ML(x)= (cid:80)N x . n=1 n This example shows that as N →∞, the ML estimate θ(cid:98)ML(x)→0. The reason is that the likelihoodisanexponentialdistribution.Therefore,thepeakisalwaysat0.Theposterioris an Erlang distribution, and therefore the peak is offset by α in the denominator. However, as N →∞ the posterior distribution is dominated by the likelihood, so the peak is shifted 529
CHAPTER 8. ESTIMATION towards 0. Finally, since the Erlang distribution is asymmetric, the mean is different from the peak. Hence, the MMSE estimate is different from the MAP estimate. 8.4.4 MMSE estimator for multidimensional Gaussian The multidimensional Gaussian has some very important uses in data science. Accordingly, we devote this subsection to the discussion of the MMSE estimate of a Gaussian. The main result is stated as follows. What is the MMSE estimator for a multi-dimensional Gaussian? Theorem 8.3. Suppose Θ∈Rd and X ∈RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX The MMSE estimator is Θ(cid:98)MMSE(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.71) The proof of this result is not difficult but it is tedious. The flow of the argument is: • Step 1: Show that the posterior distribution f (θ|x) is a Gaussian. Θ|X • Step 2: To do so we need to complete the squares for matrices. • Step 3: Once we have the f (θ|x), the posterior mean is the MMSE estimator. Θ|X The proof below can be skipped if this is your first time reading the book. Proof. The posterior PDF is f (θ,x) f (θ|x)= Θ,X Θ|X f (x) X (cid:40) (cid:20) (cid:21)T (cid:20) (cid:21)−1(cid:20) (cid:21)(cid:41) θ−µ Σ Σ θ−µ √ 1 exp −1 Θ ΘΘ ΘX Θ (2π)d+N|Σ| 2 x−µ X Σ XΘ Σ XX x−µ X = . √ 1 exp(cid:110) −1(cid:2) x−µ (cid:3)T Σ−1 (cid:2) x−µ (cid:3)(cid:111) (2π)N|ΣXX| 2 X XX X Without loss of generality, we assume that µ =µ =0. Then the posterior becomes X Θ 1 f (θ|x)= Θ|X (cid:112) (2π)d|Σ|/|Σ | XX (cid:40) 1(cid:20) θ(cid:21)T (cid:20) Σ Σ (cid:21)−1(cid:20) θ(cid:21) 1 (cid:41) ×exp − ΘΘ ΘX + xTΣ−1 x . 2 x Σ XΘ Σ XX x 2 XX (cid:124) (cid:123)(cid:122) (cid:125) H(θ,x) The tedious task here is to simplify H(θ,x). 530
8.4. MINIMUM MEAN-SQUARE ESTIMATION Regardless of what the 2-by-2 matrix inverse is, the matrix will take the form (cid:20) (cid:21)−1 (cid:20) (cid:21) Σ Σ A B ΘΘ ΘX = , Σ Σ C D XΘ XX forsomechoicesofmatricesA,B,C andD.Therefore,thefunctionH(θ,x)canbewritten as 1(cid:110) (cid:111) H(θ,x)=− θTAθ+θTBx+xTCθ+xTDx−xTΣ−1 x . (8.72) 2 XX Our goal is to complete the square for H(θ,x). To this end, we propose to write 1(cid:110) (cid:111) H(θ,x)=− (θ−Gx)TA(θ−Gx)+Q(x) , (8.73) 2 for some matrix G and function Q(·) of x only. If we compare Equation (8.72) and Equa- tion (8.73), we observe that G must satisfy G=−A−1B. Therefore,ifwecandetermineAandB,wewillknowG.IfweknowG,wehavecompleted the square for H(θ,x). If we can complete the square for H(θ,x), we can write (cid:26) (cid:27) exp{−Q(x)/2} 1 f (θ|x)= ×exp − (θ−Gx)TA(θ−Gx) . Θ|X (cid:112) (2π)d|Σ|/|Σ | 2 XX (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) constantinθ aGaussian Hence, the MMSE estimate, which is the posterior mean E[Θ|X =x], is simply Gx: θ(cid:98)MMSE(x)=E[Θ|X =x] =Gx =−A−1Bx. So it remains to determine A and B by solving the tedious matrix inversion problem. The result is:6 A=(Σ −Σ Σ−1 Σ )−1, ΘΘ ΘX XX XΘ B =−(Σ −Σ Σ−1 Σ )−1Σ Σ−1 , ΘΘ ΘX XX XΘ ΘX XX C =(Σ −Σ Σ−1Σ )−1Σ Σ−1, XX XΘ ΘΘ ΘX XΘ ΘΘ D =(Σ −Σ Σ−1Σ )−1. XX XΘ ΘΘ ΘX Therefore, plugging everything into the equation, θ(cid:98)MMSE(x)=−A−1Bx =Σ Σ−1 x. Θ,X XX For non-zero means, we can repeat the same arguments above and show that θ(cid:98)MMSE(x)=µ Θ+Σ Θ,XΣ− X1 X(x−µ X). (cid:3) 6See Matrix Cookbook https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf Section 9.1.5 ontheSchurcomplement. 531
CHAPTER 8. ESTIMATION End of the proof. Please join us again. Practice Exercise 8.9. Suppose Θ ∈ Rd and X ∈ RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX We know that the MMSE estimator is Θ(cid:98)MMSE(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.74) Find the mean squared error when using the MMSE estimator. Solution. Conditioned on X =x, according to Equation (8.70), the MMSE is MSE(Θ,Θ(cid:98)(X))=Tr{Cov[Θ|X]}. The conditional covariance Cov[Θ|X] is the covariance of the posterior distribution f (θ|x), which is Θ|X Tr{Cov[Θ|X]}=Tr{A} =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) . ΘΘ ΘX XX XΘ The overall mean squared error is (cid:16) (cid:17) (cid:20) (cid:21) MSE Θ,Θ(cid:98)(·) =E X MSE(Θ,Θ(cid:98)(X)) (cid:90) = MSE(Θ,Θ(cid:98)(x))f X(x)dx (cid:90) = Tr{Cov[Θ|X]}f (x)dx X (cid:90) = Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) f (x)dx ΘΘ ΘX XX XΘ X (cid:90) =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) f (x)dx ΘΘ ΘX XX XΘ X =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) . ΘΘ ΘX XX XΘ For multidimensional Gaussian, does MMSE = MAP? The answer is YES. Theorem 8.4. Suppose Θ∈Rd and X ∈RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX 532
8.4. MINIMUM MEAN-SQUARE ESTIMATION The MAP estimate is Θ(cid:98)MAP(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.75) Proof. The proof of this result is straightforward. If we return to the proof of the MMSE result, we note that (cid:26) (cid:27) exp{−Q(x)/2} 1 f (θ|x)= ×exp − (θ−Gx)TA(θ−Gx) . Θ|X (cid:112) (2π)d|Σ|/|Σ | 2 XX (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) constantinθ aGaussian Therefore, the maximizer of this posterior distribution, which is the MAP estimate, is θ(cid:98)MAP(x)=argmax f Θ|X(θ|x) θ 1 =argmax − (θ−Gx)TA(θ−Gx). 2 θ Taking the derivative w.r.t. θ and setting it zero, we have θ(cid:98)MAP(x)=Gx=Σ Θ,XΣ− X1 Xx. If the mean vectors are non-zero, we have θ(cid:98)MAP(x)=µ Θ+Σ ΘXΣ− X1 X(x−µ X). (cid:3) 8.4.5 Linking MMSE and neural networks The blossoming of deep neural networks since 2010 has created a substantial impact on modern data science. The basic idea of a neural network is to train a stack of matrices and nonlinear functions (known as the network weights and the neuron activation functions, respectively), among other innovative ideas, so that a certain training loss is minimized. Expressingthisbyequations,thegoalofthelearningisequivalenttosolvingtheoptimization problem (cid:20) (cid:21) g(·)=argmin E (cid:107)Θ−g(X)(cid:107)2 , (8.76) (cid:98) X,Θ g(·) where X ∈ RM is the input data and Θ ∈ Rd is the ground truth prediction. We want to find g(·) such that the error is minimized. The error we choose here is the (cid:96) -norm error (cid:107)·(cid:107)2. It is only one of many possi- 2 ble choices. You may recognize that this is exactly the same as the MMSE optimization. Therefore,theneuralnetworkwearefindinghereistheMMSEestimator.SincetheMMSE estimator is the conditional expectation of the posterior distribution, the neural network approximates the mean of the posterior distribution. Often the struggle we have with deep neural networks is whether we can find the optimal network parameters via optimization algorithms such as the stochastic gradient descent algorithms. However, if we think about this problem more deeply, the equivalence between the MMSE estimator and the posterior mean tells us that the hard part is related to the posterior distribution. In the high-dimensional landscape, it is close to impossible to determine the posterior and its mean. If we add to these difficulties and the nonconvexity of the function g, training a network is very challenging. 533
CHAPTER 8. ESTIMATION Onemisconceptionaboutneuralnetworksisthatifwecanachievealowtrainingerror, andifthemodelcanalsoachievealowtestingerror,thenthenetworkisgood.Thisisafalse sense of satisfaction. If a model can achieve very good training and testing errors, then the model is only good with respect to the error you choose. For example, if we choose the (cid:96) - 2 normerror(cid:107)·(cid:107)2 andifourmodelachievesgoodtrainingandtestingerrors(intermsof(cid:107)·(cid:107)2), we can conclude that the model does well with respect to (cid:107)·(cid:107)2. The more serious problem here, unfortunately, is that (cid:107)·(cid:107)2 is not necessarily a good metric of performance (for both trainingandtesting)becausetrainingwith(cid:107)·(cid:107)2 isequivalenttoapproximatingtheposterior mean. There is absolutely no reason to believe that in the high-dimensional landscape, the posterior mean is the optimal. If we choose the posterior mode or the posterior median, we will also obtain a result. Why are the modes and medians “worse” than the mean? In practice, it has been observed that training deep neural networks for image-processing tasksgenerallyleadstoover-smoothedimages.Thisdemonstrateshowminimizingthemean squared error (cid:107)·(cid:107)2 can be a fundamental mismatch with the problem. Is minimizing the MSE the best option? • No.MinimizingtheMSEisequivalenttofindingthemeanoftheposterior.There is no reason why the mean is the “best”. • You can find the mode of the posterior, in which case you will get a MAP estimator. • You can also find the median of the posterior, in which case you will get the minimum absolute error estimator. • Ultimately, you need to define what is “good” and what is “bad”. • The same principle applies to deep neural networks. Especially in the regression setting, why is (cid:107)·(cid:107)2 a good evaluation metric for testing (not just training)? 8.5 Summary In this chapter, we have discussed the basic principles of parameter estimation. The three building blocks are: • Likelihood f (x|θ): the PDF that we observe samples X conditioned on the un- X|Θ known parameter Θ. In the frequentist world, Θ is a deterministic quantity. In the Bayesian world, Θ is random and so it has a PDF. • Prior f (θ): the PDF of Θ. The prior f (θ) is used by all Bayesian computation. Θ Θ • Posterior f (θ|x): the PDF that the underlying parameter is Θ=θ given that we Θ|X have observed X =x. The three building blocks give us several strategies to estimate the parameters: • Maximum likelihood (ML) estimation: Maximize f (x|θ). X|Θ 534
8.6. REFERENCES • Maximum a posteriori (MAP) estimation: Maximize f (θ|x). Θ|X • Minimummean-squareestimation(MMSE):Minimizethemeansquarederror,which is equivalent to finding the mean of f (θ|x). Θ|X As discussed in this chapter, no single estimation strategy is universally “better” because one needs to specify the optimality criterion. If the goal is to minimize the mean squared error, then the MMSE estimator is the optimal strategy. If the goal is to maximize the likelihood without assuming any prior knowledge, the ML estimator would be the optimal strategy. It may appear that if we knew the ground truth parameter θ∗ we could minimize the distance between the estimated parameter θ and the true value θ∗. If the parameter is a scalar, this will work. However, if the parameter is a vector, the noise of the distance becomes an issue. For example, if one cares about the mean absolute error (MAE), the optimal estimator would be the median of the posterior distribution instead of the mean of the posterior in the MMSE case. Therefore, it is the end user’s responsibility to specify the optimality criterion. Wheneverweconsiderparameterestimation,wetendtothinkthatitisaboutestimat- ing the model parameters, such as the mean of a Gaussian PDF. While in many statistics problems this is indeed the case, parameter estimation can be much broader if we link it with regression. Specifically, a regularized linear regression problem can be formulated as a MAP estimation θ∗ =argmax (cid:107)Xθ−y(cid:107)2 + λR(θ) , (8.77) θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) −logfX|Θ(x|θ) −logfΘ(θ) for some regularization R(θ), which is also the negative log of the prior. Expressed in this way,werecognizethattheMAPestimationcanbeusedtorecoversignals.Forexample,we can model X as a linear degradation process of certain imaging systems. Then solving the MAPestimationisequivalenttofindingthebestsignalexplainingthedegradedobservation using the posterior as the criterion. There is rich literature dealing with solving MAP esti- mationproblemssimilartotheseinsubjectssuchascomputationalimaging,communication systems, remote sensing, radar engineering, and recommendation systems, to name a few. 8.6 References Basic 8-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 8 and Chapter 9. 8-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 6 and Chapter 8. 8-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 8. 8-4 HenryStarkandJohnW.Woods,ProbabilityandRandomProcesseswithApplications to Signal Processing, Prentice Hall, 3rd Edition, 2002. Chapter 5. 535
CHAPTER 8. ESTIMATION 8-5 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Chapter 12. Theoretical analysis 8-6 H. Vincent Poor, An Introduction Signal Detection and Estimation, Springer, 1998. 8-7 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory, Prentice-Hall, 1993. 8-8 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation, Springer, 2008. 8-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 2001. Chapter 8. 8-10 LarryWasserman,AllofStatistics:AConciseCourseinStatisticalInference,Springer, 2010. 8-11 Erich L. Lehmann, Elements of Large-Sample Theory, Springer, 1999. Chapter 7. 8-12 George Casella and Roger L. Berger Statistical Inference, Duxbury, 2002. Chapter 7. Machine-learning 8-13 ChristopherBishop,PatternRecognitionandMachineLearning,Springer,2006.Chap- ter 2 and Chapter 3. 8-14 Richard O. Duda, Peter E. Hart and David G. Stork, Pattern Classification, Wiley 2001. Chapter 3. 8.7 Problems Exercise 1. Let X ,...,X be a sequence of i.i.d. Bernoulli random variables with P[X = 1] = θ. 1 N n Suppose that we have observed x ,...,x . 1 N (a) Show that the PMF of X n is p Xn(x n |θ)=θxn(1−θ)1−xn. Find the joint PMF p (x ,...,x ). X1,...,XN 1 N (b) Find the maximum likelihood estimate θ(cid:98), i.e., θ(cid:98)ML =argmax log p X1,...,XN(x 1,...,x N). θ Express your answer in terms of x ,...,x . 1 N 536
8.7. PROBLEMS (c) Letθ =1/2.UseChebyshev’sinequalitytofindanupperboundforP[|Θ(cid:98)ML−θ|>0.1]. Exercise 2. Let Y = θ + W be the output of a noisy channel where the input is a scalar θ and n n W ∼N(0,1) is an i.i.d. Gaussian noise. Suppose that we have observed y ,...,y . n 1 N (a) Express the PDF of Y in terms of θ and y . Find the joint PDF of Y ,...,Y . n n 1 N (b) Findthemaximumlikelihoodestimateθ(cid:98)ML.Expressyouranswerintermsofy 1,...,y N. (c) Find E[Θ(cid:98)ML]. Exercise 3. Let X ,...,X be a sequence of i.i.d. Gaussian random variables with unknown mean θ 1 N 1 and variance θ . Suppose that we have observations x ,...,x . 2 1 N (a) Express the PDF of X in terms of x , θ and θ . Find the joint PDF of X ,...,X . n n 1 2 1 N (b) Find the maximum likelihood estimates of θ and θ . 1 2 Exercise 4. In this problem we study a single-photon image sensor. First, recall that photons arrive according to a Poisson distribution, i.e., the probability of observing k photons is λke−λ P[Y =k]= , k! whereλisthe(unknown)underlyingphotonarrivalrate.Whenphotonsarriveatthesingle- photon detector, the detector generates a binary response “1” when one or more photons are detected, and “0” when no photon is detected. (a) Let B be the random variable denoting the response of the single-photon detector. That is, (cid:40) 1, Y ≥1, B = 0, Y =0. Find the PMF of B. (b) Suppose we have obtained T independent measurements with realizations B = b , 1 1 B =b ,...,B =b .Showthattheunderlyingphotonarrivalrateλcanbeestimated 2 2 T T by (cid:32) (cid:80)T b (cid:33) λ=−log 1− t=1 t . T (c) Get a random image from the internet and turn it into a grayscale array with values between 0 and 1. Write a MATLAB or Python program to synthetically generate a sequence of T =1000 binary images. Then use the previous result to reconstruct the grayscale image. 537
CHAPTER 8. ESTIMATION Exercise 5. Consider a deterministic vector s∈Rd and random vectors f (y|θ)=Gaussian(sθ,Σ), Y|Θ f (θ)=Gaussian(µ,σ2). Θ (a) Show that the posterior distribution is given by f (θ|y)=Gaussian(m,q2), (8.78) Θ|Y where d2 =sTΣ−1s, (cid:18) 1 (cid:19)−1(cid:16) µ (cid:17) m= d2+ sTΣ−1y+ , σ2 σ2 1 q2 = . d2+ 1 σ2 (b) Show that the MMSE estimate θ(cid:98)MMSE(y) is given by σ2sTΣ−1y+µ θ(cid:98)MMSE(y)= σ2d2+1 . (8.79) (c) Show that the MSE is given by 1 MSE(Θ,Θ(cid:98)MMSE(Y))= d2+ 1 . (8.80) σ2 What happens when σ →0? (d) Give an interpretation of d2. What happens when d2 →0 and when d2 →∞? Exercise 6. Prove the following identity: (cid:20) (cid:21)−1 Σ Σ ΘΘ ΘX Σ Σ XΘ XX (cid:20) (Σ −Σ Σ−1 Σ )−1 −(Σ −Σ Σ−1 Σ )−1Σ Σ−1 (cid:21) = ΘΘ ΘX XX XΘ ΘΘ ΘX XX XΘ ΘX XX . (Σ −Σ Σ−1Σ )−1Σ Σ−1 (Σ −Σ Σ−1Σ )−1 XX XΘ ΘΘ ΘX XΘ ΘΘ XX XΘ ΘΘ ΘX Hint:Youcanperformreverseengineeringbycheckingwhethertheproductoftheleft-hand side and the right-hand side would give you the identity matrix. Exercise 7. Let X , X , X and X be four i.i.d. Poisson random variables with mean θ =4. Find the 1 2 3 4 mean and variance of the following estimators Θ(cid:98)(X) for θ and determine whether they are biased or unbiased. 538
8.7. PROBLEMS • Θ(cid:98)(X)=(X 1+X 2)/2 • Θ(cid:98)(X)=(X 3+X 4)/2 • Θ(cid:98)(X)=(X 1+2X 2)/3 • Θ(cid:98)(X)=(X 1+X 2+X 3+X 4)/4 Exercise 8. LetX ,...,X bei.i.d.randomvariableswithauniformdistributionof[0,θ].Considerthe 1 N following estimator: Θ(cid:98)(X)=max(X 1,...,X N). (8.81) (a) Show that the PDF of Θ(cid:98) is f Θ(cid:98)(θ) = N[F X(x)]N−1f X(x), where f X and F X are re- spectively the PDF and CDF of X . n (b) Show that Θ(cid:98) is a biased estimator. (c) Find the variance of Θ(cid:98). Is it a consistent estimator? (d) Find a constant c so that cΘ(cid:98) is unbiased. Exercise 9. Let X ,...,X be i.i.d. Gaussian random variables with unknown mean θ and known 1 N variance σ =1. (a) Show that the log-likelihood function is N N 1 (cid:88) logL(θ|x)=− log(2π)− (x −θ)2. (8.82) 2 2 n n=1 (b) Let X2 = 1 (cid:80)N x2 and X = 1 (cid:80)N x . Show that X2 > (X)2 if and only if N n=1 n N n=1 n (cid:80)N (x −θ)2 ≥0 for all θ. n=1 n (c) Use Python to plot the function logL(θ|x), when X =2 and X2 =1. Exercise 10. Let X ,...,X be i.i.d. uniform random variables over the interval [0,θ]. 1 N Let T =max(X ,...,X ). 1 N (a) Consider the estimator h(X)= 1 (cid:80)N X . Is h(·) an unbiased estimator? N n=1 n (b) Consider the estimator g(X)= 1 (cid:80)N X . Is g(·) an unbiased estimator? N n=1 n (c) Show that (cid:18) (cid:19) N +1 E[g(X)|T =t]= t. N 539
CHAPTER 8. ESTIMATION (d) Let g(X)=E[g(X)|T]=(cid:0)N+1(cid:1) T. Show that (cid:98) N (cid:18) (N +1)2 (cid:19) E[g(X)2]= θ2. (cid:98) N(N +2) (e) Show that (cid:18) (cid:19) 1 E[(g(X)−θ)2]= θ2. (cid:98) N(N +2) Exercise 11. The Kullback-Leibler divergence between two distributions p (x) and p (x) is defined as 1 2 (cid:90) p (x) KL(p (cid:107)p )= p (x)log 1 dx. (8.83) 1 2 1 p (x) 2 Suppose we approximate p using a distribution p . Let us choose p = Gaussian(µ,Σ). 1 2 2 Show that µ and Σ, which minimize the KL divergence, are such that µ=E [x] and Σ=E [(x−µ)(x−µ)T]. x∼p1(x) x∼p1(x) Exercise 12. (a) Recall that the trace operator is defined as tr[A] = (cid:80)d [A] . Prove the matrix i=1 i,i identity xTAx=tr[AxxT], (8.84) where A∈Rd×d. (b) Show that the likelihood function (cid:89)N (cid:26) 1 (cid:110) 1 (cid:111)(cid:27) p(D|Σ)= exp − (x −µ)TΣ−1(x −µ) (8.85) (2π)d/2|Σ|1/2 2 n n n=1 can be written as (cid:40) (cid:34) N (cid:35)(cid:41) 1 1 (cid:88) p(D|Σ)= |Σ−1|N/2exp − tr Σ−1 (x −µ)(x −µ)T . (8.86) (2π)Nd/2 2 n n n=1 (c) Let A = Σ−1Σ(cid:98)ML, and λ 1,...,λ d be the eigenvalues of A. Show that the result from part (b) leads to (cid:32) d (cid:33)N/2 (cid:40) d (cid:41) 1 (cid:89) N (cid:88) p(D|Σ)= λ exp − λ . (8.87) (2π)Nd/2|Σ(cid:98)ML|N/2 i=1 i 2 i=1 i Hint: For matrix A with eigenvalues λ ,...,λ , tr[A]=(cid:80)d λ . 1 d i=1 i (d) Find λ ,...,λ such that Equation (8.87) is maximized. 1 d 540
8.7. PROBLEMS (e) With the choice of λ i given in (d), derive the ML estimate Σ(cid:98)ML. (f) WhatwouldbethealternativewayoffindingΣ(cid:98)ML?Youdonotneedtoproveit.Just briefly describe the idea. (g) Σ(cid:98)ML is a biased estimate of the covariance matrix because E[Σ(cid:98)ML](cid:54)=Σ. Can you suggest an unbiased estimate Σ(cid:98)unbias such that E[Σ(cid:98)unbias] = Σ? You don’t need to prove it. Just state the result. 541
CHAPTER 8. ESTIMATION 542
Chapter 9 Confidence and Hypothesis InChapters7and8welearnedaboutregressionandestimation,whichallowustodetermine theunderlyingparametersofourstatisticalmodels.Afterobtainingtheestimates,wewould liketoquantifytheaccuracyoftheestimatesanddrawstatisticalconclusions.Additionally, we would like to understand the confidence of these estimates along with their statistical significance. This chapter presents a few principles that involve analyzing the confidence of theestimatesandconductinghypothesistesting.Therearetwomainquestionsthatwewill address: • How good is our estimate? This is a fundamental question about the estimator Θ(cid:98), a random variable with a PDF, a mean, and a variance.1 The estimator we construct todaymaybedifferentfromtheestimatorweconstructtomorrowduetovariationsin theobserveddata.Therefore,thequalityoftheestimatordependsontherandomness andthenumberofsamplesusedtoconstructit.Tomeasurethequalityoftheestimator we need to introduce an important concept known as the confidence. • Is there statistical significance? Suppose that we ran a campaign and observed that there is a change in the statistics. On what basis do we claim that the change is statistically significant? How should the cutoff be determined? If we claim that a result is statistically significant but there is no significance in reality, how much error will we suffer? These questions are the subjects of hypothesis testing. These two principal questions are critical for modern data science. If they are not properly answered, our statistical conclusions could potentially be flawed. A toy example: Imagine thatyou aredeveloping aCOVID-19vaccine.Youtested thevaccineon three patients, and all of them show positive responses to the vaccine. You felt excited because your vaccine has a 100% success rate. You submit your vaccine application to FDA. Within 1 second your application is rejected. Why? The answer is obvious. You only have three testing samples. How reliable can these three samples be? While you are laughing at this toy example, it raises deep statistical questions. First, why are three samples not enough? Well, it is because the variance of the estimator can potentiallybehuge.Moresamplesarebetterbecauseiftheestimatoristhesampleaverageof theindividualresponses,theestimatorwillbehavelikeaGaussianaccordingtotheCentral 1Not all random variables have a well-defined PDF, mean, and variance. E.g., a Cauchy variable does nothaveamean. 543
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Limit Theorem. The variance of this Gaussian will diminish as we have more samples. Therefore,ifwewanttocontrolthevarianceoftheestimator,weneedmoresamples.Second, even if we have many samples, how confident is this estimator with respect to the unknown population parameter? Note that the population parameter is unknown, and so we cannot measurethingssuchasthemeansquarederror.Weneedatooltoreportconfidence.Third, for simple estimators such as the sample average, we can approximate it by a Gaussian. However,iftheestimatorismorecomplicated,e.g.,thesamplemedian,howdoweestimate the variance and the confidence? Fourth, suppose that we have expanded the vaccine test to, say, 951 patients, and we have obtained some statistics. To what extent can we declare that the vaccine is effective? We need a decision rule that turns the statistics into a binary decision. Finally, even if we declare that the vaccine is effective with a confidence of 95%, what about the remaining 5%? What if we want to push the confidence to 99%? What is the trade-off? As you can see, these questions are the recurring themes of all data science problems. No matter if you are developing a medical diagnostic system, a computer vision algorithm, a speech recognition system, a recommendation system, a search engine, stock forecast, fraud detection, or robotics controls, you need to answer these questions. This chapter will introduce useful concepts related to data analysis in the form of five basic principles: 1. Confidence interval (Section 9.1). A confidence interval is a random interval that includes the true parameter. We will discuss how a confidence interval is constructed and the correct way to interpret the confidence interval. 2. Bootstrapping (Section 9.2). When constructing the confidence interval, we need the variance of the estimator. However, since we do not know the true distribution, we need an alternative way to estimate the variance. Bootstrapping is designed for this purpose. 3. Hypothesis testing (Section 9.3). Many statistical tasks require a binary decision at the end, e.g., there is a disease versus there is no disease. Hypothesis testing is a principle for making a systematic decision with statistical guarantees. 4. Neyman-Pearson decision(Section9.4).Thesimplehypothesistestingprocedurehas manylimitationsthatcanonlyberesolvedifweunderstandamoregeneralframework. We will study such a framework, called the Neyman-Pearson decision rule. 5. ROC and PR curves (Section 9.5). No decision rule is perfect. There is always a trade-off between how much we can detect and how much we will miss. The receiver operating characteristic (ROC) curve and the precision-recall (PR) curve can give us more insight into this trade-off. We will establish the equivalence between the ROC and the PR curve and correct any misconceptions about them. After reading this chapter, we hope that you will be able to apply these principles to your favorite data analysis problems correctly. With these principles, you can tell your customers or bosses the statistical significance of your conclusions. You will also be able to help your friends understand the many misconceptions that they may find on the internet. 544
9.1. CONFIDENCE INTERVAL 9.1 Confidence Interval The first topic we discuss in this chapter is the confidence interval. At a high level, the confidence interval tells us the quality of our estimator with respect to the number of sam- ples. We begin this section by reviewing the randomness of an estimator. Then we develop the concept of the confidence interval. We discuss several methods for constructing and interpreting these confidence intervals. 9.1.1 The randomness of an estimator Imagine that we have a dataset X = {X ,...,X }, where we assume that X are i.i.d. 1 N n copies drawn from a distribution f X(x;θ). We want to construct an estimator Θ(cid:98) of θ from the dataset X. For example, if f is a Gaussian distribution with an unknown mean θ, we X would like to estimate θ using the sample average Θ(cid:98). In statistics, an estimator Θ(cid:98) is also known as a statistic, which is constructed from the samples. In this book we use the terms “estimator”and“statistic”interchangeably.Writtenasequations,anestimatorisafunction of the samples: Θ(cid:98) = g(X 1,...,X N), (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) estimator functionofX where g is a function that takes the samples X 1,...,X N and returns a random variable Θ(cid:98). For example, the sample average N 1 (cid:88) Θ(cid:98) = N X n n=1 (cid:124) (cid:123)(cid:122) (cid:125) g(X1,...,XN) is an estimator because it is computed by summing the samples X ,...,X and dividing it 1 N by N. What is an estimator? • An estimator Θ(cid:98) is a function of the samples X 1,...,X N: Θ(cid:98) =g(X 1,...,X N). (9.1) • Θ(cid:98) is a random variable. It has a PDF, CDF, mean, variance, etc. Byconstruction,Θ(cid:98) isarandomvariablebecauseitisafunctionoftherandomsamples. Therefore, Θ(cid:98) has its own PDF, CDF, mean, variance, etc. Since Θ(cid:98) is a random variable, we should report both the estimator’s value and the estimator’s confidence when reporting its performance. The confidence measures the quality of Θ(cid:98) when compared to the true parameter θ. It provides a measure of the reliability of the estimator Θ(cid:98). If Θ(cid:98) fluctuates a greatdealwemaynotbeconfidentofourestimates.Let’sconsiderthefollowingexample. 545
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Example 9.1. A class of 1000 students took a test. The distribution of the score is roughly a Gaussian with mean 50 and standard deviation 20. A teaching assistant was too lazy to calculate the true population mean. Instead, he sampled a subset of 5 scores listed as follows: Student ID 1 2 3 4 5 Scores 11 97 1 78 82 He calculated the average, which is 53.8. This is a very good estimate of the class average (which is 50). What is wrong with his procedure? Solution. He was just lucky. It quite possible that if he sampled another 5 scores, he would get something very different. For example, if he looks at the 11 to 15 student scores, he could get: Student ID 11 12 13 14 15 Scores 44 29 19 27 15 In this case the average is 26.8. Both53.8and26.8arelegitimateestimates,buttheyaretherandomrealizations of a random variable Θ(cid:98). This Θ(cid:98) has a PDF, CDF, mean, variance, etc. It may be misleading to simply report the estimated value from a particular instant, so the confidence of the estimator must be specified. DistributionsofΘ(cid:98).WenextdiscussthedistributionofΘ(cid:98).Figure9.1illustratesseveral key ideas. Suppose that the population distribution f (x) is a mixture of two Gaussians. X Let θ be the mean of this distribution (somewhere between the two peak locations). We sample N =50 data points X ,...,X from this distribution. However, the 50 data points 1 N we sample today could differ from the 50 data points we sample tomorrow. If we compute the sample average from each of these finite-sample distributions, we will obtain a set of sampleaveragesΘ(cid:98).Notably,wehaveaset ofΘ(cid:98) becausetodaywehaveoneΘ(cid:98) andtomorrow we have another Θ(cid:98). By plotting the histogram of the sample averages Θ(cid:98), we will have a distribution. The histogram of Θ(cid:98) depends on several factors. According to Central Limit Theorem, the shape of f (θ) is a Gaussian because Θ(cid:98) is the average of N i.i.d. random variables. Θ(cid:98) If Θ(cid:98) is not the average of i.i.d. random variables, the shape is not necessarily a Gaussian. This results in additional complications, so we will discuss some tools for dealing with this problem. The spread of the sample distribution is mainly driven by the number of samples we have in each subdataset. As you can imagine, the more samples we have in a subdataset themoreaccuratethedistribution.Thusyouwillhaveamoreaccuratesampleaverage.The fluctuation of the sample average will also be smaller. Before we continue, let’s summarize the randomness of Θ(cid:98): What is the randomness of Θ(cid:98)? • Θ(cid:98) is generated from a finite-sample dataset. Each time we draw a finite-sample dataset, we introduce randomness. 546
9.1. CONFIDENCE INTERVAL Figure9.1:PictorialillustrationoftherandomnessoftheestimatorΘ(cid:98).Givenapopulation,ourdatasets areusuallyasubsetofthepopulation.Computingthesampleaveragefromthesefinite-sampledistribu- tions introduces the randomness to Θ(cid:98). If we plot the histogram of the sample averages, we will obtain adistribution.Themeanofthisdistributionisthepopulationmean,butthereisanontrivialamountof fluctuation. The purpose of the concept of confidence interval is to quantify this fluctuation. • IfΘ(cid:98) isthesampleaverage,thePDFis(roughly)aGaussian.IfΘ(cid:98) isnotasample average, the PDF is not necessarily a Gaussian. • The spread of the fluctuation depends on the number of samples in each sub- dataset. 9.1.2 Understanding confidence intervals The confidence interval is a probabilistic statement about Θ(cid:98). Instead of studying Θ(cid:98) as a point, we construct an interval (cid:104) (cid:105) I = Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15) , (9.2) for some (cid:15) to be determined. Note that this interval is a random interval: If we have a different realization of Θ(cid:98), we will have a different I. We call I the confidence interval for the estimator Θ(cid:98). Given this random interval, we ask: What is the probability that I includes θ? That means that we want to evaluate the probability (cid:104) (cid:105) P[θ ∈I]=P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) . WeemphasizethattherandomnessinthisprobabilityiscausedbyΘ(cid:98),notθ.Thisisbecause the interval I changes when we conduct a different experiment to obtain a different Θ(cid:98). The 547
CHAPTER 9. CONFIDENCE AND HYPOTHESIS situation is similar to that illustrated on the left-hand side of Figure 9.2. The confidence interval I changes but the true parameter θ is fixed. Figure 9.2: ConfidenceintervalistherandomintervalI =[Θ(cid:98)−(cid:15),Θ(cid:98)+(cid:15)],notthedeterministicinterval [θ−(cid:15),θ+(cid:15)]. The random interval in the former case does not require any knowledge about the true parameter θ, whereas the latter requires θ. By claiming a 95% confidence interval, we say that there is 95% chance that the random interval will include the true parameter. So if you have 100 random realizations of the confidence intervals, then 95 on average will include the true parameter. Confidence intervals can be confusing. Often the confusion arises because of the fol- lowing identity: (cid:104) (cid:105) (cid:104) (cid:105) P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) =P −(cid:15)≤θ−Θ(cid:98) ≤(cid:15) (cid:104) (cid:105) =P −(cid:15)−θ ≤−Θ(cid:98) ≤(cid:15)−θ (cid:104) (cid:105) =P θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15) . (9.3) Although the values of the two probabilities are the same, the two events are interpreted differently. The right-hand side of Figure 9.2 illustrates P[θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15)]. The interval [θ−(cid:15),θ+(cid:15)]isfixed.WhatistheprobabilitythattheestimatorΘ(cid:98) lieswithinthisdeterministic interval? To find this probability, we need to know the true parameter θ, which is not available. By contrast, the other probability P[Θ(cid:98) −(cid:15) ≤ θ ≤ Θ(cid:98) +(cid:15)] does not require any knowledge about the true parameter θ. What is the probability that the true parameter is included inside the random interval? If the probability is high, we say that there is a good chance that our confidence interval will contain the true parameter. This is observed in the left-hand side of Figure 9.2. In practice we often set P[Θ(cid:98) −(cid:15) ≤ θ ≤ Θ(cid:98) +(cid:15)] to be greater than a certain confidence level, say 95%, and then we determine (cid:15). Once we have determined (cid:15), we can claim that 548
9.1. CONFIDENCE INTERVAL with 95% probability the interval [Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15)] will include the unknown parameter θ. We do not need to know θ at any point in this process. To make this more general, we define 1−α as the confidence level for some parame- ter α. For example, if we would like to have a 95% confidence level, we set α=0.05. Then the probability inequality (cid:104) (cid:105) P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) ≥1−α (9.4) tells us that there is at least a 95% chance that the random interval I =[Θ(cid:98)−(cid:15), Θ(cid:98)+(cid:15)] will include the true parameter θ. In this case we say that I is a “95% confidence interval”. What is a 95% confidence interval? • It is a random interval [Θ(cid:98) −(cid:15),Θ(cid:98) +(cid:15)] such that there is 95% probability for it to include the true parameter θ. • It is not the deterministic interval [θ−(cid:15),θ+(cid:15)], because we never know θ. Example 9.2. After analyzing the life expectancy of people in the United States, it wasconcludedthatthe95%confidenceintervalis(77.8,79.1)yearsold.Isthefollowing claim valid? About 95% of the people in the United States have a life expectancy between 77.8 years old and 79.1 years old. Solution. No. The confidence interval tells us that with 95% probability the random interval (77.8,79.1) will include the true average. We emphasize that (77.8,79.1) is random because it is constructed from a small set of data points. If we survey another set of people we will have another interval. Since we do not know the true average, we do not know the percentage of people whoselifeexpectancyisbetween77.8yearsoldand79.1yearsold.Itcouldbethatthe true average is 80 years old, which is out of the range. It could also be that the true average is 77.9 years old, which is within the range, but only 10% of the population may have life expectancy in (77.8,79.1). Example 9.3. After studying the SAT scores of 1000 high school students, it was concluded that the 95% confidence interval is (1134, 1250) points. Is the following claim valid? There is a 95% probability that the average SAT score in the population is in the range 1134 and 1250. Solution. Yes, but it can be made clearer. The average SAT score in the population remains unknown. It is a constant and it is deterministic, so there is no probability associated with it. A better way to say this is: “There is 95% probability that the random interval 1134 and 1250 will include the average SAT score.” We emphasize that the 95% probability is about the random interval, not the unknown parameter. 549
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 9.1.3 Constructing a confidence interval Let’s consider an example. Suppose that we have a set of i.i.d. observations X ,...,X 1 N that are Gaussians with an unknown mean θ and a known variance σ2. We consider the maximum-likelihood estimator, which is the sample average: N 1 (cid:88) Θ(cid:98) = N X n. n=1 Our goal is to construct a confidence interval. Figure 9.3: Conceptual illustration of how to construct a confidence interval. Starting with the pop- ulation, we draw random subsets. Each random subset gives us an estimator, and correspondingly an interval. Before we consider the equations, let’s look at a graph illustrating what we want to achieve. Figure 9.3 shows a population distribution, which is a Gaussian in this example. WedrawN samplesfromtheGaussiantoconstructarandomsubset.Basedonthisrandom subsetweconstructtheestimatorΘ(cid:98).Sincethisestimatorisbasedontheparticularrandom subset we have, we can follow the same approach by drawing another random subset. To differentiate the estimators constructed by the different random subsets, let’s call the esti- matorsΘ(cid:98)(1) andΘ(cid:98)(2),respectively.Foreachestimatorweconstructaninterval[Θ(cid:98)−(cid:15), Θ(cid:98)+(cid:15)] to obtain two different intervals: I1 =[Θ(cid:98)(1)−(cid:15), Θ(cid:98)(1)+(cid:15)] and I2 =[Θ(cid:98)(2)−(cid:15), Θ(cid:98)(2)+(cid:15)]. If we can determine (cid:15), we have found the confidence interval. We can determine the confidence interval by observing the histogram of Θ(cid:98), which in our case is the histogram of the sample average, since the histogram of Θ(cid:98) is well-defined, 550
9.1. CONFIDENCE INTERVAL especially if we are looking at the sample average. The histogram of the sample average is a Gaussianbecausetheaverage ofN i.i.d.GaussianrandomvariablesisGaussian.Therefore, the width of this Gaussian is determined by the answer to this question: For what (cid:15) can we cover 95% of the histogram of Θ(cid:98)? To find the answer, we set up the following probability inequality:   |Θ(cid:98) −E[Θ(cid:98)]| P  (cid:113) ≤(cid:15)≥1−α. Var[Θ(cid:98)] This probability says that we want to find an (cid:15) such that the majority of Θ(cid:98) is living close to its mean. The level 1−α is our confidence level, which is typically 95%. Equivalently, we let α=0.05. In the above equation, we can define the quotient as def Θ(cid:98) −E[Θ(cid:98)] Z(cid:98) = (cid:113) . Var[Θ(cid:98)] We know that Z(cid:98) is a zero-mean unit-variance Gaussian because it is the standardized vari- able.[Note:NotallnormalizedvariablesareGaussian,butifΘ(cid:98) isaGaussianthenormalized variable will remain a Gaussian.] Thus, the probability inequality we are looking at is (cid:104) (cid:105) P |Z(cid:98)|≤(cid:15) ≥ 1−α. (cid:124) (cid:123)(cid:122) (cid:125) twotailsofastandardGaussian The PDF of Z(cid:98) is shown in Figure 9.4. As you can see, to achieve 95% confidence we need to pick an appropriate (cid:15) such that the shaded area is less than 5%. 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 (cid:113) Figure 9.4: PDF of the random variable Z(cid:98) = (Θ(cid:98) −E[Θ(cid:98)])/ Var[Θ(cid:98)]. The shaded area denotes the α=0.05 confidence level. Since P[Z(cid:98)≤(cid:15)] is the CDF of a Gaussian, it follows that P[|Z(cid:98)|≤(cid:15)]=P[−(cid:15)≤Z(cid:98)≤(cid:15)] =P[Z(cid:98)≤(cid:15)]−P[Z(cid:98)≤−(cid:15)] =Φ((cid:15))−Φ(−(cid:15)). 551
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Using the symmetry of the Gaussian, it follows that Φ(−(cid:15))=1−Φ((cid:15)) and hence P[|Z(cid:98)|≤(cid:15)]=2Φ((cid:15))−1. Equating this result with the probability inequality P[|Z(cid:98)|≤(cid:15)]≥1−α, we have that (cid:16) α(cid:17) (cid:15)≥Φ−1 1− . 2 The remainder of this problem is solvable on a computer. On MATLAB, we can call icdf to compute the inverse CDF of a standard Gaussian. On Python, the command is stats.norm.ppf. The commands are as shown below. % MATLAB code to compute the width of the confidence interval alpha = 0.05; mu = 0; sigma = 1; % Standard Gaussian epsilon = icdf(’norm’,1-alpha/2,mu,sigma) # Python code to compute the width of the confidence interval import scipy.stats as stats alph = 0.05; mu = 0; sigma = 1; # Standard Gaussian epsilon = stats.norm.ppf(1-alph/2, mu, sigma) print(epsilon) If everything is done properly, we see that for a 95% confidence level (α =0.05) the corre- sponding (cid:15) is (cid:15)=1.96. Afterdetermining(cid:15),itremainstodetermineE[Θ(cid:98)]andVar[Θ(cid:98)]inordertocompletethe probability inequality. To this end, we note that (cid:34) N (cid:35) 1 (cid:88) E[Θ(cid:98)]=E N X n =θ, n=1 (cid:34) 1 (cid:88)N (cid:35) σ2 Var[Θ(cid:98)]=Var N X n = N, n=1 if we assume that the population distribution is Gaussian(θ,σ2), where θ is unknown but σ is known. Substituting these into the probability inequality, we have that   |Θ(cid:98) −E[Θ(cid:98)]| (cid:20) σ σ (cid:21) P  (cid:113) ≤(cid:15)=P Θ(cid:98) −(cid:15)√ ≤θ ≤Θ(cid:98) +(cid:15)√ N N Var[Θ(cid:98)] (cid:20) (cid:21) σ σ =P Θ(cid:98) −1.96√ ≤θ ≤Θ(cid:98) +1.96√ , N N where we let (cid:15)=1.96 for a 95% confidence level. Therefore, the 95% confidence interval is (cid:20) (cid:21) σ σ Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ . (9.5) N N Asyoucansee,wedonotneedtoknowthevalueofθ atanypointofthederivationbecause the confidence interval in Equation (9.5) does not involve θ. This is an important difference with the other probability P[θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15)], which requires θ. 552
9.1. CONFIDENCE INTERVAL How to construct a confidence interval • Compute the estimator Θ(cid:98). • Determine the width of the confidence interval (cid:15) by inspecting the confidence level 1−α. If Θ(cid:98) is Gaussian, then (cid:15)=Φ−1(1− α). 2 • If Θ(cid:98) is not a Gaussian, replace the Gaussian CDF by the CDF of Θ(cid:98). • The confidence interval is [Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15)]. 9.1.4 Properties of the confidence interval Some important properties of the confidence interval are listed below. • Probability of Θ(cid:98) is the same as probability of Z(cid:98). First, the two random variables Θ(cid:98) and Z(cid:98) have a one-to-one correspondence. We proved the following in Chapter 6: If Θ(cid:98) ∼Gaussian(θ,σ2), then N Z(cid:98)d =ef Θ(cid:98) √−θ ∼Gaussian(0,1). (9.6) σ/ N For example, if Θ(cid:98) ∼ Gaussian(θ,σ2) with N = 1, θ = 1 and σ = 2, then a 95% N confidence level is 0.95≈P[−1.96≤Z(cid:98)≤1.96], (Z(cid:98) is within 1.96 std from Z(cid:98)’s mean) Θ(cid:98) −θ =P[−1.96≤ √ ≤1.96] σ/ N (cid:20) (cid:21) σ σ =P θ−1.96√ ≤Θ(cid:98) ≤θ+1.96√ N N =P[−2.92≤Θ(cid:98) ≤4.92]. (Θ(cid:98) is within 1.96 std from Θ(cid:98)’s mean) NotethatwhiletherangeforZ(cid:98) isdifferentfromtherangeforΘ(cid:98),theybothreturnthe sameprobability.TheonlydifferenceisthatΘ(cid:98) isconstructedbeforethenormalization and Z(cid:98) is constructed after the normalization. • Standard error.InthisestimationproblemweknowthatΘ(cid:98) isthesampleaverage.We assume that the mean θ is unknown but the variance Var[Θ(cid:98)] is known. The standard deviation of Θ(cid:98) is called the standard error: (cid:113) σ se= Var[Θ(cid:98)]= √ . (9.7) N • Critical value. The value 1.96 in our example is often known as the critical value. It is defined as (cid:16) α(cid:17) z =Φ−1 1− . (9.8) α 2 553
CHAPTER 9. CONFIDENCE AND HYPOTHESIS The z value gives us a multiplier applied to the standard error that will result in a α valuewithintheconfidenceinterval.Thisisbecause,bythedefinitionoftheconfidence interval, the interval is (cid:20) σ σ (cid:21) (cid:104) (cid:105) Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ = Θ(cid:98) −z αse, Θ(cid:98) +z αse N N • Margin of error. The margin of error is defined as σ margin of error=z √ . (9.9) α N The margin of error is also the width of the confidence interval. As the name implies, the margin of error tells us how much error the confidence interval includes when predicting the population parameter. Practice Exercise 9.1. Suppose that the number of photos a Facebook user uploads per day is a random variable with σ = 2. In a set of 341 users, the sample average is 2.9. Find the 90% confidence interval of the population mean. Solution. We set α=0.1. The z -value is α (cid:16) α(cid:17) z =Φ−1 1− =1.6449. α 2 The 90% confidence interval is then (cid:20) (cid:21) 2 2 Θ(cid:98) −1.64√ , Θ(cid:98) +1.64√ =[2.72,3.08]. 341 341 Therefore,with90%probability,theinterval[2.72,3.08]includesthepopulationmean. Example 9.4. Professional cyber-athletes have a standard deviation of σ = 73.4 actions per minute. If we want to estimate the average actions per minute of the population, how many samples are needed to obtain a margin of error < 20 at 90% confidence? Solution. With a 90% confidence level, the z -value is α (cid:16) α(cid:17) z =Φ−1 1− =Φ−1(0.95)=1.645. α 2 The margin of error is 20. So we have z α√σ =20. Moving around the terms gives us N (cid:16) σ (cid:17)2 N ≥ z =36.45. α20 Therefore, we need at least N = 37 samples to ensure a margin of error of < 20 at a 90% confidence level. 554
9.1. CONFIDENCE INTERVAL Figure 9.5: Relationships between the standard error se, the z value, and the margin of error. The α confidence level α is the area under the curve for the tails of each PDF. Theconceptsofstandarderrorse,thez value,andthemarginoferroraresummarized α in Figure 9.5. The left-hand side is the PDF of Z(cid:98). It is the normalized random variable, whichisalsothestandardGaussian.Theright-handsideisthePDFofΘ(cid:98),theunnormalized random variable. The z α value is located in the Z(cid:98)-space. It defines the range of Z(cid:98) in the PDF within which we are confident about the true parameter. The corresponding value in the Θ(cid:98)-space is the margin of error. This is found by multiplying z α with the standard deviation of Θ(cid:98), known as the standard error. Correspondingly, in the Z(cid:98)-space the standard deviation is the unity. Two further points about the confidence interval should be mentioned: • Number of Samples N.TheconfidenceintervalisafunctionofN.Asweincreasethe numberofsamples,thedistributionoftheestimatorΘ(cid:98) becomesnarrower.Specifically, if Θ(cid:98) follows a Gaussian distribution (cid:18) σ2(cid:19) Θ(cid:98) ∼Gaussian θ, , N p then Θ(cid:98) →θ as N →∞. Figure 9.6 illustrates a few examples of Θ(cid:98) as N grows. In the limit when N →∞, we observe that the interval becomes (cid:20) σ σ (cid:21) (cid:104) (cid:105) Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ −→ Θ(cid:98), Θ(cid:98) =Θ(cid:98). N N (cid:104) (cid:105) In this case, the statement θ ∈ Θ(cid:98) −1.96√σ , Θ(cid:98) +1.96√σ becomes θ = Θ(cid:98). That N N means the estimator Θ(cid:98) returns the correct true parameter θ. Of course, it is possible that E[Θ(cid:98)] (cid:54)= θ, i.e., the estimator is biased. In that case, having more samples will approach another estimate that is not θ. • Distribution of Z(cid:98). When defining the confidence interval we constructed an interme- diate variable Θ(cid:98) −θ Z(cid:98)= √ . σ/ N Since X ’s are i.i.d. Gaussian, it follows that Z is also Gaussian. This gives us a way n to calculate the probability using the standard Gaussian table. What happens when X ’s are not Gaussian? The good news is that even if X ’s are not Gaussian, for n n 555
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 4 3.5 N = 10 N = 25 3 N = 100 2.5 2 1.5 1 0.5 0 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 Figure 9.6: The PDF of Θ(cid:98) as the number of samples N grows. Here, we assume that X n are i.i.d. Gaussian random variables with mean θ=0 and variance σ2 =1. sufficiently large N, the random variable Θ(cid:98) is more or less Gaussian, because of the Central Limit Theorem. Therefore, even if X ’s are not Gaussian we can still use the n Gaussian probability table to construct α and (cid:15). 9.1.5 Student’s t-distribution In the discussions above, we estimate the population mean θ using the estimator Θ(cid:98). The assumption was that the variance σ2 was known a priori and hence is fixed. In practice, however, there are many situations where σ2 is not known. Thus we not only need to use the mean estimator Θ(cid:98) but also the variance estimator S(cid:98), which can be defined as N S(cid:98)2 d =ef N1 −1 (cid:88) (X n−Θ(cid:98))2, n=1 where Θ(cid:98) is the estimator of the mean. What is the confidence interval for Θ(cid:98)? For a confidence interval to be valid, we expect it to take the form of (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −z α√ , Θ(cid:98) +z α√ , N N which is essentially the confidence interval we have just derived but with σ replaced by S(cid:98). However, there is a problem with this. When we derive the confidence interval assuming a known σ, the z value is determined by checking the standard Gaussian α Θ(cid:98) −θ Z(cid:98)= √ , σ/ N which gives us z α = Φ−1(1−α/2). The whole derivation is based on the fact that Z(cid:98) is a standard Gaussian. Now that we have replaced σ by S(cid:98), the new random variable def Θ(cid:98) −θ T = √ (9.10) S(cid:98)/ N 556
9.1. CONFIDENCE INTERVAL is not a standard Gaussian. It turns out that the distribution of T is Student’s t-distribution with N −1 degrees of freedom. The PDF of Student’s t-distribution is given as follows. Definition 9.1. If X is a random variable following Student’s t-distribution of ν degrees of freedom, then the PDF of X is Γ(cid:0)ν+1(cid:1) (cid:18) x2(cid:19)−ν+ 21 f (x)= √ 2 1+ . (9.11) X νπΓ(cid:0)ν(cid:1) ν 2 We may compare Student’s t-distribution with the Gaussian distribution. Figure 9.7 shows the standard Gaussian and several t distributions with ν =N −1 degrees of freedom. Note that Student’s t-distribution has a similar shape to the Gaussian but it has a heavier tail. 0.4 0.35 Gaussian(0,1) t-dist, N = 11 0.3 t-dist, N = 3 0.25 t-dist, N = 2 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 9.7: The PDF of Student’s t-distribution with ν =N −1 degrees of freedom. Since T = Θ(cid:98)√−θ is a t-random variable, to determine the z α value we can follow the S(cid:98)/ N same procedure by considering the CDF of T. Let the CDF of the Student’s t-distribution with ν degrees of freedom be Ψ (z)=CDF of X at z. ν If we want P[|T|≤z ]=1−α, it follows that α (cid:16) α(cid:17) z =Ψ−1 1− . (9.12) α ν 2 Therefore, the new confidence interval, assuming an unknown S(cid:98), is (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −z α√ , Θ(cid:98) +z α√ , N N with z defined in Equation (9.12), using ν =N −1. α 557
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Practice Exercise 9.2.AsurveyaskedN =14peoplefortheirratingofamovie.As- sumethatthemeanestimatorisΘ(cid:98) andthevarianceestimatorisS(cid:98).Findtheconfidence interval. Solution. If we use Student’s t-distribution, it follows that (cid:16) α(cid:17) z =Ψ−1 1− =2.16, α 13 2 where the degrees of freedom are ν =14−1=13. Thus the confidence interval is (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −2.16√ , Θ(cid:98) +2.16√ . N N The MATLAB and Python codes to report the z value of a Student’s t-distribution α are shown below. They are both called through the inverse CDF function. In MATLAB it is icdf, and in Python it is stats.t.ppf. % MATLAB code to compute the z_alpha value of t distribution alpha = 0.05; nu = 13; z = icdf(norm,1-alpha/2,nu) # Python code to compute the z_alpha value of t distribution import scipy.stats as stats alph = 0.05 nu = 13 z = stats.t.ppf(1-alph/2, nu) print(z) Example 9.5. A class of 10 students took a midterm exam. Their scores are given in the following table. Student 1 2 3 4 5 6 7 8 9 10 Score 72 69 75 58 67 70 60 71 59 65 Find the 95% confidence interval. Solution.ThemeanandstandarddeviationofthedatasetsarerespectivelyΘ(cid:98) =66.6 and S(cid:98)=5.61. The critical z α value is determined by Student’s t-distribution: (cid:16) α(cid:17) z =Ψ−1 1− =2.26. α 9 2 558
9.1. CONFIDENCE INTERVAL The confidence interval is (cid:34) (cid:35) S(cid:98) S(cid:98) Θ(cid:98) −z α√ , Θ(cid:98) +z α√ =[62.59,70.61]. N N Therefore, with 95% probability, the interval [62.59,70.61] will include the true popu- lation mean. Remark 1. Make sure you understand the meaning of “population mean” in this example.Sincewehavetenstudents,isn’tthepopulationmeanjusttheaverageofthe ten scores? This is incorrect. In statistics, we assume that these ten students are the realizationsofsomeunderlying(unknown)randomvariableX withsomePDFf (x). X The population mean θ is therefore the expectation E[X], where the expectation is taken w.r.t. f X. The sample average Θ(cid:98), which is the average of the ten numbers, is an estimator of the population mean θ. Remark 2. You may be wondering why we are using Student’s t-distribution here when we do not even know the PDF of X. The answer is that it is an approximation. When X is Gaussian, the sample average Θ(cid:98) is a Student’s t-distribution, assuming that the variance is approximated by the sample variance S(cid:98). This result is attributed to the original paper of William Gosset, who developed Student’s t-distribution. Theaboveexamplecanbesolvedcomputationally.AnimplementationthroughPython is given below, and the MATLAB implementation is straightforward. # Python code to generate a confidence interval import numpy as np import scipy.stats as stats x = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65]) N = x.size Theta_hat = np.mean(x) # Sample mean S_hat = np.std(x) # Sample standard deviation nu = x.size-1 # degrees of freedom alpha = 0.05 # confidence level z = stats.t.ppf(1-alph/2, nu) CI_L = Theta_hat-z*S_hat/np.sqrt(N) CI_U = Theta_hat+z*S_hat/np.sqrt(N) print(CI_L, CI_U) What is Student’s t-distribution? • It was developed by William Gosset in 1908. When he published the paper he used the pseudonym Student. • We use Student’s t-distribution to model the estimator Θ(cid:98)’s PDF when the vari- ance σ2 is replaced by the sample variance S(cid:98)2. • Student’s t-distribution has a heavier tail than a Gaussian. 559
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 9.1.6 Comparing Student’s t-distribution and Gaussian We now discuss an important theoretical result regarding the relationship between a Stu- dent’s t-distribution and Gaussian distribution. The main result is that the standard Gaus- sian is a limiting distribution of the t distribution as the degrees of freedom ν →∞. Theorem 9.1. Asν →∞,theStudent’st-distributionapproachesthestandardGaus- sian distribution: νl →im ∞(cid:40) √Γ ν(cid:0) πν Γ+ 2 (cid:0)1 ν(cid:1) (cid:1)(cid:18) 1+ y ν2(cid:19)−ν+ 21(cid:41) = √1 2πe−t 22 . (9.13) 2 The proof of the theorem requires Stirling’s approximation, which is not essential for this book. Feel free to skip it if needed. Proof. There are two results we need to use: (cid:113) • Stirling’s approximation:2 Γ(z)≈ 2π (cid:0)z(cid:1)z . z e • Exponential approximation: (1+ x)−k →e−x, as k →∞. k We have that Γ(cid:0)ν+1(cid:1) (cid:113) ν2 +π 1 (cid:0)ν 2+ e1(cid:1)ν+ 21 √ νπΓ2 (cid:0)ν 2(cid:1) ≈ √ νπ2 (cid:113) 2 νπ (cid:0) 2ν e(cid:1)ν 2 2 √ (cid:114) (cid:18) (cid:19)ν 1 ν 1 ν+1 2 ν+1 = √ √ √ νπ ν+1 e ν ν √ (cid:18) (cid:19)ν 1 ν ν+1 2 = √ √ νπ 2e ν (cid:18) (cid:19)ν 1 1 2 = √ 1+ . 2πe ν Putting a limit of ν →∞, we have that (cid:18) (cid:19)ν lim √1 1+ 1 2 = √1 e1 2 = √1 . ν→∞ 2πe ν 2πe 2π The other limit follows from the fact that lim (cid:18) 1+ t2(cid:19)−ν+ 21 =e−t 22 . ν→∞ ν Combining the two limits proves the theorem. (cid:3) 2K.G.Binmore,Mathematicalanalysis:Astraightforwardapproach.CambridgeUniversityPress,1977. Section17.7.2. 560
9.2. BOOTSTRAPPING End of the proof. Please join us again. This theorem has several implications: • When N is large, S2 → σ2. The Gaussian approximation kicks in, and so Student’s t-distribution is more or less the same as the Gaussian. • Student’s t-distribution is better for small N, usually N ≤ 30. If N ≥ 30, using the Gaussian approximation suffices. • If X is Gaussian, Student’s t-distribution is an excellent model. If X is not Gaussian, Student’s t-distribution will have some issues unless N increases. 9.2 Bootstrapping When estimating the confidence interval, we focus exclusively on the sample average Θ(cid:98) = (1/N)(cid:80)N X . There are, however, many estimators that are not sample averages. For n=1 n example, we might be interested in an estimator that estimates the sample median: Θ(cid:98) = median{X ,...,X }. In such cases, the Gaussian-based analysis or the Student’s t-based 1 N analysis we just derived would not work. Steppingbackalittlefurther,itisimportanttounderstandthehierarchyofestimation. Figure 9.8 illustrates a rough breakdown of the various techniques. On the left-hand side of the tree, we have three point estimation methods: MLE, MAP, and MMSE. They are so-called point estimation methods because they are reporting a point — a single value. Thisstandsincontrasttotheright-handsideofthetree,inwhichwereporttheconfidence interval. Note that point estimates and confidence intervals do not conflict with each other. Thepointestimatesareusedfortheactualengineeringsolutionandtheconfidenceintervals areusedtoreporttheconfidenceaboutthepointestimates.Underthebranchofconfidence intervals we discussed sample average. However, if we want to study an estimator that is not the sample average, we need the technique known as the bootstrapping — a method forestimatingtheconfidenceinterval.Notably,itdoesnotgiveyouabetterpointestimate. As we have frequently emphasized, since Θ(cid:98) is a random variable, it has its own PDF, CDF, mean, variance, etc. The confidence interval introduced in the previous section pro- videsonewaytoquantifytherandomnessofΘ(cid:98).Throughoutthederivationoftheconfidence interval we need to estimate the variance Var(Θ(cid:98)). For simple problems such as the sample average,analyzingVar(Θ(cid:98))isnotdifficult.However,ifΘ(cid:98) isamorecomplicatedstatistic,e.g., the median, analyzing Var(Θ(cid:98)) may not be as straightforward. Bootstrapping is a technique that is suitable for this purpose. Why is it difficult to provide a confidence interval for estimators such as the median? A couple of difficulties arise: • Many estimators do not have a simple expression for the variance. For simple esti- mators such as the sample average Θ(cid:98) =(1/N)(cid:80)N n=1X n, the variance is σ2/N. If the 561
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.8: Hierarchy of estimation. Bootstrapping belongs to the category of confidence interval. It is used to report the confidence intervals for estimators that are not the sample averages. estimator is the median Θ(cid:98) = median{X 1,...,X N}, the variance of Θ(cid:98) will depend on theunderlyingdistributionoftheX ’s.Iftheestimatorissomethingbeyondthesam- n ple median, the variance of Θ(cid:98) can be even more complicated to determine. Therefore, techniques such as Central Limit Theorem do not apply here. • Wetypicallyhaveonlyone setofdatapoints.Wecannotre-collectmorei.i.d.samples to estimate the variance of the estimator. Therefore, our only option is to squeeze the information from the data we have been given. When do we use bootstrapping? • Bootstrapping is a technique to estimate the confidence interval. • We use bootstrapping when the estimator does not have a simple expression for the variance. • Bootstrappingallowsustoestimatethevariancewithoutre-collectingmoredata. • Bootstrapping does not improve your point estimates. 9.2.1 A brute force approach Before we discuss the idea of bootstrapping, we need to elaborate on the difficulty of esti- matingthevarianceusingrepeatedmeasurements.Supposethatwesomehowhaveaccessto the population distribution. Let us denote the CDF of this population distribution by F , X and the PDF by f . By having access to the population distribution we can synthetically X generate as many samples X ’s as we want. This is certainly hypothetical, but let’s assume n that it is possible for now. If we have full access to the population distribution, then we are able to draw K replicate datasets X1,...,XK from F : X 562
9.2. BOOTSTRAPPING X(1) ={X(1),...,X(1)}∼F , 1 N X X(2) ={X(2),...,X(2)}∼F , (9.14) 1 N X . . . X(K) ={X(K),...,X(K)}∼F . 1 N X Each dataset X(K) contains N data points, and by virtue of i.i.d. all the samples have the same underlying distribution F . X For each dataset we construct an estimator Θ(cid:98) = g(·) for some function g(·). The estimator takes the data points of the dataset X and returns a value. Since we have K datasets, correspondingly we will have K estimators: Θ(cid:98)(1) =g(X(1))=g(X(1),...,X(1)), 1 N Θ(cid:98)(2) =g(X(2))=g(X(2),...,X(2)), (9.15) 1 N . . . Θ(cid:98)(K) =g(X(K))=g(X(K),...,X(K)). 1 N Note that these estimators g(·) can be anything. It can be the sample average or it can be the sample median. There is no restriction. SinceweareinterestedinconstructingtheconfidenceintervalforΘ(cid:98),weneedtoanalyze the mean and variance of Θ(cid:98). The true mean and the estimated mean of Θ(cid:98) are E[Θ(cid:98)]=true mean of Θ(cid:98), (9.16) M(Θ(cid:98))=estimated mean based on Θ(cid:98)(1),...,Θ(cid:98)(K) K K d =ef 1 (cid:88) Θ(cid:98)(k) = 1 (cid:88) g(X(k)), (9.17) K K k=1 k=1 respectively. Similarly, the true variance and the estimated variance of Θ(cid:98) are Var[Θ(cid:98)]=true variance of Θ(cid:98), (9.18) V(Θ(cid:98))=estimated variance based on Θ(cid:98)(1),...,Θ(cid:98)(K) K d =ef 1 (cid:88)(cid:16) Θ(cid:98)(k)−M(Θ(cid:98))(cid:17)2 K k=1 K 1 (cid:88)(cid:16) (cid:17)2 = g(X(k))−M(Θ(cid:98)) . (9.19) K k=1 These two equations should be familiar: Since Θ(cid:98) is a random variable, and {Θ(cid:98)(k)} are i.i.d. copies of Θ(cid:98), we can compute the average of Θ(cid:98)(1),...,Θ(cid:98)(K) and the corresponding variance. AsthenumberofrepeatedtrialsK approaches∞,theestimatedvarianceV(Θ(cid:98))willconverge to Var(Θ(cid:98)) according to the law of large numbers. Wecansummarizetheprocedurewehavejustoutlined.Toproduceanestimateofthe variance, we run the algorithm below. 563
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Algorithm 1: Brute force method to generate an estimated variance • Assume: We have access to F . X • Step 1: Generate datasets X(1),...,X(K) from F . X • Step 2: Compute M(Θ(cid:98)) and V(Θ(cid:98)) based on the samples. • Output: The estimated variance is V(Θ(cid:98)). The problem, however, is that we only have one dataset X(1). We do not have access to X(2),...,X(K), and we do not have access to F . Therefore, we are not able to approxi- X matethevarianceusingtheabovebruteforcesimulation.Bootstrappingisacomputational technique to mimic the above simulation process by using the available data in X(1). 9.2.2 Bootstrapping The idea of bootstrapping is illustrated in Figure 9.9. Imagine that we have a population CDFF andPDFf .Thedatasetwehaveinhand,X,isacollectionoftherandomrealiza- X X tionsoftherandomvariableX.ThisdatasetX containsN datapointsX ={X ,...,X }. 1 N Figure 9.9: A conceptual illustration of bootstrapping. Given the observed dataset X, we synthetically construct K bootstrapped datasets (colored in yellow) by sampling with replacement from X. We then compute the estimators, e.g., computing the median, for every bootstrapped dataset. Finally, we construct the estimator’s histogram (in blue) to compute the bootstrapped mean and variance. In bootstrapping, we synthesize K bootstrapped datasets Y(1),...,Y(K), where each bootstrappeddatasetY(k) consistsofN samplesredrawnfromX.Essentially,wedrawwith replacement N samples from the observed dataset X: Y(1) ={Y(1),...,Y(1)}=N random samples from X, 1 N . . . Y(K) ={Y(K),...,Y(K)}=N random samples from X. 1 N 564
9.2. BOOTSTRAPPING Afterward,weconstructourestimatorΘ(cid:98) accordingtoourdesiredfunctiong(·).Forexample, if g(·)= median, we have Θ(cid:98)(1) =g(Y(1))=median(Y(1)), boot . . . Θ(cid:98)(K) =g(Y(K))=median(Y(K)). boot Then, we define the bootstrapped mean and the bootstrapped variance as K M boot(Θ(cid:98))= K1 (cid:88) Θ(cid:98)( bk o) ot, (9.20) k=1 K V boot(Θ(cid:98))= K1 (cid:88)(cid:16) Θ(cid:98)( bk o) ot−M boot(Θ(cid:98))(cid:17)2 . (9.21) k=1 The procedure we have just outlined can be summarized as follows. Algorithm 2: Bootstrapping to generate an estimated variance • Assume: We do NOT have access to F , but we have one dataset X. X • Step 1: Generate datasets Y(1),...,Y(K) from X, by sampling with replacement from X. • Step 2: Compute M boot(Θ(cid:98)) and V boot(Θ(cid:98)) based on the samples. • Output: The bootstrapped variance is V boot(Θ(cid:98)). The only difference between this algorithm and the previous one is that we are not synthe- sizing data from the population but rather from the observed dataset X. What makes bootstrapping work? The basic principle of bootstrapping is based on three approximations: (a) Var F(Θ(cid:98)) ≈ V full(Θ(cid:98)) )b( ≈ (c) Var F(cid:98)(Θ(cid:98)) ≈ V boot(Θ(cid:98)) In this set of equations, the ultimate quantity we want to know is Var F(Θ(cid:98)), which is the variance of Θ(cid:98) under F. (By “under F” we mean that the variance was found by integrating with respect to the distribution F .) However, since we do not have access to F, we have X to approximate Var F(Θ(cid:98)) by V full(Θ(cid:98)). V full(Θ(cid:98)) is the sample variance computed from the K hypothetical datasets X(1),...,X(K). We call it “full” because we can generate as many hypothetical datasets as we want. It is marked as the approximation (a) above. In the bootstrapping world, we approximate the underlying distribution F by some other distribution F(cid:98). For example, if F is the CDF of a Gaussian distribution, we can chooseF(cid:98) tobethefinite-samplestaircasefunctionapproximatingF.Inourcase,weusethe observeddatasetX toserveasaproxyF(cid:98)toF.Thisisthesecondapproximation,markedby 565
CHAPTER 9. CONFIDENCE AND HYPOTHESIS (b). Normally, if you have a reasonably large X, it is safe to assume that this finite-sample dataset X has a CDF F(cid:98) that is close to the true CDF F. The third approximation is to find a numerical estimate Var (Θ(cid:98)) via the simulation F(cid:98) procedure we have just outlined. This is essentially the same line of argument for (a) but now applied to the bootstrapping world. We mark this approximation by (c). Its goal is to approximate Var F(cid:98)(Θ(cid:98)) via V boot(Θ(cid:98)). The three approximations have their respective influence on the accuracy of the boot- strapped variance: How does bootstrapping work? • It is based on three approximations: • (a): A hypothetical approximation. The best we can do is that we have access to F. It is practically impossible to achieve, but it gives us intuition. • (b): Approximate F by F(cid:98), where F(cid:98) is the empirical CDF of the observed data. This is usually the source of error. The approximation error reduces when you use more samples to approximate F. • (c): Approximate the theoretical bootstrapped variance by a finite approxima- tion.Thisapproximationerrorisusuallysmallbecauseyoucangenerateasmany bootstrapped datasets as you want. One “mysterious” property of bootstrapping is the sampling with replacement scheme used to synthesize the bootstrapped samples. The typical questions are: • (1) Why does sampling from the observed dataset X lead to meaningful boot- strapped datasets Y(1),...,Y(K)? To answer this question we consider the following toy example. Suppose we have a dataset X containing N = 20 samples, as shown below. X = [0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2 2] ThisdatasetisgeneratedfromarandomvariableX withaPDFf(cid:98)havingthreestates: 0 (30%), 1 (20%), 2 (50%). As we draw samples from X, the percentage of the states will determine the likelihood of one state being drawn. For example, if we randomly pick a sample Y from X, we have a 30% chance of having Y to be 0, 20% chance n n of having it to be 1, and 50% chance of having it to be 2. Therefore, the PDF of Y n (the randomly drawn sample from X) will be 0 (30%), 1 (20%), 2 (50%), the same as the original PDF. If you think about this problem more deeply, by “sampling with replacement” we essentially assign each X with an equal probability of 1/N. If one n of the states is more popular, the individual probabilities will add to form a higher probability mass. • (2) Why can’t we do sampling without replacement, aka permutation? We need to understandthatsamplingwithoutreplacementisthesameaspermutingthedatainX. BypermutingthedatainX,thesimpleprobabilityassignmentssuchasP[X =0]= 6 , 20 P[X =1]= 4 and P[X =2]= 10 will be destroyed. Moreover, permuting the data 20 20 does not change the mean and variance of the data because we are only shuffling the order. As far as constructing the confidence interval is concerned, shuffling the order is not useful. 566
9.2. BOOTSTRAPPING On computers it is easy to generate the bootstrapped dataset, along with their mean and variance. In MATLAB the key step is to call a for loop. Inside the for loop, we draw N randomindicesrandifrom1toN andpickthesamples.TheestimatorThetahatisthen constructed by calling your target estimator function g(·). In this example the estimator is the median. After the for loop, we compute the mean and variance of Θ(cid:98). These are the bootstrapped mean and variance, respectively. % MATLAB code to estimate a bootstrapped variance X = [72, 69, 75, 58, 67, 70, 60, 71, 59, 65]; N = size(X,2); K = 1000; Thetahat = zeros(1,K); for i=1:K % repeat K times idx = randi(N,[1, N]); % sampling w/ replacement Y = X(idx); Thetahat(i) = median(Y); % estimator end M = mean(Thetahat) % bootstrapped mean V = var(Thetahat) % bootstrapped variance The Python commands are similar. We call np.random.randint to generate random integers and we pick samples according to Y = X[idx]. After generating the bootstrapped dataset, we compute the bootstrap estimators Thetahat. # Python code to estimate a bootstrapped variance import numpy as np X = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65]) N = X.size K = 1000 Thetahat = np.zeros(K) for i in range(K): idx = np.random.randint(N, size=N) Y = X[idx] Thetahat[i] = np.median(Y) M = np.mean(Thetahat) V = np.var(Thetahat) After we have constructed the bootstrapped variance, we can define the bootstrapped standard error as (cid:113) s (cid:98)e boot = V boot(Θ(cid:98)). (9.22) Accordingly we define the bootstrapped confidence interval as (cid:2) (cid:3) I = Θ(cid:98) −z αs (cid:98)e boot, Θ(cid:98) +z αs (cid:98)e boot , (9.23) where z is the critical value of the Gaussian. α The validity of the confidence intervals constructed by bootstrapping is subject to the validity of z α. If Θ(cid:98) is roughly a Gaussian, the bootstrapped confidence interval will be 567
CHAPTER 9. CONFIDENCE AND HYPOTHESIS reasonablygood.IfΘ(cid:98) isnotGaussian,thereareadvancedmethodstoreplacez α withbetter estimates. This topic is beyond the scope of this book; we refer interested readers to Larry Wasserman, All of Statistics, Springer 2003, Chapter 8. 9.3 Hypothesis Testing Imagine that you are a vaccine company developing COVID-19 vaccines. You gave the vaccine to 934 patients, and 928 patients have developed antigens. How confident can you be that your vaccine is effective? Questions like this are becoming more common nowadays insituationsinwhichweneedtomakestatisticallyinformedchoicesbetweenYESandNO. The subject of this section is hypothesis testing — a principled statistical procedure used to evaluate statements that should be accepted or rejected. 9.3.1 What is a hypothesis? A hypothesis is a statement that requires testing by observation to determine whether it is true or false. A few examples: • The coin is unbiased. • Students entering the graduate program have GPA ≥3. • More people like orange juice than lemonade. • Algorithm A performs better than Algorithm B. As you can see from these examples, a hypothesis is something we can test based on the data.Therefore,being“correct”or“wrong”dependsonthestatisticswehaveandthecutoff threshold. Accepting or rejecting a hypothesis does not mean that the statement is correct or wrong, since the truth is unknown. If we accept a hypothesis, we have made a better decisionsolelybasedonthestatisticalevidence.Itispossiblethattomorrowwhenyouhave collected more data we may reject a previously accepted hypothesis. Theprocedurefortestingwhetherahypothesisshouldbeacceptedorrejectedisknown as hypothesis testing. In hypothesis testing, we often have two opposite hypotheses: • H : Null hypothesis. It is the “status quo”, or the current status. 0 • H : Alternative hypothesis. It is the alternative to the null hypothesis. 1 To better understand hypothesis testing, consider a courthouse. By default, any person being prosecuted is assumed to be innocent. The police need to show sufficient evidence in order to prove the person guilty. The null hypothesis is the default assumption. Hypothesis testing asks whether we have strong enough evidence to reject the null hypothesis. If our evidence is not strong enough, we must assume that the null hypothesis is possibly true. Example 9.6. Suggest a null hypothesis and an alternative hypothesis regarding whether a coin is unbiased. Solution: Let θ be the probability of getting a head. 568
9.3. HYPOTHESIS TESTING • H : θ =0.5, and H : θ >0.5. This is a one-sided alternative. 0 1 • H : θ =0.5, and H : θ <0.5. This is another one-sided alternative. 0 1 • H : θ =0.5, and H : θ (cid:54)=0.5. This is a two-sided alternative. 0 1 PracticeExercise9.3.Suggestanullandanalternativehypothesisregardingwhether more than 62% of people in the United States use Microsoft Windows. Solution:LetθbetheproportionofpeopleusingMicrosoftWindowsinUnitedStates. • H : θ ≥0.62, and H : θ <0.62. This is a one-sided alternative. 0 1 PracticeExercise9.4.Suggestanullandanalternativehypothesisregardingwhether self-checkout at Walmart is faster than using a cashier. Solution: Let θ be the proportion of people that check out faster with self-checkout.. • H : θ ≥0.5, and H : θ <0.5. This is a one-sided alternative. 0 1 9.3.2 Critical-value test In hypothesis testing, there are two major approaches: the critical-value test, and the p-valuetest.Thetwotestsaremoreorlessequivalent.Ifyourejectthenullhypothesisusing the critical-value test, you will reject the hypothesis using the p-value. In this subsection, we will discuss the critical-value test. Let us consider a toy problem: Suppose that we have a 4-sided die and our goal is to test whether the die is unbiased. To do so, we define the null and the alternative hypotheses as • H : θ =0.25, which is our default belief. 0 • H : θ >0.25, which is a one-sided alternative. 1 There is no particular reason for considering the one-sided alternative other than the fact thatthecalculationisslightlyeasier.Youarewelcometoconsiderthetwo-sidedalternative. Wemustobtaindatapriortoconductinganyhypothesistesting.Let’sassumethatwe have thrown the die N =1000 times. We find that “3” appears 290 times (we could just as wellhavechosen1,2,or4).WeletX ,...,X betheN =1000binaryrandomvariables 1 1000 representingwhetherwehaveobtaineda“3”ornot.Ifthetrueprobabilityisθ =0.25,then we will have P[X =3]=θ =0.25 and P[X (cid:54)=3]=1−θ =0.75. We know that we cannot n n access the true probability, so we can only construct an estimator of the probability: N 1 (cid:88) Θ(cid:98) = N X n. n=1 In this experiment, we can show that Θ(cid:98) =290/1000=0.29. To make our problem slightly easier, we pretend that we know the variance Var[X ]. n In practice, we certainly do not know Var[X ], and so we need to estimate the variance. If n 569
CHAPTER 9. CONFIDENCE AND HYPOTHESIS we knew the variance, it should be Var[X ] = θ(1−θ) = 0.25(1−0.25) = 0.1875, because n X is a Bernoulli random variable with a mean θ. n The question asked by hypothesis testing is: How far is “Θ(cid:98) = 0.29” from “θ = 0.25”? If the statistic generated by our data, Θ(cid:98) = 0.29, is “far” from the hypothesized θ = 0.25, then we need to reject H because H says that θ = 0.25. However, if there is no strong 0 0 evidence that θ > 0.25, we will need to assume that H may possibly be true. So the key 0 question is what is meant by “far”. Formanyproblemslikethisone,itispossibletoanalyzethePDFofΘ(cid:98).SinceΘ(cid:98) isthe sample average of a sequence of Bernoulli random variables, it follows that Θ(cid:98) is a binomial (withascalingconstant1/N).IfN islargeenough,e.g.,N ≥30,theCentralLimitTheorem tells us that Θ(cid:98) is also very close to a Gaussian. Therefore, we can more or less claim that (cid:18) σ2(cid:19) Θ(cid:98) ∼Gaussian θ, . N With a simple translation and scaling, we can normalize Θ(cid:98) to obtain Z(cid:98): Θ(cid:98) −θ Z(cid:98)= √ ∼Gaussian(0,1). σ/ N Figure 9.10 illustrates the range of values for this problem. There are two axes: the Θ(cid:98)- axis (which is the estimator) and the Z(cid:98)-axis (which is the normalized variable). The values corresponding to each axis are shown in the figure. For example. Θ(cid:98) = 0.29 is equivalent to Z(cid:98) = 2.92, and Θ(cid:98) = 0.25 is equivalent to Z(cid:98) = 0, etc. Therefore, when we ask how far “Θ(cid:98) = 0.29” is from “θ = 0.25”, we can map this question from the Θ(cid:98)-axis to the Z(cid:98)-axis, and ask the relative position of Z(cid:98) from the origin. Figure 9.10: The mapping between Θ(cid:98) and Z(cid:98). To decide whether we want to reject or keep H 0, the critical-value approach compares Z(cid:98) relative to the critical value z α. Onacomputer,obtainingthesevaluesisquitestraightforward.UsingMATLAB,find- ing Z(cid:98) can be done by calling the following commands. The Python code is analogous. % MATLAB command to estimate the Z_hat value. Theta_hat = 0.29; % Your estimate 570
9.3. HYPOTHESIS TESTING theta = 0.25; % Your hypothesis sigma = sqrt(theta*(1-theta)); % Known standard deviation N = 1000; % Number of samples Z_hat = (Theta_hat - theta)/(sigma/sqrt(N)); # Python command to estimate the Z_hat value import numpy as np Theta_hat = 0.29 # Your estimate theta = 0.25 # Your hypothesis N = 1000 # Number of samples sigma = np.sqrt(theta*(1-theta)) # Known standard deviation Z_hat = (Theta_hat - theta)/(sigma / np.sqrt(N)) print(Z_hat) One essential element of hypothesis testing is the cutoff threshold, which is defined through the critical level α. It is the area under the curve of the PDF of Z(cid:98). Typically, α is chosen to be a small value, such as α = 0.05 (corresponding to a 5% margin). The corresponding cutoff is known as the critical value. It is defined as z =cutoff location where area under the curve is α. α If Z(cid:98) is Gaussian(0,1) and if we are looking at the right-hand tail, it follows that z =Φ−1(1−α). (9.24) α In our example, we find that z =1.65, which is marked in Figure 9.10. 0.05 On computers, determining the critical value z is straightforward. In MATLAB the α command is icdf, and in Python the command is stats.norm.ppf. % MATLAB code to compute the critical value alpha = 0.05; z_alpha = icdf(’norm’, 1-alpha, 0, 1); # Python code to compute the critical value import scipy.stats as stats alpha = 0.05 z_alpha = stats.norm.ppf(1-alpha, 0, 1) Do we have enough evidence to reject H in this example? Of course! The estimated 0 value Θ(cid:98) = 0.29 is equivalent to Z(cid:98) = 2.92, which is much too far from the cutoff z α = 1.65. In other words, we conclude that at a 5% critical level we have strong evidence to believe that the die is biased. Therefore, we need to reject H . 0 This conclusion makes a lot of sense if you think about it carefully. The estimator Θ(cid:98) =0.29 is obtained from N =1000 independent experiments. If we were only conducting N = 20 experiments, it might be consistent with the null hypothesis to have Θ(cid:98) = 0.29. However, if we have N = 1000 experiments, having Θ(cid:98) = 0.29 does not seem likely when there is no systematic bias. If there is no systematic bias, the estimator Θ(cid:98) should slightly jitter around Θ(cid:98) = 0.25, but it is quite unlikely to vary wildly to Θ(cid:98) = 0.29. Thus, based on the available statistics, we decide to reject the null hypothesis. 571
CHAPTER 9. CONFIDENCE AND HYPOTHESIS The decision based on comparing the critical value is known as the critical-value test. The idea (for testing a right-hand tail of a Gaussian random variable) is summarized in three steps: How to conduct a critical-value test √ • Set a critical value z α. Compute Z(cid:98)=(Θ(cid:98) −θ)/(σ/ N). • If Z(cid:98)≥z α, then reject H 0. • If Z(cid:98)<z α, then keep H 0. If you are testing a left-hand tail, you can switch the order of the inequalities. The critical-value test belongs to a larger family of testing procedures based on deci- sion theory. To give you a preview of the general theory of hypothesis testing, we define a decision rule,afunctionthatmapsarealizationoftheestimatortoabinarydecisionspace. In our problem the estimator is Z(cid:98) (or equivalently Θ(cid:98)). We denote its realization by z (cid:98). The binary decision space is {H , H }, corresponding to whether we want to claim H or H . 0 1 0 1 Claiming H is equivalent to keeping H , and claiming H is equivalent to rejecting H . 0 0 1 0 For the critical-value test, the decision rule δ(·) : R → {0,1} is given by the equation (for testing a right-hand tail): (cid:40) 1, if z ≥z , (claim H ), δ(z)= (cid:98) α 1 (9.25) (cid:98) 0, if z <z , (claim H ). (cid:98) α 0 Example 9.7. It was found that only 35% of the children in a kindergarten eat broccoli. The teachers conducted a campaign to get more kids to eat broccoli, after whichitwasfoundthat390kidsoutof1009kidsreportedthattheyhadeatenbroccoli. Has the campaign successfully increased the number of kids eating broccoli? Assume that the standard deviation is known. Solution. We setup the null and the alternative hypothesis. H : θ =0.35, H : θ >0.35. 0 1 We construct an estimator Θ(cid:98) = (1/N)(cid:80)N n=1X n, where X n is Bernoulli with proba- bility θ. Based on θ, σ2 = θ(1−θ) = 0.227. (Again, in practice we do not know the true variance, but in this problem we pretend that we know it.) By the Central Limit Theorem, Θ(cid:98) is roughly a Gaussian. We compute the test statistics Θ(cid:98) = 390 = 0.387. Standardization gives Z(cid:98) = Θ(cid:98)√−θ = 2.432. At a 5% 1009 σ/ N critical level, we have that z α = 1.65. So Z(cid:98) = 2.432 > 1.65 = z α, and hence we need to reject the null hypothesis. Even if we choose a 1% critical level so that z = 2.32, α our estimator Z(cid:98)=2.432>2.32=z α will still reject the null hypothesis. A graphical illustration of this problem is shown in Figure 9.11. It can be seen that Θ(cid:98) =0.387 is actually quite far away from the cutoff 1.65. Thus, we need to reject the null hypothesis. 572
9.3. HYPOTHESIS TESTING Figure 9.11: Example of a critical-value test. In this example, the test statistic Θ(cid:98) = 0.387 is equivalent to Z(cid:98) = 2.432, which is significantly larger than the cutoff z α = 1.65. Therefore, we havestrongevidencetorejectthenullhypothesis,becausetheprobabilityofobtainingΘ(cid:98) =0.387 is very low if H is true. 0 9.3.3 p-value test An alternative to the critical-value test is the p-value test. Instead of looking at the cutoff valuez ,weinspecttheprobabilityofobtainingourobservationifH istrue.Tounderstand α 0 how the p-value test works, we consider another toy problem. Suppose that we have two hypotheses about flipping a coin: • H : θ =0.9, which is our default belief. 0 • H : θ <0.9, which is a one-sided alternative. 1 It was found that with N = 150 coin flips, the coin landed on heads 128 times. Thus the estimator is Θ(cid:98) = 128 =0.853. Then, by following our previous procedures, we have that 150 Θ(cid:98) −θ 0.853−0.9 Z(cid:98)= √ = (cid:113) =−1.92. σ/ N 0.9(1−0.9) 150 At this point we can follow the previous subsection by computing the critical value z α and make the decision. However, let’s take a different route. We want to know what is the probability under the curve if we integrate the PDF of Z(cid:98) from −∞ to −1.92. This is easy. Since Z(cid:98) is Gaussian(0,1), it follows from the CDF of a Gaussian that P[Z(cid:98)≤−1.92]=0.0274. (cid:124) (cid:123)(cid:122) (cid:125) p-value Referring to Figure 9.12, the value 0.0274 is the pink area under the curve, which is the PDFofZ(cid:98).Sincetheareaunderthecurveislessthanthecriticallevelα(say5%),wereject the null hypothesis. On computers, computing the p-value is done using the CDF commands. % MATLAB code to compute the p-value p = cdf(’norm’, -1.92, 0, 1); 573
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure9.12:Thep-valuetestasksustolookattheprobabilityofZ(cid:98)≤z (cid:98).Ifthisprobability(thep-value) is less than the critical level α, we have significant evidence to reject the null hypothesis. # Python code to compute the p-value import scipy.stats as stats p = stats.norm.cdf(-1.92,0,1) In this example, the probability P[Z(cid:98) ≤ −1.92] is known as the p-value. It is the probability of Z(cid:98) ≤ z, under the distribution mandated by the null hypothesis, where z is the (normalized) estimated value based on data. Using our example, z is −1.92. By “distributionmandatedbythenullhypothesis”wemeanthatthePDFofZ(cid:98)isthePDFthat the null hypothesis wants. In the above example the PDF is Gaussian(0,1), corresponding √ to Gaussian(θ,σ/ N) for Θ(cid:98). More formally, the p-value for a left-hand tail test is defined as p-value(z (cid:98))=P[Z(cid:98)≤z (cid:98)], where z (cid:98) is the random realization of Z(cid:98) estimated from the data. The decision rule based on the p-value is (for the left-hand tail): (cid:40) δ(z)= 1, P[Z(cid:98)≤z (cid:98)]<α (claim H 1), (9.26) (cid:98) 0, P[Z(cid:98)≤z (cid:98)]≥α (claim H 0). Ifthealternativehypothesisisright-handed,thentheprobabilitybecomesP[Z(cid:98)≥z (cid:98)]instead. Relationship between critical-value and p-value tests. There is a one-to-one corre- spondence between the p-value and the critical value. In the p-value test, if Z(cid:98) is Gaussian, it follows that p-value=P[Z(cid:98)≤z (cid:98)]=Φ(z (cid:98)), where Φ is CDF of the standard Gaussian. Taking the inverse, the corresponding z is (cid:98) z =Φ−1(p-value). (cid:98) 574
9.3. HYPOTHESIS TESTING In practice, we do not need to take any inverse of the p-value to obtain z because it is (cid:98) directly available from the data. To test the p-value, we compare it with the critical level α by checking p-value<α. Taking the inverse of both sides, it follows that the decision rule is equivalent to Φ−1(p-value)<Φ−1(α), (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) z(cid:98) zα where the quantity on the right-hand side is the critical value z . Therefore, if the test α statistic fails in the p-value test it will also fail in the critical-value test, and vice versa. What is the difference between the critical-value test and p-value test? • Critical-value test: Compare w.r.t. critical value, which is the cutoff on the Z- axis. • p-value test: Compare w.r.t. α, which is the probability. • Both will give you the same statistical conclusion. So it does not matter which one you use. Example 9.8.WeflipacoinforN =150timesandfindthat128areheads.Consider two hypotheses • H : θ =0.9, which is our default belief. 0 • H : θ (cid:54)=0.9, which is a two-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject H ? 0 Solution. We know that Θ(cid:98) =128/150=0.853. The normalized statistic is Θ(cid:98) −θ 0.853−0.9 Z(cid:98)= √ = (cid:113) =−1.92. σ/ N 0.9(1−0.9) 150 To compute the p-value, we observe that the two-sided test means that we consider the two tails. Thus, we have p-value=P[|Z(cid:98)|>1.92] =2×P[Z(cid:98)>1.92] =2×0.0274=0.055. For a critical level of α = 0.05, the p-value is larger. This means that the probability of obtaining |Z| > 1.92 is not extreme enough. Therefore, we do not have sufficient evidence to reject the null hypothesis. 575
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If we take the critical-value test, we will reach the same conclusion. The critical value for α=0.05 is determined by taking the inverse CDF at 1−0.025, giving (cid:16) α(cid:17) z =Φ−1 1− =1.96. α 2 Since Z(cid:98) = 1.92 has not passed this threshold, we conclude that there is not enough evidence to reject the null hypothesis. Figure 9.13: Example of a two-sided test using the p-value and the z -value. α 9.3.4 Z-test and T-test The critical-value test and the p-value tests are generic tools for hypothesis testing. In this subsection we introduce the Z-test and the T-test. It is important to understand that the Z-test and the T-test refer to the distributional assumptions we make about the variance. They define the distribution we use to conduct the test but not the tools. In fact, both the Z-test and the T-test can be implemented using the critical-value test or the p-value test. Figure 9.14 illustrates the hierarchy of the tests. Figure 9.14: When conducting a hypothesis testing of the sample average, we may or may not know thevariance.Ifweknowthevariance,weusetheGaussiandistributiontoconducteitherap-valuetest or a critical-value test. If we do not know the variance, we use Student’s t-distribution. The difference between the Gaussian distribution and the T distribution is mainly attributable to the knowledge about the population variance. If the variance is known, the distribution of the estimator (which in our case is the sample average) is Gaussian. If 576
9.3. HYPOTHESIS TESTING the variance is estimated from the sample, the distribution of the estimator will follow a Student’s t-distribution. To introduce the Z-test and the T-test we consider the following two examples. The first example is a Z-test. Example 9.9 (Z-test). Suppose we have a Gaussian random variable with unknown mean θ and a known variance σ = 11.6. We draw N = 25 samples and construct an estimator Θ(cid:98) =80.94. We propose two hypotheses: • H : θ =85, which is our default belief. 0 • H : θ <85, which is a one-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject the null hypothesis? Solution. The test statistic is Θ(cid:98) −θ Z(cid:98)= √ =−1.75. σ/ N Since the individual samples are assumed to follow a Gaussian, the sample average Θ(cid:98) is also a Gaussian. Hence, Z(cid:98) is distributed according to Gaussian(0,1). Figure 9.15: A one-sided Z-test using the p-value and the z -value. α For a critical level of 0.05, a one-sided critical value is z =Φ−1(1−α)=−1.645. α Since Z(cid:98) =−1.75, which is more extreme than the critical value, we conclude that we need to reject H . 0 If we use the p-value test, we have that the p-value is P[Z(cid:98)≤−1.75]=Φ(−1.75)=0.0401. Since the p-value is smaller than the critical level α=0.05, it implies that Z(cid:98)=−1.75 is more extreme. Hence, we reject H . 0 577
CHAPTER 9. CONFIDENCE AND HYPOTHESIS ThefollowingexampleisaT-test.InaT-testwedonotknowthepopulationvariance butonlyknowthesamplevarianceS(cid:98).ThustheteststatisticweuseisaT randomvariable. Example 9.10 (T-test). Suppose we have a Gaussian random variable with unknown mean θ and an unknown variance σ. We draw N = 100 samples and construct an estimator Θ(cid:98) =130.1, with a sample variance S(cid:98)=21.21. We propose two hypotheses: • H : θ =120, which is our default belief. 0 • H : θ (cid:54)=120, which is a two-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject the null hypothesis? Solution. The test statistic is Θ(cid:98) −θ T(cid:98)= √ =4.762. S(cid:98)/ N Note that while the sample average Θ(cid:98) is a Gaussian, the test statistic T(cid:98) is distributed according to a T distribution with N −1 degrees of freedom. For a critical level of 0.05, a two-sided critical value is (cid:16) α(cid:17) t =Ψ−1 1− =1.984. α 99 2 Since T(cid:98) = 4.762, which is more extreme than the critical value, we conclude that we need to reject H . 0 If we use the p-value test, we have that the p-value is P[|T(cid:98)|≥4.762]=2×P[T(cid:98)≥4.762] =3.28×10−6. Since the p-value is (much) smaller than the critical level α = 0.05, it implies that |T(cid:98)|≥4.762 is quite extreme. Hence, we reject H 0. Figure 9.16: A two-sided T-test using the p-value and the z -value. α 578
9.4. NEYMAN-PEARSON TEST Forthisexample,theMATLABandPythoncommandstocomputet andthep-value α are % MATLAB code to compute critical-value and p-value t_alpha = icdf(’t’, 1-0.025, 99); p = 1-cdf(’t’, 4.762, 99); # Python code to compute critical value and p-value import scipy.stats as stats t_alpha = stats.t.ppf(1-0.025,99) p = 1-stats.t.cdf(4.762,99) What are the Z-test and the T-test? • Both are hypothesis testings for the sample averages. • Z-test: Assume known variance. Hence, use the Gaussian distribution. • T-test: Assume unknown variance. Hence, use the Student’s t-distribution. Remark. We are exclusively analyzing the sample average in this section. There are other types of estimators we can analyze. For example, we can discuss the difference between the twomeans,theratiooftworandomvariables,etc.Ifyouneedtoolsforthesemoreadvanced problems, please refer to the reference section at the end of this chapter. 9.4 Neyman-Pearson Test The hypothesis testing procedures we discussed in the previous section are elementary in the sense that we have not discussed much theory. This section aims to fill the gap so that you can understand hypothesis testing from a broader perspective. This generalization will also help to bridge statistics to other disciplines such as classification in machine learning and detection in signal processing. We call this theoretical analysis the Neyman-Pearson framework. 9.4.1 Null and alternative distributions Whenwediscussedhypothesistestingintheprevioussection,wefocusedexclusivelyonthe null hypothesis H . Regardless of whether we are studying the Z-test or the T-test, using 0 the critical value or the p-value, all the distributions are associated with the distribution under H . 0 What do we mean by “distribution under H 0”? Using Θ(cid:98) as an example, the PDF of Θ(cid:98) is assumed to be Gaussian(θ,σ2/N). This Gaussian, centered at θ, is the distribution assumed under H . As we decide whether to keep or rejectH , we look at the critical value 0 0 and the p-value of the test statistic under Gaussian(θ,σ2/N). 579
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Importantly, the analysis of hypothesis testing is not just about H — it is also about 0 the alternative hypothesis H , which uses a different PDF. For example, H could use 1 1 Gaussian(θ(cid:48),σ2/N)forθ(cid:48) >θ.Therefore,forthesametestingstatisticΘ(cid:98),wecancheckhow close it is to H . 1 To capture both distributions, we define f (y)=f (y |H ), 0 Y 0 f (y)=f (y |H ). 1 Y 1 The first PDF defines the distribution when the true model is H . The second PDF is the 0 distribution when the true model is H . 1 Example9.11.ConsideranestimatorY ∼Gaussian(θ,σ2/N).Definetwohypotheses H :θ =120 and H :θ >120. The two PDFs are then 0 1 f (y)=f (y|H )=Gaussian(120,σ2/N), 0 Y 0 f (y)=f (y|H )=Gaussian(θ(cid:48),σ2/N), θ(cid:48) >120. 1 Y 1 A graph of the two distributions is shown in Figure 9.17. In this figure we plot the PDF under the null hypothesis and the PDF under an alternative hypothesis. The decision is based on the null, where we marked the critical value. Figure 9.17: The PDF of the estimator under hypotheses H and H . The yellow region defines the 0 1 rejection zone R . If the estimator has a realization Y = y that falls into the rejection zone R , we α α need to reject H . 0 Students are frequently confused about the exact equation of the PDF under H . If 1 the alternative hypothesis is defined as θ > 120, shall we define the PDF as a Gaussian centered at 130 or 151.4? They are both valid alternative hypotheses. The answer is that we are going to express all equations based on θ(cid:48). For example, if we want to analyze the prediction error (this term will be explained later), the prediction error will be a function of θ(cid:48). If θ(cid:48) is close to θ, we will expect a larger prediction error. However, if θ(cid:48) is far away from θ, the prediction error may be small. Whenever we discuss hypothesis testing, a decision rule is always implied. A decision ruleisamappingδ(·)fromsamplespaceY oftheteststatisticY (orΘ(cid:98) ifyouprefer)tothe 580
9.4. NEYMAN-PEARSON TEST binary space of {0,1}: (cid:40) 1, if y ∈R , (we will reject H ), δ(y)= α 0 (9.27) 0, if y (cid:54)∈R , (we will keep H ). α 0 Here R is the rejection zone. For example, in a one-sided testing at a critical level α, the α rejection zone is R = {y ≥ Φ−1(1−α)}. Therefore, as long as y ≥ Φ−1(1−α), we will α reject the null hypothesis. Otherwise, we will keep the null hypothesis. A rejection zone can be one-sided, two-sided, or even more complicated. Example 9.12. Consider H : θ = 0.35 and H : θ > 0.35. It was found that the 0 1 sample average o √ver 1009 samples is Θ(cid:98) =0.387, with σ2 =0.227. The normalized test statistic is Z(cid:98) = N(Θ(cid:98) −θ)/σ =2.432. At a 5% critical level, define the decision rule based on the critical-value approach. Solution. If α = 0.05, it follows that z = Φ−1(1 − 0.05) = 1.65. Therefore, the α decision rule is (cid:40) 1, if z ≥1.65, (we will reject H ), δ(z)= (cid:98) 0 (cid:98) 0, if z <1.65, (we will keep H ), (cid:98) 0 where z (cid:98)is the realization of Z(cid:98). In this particular problem, we have z (cid:98)= 2.432. Thus, according to the decision rule, we need to reject H . 0 A decision rule is something you create. You do not need to follow the critical-value or the p-value procedure — you can create your own decision rule. For example, you can say “reject H when |y|>0.000001”. There is nothing wrong with this decision rule except 0 that you will almost always reject the null hypothesis (so it is a bad decision rule). See Figure 9.18 for a graph of a similar example. If you follow the critical-value or the p-value procedures,itturnsoutthattheresultingdecisionruleisequivalenttosomeformofoptimal decisionrule.ThisconceptistheNeyman-Pearsonframework,whichwewillexplainshortly. 9.4.2 Type 1 and type 2 errors Since hypothesis testing is about applying a decision rule to the test statistics, and since no decision rule is perfect, it is natural to ask about the error expected from a particular decisionrule.Inthissubsectionwedefinethedecisionerror.However,theterminologyvaries from discipline to discipline. We will explain the decision error first through the statistics perspective and then through the signal processing perspective. Twotablesofthecasesthatcanbegeneratedbyabinarydecision-makingprocessare shown in Figure 9.19. The columns of the tables are the true statements, i.e., whether the test statistic has a population distribution under H or H . The rows of the tables are the 0 1 statements predicted by the decision rule, i.e., whether we should declare the statistics are from H or H . Each combination of the truth and prediction has a label: 0 1 • True positive: The truth is H , and you declare H . 1 1 • True negative: The truth is H , and you declare H . 0 0 • False positive: The truth is H , and you declare H . 0 1 581
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.18: Two possible decision rules δ (y) and δ (y). In this example, δ (y) is designed according 1 2 1 tothecritical-valueapproachatα=0.025,whereasδ (y)isarbitrarilydesigned.Botharevaliddecision 2 rules, although δ should not be used because it tends to reject the null hypothesis more often than 2 desired. • False negative: The truth is H , and you declare H . 1 0 Differentcommunitieshavedifferentwaysoflabelingthesequantities.Inthestatisticscom- munity the false negative rate (i.e., the number of false negative cases divided by the total number of cases) is called the type 2 error, and the false positive rate is called the type 1 error. The true positive rate is called the power of the decision rule. In the engineering community (e.g., radar engineering and signal processing) the ob- jective is to detect whether a target (e.g., a missile or an enemy aircraft) is present. In this context, the false positive rate is known as the probability of false alarm, since personnel willbealertedwhennotargetispresent.Thefalsenegativerateisknownastheprobability of miss because you miss a target. If the truth is H and the prediction is also H , we call 1 1 this the probability of detection. Figure 9.19: Terminologies used in labeling the prediction error. The terms “Type 1 error” and “Type 2 error” are commonly used by the statistics community, whereas the terms “false alarm”, “miss” and “detection” are more often used in the engineering community. ThediagraminFigure 9.20willhelptoclarifythesedefinitions.Giventwohypotheses H andH ,thereexiststhecorrespondingdistributionsf (y)andf (y),whicharethePDFs 0 1 0 1 582
9.4. NEYMAN-PEARSON TEST of the test statistics Y (or Θ(cid:98) if you prefer). Supposing that our decision rule is to declare H whenY ≥η forsomeη,forexample,η =1.65fora5%criticallevel,therearetwoareas 1 under the curve that we need to consider. • Type 1 / False alarm. The blue region under the curve represents the probability of declaring H (i.e., we choose to reject the null) while the truth is actually H (i.e., we 1 0 should have not rejected the null). Mathematically, this probability is (cid:90) p =P[Y ≥η |H ]= f (y)dy. (9.28) F 0 0 y≥η • Type2/Miss.Thepinkregionunderthecurverepresentstheprobabilityofdeclaring H (i.e., we choose to keep the null) while the truth is actually H (i.e., we should 0 1 have rejected the null). Mathematically, this probability is (cid:90) p =P[Y <η |H ]= f (y)dy. (9.29) M 1 1 y<η Figure 9.20: Definition of type 1 and type 2 errors. The power of the decision rule is also known as the detection. It is defined as p =P[Y ≥η |H ]. (9.30) D 1 A plot illustrating the power of the decision rule is shown in Figure 9.21. Since p is the D conditional probability of Y ≥η given H , it is the complement of p , and so we have the 1 M identity p =1−p . D M Some communities refer to the above quantities in terms of the counts instead of the probabilities. The difference is that the probabilities are normalized to [0,1] whereas the counts are just the raw integers obtained from running an experiment. We prefer to use the probabilities because they are the theoretical values. If you tell us the distributions f and 0 f ,wecanreporttheprobabilities.Thecounts,bycontrast,arejustanotherformofsample 1 statistics.Thenumberofcountstodaymaybedifferentfromthenumberofcountstomorrow 583
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.21: The power of the decision rule is the area under the curve of f , integrated for y inside 1 the rejection zone. because they are obtained from the experiments. The difference between probabilities and counts is analogous to the difference between PMFs and histograms. Since the probability of errors changes as the decision rule changes, it is necessary to definep ,p andp asfunctionsofδ.Inaddition,hypothesistestingisnotlimitedtoone- F D M sidedtests.WecandefinetherejectionzoneasR ={y |reject H using a critical level α}. α 0 The probabilities p and p are defined as F M (cid:90) (cid:90) p (δ)= δ(y)f (y)dy = f (y)dy, (9.31) F 0 0 y∈Rα (cid:90) (cid:90) p (δ)= δ(y)f (y)dy = f (y)dy. (9.32) M 1 1 y(cid:54)∈Rα Using the property that p =1−p , we have that D M (cid:90) p (δ)=1−p (δ)= f (y)dy. (9.33) D M 1 y∈Rα Note that the rejection zone does not need to depend on α. You can arbitrarily define the rejection zone, and the probabilities p , p , and p can still be defined. F M D Example 9.13. Find p (δ ) and p (δ ) for the decision rule in Figure 9.18. F 1 F 2 Solution. Since f is a Gaussian with zero mean and unit variance, it follows that 0 p F(δ 1)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(1.92)=0.025, 2π 1.96 p F(δ 2)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(0.5)=0.3085. 2π 0.5 9.4.3 Neyman-Pearson decision At this point you have probably observed something about the critical-value test and the p-value test. Among the four types of decision combinations, we are looking at the false 584
9.4. NEYMAN-PEARSON TEST positive rate, or the probability of false alarm p (δ). The critical-value test requires us to F findδ suchthatp (δ)isequaltoα.Thatis,ifyoutellusthecriticallevelα(e.g.,α=0.05), F we will find a decision rule (by telling you the cutoff) such that the false alarm rate is α. Consider an example: Example 9.14. Let α = 0.05. Assume that f is a Gaussian with zero-mean and 0 unit-variance. Let us do a one-sided test for H :θ =0 versus H :θ >0. Find δ such 0 1 that p (δ)=α. F Solution. Let the decision rule δ be (cid:40) 1, y ≥η, δ(y)= 0, y <η. Our goal is to find η. The probability of false alarm is p F(δ)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(η). 2π η Equatingthisto α,itfollows that 1−Φ(η)=α implies η =Φ−1(1−α)=1.65.Sothe decision rule becomes (cid:40) 1, y ≥1.65, δ(y)= 0, y <1.65. Ifyouapplythisdecisionrule,youareguaranteedthatthefalsealarmrateisα=0.05. But why should we aim for p (δ) equal to α? Isn’t a lower false alarm rate better? F Indeed, we would not mind having a lower false alarm, so we are happy to have any δ that satisfies p (δ) ≤ α. However, changing the equality to an inequality means that we F now have a set of δ instead of a unique δ. More important, we need to pay attention to the trade-off between p (δ) and p (δ). The smaller the p (δ) a decision rule δ provides, F D F the smaller the p (δ) you can achieve. This is immediately apparent from Figure 9.20 and D Figure 9.21. (If you move the cutoff to the right, the gray area and the blue area will both shrink.) Therefore, the desired optimization should be formulated as: From all the decision rules δ that have a false alarm rate of no larger than α, we pick the one that maximizes the detection rate. The resulting decision rule is known as the Neyman-Pearson decision rule. Definition 9.2. The Neyman-Pearson decision rule is defined as the solution to the optimization δ∗ =argmax p (δ), D δ subject to p (δ)≤α. (9.34) F Figure 9.22illustratestwodecisionrulesδ∗(y)andδ(y).Thefirstdecisionruleδ∗(y)is obtained according to the critical-value approach, with α=0.025. As we will prove shortly, this is also the optimal Neyman-Pearson decision rule for a one-sided hypothesis testing at α = 0.025. The second decision rule δ(y) has a harsher cutoff, meaning that you need an extreme test statistic to reject the null hypothesis. Clearly, the p-value obtained by δ(y) is 585
CHAPTER 9. CONFIDENCE AND HYPOTHESIS less than α = 0.025. Thus, δ(y) is a valid decision rule according to the Neyman-Pearson formulation. However, δ(y) is not optimal because the detection rate is not maximized. Figure 9.22: Twodecisionrulesδ(y)andδ∗(y).Assumethatα=0.025.Thenδ(y)isoneofthemany feasible choices in the Neyman-Pearson optimization, but δ∗(y) is the optimal solution. Because of the complementary behavior of p and p , it follows that p is maximized F D D whenp hitstheupperbound.Ifwewanttomaximizethedetectionrateweneedtostretch F the false alarm rate as much as possible. As a result, the Neyman-Pearson solution occurs when p (δ)=α, i.e., when the equality is met. F TheNeyman-Pearsonframeworkisageneralframeworkforalldistributionsf andf , 0 1 asopposedtothecritical-valueandp-valueexamples,whichareeitherGaussianorStudent’s t-distribution. The solution to the Neyman-Pearson optimization is a decision rule known as the likelihood ratio test. The likelihood ratio is defined as follows. Definition 9.3. The likelihood ratio for two distributions f (y) and f (y) is 1 0 f (y) L(y)= 1 . (9.35) f (y) 0 It turns out that the solution to the Neyman-Pearson optimization takes the form of the likelihood ratio. Theorem 9.2. The solution to the Neyman-Pearson optimization is a decision rule that checks the likelihood ratio (cid:40) 1, L(y)≥η, δ∗(y)= (9.36) 0, L(y)<η, for some decision boundary η which is a function of the critical level α. 586
9.4. NEYMAN-PEARSON TEST What is so special about Neyman-Pearson decision rule? • Itistheoptimaldecision.Itsoptimalityisdefinedw.r.t.maximizingthedetection rate while keeping a reasonable false alarm rate: δ∗ =argmax p (δ), D δ subject to p (δ)≤α. F • If your goal is to maximize the detection rate while maintaining the false alarm rate, you cannot do better than Neyman-Pearson. • Its solution is the likelihood ratio test: (cid:40) 1, L(y)≥η, δ∗(y)= 0, L(y)<η, where L(y)=f (y)/f (y) is the likelihood ratio. 1 0 • The critical-value test and the p-value test are special cases of the Neyman- Pearson test. Deriving the solution to the Neyman-Pearson optimization can be skipped if this is your first time reading the book. Proof. Given α, choose δ∗ such that the false alarm rate is maximized: p (δ∗)=α. Then, F by substituting the definition of δ∗ into the false alarm rate, (cid:90) ∞ α=p (δ∗)= δ∗(y)f (y)dy F 0 −∞ (cid:90) (cid:90) = 1·f (y)dy+ 0·f (y)dy. (9.37) 0 0 L(y)≥η L(y)<η Now, consider another decision rule δ that is not optimal but is feasible. That means that δ satisfies p (δ)≤α. Therefore, F (cid:90) ∞ α≥p (δ)= δ(y)f (y)dy F 0 −∞ (cid:90) (cid:90) = δ(y)·f (y)dy+ δ(y)·f (y)dy. (9.38) 0 0 L(y)≥η L(y)<η Our goal is to show that p (δ∗)≥p (δ), because by proving this result we can claim that D D δ∗ maximizes the detection rate. By combining Equation (9.37) and Equation (9.38), we have 0≤p (δ∗)−p (δ) F F (cid:90) (cid:90) = (1−δ(y))f (y)dy− δ(y)f (y)dy. (9.39) 0 0 L(x)≥η L(y)<η 587
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Define L(y)= f1(y). Then L(y)≥η if and only if f (y)≥ηf (y). So, f0(y) 1 0 (cid:90) (cid:90) p (δ∗)−p (δ)= (1−δ(y))f (y)dy− δ(y)f (y)dy D D 1 1 L(y)≥η L(y)<η (cid:90) (cid:90) = (1−δ(y))ηf (y)dy− δ(y)ηf (y)dy 0 0 L(y)≥η L(y)<η (cid:34) (cid:35) (cid:90) (cid:90) =η (1−δ(y))f (y)dy− δ(y)f (y)dy ≥0, 0 0 L(y)≥η L(y)<η where the last inequality holds because of Equation (9.39). Therefore, we conclude that δ∗ maximizes p . (cid:3) D End of the proof. Please join us again. At this point, you may object that the likelihood ratio test (i.e., the Neyman-Pearson decision rule) is very different from the hypothesis testing examples we have seen in the previous chapter because now we need to handle the likelihood ratio L(y). Rest assured that they are the same, as illustrated by the following example. Example 9.15. Consider two hypotheses: H : Y ∼ Gaussian(0,σ2), and H : Y ∼ 0 1 Gaussian(µ,σ2), with µ > 0. Construct the Neyman-Pearson decision rule (i.e., the likelihood ratio test). Solution. Let us first define the likelihood functions. It is clear from the description that 1 (cid:26) y2 (cid:27) 1 (cid:26) (y−µ)2(cid:27) f (y)= √ exp − and f (y)= √ exp − . 0 2πσ2 2σ2 1 2πσ2 2σ2 Therefore, the likelihood ratio is (cid:26) (cid:27) f (y) 1 L(y)= 1 =exp − (µ2−2µy) . f (y) 2σ2 0 The likelihood ratio test states that the decision rule is (cid:40) 1, L(y)≥η, δ∗(y)= 0, L(y)<η. So it remains to simplify the condition L(y)(cid:82)η. To this end, we observe that 1 L(y)≥η ⇐⇒ − (µ2−2µy)≥logη 2σ2 µ σ2 ⇐⇒ y ≥ − logη. 2 µ (cid:124) (cid:123)(cid:122) (cid:125) d=efτ 588
9.4. NEYMAN-PEARSON TEST Therefore,insteadofdeterminingη,wejustneedtodefineτ becausethedecisionrules based on η and τ are equivalent. Todetermineτ,Neyman-Pearsonstatesthatp (δ)≤α(andattheoptimalpoint F the equality has to hold). Substituting this criterion into the decision rule, (cid:90) α=p (δ)= f (y)dy F 0 L(y)≥η (cid:90) = f (y)dy 0 y≥τ =(cid:90) √ 1 e− 2y σ2 2 dy 2πσ2 y≥τ (cid:16)τ(cid:17) =1−Φ . σ Taking the inverse of the CDF, we obtain τ: τ =σΦ−1(1−α). Putting everything together, the final decision rule is (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). √ So if α = 0.05 we will reject H when y ≥ 1.65σ. We can also replace σ by σ/ N if 0 the estimator is constructed from multiple measurements. The above example tells us that even though the likelihood ratio test may appear complicatedatfirstglance,thedecisionisthesameasthegoodoldhypothesistestingrules we have derived. The flexibility we have gained with the likelihood ratio test is the variety of distributions we can handle. Instead of restricting ourselves to Gaussians or Student’s t-distribution (which exclusively focuses on the sample averages), the likelihood ratio test allows us to consider any distributions. The exact decision rule could be less obvious, but the method is generalizable to a broad range of problems. Practice Exercise 9.5. In a telephone system, the waiting time is defined as the inter-arrival time between two consecutive calls. However, it is known that sometimes the waiting time can be mistakenly recorded as the time between three consecutive calls (i.e., by skipping the second one). Since the interarrival time of an independent PoissonprocessiseitheranexponentialrandomvariableoranErlangrandomvariable, depending on how many occurrences we are counting, we define the hypotheses (cid:40) (cid:40) e−y, y ≥0, ye−y, y ≥0, f (y)= and f (y)= 0 1 0, y <0, 0, y <0. Suppose we are given one measurement Y = y. Find the Neyman-Pearson decision rule for α=0.05. 589
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Solution. The likelihood ratio is f (y) ye−y L(y)= 1 = =y, y ≥0. f (y) e−y 0 Substituting this into the decision rule, we have (cid:40) 1, L(y)≥η ⇐⇒y ≥η, δ∗(y)= 0, L(y)<η ⇐⇒y <η. It remains to determine η. Inspecting p (δ), we have that F (cid:90) α=p (δ∗)= f (y)dy F 0 L(y)≥η (cid:90) = e−y dy =e−η. y≥η Setting e−η =α, we have that α=−logα. Hence, the decision rule is (cid:40) 1, L(y)≥η ⇐⇒y ≥−logα, δ∗(y)= 0, L(y)<η ⇐⇒y <−logα. For α = 0.05, we reject the null hypothesis when y ≥ 2.9957. Figure 9.23 illustrates the hypothesis testing rule. Figure 9.23: Neyman-Pearson decision rule at α=0.05. Remark. This example is instructive in that we have only one measurement Y =y. If we have repeated measurements and take the average, then the Central Limit The- orem will kick in. In that case, we can resort to our favorite Gaussian distribution or Student’s t-distribution instead of dealing with the exponential and the Erlang distributions. However, the example demonstrates the usefulness of Neyman-Pearson, especially when the distributions are complicated. 590
9.5. ROC AND PRECISION-RECALL CURVE 9.5 ROC and Precision-Recall Curve Beingabinary decisionrule,thehypothesistestingproceduresharesmanysimilaritieswith a two-class classification algorithm.3 Given a testing statistic or a testing sample, both the hypothesis testing and a classification algorithm will report YES or NO. Therefore, any performance evaluation metric developed for hypothesis testing is equally applicable to classification and vice versa. Thetopicwestudyinthissectionisthereceiveroperatingcharacteristic(ROC)curve and the precision-recall (PR) curve. The ROC curve and the PR curve are arguably the mostpopularmetricsinmodernmachinelearning,inparticularforclassification,detection, and segmentation tasks in computer vision. There are many unresolved questions about thesetwocurvesandtherearemanydebatesabouthowtousethem.Ourgoalisnottoadd another voice to the debate; rather, we would like to fill in the gap between the hypothesis testing theory (particularly the Neyman-Pearson framework) and these two sets of curves. We will establish the equivalence between the two curves and leave the open-ended debates to you. 9.5.1 Receiver Operating Characteristic (ROC) Our approach to understanding the ROC curve and the PR curve is based on the Neyman- Pearson framework. Under this framework, we know that the optimal decision rule w.r.t to the Neyman-Pearson criterion is the solution to the optimization δ∗(α)=argmax p (δ) D δ subject to p (δ)≤α. F As a result of this optimization, the decision rule δ∗ will achieve a certain false alarm rate p (δ∗) and detection rate p (δ∗). Clearly, the decision rule δ∗ changes as we change the F D critical level α. Accordingly we write δ∗ as δ∗(α) to reflect this dependency. Whatthisobservationimpliesisthataswesweepthroughtherangeofα’s,weconstruct different decision rules, each one with a different p and p . If we denote the decision rules F D by δ ,δ ,...,δ , we have M pairs of false alarm rate p and detection rate p : 1 2 M F D • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). 1 F 1 D 1 • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). 2 F 2 D 2 . . • . • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). M F M D M 3Inaclassificationalgorithm,thegoalistolookatthetestingsampleyandcomputecertainthresholding (cid:40) 1, wTφ(y)≥τ criteria.Forexample,atypicaldecisionruleofaclassificationalgorithmisδ(y)= .Here, 0, wTφ(y)<τ youcanthinkofthevectorw astheregressioncoefficient,andφ(·)issomekindoffeaturetransform.The equation says that class 1 will be reported if the inner product is larger than a threshold τ, and class 0 will be reported otherwise. Therefore, a binary classification, when written in this form, is the same as a hypothesistestingprocedure. 591
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If we plot p (δ) on the y-axis as a function of p (δ) on the x-axis, we obtain a curve shown D F in Figure 9.24 (see the example below for the problem setting). The black curve shown on the right is known as the receiver operating characteristic (ROC) curve. Figure9.24:AnexampleofanROCcurve,whereweconsidertwohypotheses:H :Y ∼Gaussian(0,2), 0 and H : Y ∼ Gaussian(3,2). We construct the Neyman-Pearson decision rule for a range of critical 1 levels α. For each α we compute the theoretical p (α) and p (α), shown on the left-hand side of the F D figure. The pair of (p ,p ) is then plotted as the right-hand side curve by sweeping the α’s. D F The setup of the figure follows the example below. Example 9.16. We consider two hypotheses: H :Y ∼Gaussian(0,2), and H :Y ∼ 0 1 Gaussian(3,2). Derive the Neyman-Pearson decision rule and plot the ROC curve. Solution. We construct a Neyman-Pearson decision rule: (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). whereτ isatunablethreshold.Forexample,ifα=0.05,thenσΦ−1(1−0.05)=3.2897, and if α = 0.1, then σΦ−1(1−0.1) = 2.5631. Therefore, the false alarm rate and the detection rate are functions of the critical level α. For this particular example, we have the false alarm rate and detection rate in closed form, as functions of α: p F(α)=(cid:90) ∞ √ 1 e− 2y σ2 2 dy σΦ−1(1−α) 2πσ2 (cid:18) σΦ−1(1−α)(cid:19) =1−Φ =α, σ 592
9.5. ROC AND PRECISION-RECALL CURVE p D(α)=(cid:90) ∞ √ 1 e−(y 2− σµ 2)2 dy σΦ−1(1−α) 2πσ2 (cid:16) µ(cid:17) =1−Φ Φ−1(1−α)− . σ These give us the two curves on the left-hand side of Figure 9.24. What is an ROC curve? • It is a plot showing p on the y-axis and p on the x-axis. D F • p = detection rate (also known as the power of the test). D • p = false alarm rate (also known as the type 1 error of the test). F TheROCcurvetellsusthebehaviorofthedecisionruleaswechangethethresholdα. A graphical illustration is shown in Figure 9.25. There are a few key observations we need to pay attention to: Figure 9.25: Interpreting the ROC curve. • The ROC curve must go through (0,0). This happens when you always keep the null hypothesis or always declare class 0, no matter what observations. If you always keep H , certainly you will not make any false positive (or false alarm), because you 0 will never say H is wrong. Therefore, the detection rate (or the power of the test) is 0 also 0. This is a useless decision rule for both classification and hypothesis testing. • TheROCcurvemustgothrough(1,1).Thishappenswhenyoualwaysrejectthenull hypothesis, no matter what observations we have. If you always reject H , you will 0 always say that “there is a target”. As far as detection is concerned, you are perfect 593
CHAPTER 9. CONFIDENCE AND HYPOTHESIS because you have not missed any targets. However, the false positive rate is also high because you will falsely declare a target when there is nothing. Therefore, this is also a useless decision rule. • TheROCcurvetellsustheoperatingpointofthedecisionruleaswechangethethresh- old. A threshold is a universal concept for both hypothesis testing and classification. In hypothesis testing, we have the critical level α, say 0.05 or 0.1. In classification, we also have a threshold for judging whether a sample should be classified as class 1 or class0.Ofteninclassification,theintermediateestimatesareprobabilitiesordistances to decision boundaries. These real numbers need to be binarized to generate a binary decision. The ROC curve tells us that if you pick a threshold, your decision rule will have a certain p and p as predicted by the curve. If you want to tolerate a higher F D p , you can move along the curve to find your operating point. F • The ideal operating point on a ROC curve is when p =0 and p =1. However, this F D is a hypothetical situation that does not happen in any real decision rule. 9.5.2 Comparing ROC curves BecauseofhowtheROCcurvesareconstructed,everybinarydecisionrulehasitsownROC curve. Typically, when one tries to compare classification algorithms, the area under the curve(AUC)occupiedbytheROCcurveiscompared.AdecisionrulehavingalargerAUC is often a “better” decision rule. Toillustratetheideaofcomparingestimators,weconsideratrivialdecisionrulebased on a blind guess. Example 9.17. (A blind guess decision) Consider a decision rule that we reject H 0 withprobabilityαandkeepH withprobability1−α.Wecallthisablindguess,since 0 the decision rule ignores observation y. Mathematically, this trivial decision rule is (cid:40) 1, with probability α, δ(y)= 0, with probability 1−α. Find p , p , and AUC. F D Solution.Forthisdecisionrulewecomputeitsfalsepositiverate(orfalsealarmrate) and its true positive rate (or detection rate). However, since δ(y) is now random, we needtotaketheexpectationoverthetworandomstatesthatδ(y)cantake.Thisgives us (cid:20)(cid:90) (cid:21) p (α)=E δ(y)f (y)dy F 0 (cid:90) (cid:90) = 1·f (y)dyP[δ(y)=1]+ 0·f (y)dyP[δ(y)=0] 0 0 (cid:90) =α f (y)dy =α. 0 594
9.5. ROC AND PRECISION-RECALL CURVE Similarly, the detection rate is (cid:20)(cid:90) (cid:21) (cid:90) p (α)=E δ(y)f (y)dy =α f (y)dy =α. D 1 1 If we plot p as a function of p , we notice that the function is a straight line going D F from (0,0) to (1,1). This decision rule is useless. Comparing this with the Neyman- Pearson decision rule, it is clear that Neyman-Pearson has a larger AUC. The AUC for this trivial decision rule is the area of the triangle, which is 0.5. Figure 9.26: The ROC curve of the blind guess decision rule is a straight line. The AUC is 0.5. If you set α=0.5, then the decision rule becomes (cid:40) 1, with probability 1, δ(y)= 2 0, with probability 1. 2 This is equivalent to flipping a fair coin with probability 1/2 of declaring H and 1/2 0 declaring H . Its operating point is the yellow circle. 1 Computing the AUC can be done by calling special library functions. However, to spell out the details we demonstrate something more elementary. The program below is a piece of MATLAB code plotting two ROC curves corresponding to two different decision rules. The first decision rule is the trivial decision rule, where we have just shown that p (α) = p (α) = α. The second decision rule is the Neyman-Pearson decision rule, for F D which we showed in Figure 9.24 that p (α) = α and p (α) = 1−Φ(cid:0) Φ−1(1−α)− µ(cid:1) . F D σ Using the MATLAB code below, we can plot the two ROC curves shown in Figure 9.26. % MATLAB code to plot ROC curve sigma = 2; mu = 3; alphaset = linspace(0,1,1000); PF1 = zeros(1,1000); PD1 = zeros(1,1000); PF2 = zeros(1,1000); PD2 = zeros(1,1000); for i=1:1000 alpha = alphaset(i); 595
CHAPTER 9. CONFIDENCE AND HYPOTHESIS PF1(i) = alpha; PD1(i) = alpha; PF2(i) = alpha; PD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma); end figure; plot(PF1, PD1,’LineWidth’, 4, ’Color’, [0.8, 0, 0]); hold on; plot(PF2, PD2,’LineWidth’, 4, ’Color’, [0, 0, 0]); hold off; To compute the AUC we perform a numerical integration: (cid:90) (cid:88) AUC= p (α)·dp (α)≈ p (α )·∆p (α ) D F D i F i i (cid:88) (cid:2) (cid:3) = p (α )· p (α )−p (α ) , D i F i F i−1 i where α is the ith critical level we use to plot the ROC curve. (We assume that the α’s are i sorted in ascending order.) In MATLAB, the commands are auc1 = sum(PD1.*[0 diff(PF1)]) auc2 = sum(PD2.*[0 diff(PF2)]) The AUC of the two decision rules computed by MATLAB are 0.8561 and 0.5005, respec- tively.Thesmallslackof0.0005iscausedbythenumericalapproximationatthetail,which can be ignored as long as you are consistent for all the ROC curves. The commands for Python are analogous to the commands for MATLAB. # Python code to plot ROC curve import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats sigma = 2; mu = 3; alphaset = np.linspace(0,1,1000) PF1 = np.zeros(1000); PD1 = np.zeros(1000) PF2 = np.zeros(1000); PD2 = np.zeros(1000) for i in range(1000): alpha = alphaset[i] PF1[i] = alpha PD1[i] = alpha PF2[i] = alpha PD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) plt.plot(PF1,PD1) plt.plot(PF2,PD2) To compute the AUC, the Python code is (continuing from the previous code): 596
9.5. ROC AND PRECISION-RECALL CURVE auc1 = np.sum(PD1 * np.append(0, np.diff(PF1))) auc2 = np.sum(PD2 * np.append(0, np.diff(PF2))) It is possible to get a decision rule that is worse than a blind guess. The following example illustrates a trivial setup. Practice Exercise 9.6. (Flipped Neyman-Pearson). Consider two hypotheses H =Gaussian(0,σ2), 0 H =Gaussian(µ,σ2), µ>0. 1 Let α be the critical level. The Neyman-Pearson decision rule is (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). Now, consider a flipped Neyman-Pearson decision rule (cid:40) 1, y <σΦ−1(1−α), δ+(y)= 0, y ≥σΦ−1(1−α). Find p , p , and AUC for the new decision rule δ+. F D Solution. Since we flip the rejection zone, the probability of false alarm is (cid:90) p (α)= δ+(y)f (y)dy F 0 =(cid:90) σΦ−1(1−α) √ 1 e− 2y σ2 2 dy 2πσ2 −∞ (cid:18) σΦ−1(1−α)(cid:19) =Φ σ =1−α. Similarly, the probability of detection is (cid:90) p (α)= δ+(y)f (y)dy D 1 =(cid:90) σΦ−1(1−α) √ 1 e−(y 2− σµ 2)2 dy 2πσ2 −∞ (cid:18) σΦ−1(1−α)−µ(cid:19) =Φ σ (cid:16) µ(cid:17) =Φ Φ−1(1−α)− . σ 597
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If you plot p as a function of p , you will obtain a curve shown in Figure 9.27. D F The AUC for this flipped decision rule is 0.1439, whereas that for Neyman-Pearson is 0.8561. The two numbers are complements of each other, meaning that their sum is unity. Figure 9.27: The ROC curve of a flipped Neyman-Pearson decision rule. What if we arbitrarily construct a decision rule that is neither Neyman-Pearson nor the blind guess? The following example demonstrates one possible choice. Practice Exercise 9.7. Consider two hypotheses H =Gaussian(0,σ2), 0 H =Gaussian(µ,σ2), µ>0. 1 Let α be the critical level. Consider the following decision rule: (cid:40) 1, |y|≥σΦ−1(1−α), δ♣(y)= 0, |y|<σΦ−1(1−α). Find p , p , and AUC for the new decision rule δ♣. F D Solution. The probability of false alarm is (cid:90) p (α)= δ♣(y)f (y)dy F 0 =1−(cid:90) σΦ−1(1−α) √ 1 e− 2y σ2 2 dy −σΦ−1(1−α) 2πσ2 =1−Φ(cid:0) Φ−1(1−α)(cid:1) +Φ(cid:0) −Φ−1(1−α)(cid:1) =2α. 598
9.5. ROC AND PRECISION-RECALL CURVE Similarly, the probability of detection is (cid:90) p (α)= δ♣(y)f (y)dy D 1 =1−(cid:90) σΦ−1(1−α) √ 1 e−(y 2− σµ 2)2 dy −σΦ−1(1−α) 2πσ2 (cid:18) σΦ−1(1−α)−µ(cid:19) (cid:18) −σΦ−1(1−α)−µ(cid:19) =1−Φ +Φ σ σ (cid:16) µ(cid:17) (cid:16) µ(cid:17) =1−Φ Φ−1(1−α)− +Φ −Φ−1(1−α)− . σ σ If you plot p as a function of p , you will obtain a curve shown in Figure 9.28. D F The AUC for this proposed decision rule is 0.7534, whereas that of Neyman-Pearson is 0.8561. Therefore, the Neyman-Pearson decision rule is better. Figure 9.28: The ROC curve of a proposed decision rule. The MATLAB code we used to generate Figure 9.28 is shown below. Note that we need to separate the calculations of the two curves, because the proposed curve can only take 0<α<0.5. The Python code is implemented analogously. % MATLAB code to generate the ROC curve. sigma = 2; mu = 3; PF1 = zeros(1,1000); PD1 = zeros(1,1000); PF2 = zeros(1,1000); PD2 = zeros(1,1000); alphaset = linspace(0,0.5,1000); for i=1:1000 alpha = alphaset(i); PF1(i) = 2*alpha; PD1(i) = 1-(normcdf(norminv(1-alpha)-mu/sigma)-... normcdf(-norminv(1-alpha)-mu/sigma)); end alphaset = linspace(0,1,1000); 599
CHAPTER 9. CONFIDENCE AND HYPOTHESIS for i=1:1000 alpha = alphaset(i); PF2(i) = alpha; PD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma); end figure; plot(PF1, PD1,’LineWidth’, 4, ’Color’, [0.8, 0, 0]); hold on; plot(PF2, PD2,’LineWidth’, 4, ’Color’, [0, 0, 0]); hold off; import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats sigma = 2; mu = 3; PF1 = np.zeros(1000); PD1 = np.zeros(1000) PF2 = np.zeros(1000); PD2 = np.zeros(1000) alphaset = np.linspace(0,0.5,1000) for i in range(1000): alpha = alphaset[i] PF1[i] = 2*alpha PD1[i] = 1-(stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) \ -stats.norm.cdf(-stats.norm.ppf(1-alpha)-mu/sigma)) alphaset = np.linspace(0,1,1000) for i in range(1000): alpha = alphaset[i] PF2[i] = alpha PD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) plt.plot(PF1, PD1) plt.plot(PF2, PD2) 9.5.3 The ROC curve in practice If the Neyman-Pearson decision rule is the optimal rule, why don’t we always use it? The problem is that in practice we may not have access to the distributions. For example, if we classify images, how do we know that the data follows a Gaussian distribution or a mixture of distributions? Consequently, the ROC curves we discussed in the subsections above are the theoretical ROC curves. In practice, we plot the empirical ROC curves. Plotting an empirical ROC curve for a binary classification method (and hypothesis testing) is intuitive. The ingredients we need are a set of scores and a set of labels. The scores are the probability values determining the likelihood of a sample belonging to one class. Generally speaking, for empirical data this requires looking at the training data, building a model, and computing the likelihood. We will not go into the details of how a binary classifier is built. Instead, we assume that you have already built a binary classifier and have obtained the scores. Our goal is to show you how to plot the ROC curve. 600
9.5. ROC AND PRECISION-RECALL CURVE The following MATLAB code uses a dataset fisheriris. The code builds a binary classifier and returns the scores. % MATLAB code to train a classification algorithm. % Do not worry if you cannot understand this code. % It is not the focus on this book. load fisheriris pred = meas(51:end,1:2); resp = (1:100)’>50; mdl = fitglm(pred,resp,’Distribution’,’binomial’,’Link’,’logit’); scores = mdl.Fitted.Probability; labels = [ones(1,50), zeros(1,50)]; save(’ch9_ROC_example_data’,’scores’,’labels’); To give you an idea of how the scores of the classifier look, we plot the histogram of thescoresinFigure 9.29.Asyoucansee,thereisnocleardivisionbetweenthetwoclasses. No matter what threshold τ we use, some cases will be misclassified. Figure 9.29: The distribution of probability scores obtained from a binary classifier for the dataset fisheriris.Thegreenverticallinesrepresentthethresholdforturningthescoresintobinarydecisions. Anyscoregreaterthanτ willbeclassifiedasClass1,andanyscorethatislessthanτ willbeclassified as Class 0. These predicted labels would then be compared to the true labels to plot the ROC curve. Recall that the ROC curve is a function of p versus p . Using terminology from D F statistics, p is the true positive rate and p is the false positive rate. By sweeping a range D F of decision thresholds (over the scores), we can compute the corresponding p ’s and p ’s. F D On a computer this can be done by setting up two columns of labels: the true label labels and the predicted labels prediction. For any threshold τ, we binarize the scores to turn them into a decision vector. Then we count the number of true positives, true negatives, false positives, and false negatives. The total of these numbers will give us p and p . F D In MATLAB, the above description can be easily implemented by sweeping through the range of τ. % MATLAB code to generate an empirical ROC curve load ch9_ROC_example_data 601
CHAPTER 9. CONFIDENCE AND HYPOTHESIS tau = linspace(0,1,1000); for i=1:1000 idx = (scores <= tau(i)); predict = zeros(1,100); predict(idx) = 1; true_positive = 0; true_negative = 0; false_positive = 0; false_negative = 0; for j=1:100 if (predict(j)==1) && (labels(j)==1) true_positive = true_positive + 1; end if (predict(j)==1) && (labels(j)==0) false_positive = false_positive + 1; end if (predict(j)==0) && (labels(j)==1) false_negative = false_negative + 1; end if (predict(j)==0) && (labels(j)==0) true_negative = true_negative + 1; end end PF(i) = false_positive/50; PD(i) = true_positive/50; end plot(PF, PD, ’LineWidth’, 4, ’Color’, [0, 0, 0]); The Python codes of this problem are similar. We give them here for completeness. # Python code to generate an empirical ROC curve import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats scores = np.loadtxt(’ch9_ROC_example_data.txt’) labels = np.append(np.ones(50), np.zeros(50)) tau = np.linspace(0,1,1000) PF = np.zeros(1000) PD = np.zeros(1000) for i in range(1000): idx = scores<= tau[i] predict = np.zeros(100) predict[idx] = 1 true_positive = 0; true_negative = 0 false_positive = 0; false_negative = 0 for j in range(100): if (predict[j]==1) and (labels[j]==1): true_positive += 1 if (predict[j]==1) and (labels[j]==0): false_positive += 1 if (predict[j]==0) and (labels[j]==1): false_negative += 1 if (predict[j]==0) and (labels[j]==0): true_negative += 1 PF[i] = false_positive/50 PD[i] = true_positive/50 plt.plot(PF, PD) 602
9.5. ROC AND PRECISION-RECALL CURVE The empirical ROC curve for this problem is shown in Figure 9.30. Each point on the curve is a coordinate (p ,p ), evaluated at a particular threshold τ. Mathematically, the F D decision rule we used was (cid:40) 1, score(y)≥τ, δ(y)= 0, score(y)<τ. For every τ, we have a false alarm rate and a detection rate. Since this is an empirical dataset with only 100 samples, there are many occasions where p does not change but F p increases, or p stays constant but p increases. For this particular example, we can D D F compute the AUC, which is 0.7948. Figure 9.30: The empirical ROC curve for the dataset fisheriris, using a classifier based on the logistic regression. Note that the empirical ROC is rough. It does not have the smooth concave shape of the theoretical ROC curve. One can prove that if the decision rule is Neyman-Pearson, i.e., if we conduct a likelihood ratio test, then the resulting ROC curve is concave. Otherwise, you can still obtain an empirical ROC curve for real datasets and classifiers. However, the shape is not necessarily concave. 9.5.4 The Precision-Recall (PR) curve Inmoderndatascience,analternativeperformancemetrictotheROCcurveistheprecision- recall (PR) curve. The precision and recall are defined as follows. Definition 9.4. Let TP = true positive, FP = false positive, FN = false negative. The precision is defined as TP p precision= = D , (9.40) TP+FP p +p D F and the recall is defined as TP p recall= = D =p . (9.41) TP+FN p +p D D M 603
CHAPTER 9. CONFIDENCE AND HYPOTHESIS In this definition, TP, FP, and FN are the numbers of samples that are classified as true positive,falsepositive,andfalsenegative,respectively.However,bothprecisionandrecallare definedasratiosofnumbers.Theratioscanbeequivalentlydefinedthroughtherates.Using ourterminology,thisgivesusthedefinitionsintermsofp ,p andp .Sincep =1−p , D F M D M it also holds that the recall is p . D Let us take a moment to consider the meanings of precision and recall. Precision is defined as TP # true positives precision= = . (9.42) TP+FP total # positives you claim The numerator of the precision is the number of true positive samples and the denominator is the total number of positives that you claim. This includes the true positives and the false positives. Therefore, precision measures how trustworthy your claim is. There are two scenarios to consider: • High precision: This means that among all the positives you claim, many of them are the true positives. Therefore, whatever you claim is trustworthy. One possibility for obtaining a high precision is that the critical level α of the Neyman-Pearson decision ruleapproaches1.Inotherwords,youareveryacceptingofthenullhypotheses.Thus, whenever you reject, it will be a reliable reject. • Low precision: This means that you are overclaiming the positives, and so there are many false positives. Thus, even though you claim many positives, not all are trust- worthy. One reason why low precision occurs is that you are too eager to reject the null. Thus you tend to overkill the unnecessary cases. A similar analysis can be applied to the recall. The recall is defined as TP # true positives recall= = . (9.43) TP+FN total # positives in the distribution The difference between the recall and the precision is the denominator. For recall, the denominator is the total number of positives in the distribution. We are not interested in knowing what you have claimed but in knowing how many of them are there in the distribution.Ifyouexaminethedefinitionusingp ,youcanseethatrecallistheprobability D of detection — how successfully you can detect a target. A high recall and a low recall can occur in two situations: • High recall: This means that you are very good at detecting the target or rejecting thenullappropriately.Ahighrecallcanhappenwhenthecriticallevelαislowsothat you never miss a target. However, if the critical level α is low, you will suffer from a low precision. • Low recall: This means that you are too accepting of the null hypotheses, and so you never claim that there is a target. As a result the number of successful detections is low.However,havingalowrecallcanbuyyouhighprecisionbecauseyoudonotreject the null unless it has extreme evidence (hence there is no false alarm.) As you can see from the discussions above, the precision-recall has a trade-off, just as theROCcurvedoes.SincethePRcurveandROCcurvearederivedfromp andp ,there F D isaone-to-onecorrespondence.Thiscanbeprovedbyrearrangingthetermsintheprevious theorem. 604
9.5. ROC AND PRECISION-RECALL CURVE Theorem 9.3. The false alarm rate p and the detection rate p can be expressed F D in terms of the precision and recall as recall(1−precision) p = , (9.44) F precision p =recall. D This result implies that whenever we have an ROC curve we can convert it to a PR curve. Moreover, whenever we have a PR curve we can convert it to an ROC curve. Therefore, there is no additional information one can squeeze out by converting the curves. What we can claim, at most, is that the two curves offer different ways of interpreting the decision rule. To illustrate the equivalence between an ROC curve and a PR curve, we plot two differentdecisionrulesinFigure9.31.AnypointontheROCcurvewillhaveacorresponding point on the PR curve, and vice versa. Figure 9.31: There is a one-to-one correspondence between the ROC curve and the PR curve. The MATLAB and Python codes for generating the PR curve are straightforward. Assuming that we have run the code used to generate Figure 9.28, we plot the PR curve as follows (this will give us Figure 9.31). % MATLAB code to generate a PR curve precision1 = PD1./(PD1+PF1); precision2 = PD2./(PD2+PF2); recall1 = PD1; recall2 = PD2; plot(recall1, precision1, ’LineWidth’, 4); hold on; plot(recall2, precision2, ’LineWidth’, 4); hold off; 605
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Practice Exercise 9.8. Suppose that the decision rule is a blind guess: (cid:40) 1, with probability α, δ(y)= 0, with probability 1−α, Plot the ROC curve and the PR curve. Solution:Aswehaveshownearlier,p (α)andp (α)forthisdecisionrulearep (α)= F D F α and p (α)=α. Therefore, D p α 1 precision= D = = , and recall=p =α. p +p α+α 2 D D F Thus the PR curve is a straight line with a level of 0.5. Figure 9.32: The PR curve of a blind-guess decision rule is a straight line. Practice Exercise 9.9. Convert the ROC curve in Figure 9.30 to a PR curve. Solution:Theconversionisdonebyfirstcomputingp andp .Definingtheprecision F D and recall in terms of p and p , we plot the PR curves below. F D Figure 9.33: The PR curve of a real dataset. 606
9.6. SUMMARY As you can see from the figure, the PR curve behaves very differently from the ROC curve. It is sometimes argued that the two curves can be interpreted differently, even though they describe the same decision rule for the same dataset. 9.6 Summary In this chapter, we have discussed five principles for quantifying the confidence of an esti- matorandmakingstatisticaldecisions.Tosummarizethechapter,weclarifyafewcommon misconceptions about these topics. • Confidence interval. Students frequently become confused about the meaning of a confidence interval. It is not the interval that 95% of the samples will fall inside. It is also not the interval within which the estimator has a 95% chance to show up. A confidence interval is a random interval that has a 95% chance of including the populationparameter.Abetterwaytothinkaboutaconfidenceintervalistothinkof it as an alternative to a point estimate. A point estimate only gives a point, whereas aconfidenceintervalextendsthepointtoaninterval.Alltherandomnessofthepoint estimate is also there in the confidence interval. However, if the confidence interval is narrow, there is a good chance for the point estimate to be accurate. • Bootstrapping. The most common misconception about bootstrapping is that it can createsomethingfromnothing.Anothermisconceptionisthatbootstrappingcanmake your estimates better. Both beliefs are wrong. Bootstrapping is a technique for esti- mating the estimator’s variance, and consequently it provides a confidence interval. Bootstrapping does not improve the point estimate, no matter how many bootstrap- ping samples you synthesize. Bootstrapping works because the sampling with the re- placement step is equivalent to drawing samples from the empirical distribution. The whole process relies on the proximity between the empirical distribution and the true population.Ifyoudonothaveenoughsamplesandtheempiricaldistributiondoesnot approximate the population, bootstrapping will not work. Therefore, bootstrapping does not create something from nothing; it uses whatever you have and tells you how reliable the estimate is. • Hypothesis testing. Students are often overwhelmed at first by the great number of tests one can use for hypothesis testing, e.g., p-value, critical value, Z-test, T-test, χ2 test, F-test, etc. Our advice is to forget about them and remember that hypothesis testing is a court trial. Your job is to decide whether you have enough evidence to declare that the defendant is guilty. To reach a guilty verdict, you need to make sure that the test statistic is unlikely to happen. Therefore, the best practice is to draw the distributions of the test statistic and ask yourself how likely is it that the test statistic has such a value. When you draw the pictures of the distributions, you will know whether you should use a Gaussian Z, a Student’s t, a χ2, a F-statistic, etc. Whenyouexaminethelikelihoodoftheteststatistic,youwillknowwhetheryouwant to use the p-value or the critical value. If you follow this principle, you will never be confused by the oceans of tests you find in the textbooks. 607
CHAPTER 9. CONFIDENCE AND HYPOTHESIS • Neyman-Pearson. Beginners often find Neyman-Pearson abstract and do not under- stand why it is useful. In this chapter, however, we have explained why we need to understand Neyman-Pearson. It is a very general framework for many kinds of hy- pothesistestingproblems.Allitsaysisthatifwewanttomaximizethedetectionrate while maintaining the false alarm rate, then the optimal testing procedure boils down tothecritical-valuetestandthep-valuetest.Thisgivesusacertificatethatourusual hypothesis testing is optimal according to the Neyman-Pearson framework. • ROC and PR curves. On the internet nowadays there is a huge quantity of articles, blogs, and tutorials about how to plot the ROC curve and the PR curve. Often these curvesareexplainedthroughprogrammingexamplessuchasPython,R,orMATLAB. OuradviceforstudyingtheROCcurveandthePRcurveistogobacktotheNeyman- Pearson framework. These two curves do not come out of the blue. The ROC curve is the natural figure explaining the objective and the constraint in the Neyman-Pearson framework. By changing the coordinates, we obtain the PR curve. Therefore, the two curves are the same in terms of the amount of information, but they offer different interpretations. 9.7 Reference Confidence Interval 9-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 9.1. 9-2 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.3. 9-3 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.2. 9-4 Larry Wasserman, All of Statistics, Springer 2003. Chapter 6. 9-5 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 8.4. Bootstrapping 9-6 TrevorHastie,RobertTibshirani,andJeromeFriedman,ElementsofStatisticalLearn- ing, Springer, 2nd Edition. Chapter 8.2. 9-7 Larry Wasserman, All of Statistics, Springer 2003. Chapter 8. 9-8 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.4. 9-9 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.9. 608
9.8. PROBLEMS Hypothesis Testing 9-10 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.5. 9-11 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 8. 9-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 8.5. 9-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 9. 9-14 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.3. 9-15 Larry Wasserman, All of Statistics, Springer 2003. Chapter 10. 9-16 Laura Simon, Introduction to Mathematical Statistics, Penn State University STAT 415Textbook,Online materials.Accessed12/2020. https://online.stat.psu.edu/ stat415/ Neyman-Pearson and ROC curves 9-17 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 8. 9-18 H. Vincent Poor, An Introduction Signal Detection and Estimation, Springer, 1998. 9-19 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation, Springer, 2008. 9-20 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory, Prentice-Hall, 1993. 9-21 StevenM.Kay,FundamentalsofStatisticalSignalProcessing:DetectionTheory,Prentice- Hall, 1998. 9.8 Problems Exercise 1. Consideri.i.d.GaussianrandomvariablesX ,...,X withanunknownmeanθandaknown 1 N varianceσ2 =1.SupposeN =30.Findtheconfidencelevel1−αfortheconfidenceintervals of the mean Θ(cid:98): (a) I =[Θ(cid:98) − 2√.14σ,Θ(cid:98) + 2√.14σ] N N (b) I =[Θ(cid:98) − 1√.85σ,Θ(cid:98) + 1√.85σ] N N 609
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Exercise 2. Suppose that we have conducted an experiment with N = 100 samples. A 95% confidence interval of the mean was 0.45≤µ≤0.82. (a) Woulda99%confidenceintervalcalculatedfromthesampledatabewiderornarrower? (b) Is it correct to interpret the confidence interval as saying that there is a 95% chance thatµisbetween0.49and0.82?Youmayansweryes,no,orpartiallycorrect.Explain. (c) Is it correct to say that if we conduct the experiment 1000 times, there will be 950 confidence intervals that will contain µ? You may answer yes, no, or partially correct. Explain. Exercise 3. Supposethatwehaveconductedanexperiment.Weknowthatσ =25.WeobtainedN =20 samples and found that the sample mean is Θ(cid:98) =1014. (a) Construct a 95% two-sided confidence interval of Θ(cid:98). (b) Construct a 95% one-sided confidence interval (the lower tail) of Θ(cid:98). Exercise 4. Let X 1,...,X N be i.i.d. Gaussian with X n ∼ Gaussian(0,1). Let Y n = eXn, and suppose we have N =100 samples. We want to compute a 95% confidence interval for skewness. (a) Randomly subsample the dataset with B = 30 samples. Repeat the exercise 5 times. Plot the resulting histograms using MATLAB or Python. (b) Repeat(a)forM =500timesandcomputethe95%bootstrappedconfidenceinterval of the skewness. (c) Try using a larger B = 70 and a smaller B = 10. Report the 95% bootstrapped confidence interval of the skewness. Exercise 5. Let X 1,...,X N be i.i.d. uniform with X n ∼ Uniform(0,θ). Let Θ(cid:98) = max{X 1,...,X N}. Generate a dataset of N =50 with θ =1. (a) Find the distribution of the estimator Θ(cid:98). (b) Show that P[Θ(cid:98) =θ]=1−(1−(1/n))N. Thus, as N →∞, we have P[Θ(cid:98) =θ]=0. (c) Use Python or MATLAB to generate the histogram of Θ(cid:98) from bootstrapping. How does the bootstrapped histogram look as N grows? Why? Exercise 6. Let X be a Gaussian random variable with unknown mean and unknown variance. It was found that with N =15, N N (cid:88) (cid:88) X =250, X2 =10000. n n n=1 n=1 610
9.8. PROBLEMS Find a 95% confidence interval of the mean of X. Exercise 7. Let Θ(cid:98) be the sample mean of a dataset containing N samples. It is known that the samples are drawn from Gaussian(θ,32). Find N such that P[Θ(cid:98) −1≤θ ≤Θ(cid:98) +1]=0.95. Exercise 8. Which of the following statements are valid hypothesis testing problems? (a) H : µ=25 and H : µ(cid:54)=25. 0 1 (b) H : σ >10 and H : σ =10. 0 1 (c) H : X =50 and H : X (cid:54)=50. 0 1 (d) H : p-value = 0.1, H : p-value = 0.5. 0 1 Exercise 9. It is claimed that the mean is θ = 12 with a standard deviation 0.5. Consider H : θ = 12 0 and H 1: θ < 12. Ten samples are obtained, and it is found that Θ(cid:98) = 13.5. With a 95% confidence level, should we accept or reject the null hypothesis? Exercise 10. Consider a hypothesis testing problem: H : θ = 175 versus an alternative hypothesis H : 0 1 θ >175. Assume N =10 and σ =20. (a) Find the type 1 error if the critical region is Θ(cid:98) >185. (b) Find the type 2 error if the true mean is 195. Exercise 11. Consider H : θ = 30000 versus an alternative hypothesis H : θ > 30000. Suppose N = 16, 0 1 and let σ =1500. (a) If we want α=0.01, what is z ? α (b) What is the type 2 error when θ =31000? Exercise 12. Let W ∼Gaussian(0,σ2), and consider two hypotheses: n H : X =θ +W , n=1,...,N, 0 n 0 n H : X =θ +W , n=1,...,N. 1 n 1 n Let X =(1/N)(cid:80)N X . n=1 n 611
CHAPTER 9. CONFIDENCE AND HYPOTHESIS (a) Show that the likelihood of observing X ,...,X given H is 1 N 0 (cid:40) N (cid:41) 1 1 (cid:88) f (x|H )= exp − (X −θ )2 . X 0 (2πσ2)N/2 2σ2 n 0 n=1 (b) Find the likelihood f (x|H ) of observing X ,...,X given H . X 1 1 N 1 (c) The likelihood ratio test states that f (x|H ) X 1 ≷H1 τ. f X(x|H 0) H0 Show that the likelihood ratio test is given by θ +θ σ2logτ X ≷H1 0 1 + . H0 2 N(θ 1−θ 0) 612
Chapter 10 Random Processes In modern data science, many problems involve time. The stock market changes every minute;aspeechsignalchangeseverymillisecond;acarchangesitssteeringangleconstantly; the examples are endless. A common theme among all these examples is randomness. We do not know whether a stock will go up or down tomorrow, although we may be able to make some predictions based on previous observations. We do not know the next word of a sentence, but we can guess based on the context. Random processes are tools that can be appliedtothesesituations.Wetreatarandomprocessasaninfinitelylongvectorofrandom variables wherethe correlationsbetween theindividualvariables definethe statisticalprop- erties of the process. If we can determine these correlations, we will be able to summarize the past and predict the future. The objective of this chapter is to introduce the basic concepts of random processes. Given the breadth of the subject, we can only cover the most elementary results, but they are sufficient for many engineering and data science problems. However, there are complex situations for which these elementary results will be insufficient. The references at the end of this chapter contain more in-depth discussions of random processes. Plan of this chapter We begin by outlining the definition of random processes and ways to characterize their randomnessinSection10.1.InSection10.2wediscussthemeanfunction,theautocorrelation function, and the autocovariance function of a random process. In Section 10.3 we look at a special subclass of random processes known as the wide-sense stationary processes. Wide- sense stationary processes allow us to use tools in the Fourier domain to make statistical statements. Based on wide-sense stationary processes, we discuss power spectral density in Section10.4.Withthisconcept,wecanaskwhatwillhappentotherandomprocesswhenwe passitthroughalineartransformation.InSection10.5wediscusssuchinteractionsbetween therandomprocessandalineartime-invariantsystem.Finally,wediscussapracticalusage of random processes in the subject of optimal linear filters in Section 10.6. 613
CHAPTER 10. RANDOM PROCESSES 10.1 Basic Concepts 10.1.1 Everything you need to know about a random process Here is the single most important thing you need to remember about random processes: What is a random process? A random process is a function indexed by a random key. That’s it. Now you may be wondering what exactly a “function indexed by a random key” means. To help you see the picture, we consider two examples. Example 10.1. We consider a set of straight lines. We define two random variables a and b that are uniformly distributed in a certain range. We then define a function: f(t)=at+b, −2≤t≤2. (10.1) Clearly,f(t)isafunctionoftimet.Butsinceaandbarerandom,f(t)isalsorandom. The randomness iscausedby a and b. To emphasize this dependency, we write f(t) as f(t,ξ)=a(ξ)t+b(ξ), −2≤t≤2, where ξ ∈ Ω denotes the random index of the constants (a,b) and Ω is the sample space of ξ. Therefore, by picking a different pair of constants (a(ξ),b(ξ)), we will have a different function f(t,ξ), which in our case is a straight line of different slope and y-intercept. 1 0.5 0 -0.5 -1 -2 -1 0 1 2 t )t(f Figure 10.1: The set of straight lines f(x)=ax+b where a,b∈R. As a special case of the example, suppose that the sample space contains only two pairs of constants: (a,b) = (1.2,0.6) and (a,b) = (−0.75,1.8). The probability of 614
10.1. BASIC CONCEPTS getting either pair is 1. Then the function f(t,ξ) will take two forms: 2 (cid:40) 1.2t+0.6, with probability 1, f(t,ξ)= 2 −0.75t+1.8, with probability 1. 2 Every time you pick a sample you pick one of the two functions, either f(t,ξ ) or 1 f(t,ξ ).Sowesaythatf(t,ξ)isarandomprocessbecauseitisafunctionf(t)indexed 2 by a random key ξ. Example 10.2. This example studies the function f(t)=cos(ω t+Θ), −1≤t≤1, 0 whereΘisarandomphasedistributeduniformlyovertherange[0,2π].Dependingon therandomnessofΘ,thefunctionf(t)willtakeadifferentphaseoffset.Toemphasize this dependency, we write f(t,ξ)=cos(ω t+Θ(ξ)), −1≤t≤1. (10.2) 0 2 1 0 -1 -2 -1 -0.5 0 0.5 1 t )t(f Figure 10.2: The set of phase-shifted cosines f(t)=cos(ω t+θ) where θ∈[0,2π]. 0 Again,ξ denotestheindexoftherandomvariableΘ.SinceΘisdrawnuniformly from the interval [0,2π], the following functions are two possible realizations: (cid:18) (cid:19) 3π f(t,ξ )=cos ω t+ , −1≤t≤1, 1 0 4 (cid:18) (cid:19) 7π f(t,ξ )=cos ω t− , −1≤t≤1. 2 0 3 Just as with the previous example, f(t) is a function indexed by a random key ξ. Thesetwoexamplesshouldgiveyouafeelingforwhattoexpectfromarandomprocess. A random process is quite similar to a random variable because they are both contained in a certain sample space. For (discrete) random variables, the sample space is a collection of outcomes {ξ ,ξ ,...,ξ }. The random variable X : F → R is a mapping that maps 1 2 N ξ to X(ξ ), where X(ξ ) is a number. For random processes, the sample space is also n n n 615
CHAPTER 10. RANDOM PROCESSES {ξ ,ξ ,...,ξ }. However, the mapping X does not map ξ to a number X(ξ ) but to a 1 2 N n n functionX(t,ξ ).Afunctionhasthetimeindext,whichisabsentinthenumber.Therefore, n for the same ξ , X(t ,ξ ) can take one value and X(t ,ξ ) can take another value. n 1 n 2 n Figure 10.3: The sample space of a random process X(t,ξ) contains many functions. Therefore, each random realization is a function. Figure 10.3 shows the sample space of a random process. Each outcome in the sample space is a function. The probability of getting a function is specified by the probability mass or the probability density of the associated random key ξ. If you put your hand into the sample space, the sample you pick will be a function that will change with time and is indexed by the random key. From our discussions of joint random variables in Chapter 5, you can think of the function as a vector. When you pull a sample from the sample space, you pull the entire vector and not just an element. 10.1.2 Statistical and temporal perspectives Sincearandomprocessisafunctionindexedbyarandomkey,itisatwo-dimensionalobject. It is a function both of time t and of the random key ξ. That’s why we use the notation X(t,ξ) to denote a random process. These two axes play different roles, as illustrated in Figure 10.4. Temporal perspective: Let us fix the random key at ξ = ξ . This gives us a function 0 X(t,ξ ). Since ξ is already fixed at ξ , we are looking at a particular realization drawn 0 0 from the sample space. This realization is expressed as a function X(t,ξ ), which is just 0 a deterministic function that evolves over time. There is no randomness associated with it. This is analogous to a random variable. While X itself is a random variable, by fixing the random key ξ = ξ , X(ξ ) is just a real number. For random processes, X(t,ξ ) now 0 0 0 becomes a function. SinceX(t,ξ )isafunctionthatevolvesovertime,weviewitalongthehorizontalaxis. 0 For example, we can study the sequence X(t ,ξ ),X(t ,ξ ),...,X(t ,ξ ), 1 0 2 0 K 0 where t ,...,t are the time indices of the function. This sequence is deterministic and is 1 K just a sequence of numbers, although the numbers evolve as t changes. Statistical perspective: The other perspective, which could be slightly more abstract, is the statistical perspective. Let us fix the time at t=t . The random key ξ can take any 0 616
10.1. BASIC CONCEPTS (a) Temporal perspective (b) Statistical perspective Figure 10.4: Temporal and statistical perspectives of a random process. For the temporal perspective (which we call the horizontal perspective), we fix the random key ξ and look at the function in time. For the statistical perspective (which we call the vertical perspective), we fix the time and look at the function at different random keys. statedefinedinthesamplespace.Soifthesamplespacecontains{ξ ,...,ξ },thesequence 1 N {X(t ,ξ ),...,X(t ,ξ )} is a sequence of random variables, because the ξ’s can go from 0 1 0 N one state to another state. A good way to visualize the statistical perspective is the vertical perspective in which we write the sequence as a vertical column of random variables: X(t ,ξ ) 0 1 X(t ,ξ ) 0 2 . . . X(t ,ξ ) 0 N That is, if you fix the time at t = t , you are getting a sequence of random variables. The 0 probability of getting a particular value X(t ) depends on which random state you land on. 0 Why do we bother to differentiate the temporal perspective and the statistical per- spective? The reason is that the operations associated with the two are different, even if sometimes they give you the same result. For example, if we take the temporal average of the random process, we get a number: 1 (cid:90) T X(ξ)= X(t,ξ)dt. (10.3) T 0 Wecallthisthe“temporalaverage”becausewehaveintegratedthefunctionovertime.The resulting value will not change with time. However, X(ξ) depends on the random key you provide. If you pick a different random realization, X(ξ) will take a different value. So the temporal average is a random variable. On the other hand, if we take the statistical average of the random process, we get (cid:90) E[X(t)]= X(t,ξ)p(ξ)dξ, (10.4) Ω 617
CHAPTER 10. RANDOM PROCESSES where p(ξ) is the PDF of the random key ξ. We call this the statistical average because we have taken the expectation over all possible random keys. The resulting object E[X(t)] is deterministic but a function of time. No matter how you look at the temporal average or the statistical average, they are different with the following exception: that X(ξ)=const and E[X(t)]=const, for example, X(ξ) = E[X(t)] = 0. This happens only for some special (and useful) random processes known as ergodic processes that allow us to approximate the statistical average using the temporal average, with some guarantees derived from the law of large numbers. We will return to this point later. Example 10.3. Let A∼Uniform[0,1]. Define X(t,ξ)=A(ξ)cos(2πt). In this example, the magnitude A(ξ) is a random variable depending on the random key ξ. For example if we draw ξ , perhaps we will get a value A(ξ ) = 0.5. 1 1 Then X(t,ξ ) = 0.5cos(2πt). To take another example, if we draw ξ , we may get 1 2 A(ξ ) = 1. Then X(t,ξ ) = 1cos(2πt). Figure 10.5 shows a few random realizations 2 2 of the cosines. We can look at X(t,ξ) from the statistical and the temporal views. 1 X (t) 1 X (t) 2 0.5 X (t) 3 X (t) 4 X (t) 5 0 -0.5 -1 -2 -1 0 1 2 Figure 10.5: Five different realizations of the random process X(t)=Acos(2πt). • Statistical View: Fix t (for example t=10). In this case, we have X(t,ξ)=A(ξ)cos(2π(10)) =A(ξ)cos(20π), which is a random variable because cos(20π) is a constant. The randomness of X comes from the fact that A(ξ)∼ Uniform[0,1]. • Temporal View: Fix ξ (for example A(ξ)=0.7). In this case, we have X(t,ξ)=0.7cos(2πt), which is a deterministic function of t. 618
10.1. BASIC CONCEPTS Example 10.4. Let A be a discrete random variable with a PMF 1 1 P(A=+1)= and P(A=−1)= . 2 2 We define the function X[n,ξ] = A(ξ)(−1)n. In this example, A can only take two states. If A=+1, then X[n,ξ]=(−1)n. If A=−1, then X[n,ξ]=(−1)n+1. 1.5 1.5 X(n) X(n) 1 2 1 1 0.5 0.5 0 0 -0.5 -0.5 -1 -1 -1.5 -1.5 0 1 2 3 4 5 0 1 2 3 4 5 Figure 10.6: Realizations of the random process X[n]=A(−1)n. ThegraphicalillustrationofthisexampleisshowninFigure 10.6.Again,wecan look at X[n,ξ] from two views. • Statistical View: Fix n, say n=10. Then, (cid:40) (−1)10 =1, with prob1/2, X(ξ)= (−1)11 =−1, with prob1/2, which is a Bernoulli random variable. • Temporal View: Fix ξ. Then, (cid:40) (−1)n, if A=+1, X[n]= (−1)n+1, if A=−1, which is a time series. Inthisexample,weseethatthesamplespaceofX(n,ξ)consistsofonlytwofunctions with probabilities 1 P(X[n]=(−1)n)= , 2 1 P(X[n]=(−1)n+1)= , 2 Therefore, if there is a sequence outside the sample space, e.g., P(cid:0) X[n]=(cid:2) 1 1 1 −1 1 −1 ··· (cid:3)(cid:1) =0 then the probability of obtaining that sequence is 0. 619
CHAPTER 10. RANDOM PROCESSES What do we mean by statistical average and temporal average? • Statistical average: Take the expectation of X(t,ξ) over ξ. This is the vertical average. • Temporal average: Take the expectation of X(t,ξ) over t. This is the horizontal average. • In general, statistical average (cid:54)= temporal average. 10.2 Mean and Correlation Functions Given a random variable, we often want to know the expectation and variance, and often we also want to know the expectation and variance for the random processes. Nevertheless, we need to consider the time axis. In this section, we discuss the mean function and the autocorrelation function. 10.2.1 Mean function Definition 10.1. The mean function µ (t) of a random process X(t) is X µ (t)=E[X(t)]. (10.5) X Let’s consider the “expectation” of X(t). Recall that a random process is actually X(t,ξ) whereξ istherandomkey.Therefore,theexpectationistakenwithrespecttoξ,ortostate it more explicitly, (cid:90) µ (t)=E[X(t)]= X(t,ξ)p(ξ)dξ, X Ω wherep(ξ)isthePDFoftherandomkey.Thisisanabstractdefinition,butitisnotdifficult to understand if you follow the example below. Example 10.5. Let A∼Uniform[0,1], and let X(t)=Acos(2πt). Find µ (t). X Solution. The solution to this problem is actually very simple: µ (t)=E[X(t)]=E[Acos(2πt)] X 1 =cos(2πt)E[A]= cos(2πt). 2 So the answer is µ (t)= 1cos(2πt). X 2 We can link the equations to the definition more explicitly. To do so, we rewrite X(t) as X(t,ξ)=A(ξ)cos(2πt). 620
10.2. MEAN AND CORRELATION FUNCTIONS Then we take the expectation over A: (cid:90) (cid:90) 1 µ (t)= X(t,a)p (a)da= acos(2πt)·1da X A Ω 0 (cid:20) a2(cid:21)1 1 =cos(2πt) = cos(2πt). 2 2 0 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.7: The mean function of X(t)=Acos(2πt). An illustration is provided in Figure 10.7, in which we observe many random realizations of the random process X(t,ξ). On top of these, we also see the mean function. The way to visualize the mean function is to use the statistical perspective. That is, fix a time t and look at all the possible values that the function can take. For example,ifwefixt=t ,thenwewillhaveasetofrealizationsofonerandomvariable: 0 (cid:26) (cid:27) 0.71cos(2πt ), 0.58cos(2πt ), ..., 0.93cos(2πt ) → take expectation 0 0 0 Therefore,whenwetaketheexpectation,itisthatoftheunderlyingrandomvariable. If we move to another timestamp t=t , we will have a different expectation because 1 cos(2πt ) now becomes cos(2πt ). 0 1 The MATLAB/Python codes used to generate Figure 10.7 are shown below. You can also replace the line 0.5*cos(2*pi*t) by the mean function mean(X) (in MATLAB). % MATLAB code for Example 10.5 x = zeros(1000,20); t = linspace(-2,2,1000); for i=1:20 X(:,i) = rand(1)*cos(2*pi*t); end plot(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; plot(t, 0.5*cos(2*pi*t), ’LineWidth’, 4, ’Color’, [0.6 0 0]); # Python code for Example 10.5 x = np.zeros((1000,20)) 621
CHAPTER 10. RANDOM PROCESSES t = np.linspace(-2,2,1000) for i in range(20): x[:,i] = np.random.rand(1)*np.cos(2*np.pi*t) plt.plot(t,x,color=’gray’) plt.plot(t,0.5*np.cos(2*np.pi*t),color=’red’) plt.show() Example 10.6. Let Θ∼Uniform[−π,π], and let X(t)=cos(ωt+Θ). Find µ (t). X Solution. (cid:90) π 1 µ (t)=E[cos(ωt+Θ)]= cos(ωt+θ)· dθ =0. X 2π −π Again, as in the previous example, we can try to map this simple calculation with the definition. Write X(t) as X(t,ξ)=cos(ωt+Θ(ξ)). Then the expectation is (cid:90) µ (t)= cos(ωt+θ)p (θ)dθ X Θ Ω (cid:90) π 1 = cos(ωt+θ)· dθ =0. 2π −π 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.8: The mean function of X(t)=cos(ωt+Θ). Figure 10.8 illustrates the random realizations for X(t) = cos(ωt+Θ) and the meanfunction.Thezeromeanshouldnotbeasurprisebecauseifwetakethestatistical average (the vertical average) across all the possible values at any time instant, the positive and negative values of the realizations will make the mean zero. Weshouldemphasizethatthestatisticalaverageisnotthesameasthetemporal average, even if they give you the same value. Why do we say that? If we calculate the temporal average of the function cos(ωt+θ ) for a specific value Θ=θ , then we 0 0 622
10.2. MEAN AND CORRELATION FUNCTIONS have 1 (cid:90) T X = cos(ωt+θ )dt=0, T 0 0 assuming that T is a multiple of the cosine period. This implies that the temporal average is zero, which is the same as the statistical average. This gives us an example inwhichthestatisticalaverageandthetemporalaveragehavethesamevalue,although we know they are two completely different things. The MATLAB/Python codes used to generate Figure 10.8 are shown below. % MATLAB code for Example 10.6 x = zeros(1000,20); t = linspace(-2,2,1000); for i=1:20 X(:,i) = cos(2*pi*t+2*pi*rand(1)); end plot(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; plot(t, 0*cos(2*pi*t), ’LineWidth’, 4, ’Color’, [0.6 0 0]); # Python code for Example 10.6 x = np.zeros((1000,20)) t = np.linspace(-2,2,1000) for i in range(20): Theta = 2*np.pi*(np.random.rand(1)) x[:,i] = np.cos(2*np.pi*t+Theta) plt.plot(t,x,color=’gray’) plt.plot(t,np.zeros((1000,1)),color=’red’) plt.show() Example10.7.Letusconsideradiscrete-timerandomprocess.LetX[n]=Sn,where S ∼Uniform[0,1]. Find µ [n]. X (cid:90) 1 1 µ [n]=E[sn]= sn ds= . X n+1 0 In this example the randomness goes with the constant s. Thus, if we write X[n] as X[n,ξ]=[S(ξ)]n, the expectation is (cid:90) (cid:90) 1 1 E[X[n]]= snp (s)ds= sn·1ds= . S n+1 Ω 0 The graphical illustration is provided in Figure 10.9. 623
CHAPTER 10. RANDOM PROCESSES 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 0 5 10 15 20 Figure 10.9: The mean function of X[n]=Sn, where S ∼Uniform[0,1]. TheMATLABcodeusedtogenerateFigure 10.9isshownbelow.WeskipthePython implementation because it is straightforward. % MATLAB code for Example 10.7 t = 0:20; for i=1:20 X(:,i) = rand(1).^t; end stem(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; stem(t, 1./(t+1), ’LineWidth’, 2, ’MarkerSize’, 8); 10.2.2 Autocorrelation function Inrandomprocesses,thenotionsof“variance”and“covariance”aretrickierthanforrandom variables. Let us first define the concept of an autocorrelation function. Definition 10.2. The autocorrelation function of a random process X(t) is R (t ,t )=E[X(t )X(t )]. (10.6) X 1 2 1 2 R (t ,t ) is not difficult to calculate — just integrate X(t )X(t ) using the appropriate X 1 2 1 2 PDFs. Example 10.8. Let A∼Uniform[0,1], X(t)=Acos(2πt). Find R (t ,t ). X 1 2 Solution. R (t ,t )=E[Acos(2πt )Acos(2πt )] X 1 2 1 2 1 =E[A2]cos(2πt )cos(2πt )= cos(2πt )cos(2πt ). 1 2 3 1 2 624
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.9. Let Θ∼Uniform[−π,π], X(t)=cos(ωt+Θ). Find R (t ,t ). X 1 2 Solution. R (t ,t )=E[cos(ωt +Θ)cos(ωt +Θ)] X 1 2 1 2 1 (cid:90) π = cos(ωt +θ)cos(ωt +θ)dθ 2π 1 2 −π (a) 1 (cid:90) π 1(cid:20) (cid:21) = cos(ω(t +t )+2θ)+cos(ω(t −t )) dθ 2π 2 1 2 1 2 −π 1 (cid:16) (cid:17) = cos ω(t −t ) , 2 1 2 where in (a) we applied the trigonometric formula: 1 cosAcosB = [cos(A+B)+cos(A−B)], 2 Asyoucansee,thecalculationsarenotdifficult.Thetrickythingistheinterpretation of R (t ,t ). X 1 2 How do we understand the meaning of E[X(t )X(t )]? 1 2 E[X(t )X(t )] is analogous to the correlation E[XY] between two random variables 1 2 X and Y. The autocorrelation function E[X(t )X(t )] is analogous to the correlation E[XY] in rela- 1 2 tion to a pair of random variables. In our discussions of E[XY], we mentioned that E[XY] couldberegardedastheinnerproductoftwovectors,andsoitisameasureofthecloseness between X and Y. Now, if we substitute X and Y with X(t ) and X(t ) respectively, then 1 2 weareeffectivelyaskingabouttheclosenessbetweenX(t )andX(t ).So,inanutshell,the 1 2 autocorrelation function tells us the correlation between the function at two different time stamps. What do we mean by the correlation between two timestamps? Remember that X(t ) 1 and X(t ) are two random variables. Consider the following example. 2 Example10.10.LetX(t)=Acos(2πt),whereA∼Uniform[0,1].FindE[X(0)X(0.5)]. Solution. If X(t)=Acos(2πt), then X(0)=Acos(0)=A, X(0.5)=Acos(π)=−A. When you have two random variables, you consider their correlations. Using this ex- 625
CHAPTER 10. RANDOM PROCESSES ample, we have that E[X(0)X(0.5)]=−E[A·A] 1 =−E[A2]=− . 3 A picture will reveal what is happening. Figure 10.10 presents the realizations of the random process X(t) = Acos(2πt). If we consider X(0) and X(0.5), each of them is a random variable, and thus we can ask about their PDFs. It is obvious from the illustration that the random variable X(0) has a PDF that is a uniform distribution from 0 to 1, whereastherandomvariableX(0.5)hasaPDFthatisauniformdistributionfrom−1to0. Mathematically, the PDFs are (cid:40) (cid:40) 1, 0≤x≤1, 1, −1≤x≤0, f (x)= and f (x)= X(0) X(0.5) 0, otherwise 0, otherwise. Since X(0) and X(0.5) have their own PDFs, we can calculate their correlation. This will give us E[X(0)X(0.5)] which after some calculations is E[X(0)X(0.5)]=−1. 3 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.10: The autocorrelation between X(0) and X(0.5) should be regarded as the correlation between two random variables. Each random variable has its own PDF. We can now consider the autocorrelation for any t and t . When you are evaluating 1 2 the autocorrelation function, you are not just evaluating at t = 0 and t = 0.5, you are also evaluating the correlation for all pairs of t and t . Now you want to know what the 1 2 correlation is between t = 0 and t = 0.5, t = 2 and t = 3.1, etc. Of course, there are infinitely many pairs of time instants. The point of the autocorrelation function is to tell youthecorrelationof allthepairs.Inotherwords,ifwetellyouR (t ,t ),youwillbeable X 1 2 to plug in a value of t and a value of t and tell us the correlation at (t ,t ). How is this 1 2 1 2 possible? To find out, let’s consider the following example. 626
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.11.LetA∼Uniform[0,1],X(t)=Acos(2πt).FindR (0,0.5),anddraw X R (t ,t ). X 1 2 Solution. From the previous example, we know that 1 R (t ,t )= cos(2πt )cos(2πt ). X 1 2 3 1 2 Therefore, R (0,0.5) = 1cos(2π0)cos(2π0.5) = −1, which is the same as if we had X 3 3 computed it from the first principle. Theautocorrelationfunctiontellsyouhowonepointofatimeseriesiscorrelated withanotherpointofthetimeseries.IfR (t ,t )givesahighvalue,thenitmeansthe X 1 2 random variables at t and t have a strong correlation. To understand this, suppose 1 2 we let t =0, and let us vary t . Then 1 2 1 R (0,t )= cos(2π0)cos(2πt ) X 2 3 2 1 = cos(2πt ). 3 2 This is a periodic function that cycles through itself whenever t is an integer. As 2 we recall from Figure 10.10, if t = 0.5, the random variable X(t ) will take only 2 2 the negative values, but otherwise it is correlated with X(0). On the other hand, if t =0.25, then Figure 10.10 says that the random variable X(t ) is a constant 0, and 2 2 so the correlation with X(0) is zero. Clearly, R (t ,t ) is a 2-dimensional function of t and t . You need to tell R X 1 2 1 2 X which of the two time instants you want to compare, and then R will tell you the X correlation. So no matter what happens, you must specify two time instants. Because R (t ,t )isa2-dimensionalfunction,wecanvisualizeitbycalculatingallthepossible X 1 2 values it takes. For example, if R (t ,t )= 1cos(2πt )cos(2πt ), we can plot R as X 1 2 3 1 2 X a function of t and t . Figure 10.11 shows the plot. 1 2 -1 1 -0.75 -0.5 0.5 -0.25 0 0 0.25 -0.5 0.5 0.75 -1 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 t 1 t 2 Figure 10.11: The autocorrelation function R (t ,t )= 1cos(2πt )cos(2πt ). X 1 2 3 1 2 627
CHAPTER 10. RANDOM PROCESSES The MATLAB/Python code for Figure 10.11 is shown below. % MATLAB code for Example 10.11 t = linspace(-1,1,1000); R = (1/3)*cos(2*pi*t(:)).*cos(2*pi*t); imagesc(t,t,R); # Python code for Example 10.11 import numpy as np import matplotlib.pyplot as plt t = np.linspace(-1,1,1000) R = (1/3)*np.outer(np.cos(2*np.pi*t), np.cos(2*np.pi*t)) plt.imshow(R, extent=[-1, 1, -1, 1]) plt.show() To understand the 2D function shown on the right hand side of Figure 10.11, we can take a closer look by drawing Figure 10.12. For any two time instants t and t , we have 1 2 two random variables X(t ) and X(t ). The joint expectation E[X(t )X(t )] will return us 1 2 1 2 some value, and this is a point in the 2D plot R (t ,t ). The value tells us the correlation X 1 2 between X(t ) and X(t ). In the example in which t =0 and t =0.5, the correlation is 1 2 1 2 −1. Interestingly, if we pick another pair of time instants t = −0.5 and t = 0, the joint 3 1 2 expectation is E[X(−0.5)X(0)]=−1, which is the same value. However, this −1 is located 3 3 at a different valley than E[X(0)X(0.5)] in the 2D plot. Figure 10.12: To understand the autocorrelation function, pick two time instants t and t , and then 1 2 evaluate the joint expectation E[X(t )X(t )]. 1 2 The above example shows a periodic autocorrelation function. The fact that it is peri- odic is coincidental because the random process X(t) is a periodic function. In general, an arbitraryrandomprocesscanhaveanarbitraryautocorrelationfunctionthatisnotperiodic. There are, of course, various properties of the autocorrelation functions and special types of autocorrelation functions. We will study one of them, called the wide-sense stationary processes, later. 628
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.12. Let Θ∼Uniform[−π,π], X(t)=cos(ωt+Θ). Draw the autocorrela- tion function R (t ,t ). X 1 2 Solution. From the previous example we know that 1 (cid:16) (cid:17) R (t ,t )= cos ω(t −t ) . X 1 2 2 1 2 Figure 10.13 shows the realizations, and the mean and autocorrelation functions. Note that the autocorrelation function has a structure: Every row is a shifted versionofthepreviousrow.WecallthisaToeplitzstructure.Anautocorrelationwith aToeplitzstructureisspecifiedonceweknowanyoftherows.AToeplitzstructurealso implies that the autocorrelation function does not depend on the pair (t ,t ) but only 1 2 onthedifferencet −t .Inotherwords,R (0,1)isthesameasR (11.6,12.6),andso 1 2 X X knowing R (0,1) is enough to know all R (t ,t +t). Not all random processes have X X 0 0 aToeplitzautocorrelationfunction.RandomprocesseswithaToeplitzautocorrelation function are “nice” processes that we will study in detail later. -1 1 -0.75 -0.5 0.5 -0.25 0 0 0.25 -0.5 0.5 0.75 -1 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 t 1 t 2 (cid:16) (cid:17) Figure 10.13: The autocorrelation function R (t ,t )= 1cos ω(t −t ) . X 1 2 2 1 2 The MATLAB code used to generate Figure 10.13 is shown below. % MATLAB code for Example 10.12 t = linspace(-1,1,1000); R = Toeplitz(0.5*cos(2*pi*t(:))); imagesc(t,t,R); grid on; xticks(-1:0.25:1); yticks(-1:0.25:1); 629
CHAPTER 10. RANDOM PROCESSES Practice Exercise 10.1. Let Θ∼Uniform[0,2π], X(t)=cos(ωt+Θ). Find the PDF of X(0). Solution. Let Z =X(0)=cosΘ. Then the CDF of Z is F (z)=P[Z ≤z] Z =P[cosΘ≤z] =P[cos−1z ≤Θ≤2π−cos−1z] cos−1z =1− . π Then by the fundamental theorem of calculus, 1 f (z)= √ . Z π 1−z2 A similar concept to the autocorrelation function is the autocovariance function. The idea is to remove the mean before computing the correlation. This is analogous to the covariance Cov(X,Y)=E[(X −µ )(Y −µ )] as opposed to the correlation E[XY] in the X Y random variable case. Definition 10.3. The autocovariance function of a random process X(t) is C (t ,t )=E[(X(t )−µ (t ))(X(t )−µ (t ))]. (10.7) X 1 2 1 X 1 2 X 2 As one might expect, the autocovariance function is closely related to the autocorrelation function. Theorem 10.1. C (t ,t )=R (t ,t )−µ (t )µ (t ). (10.8) X 1 2 X 1 2 X 1 X 2 Proof. Plugging in the definition, we have that C (t ,t )=E[X(t )X(t )−X(t )µ (t )−X(t )µ (t )+µ (t )µ (t )] X 1 2 1 2 1 X 2 2 X 1 X 1 X 2 =R (t ,t )−µ (t )µ (t )−µ (t )µ (t )+µ (t )µ (t ) X 1 2 X 1 X 2 X 1 X 2 X 1 X 2 =R (t ,t )−µ (t )µ (t ). (cid:3) X 1 2 X 1 X 2 Practice Exercise 10.2. If X(t)=Acos(2πt) for A∼Uniform[0,1], find C (t ,t ). X 1 2 630
10.2. MEAN AND CORRELATION FUNCTIONS Solution. 1 1 1 C (t ,t )= cos(2πt )cos(2πt )− cos(2πt )· cos(2πt ) X 1 2 3 1 2 2 1 2 2 1 = cos(2πt )cos(2πt ). 12 1 2 Practice Exercise 10.3. Suppose X(t)=cos(ωt+Θ) for Θ∼Uniform[−π,π]. Find C (t ,t ). X 1 2 Solution. C (t ,t )=R (t ,t )−µ (t )µ (t ) X 1 2 X 1 2 X 1 X 2 (cid:18) (cid:19) (cid:18) (cid:19) 1 1 = cos ω(t −t ) −0·0= cos ω(t −t ) . 2 1 2 2 1 2 In some problems we are interested in the correlation between two random processes X(t) and Y(t). This gives us the cross-correlation and the cross-covariance functions. Definition 10.4. The cross-correlation function of X(t) and Y(t) is R (t ,t )=E[X(t )Y(t )]. (10.9) X,Y 1 2 1 2 Definition 10.5. The cross-covariance function of X(t) and Y(t) is C (t ,t )=E[(X(t )−µ (t ))(Y(t )−µ (t ))]. (10.10) X,Y 1 2 1 X 1 2 Y 2 Remark. If µ (t )=µ (t )=0, then C (t ,t )=R (t ,t )=E[X(t )Y(t )]. X 1 Y 2 X,Y 1 2 X,Y 1 2 1 2 10.2.3 Independent processes Howdoweestablishindependencefortworandomprocesses?Weknowthatfortworandom variables to be independent, the joint PDF can be written as a product of two PDFs: f (x,y)=f (x)f (y). (10.11) X,Y X Y If we extrapolate this idea to random processes, a natural formulation would be f (x,y)=f (x)f (y). (10.12) X(t),Y(t) X(t) Y(t) But this definition has a problem because X(t) and Y(t) are functions. It is not enough to just look at one time index, say t=t . The way to think about this situation is to consider 0 a pair of random vectors X and Y. When you say X and Y are independent, you require f (x,y) = f (x)f (y). The PDF f (x) itself is a joint distribution, i.e., f (x) = X,Y X Y X X f (x ,...,x ). Therefore, for random processes, we need something similar. X1,...,XN 1 N 631
CHAPTER 10. RANDOM PROCESSES Definition 10.6. Two random processes X(t) and Y(t) are independent if for any t ,...,t , 1 N f (x ,...,x ,y ,...,y ) X(t1),...,X(tN),Y(t1),...,Y(tN) 1 N 1 N =f (x ,...,x )×f (y ,...,y ). X(t1),...,X(tN) 1 N Y(t1),...,Y(tN) 1 N This definition is reminiscent of f (x,y) = f (x)f (y). The requirement here is that X,Y X Y the factorization holds for any N, including very small N and very large N, because X(t) and Y(t) are infinitely long. Independence means that the behavior of one process will not influence the behavior of the other process. We define uncorrelated as follows. Definition 10.7. Two random processes are X(t) and Y(t) uncorrelated if E[X(t )Y(t )]=E[X(t )]E[Y(t )], (10.13) 1 2 1 2 Independence implies uncorrelation, as we can see from the following. If X(t) and Y(t) are independent, it follows that (cid:90) E[X(t )Y(t )]= X(t ,ξ)Y(t ,ζ)f (ξ,ζ)dξ dζ 1 2 1 2 X,Y (cid:90) = X(t ,ξ)Y(t ,ζ)f (ξ)f (ζ)dξ dζ, independence 1 2 X Y (cid:90) (cid:90) = X(t ,ξ)f (ξ)dξ Y(t ,ζ)f (ζ)dζ =E[X(t )]E[Y(t )]. 1 X 2 Y 1 2 If two random processes are uncorrelated, they are not necessarily independent. ⇒ Independent X and Y uncorrelated X and Y (cid:58) Example 10.13. Let Y(t) = X(t)+N(t), where X(t) and N(t) are independent. Then R (t ,t )=E[X(t )Y(t )]=E[X(t )(X(t )+N(t ))] X,Y 1 2 1 2 1 2 2 =R (t ,t )+µ (t )µ (t ). X 1 2 X 1 N 2 10.3 Wide-Sense Stationary Processes 632
10.3. WIDE-SENSE STATIONARY PROCESSES As we have seen in the previous sections, some random processes have a “nice” autocor- relation function, in the sense that the 2D function R (t ,t ) has a Toeplitz structure. X 1 2 Randomprocesseswiththispropertyareknownaswide-sense stationary(WSS)processes. WSS processes belong to a very small subset in the entire universe of random processes, but they are practically the most useful ones. Before we discuss how to use them, we first present a formal definition of a WSS process.1 10.3.1 Definition of a WSS process Definition 10.8. A random process X(t) is wide-sense stationary if: 1. µ (t)=constant, for all t, and X 2. R (t ,t )=R (t −t ) for all t ,t . X 1 2 X 1 2 1 2 There are two criteria that define a WSS process. The first criterion is that the mean is a constant.Thatis,themeanfunctiondoesnotchangewithtime.Thesecondcriterionisthat the autocorrelation function only depends on the difference t −t and not on the absolute 1 2 starting point. For example, R (0.1,1.1) needs to be the same as R (6.3,7.3), because the X X intervals are both 1. How can these two criteria be mapped to the Toeplitz structure we discussed in the previous examples? Figure 10.14 shows the autocorrelation function R (t ,t ), which is a X 1 2 2D function. We take three cross sections corresponding to t =−0.13, t =0 and t =0.3. 2 2 2 Asyoucanseefromthefigure,eachR (t ,t )isashiftedversionofanotherone.Toobtain X 1 2 any value R (t ,t ) on the function, there is no need to probe to the 2D map; you only X 1 2 need to probe to the red curve and locate the position marked as t −t , and you will be 1 2 able to obtain the value R (t ,t ). X 1 2 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 t 1 t 2 1 t = -0.13 2 t = 0 2 0.5 t = 0.3 2 0 -0.5 -1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 t 1 (cid:16) (cid:17) Figure 10.14: Cross sections of the autocorrelation function R (t ,t )= 1cos ω(t −t ) . X 1 2 2 1 2 Not all random processes have a Toeplitz autocorrelation function. For example, the random process X(t)=Acos(2πt) is not a WSS process, because the autocorrelation func- 1Manytextbooksintroducestrictlystationaryprocessesbeforediscussingawide-sensestationaryprocess. Weskiptheformerbecause,throughoutourbook,weonlyuseWSSprocesses.Readersinterestedinstrictly stationaryprocessescanconsultthereferenceslistedattheendofthischapter. 633
CHAPTER 10. RANDOM PROCESSES tion is 1 R (t ,t )= cos(2πt )cos(2πt ), X 1 2 3 1 2 which cannot be written as the difference t −t . 1 2 Remark 1. WSS processes can also be defined using the autocovariance function instead of the autocorrelation function, because if a process is WSS, then the mean function is a constant. If the mean function is a constant, then C (t ,t ) = R (t ,t ) − µ2. So any X 1 2 X 1 2 geometric structure that R possesses will be translated to C , as the constant µ2 will not X X influence the geometry. Therefore, it is equally valid to say that a WSS process has C (t ,t )=C (t −t ). X 1 2 X 1 2 Remark 2. Because a WSS is completely characterized by the difference t −t , there is 1 2 no need to keep track of the absolute indices t and t . We can rewrite the autocorrelation 1 2 function as R (τ)=E[X(t+τ)X(t)]. (10.14) X There is nothing new in this equation: It only says that instead of writing R (t+τ,t), we X can write R (τ) because the time index t plays no role in terms of R . Thus from now on, X X for any WSS processes we will write the autocorrelation function as R (τ). X 10.3.2 Properties of R (τ) X When X(t) is WSS, R (τ) has several important properties. X Corollary 10.1. R (0)= average power of X(t). X Proof. Since R (0)=E[X(t+0)X(t)]=E[X(t)2], X and since E[X(t)2] is the average power, R (0) is the average power of X(t). (cid:3) X Corollary 10.2. R (τ) is symmetric. That is, R (τ)=R (−τ). X X X Proof.NotethatR (τ)=E[X(t+τ)X(t)].Byswitchingtheorderofmultiplicationinthe X expectation, we have E[X(t+τ)X(t)]=E[X(t)X(t+τ)]=R (−τ). X (cid:3) Corollary 10.3. 2(R (0)−R (τ)) P(|X(t+τ)−X(τ)|>(cid:15))≤ X X . (cid:15)2 634
10.3. WIDE-SENSE STATIONARY PROCESSES This result says that if R (τ) is slowly decaying from R (0), the probability of having a X X large deviation |X(t+τ)−X(τ)| is small. Proof. P(|X(t+τ)−X(τ)|>(cid:15))≤E[(X(t+τ)−X(τ))2]/(cid:15)2 (cid:16) (cid:17) = E[X(t+τ)2]−2E[X(t+τ)X(t)]+E[X(t)2] /(cid:15)2 (cid:16) (cid:17) = 2E[X(t)2]−2E[X(t+τ)X(t)] /(cid:15)2 (cid:16) (cid:17) =2 R (0)−R (τ) /(cid:15)2. X X (cid:3) Corollary 10.4. |R (τ)|≤R (0), for all τ. X X Proof. By Cauchy’s inequality E[XY]2 ≤E[X2]E[Y2], we can show that R (τ)2 =E[X(t)X(t+τ)]2 X ≤E[X(t)2]E[X(t+τ)2] =E[X(t)2]2 =R (0)2. X (cid:3) 10.3.3 Physical interpretation of R (τ) X How should we understand the autocorrelation function R (τ) for WSS processes? Cer- X tainly, by definition, R (τ)=E[X(t+τ)X(t)] means that we can analyze R (τ) from the X X statistical perspective. But in this section we want to take a slightly different approach by answering the question from a computational perspective. Consider the following function: 1 (cid:90) T def R(cid:98)X(τ) = 2T X(t+τ)X(t)dt. (10.15) −T ThisfunctionisthetemporalaverageofX(t+τ)X(t),asopposedtothestatisticalaverage. Why do we want to consider this temporal average? We first show the main result, that E[R(cid:98)X(τ)]=R X(τ). Lemma 10.1. Let R(cid:98)X(τ)d =ef 21 T (cid:82) −T T X(t+τ)X(t)dt. Then (cid:104) (cid:105) E R(cid:98)X(τ) =R X(τ). (10.16) Proof. (cid:104) (cid:105) 1 (cid:90) T E R(cid:98)X(τ) = 2T E[X(t+τ)X(t)] dt −T 1 (cid:90) T 1 (cid:90) T = R (τ)dt=R (τ) dt=R (τ). 2T X X 2T X −T −T 635
CHAPTER 10. RANDOM PROCESSES (cid:3) This lemma implies that if the signal X(t) is long enough, we can approximate R (τ) X by R(cid:98)X(τ). The approximation is asymptotically consistent, in the sense that E[R(cid:98)X(τ)] = R X(τ). Now, the more interesting question is the interpretation of R(cid:98)X(τ). What is it? How should we understand R(cid:98)X(τ)? R(cid:98)X(τ) is the “unflipped convolution”, or correlation, of X(τ) and X(t+τ). Correlation is analogous to convolution. For convolution, the definition is (cid:90) T Y(τ)= X(t−τ)X(t)dt, (10.17) −T whereas for correlation, the definition is (cid:90) T Y(τ)= X(t+τ)X(t)dt. (10.18) −T Clearly, R(cid:98)X(τ) is the latter. A graphical illustration of the difference between convolution andcorrelationisprovidedinFigure 10.15.Theonlydifferencebetweenthetwoisthatthe correlation does not flip the function, whereas the convolution does flip the function. 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 -0.1 -0.1 -10 -5 0 5 10 -10 -5 0 5 10 (a) Convolution (b) Correlation Figure 10.15: Thedifferencebetweenconvolutionandcorrelation.Inconvolution,thefunctionX(t)is flipped before we compute the result. For correlation, the function is not flipped. Thetemporalcorrelationiseasytovisualize.StartingwiththefunctionX(t+τ),ifyou makeτ largerorsmaller,theneffectivelyyouareshiftingX(t)leftorright.Theintegration (cid:82)T X(t+τ)X(t) dt calculates the energy accumulated. If the integral is large, there is a −T strong correlation between X(t) and X(t+τ). Otherwise the correlation is small. Here is an extreme example: 636
10.3. WIDE-SENSE STATIONARY PROCESSES Example 10.14. Consider a random process X(t) such that for every t, X(t) is an i.i.d. Gaussian random variable with zero mean and unit variance. Then (cid:40) E[X2(t)], τ =0, R (τ)=E[X(t+τ)X(t)]= X E[X(t+τ)]E[X(t)], τ (cid:54)=0. Using the fact that X(t) is i.i.d. Gaussian for all t, we can show that E[X2(t)]=1 for any t, and E[X(t+τ)]E[X(t)]=0. Therefore, we have (cid:40) 1, τ =0, R (τ)= X 0. τ (cid:54)=0. The equation says that since the random process is i.i.d. Gaussian, shifting and in- tegrating will give maximum correlation at the origin. As soon as the shift is not at the origin, the correlation is zero. This makes sense because the samples are just i.i.d. Gaussian. One pixel offset is enough to destroy any correlation. Now let’s calculate the temporal correlation. We know that 1 (cid:90) T R(cid:98)X(τ)= 2T X(t+τ)X(t)dτ. −T This equation says that we shift X(t) to the left and right and then integrate. If τ is not zero, the product X(t+τ)X(t) will sometimes be positive and sometimes be negative. After integrating the entire period, we cancel out most of the terms. Let’s plot the functions and see if all these steps make sense. In Figure 10.16(a), we show two random realizations of the random process X(t). They are just i.i.d. Gaussian samples. In Figure 10.16(b) we plot the temporal autocorrelation function R(cid:98)X(τ). Since R(cid:98)X(τ) itself is a random process, it has different realizations. We plot two random realizations, which are computed based on shifting and integrating X(t). In the same plot, we also show the statistical expectation R (τ). As we can see from the plot, X the temporal correlation and the statistical correlation match reasonably well except for the fluctuation in R(cid:98)X(τ), which is expected because it is computed from a finite number of samples. 637
CHAPTER 10. RANDOM PROCESSES 4 1.2 correlation of sample 1 3 correlation of sample 2 1 auto-correlation function 2 0.8 1 0.6 0 0.4 -1 -2 0.2 -3 0 -4 -0.2 0 200 400 600 800 1000 0 500 1000 1500 2000 (a) X(t) (b) R(cid:98)X(τ) Figure 10.16: (a)ArandomprocessX(t)withtwodifferentrealizations.(b)Aswecalculatethe temporal correlation of each of the two realizations, we obtain a noisy function that is nearly an impulse. If we take the average of many of these realizations, we obtain a pure delta function. On a computer, the commands to do the autocorrelation function are xcorr in MAT- LAB and np.correlate in Python. Below are the codes used to generate Figure 10.16. % MATLAB code to demonstrate autocorrelation N = 1000; % number of sample paths T = 1000; % number of time stamps X = 1*randn(N,T); xc = zeros(N,2*T-1); for i=1:N xc(i,:) = xcorr(X(i,:))/T; end plot(xc(1,:),’b:’, ’LineWidth’, 2); hold on; plot(xc(2,:),’k:’, ’LineWidth’, 2); # Python code to demonstrate autocorrelation N = 1000 T = 1000 X = np.random.randn(N,T) xc= np.zeros((N,2*T-1)) for i in range(N): xc[i,:] = np.correlate(X[i,:],X[i,:],mode=’full’)/T plt.plot(xc[0,:],’b:’) plt.plot(xc[1,:],’k:’) plt.show() Under what conditions will R(cid:98)X(τ)→R X(τ) as T →∞? The answer to this question is provided by an important theorem called Mean-Square Ergodic Theorem, which can be thought of as the random process version of the weak law of large numbers. We leave the discussion of the mean ergodic theorem to the Appendix. 638
10.4. POWER SPECTRAL DENSITY Everything you need to know about a WSS process • The mean of a WSS process is a constant (does not need to be zero) • Thecorrelationfunctiononlydependsonthedifference,soR (t ,t )isToeplitz. X 1 2 • You can write R (t ,t ) as R (τ), where τ =t −t . X 1 2 X 1 2 • R (τ) tells you how much correlation you have with someone located at a time X instant τ from you. 10.4 Power Spectral Density BeginningwiththissectionwearegoingtofocusonWSSprocesses.ByWSS,wemeanthat the autocorrelation function R (t ,t ) has a Toeplitz structure. Putting it in other words, X 1 2 we assume R (t ,t ) can be simplified to R (τ), where τ = t −t . We call this property X 1 2 X 1 2 time invariance. 10.4.1 Basic concepts AssumingthatR (τ)issquareintegrable,i.e.,(cid:82)∞ R (τ)2 dτ <∞,wecannowdefinethe X −∞ X Fourier transform of R (τ) which is called the power spectral density. X Theorem 10.2 (Einstein-Wiener-Khinchin Theorem). The power spectral density S (ω) of a WSS process is X (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ =F(R (τ)), X X X −∞ assuming that (cid:82)∞ R (τ)2 dτ <∞ so that the Fourier transform of R (τ) exists. −∞ X X Practice Exercise 10.4. Let R (τ)=e−2α|τ|. Find S (ω). X X Solution. Using the Fourier transform table, 4α S (ω)=F{R (τ)}= . X X 4α2+ω2 Figure 10.17 shows the autocorrelation function and the power spectral density. 639
CHAPTER 10. RANDOM PROCESSES 1 1 R ( ) S ( ) 0.8 X 0.8 X 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -2 -1 0 1 2 -10 -5 0 5 10 Figure 10.17: Example for R (τ)=e−2α|τ|, with α=1. X Why is Theorem 10.2 a theorem rather than a definition? This is because power spectral densityhasitsdefinition.Thereisnowaythatyoucangetany“power”informationmerely bylookingattheFouriertransformofR (τ).Wewilldiscusstheoriginofthepowerspectral X density later, but for now, we only need to know that S (ω) is the Fourier transform of X R (τ). X Remark. The power spectral density is defined for WSS processes. If the process is not WSS, then R will be a 2D function instead of a 1D function of τ, so we cannot take the X Fourier transform in τ. We will discuss this in detail shortly. PracticeExercise10.5.LetX(t)=acos(ω t+Θ), Θ∼Uniform[0,2π].FindS (ω). 0 X Solution. We know that the autocorrelation function is a2 R (τ)= cos(ω τ) X 2 0 a2 (cid:18) ejω0τ +e−jω0τ(cid:19) = . 2 2 By taking the Fourier transform of both sides, we have a2 (cid:20) 2πδ(ω−ω )+2πδ(ω+ω )(cid:21) S (ω)= 0 0 X 2 2 πa2 = [δ(ω−ω )+δ(ω+ω )]. 2 0 0 The result is shown in Figure 10.18. 0.5 2 R ( ) S ( ) X X 1.5 0 1 0.5 -0.5 0 -2 -1 0 1 2 -10 -5 0 5 10 640
10.4. POWER SPECTRAL DENSITY Figure 10.18: Example for R (τ)= a2 cos(ω τ), with a=1 and ω =2π. X 2 0 0 Practice Exercise 10.6. Let S X(ω)= N 20rect( 2ω W). Find R X(τ). Solution. Since S (ω)=F(R (τ)), the inverse holds: X X N W R (τ)= 0 sinc(Wτ). X 2 π This example shows what we call the bandlimited white noise. The power spectral density S (ω) is uniform, meaning that it covers all frequencies (or wavelengths in X optics). It is called “white noise” because white light is essentially a mixture of all wavelengths. The bandwidth of the power spectral density W defines the zero crossings of R (τ). It is easy to show that when W → ∞, R (τ) converges to a delta function. X X ThishappenswhenX(t)isi.i.d.Gaussian.Therefore,thepureGaussiannoiserandom process is also known as the white noise process. Reshaping the i.i.d. Gaussian noise toanarbitrarypowerspectraldensitycanbedonebypassingitthroughalinearfilter, as we will explain later. 2 1.5 R ( ) S ( ) 1.5 X X 1 1 0.5 0.5 0 0 -0.5 -0.5 -2 -1 0 1 2 -10 -5 0 5 10 Figure 10.19: Example for S (ω)= N0rect( ω ), with N =2 and W =5. X 2 2W 0 Finding S (ω) from R (τ) is straightforward, at least in principle. The more inter- X X esting questions to ask are: (1) Why do we need to learn about power spectral density? (2) Why do we need WSS to define power spectral density? How is power spectral density useful? • Powerspectraldensitiesareusefulwhenwepassarandomprocessthroughsome linear operations, e.g., convolution, running average, or running difference. • Power spectral densities are the Fourier transforms of the autocorrelation func- tions. Fourier transforms are useful for speeding up computation and drawing random samples from a given power spectral density. Arandomprocessitselfisnotinterestinguntilweprocessit;therearemanywaystodo this.Themostbasicoperationistosendtherandomprocessthroughalineartime-invariant system, e.g., a convolution. Convolution is equivalent to filtering the random process. For example, if the input process contains noise, we can design a linear time-invariant filter to 641
CHAPTER 10. RANDOM PROCESSES denoise the random process. The power spectral density, which is the Fourier transform of the autocorrelation function, makes the filtering easier because everything can be done in the spectral (Fourier) domain. Moreover, we can analyze the performance and quantify the limit using standard results in Fourier analysis. For some specialized problems such as imaging through atmospheric turbulence, the distortions happen in the phase domain. This canbesimulatedbydrawingsamplesfromthepowerspectraldensity,e.g.,theKolmogorov spectrum or the von K´arm´an spectrum. Power spectral densities have many important engineering applications. Why does the power spectral density require wide-sense stationarity? • If a process is WSS, then R will have a Toeplitz structure. X • A Toeplitz matrix is important. If you do eigendecomposition to a Toeplitz ma- trix, the eigenvectors are the Fourier bases. • So if R is Toeplitz, then you can diagonalize it using the Fourier transform. X • Therefore, the power spectral density can be defined. Why does power spectral density require WSS? This has to do with the Toeplitz structure of the autocorrelation function. To make our discussion easier let us discretize the autocorrelation function R (t ,t ) by considering R [m,n]. (You can do a mental X 1 2 X calculation by converting t to integer indices m, and t to n. See any textbook on signals 1 2 and systems if you need help. This is called the “discrete time signal”.) Following the range of t and t , R [m,n] can be expressed as: 1 2 X   R [0] R [1] ··· R [N −1] X X X  R X[1] R X[0] ··· R X[N −2] R=   . . . . . . ... . . .   , R [N −1] R [N −1] ··· R [0] X X X whereweusedthefactthatR [m,n]=R [m−n]forWSSprocessesandR [k]=R [−k]. X X X X We call the resulting matrix R the autocorrelation matrix, which is a discretized version of the autocorrelation function R (t ,t ). Looking at R, we again observe the Toeplitz X 1 2 structure. For example, Figure 10.20 shows one Toeplitz structure and one non-Toeplitz structure. Any Toeplitz matrix R can be diagonalized using the Fourier transforms. That is, we can write R as R=FHΛF, whereF isthe(discrete)Fourier transform matrixandΛisadiagonalmatrix.Thiscanbe understood as the eigendecomposition of R. The important point here is that only Toeplitz matrices can be eigendecomposed using the Fourier transforms; an arbitrary symmetric matrixcannot.Figure 10.20illustratesthispoint.IfyourmatrixisToeplitz,youcandiago- nalizeit,andhenceyoucandefinethepowerspectraldensity,justasinthefirstexample.If yourmatrixisnotToeplitz,thenthepowerspectraldensityisundefined.TogettheToeplitz matrix, you must start with a WSS process. Before moving on, we define cross power spectral densities, which will be useful in some applications. 642
10.4. POWER SPECTRAL DENSITY Figure10.20:WeshowtwoautocorrelationfunctionsR [m,n]ontheleft-handside.Thefirstautocor- X relation function comes from a WSS process that has a Toeplitz structure. The second autocorrelation function does not have Toeplitz structure. For the Toeplitz matrix, we can diagonalize it using the Fourier transform. The eigenvalues are the power spectral density. Definition 10.9. The cross power spectral density between two random processes X(t) and Y(t) is S (ω)=F(R (τ)) where R (τ)=E[X(t+τ)Y(t)], X,Y X,Y X,Y (10.19) S (ω)=F(R (τ)) where R (τ)=E[Y(t+τ)X(t)]. Y,X Y,X Y,X Remark. In general, S (ω) (cid:54)= S (ω). Rather, since R (τ) = R (−τ), we have X,Y Y,X X,Y Y,X S (ω)=S (ω). X,Y Y,X 10.4.2 Origin of the power spectral density To understand the power spectral density, it is crucial to understand where it comes from and why it is the Fourier transform of the autocorrelation function. We begin by assuming that X(t) is a WSS random process with mean µ and auto- X correlation R (τ). We now consider the notion of power. Consider a random process X(t). X The power within a period [−T,T] is 1 (cid:90) T P(cid:98)X = 2T |X(t)|2 dt. −T P(cid:98)X defines the power because the integration alone is the energy, and the normalization by 1/2T gives us the power. However, there are two problems. First, since X(t) is random, the power P(cid:98)X is also random. Is there a way we can eliminate the randomness? Second, T is a finite period of time. It does not capture the entire process, and so we do not know the power of the entire process. A natural solution to these two problems is to consider (cid:34) (cid:35) 1 (cid:90) T P d =efE lim |X(t)|2 dt . (10.20) X T→∞2T −T 643
CHAPTER 10. RANDOM PROCESSES Here, we take the limit of T to infinity so that we can compute the power of the entire process. We also take the expectation to eliminate the randomness. Therefore, P can be X regarded as the average power of the complete random process X(t). Next, we need one definition and one lemma. The definition defines S (ω), and the X lemma will link S (ω) with the power P . X X Definition 10.10. The power spectral density (PSD) of a WSS process is defined as (cid:104) (cid:105) E |X(cid:101)T(ω)|2 S (ω)= lim , (10.21) X T→∞ 2T where (cid:90) T X(cid:101)T(ω)= X(t)e−jωt dt (10.22) −T is the Fourier transform of X(t) limited to [−T,T]. This definition is abstract, but in a nutshell, it simply considers everything in the Fourier domain. The ratio |X(cid:101)T(ω)|2/2T is the power, but in the frequency domain. The reason is thatifX(t)isFouriertransformable,thenParseval’s theoremwillhold.Parseval’stheorem states that energy in the original space is conserved in the Fourier space. Since the ratio |X(cid:101)T(ω)|2/2T istheenergydividedbytime,itisthepower.However,thisisstillnotenough to help us understand power spectral density: We need a lemma. Lemma 10.2. Define (cid:34) (cid:35) 1 (cid:90) T P d =efE lim |X(t)|2 dt . X T→∞2T −T Then 1 (cid:90) ∞ P = S (ω)dω. (10.23) X 2π X −∞ Thelemmahastobereadtogetherwiththepreviousdefinition.Ifwecanprovethelemma, we know that by integrating S (ω) we will obtain the power. Therefore, S (ω) can be X X viewed as a density function, specifically the density function of the power. S (ω) is called X thepowerspectraldensitybecauseeverythingisdefinedintheFourierdomain.Puttingthis all together gives us “power spectral density”. Proof. First, we recall that P is the expectation of the average power of X(t). Let X (cid:26) X(t) −T ≤t≤T, X (t)= T 0 otherwise. It follows that integrating over −∞ to ∞ is equivalent to (cid:90) ∞ (cid:90) T |X (t)|2 dt= |X(t)|2 dt. T −∞ −T 644
10.4. POWER SPECTRAL DENSITY By Parseval’s theorem, energy is conserved in both the time and the frequency domain: (cid:90) ∞ 1 (cid:90) ∞ |X T(t)|2 dt= 2π |X(cid:101)T(ω)|2 dω. −∞ −∞ Therefore, P satisfies X (cid:34) (cid:35) 1 (cid:90) T P =E lim |X(t)|2 dt X T→∞2T −T (cid:20) 1 1 (cid:90) ∞ (cid:21) =E Tl →im ∞2π2T −∞|X(cid:101)T(ω)|2 dω 1 (cid:90) ∞ 1 (cid:104) (cid:105) = 2π −∞Tl →im ∞2TE |X(cid:101)T(ω)|2 dω. (cid:124) (cid:123)(cid:122) (cid:125) d=efSX(ω) (cid:3) The power spectral densities are functions whose integrations give us the power. If we want to determine the power of a random process, the Einstein-Wiener-Khinchin theorem (Theorem 10.2) says that S (ω) is just the Fourier transform of R (τ): X X (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ =F(R (τ)). X X X −∞ The proof of the Einstein-Wiener-Khinchin theorem is quite intricate, so we defer the proof to the Appendix. The significance of the theorem is that it turns an abstract quantity, the power spectral density, into a very easily computable quantity, namely the Fouriertransformoftheautocorrelationfunction.Fornow,wewillhappilyusethistheorem because it saves us a great deal of trouble when we want to determine the power spectral density from the first principles. 645
CHAPTER 10. RANDOM PROCESSES 10.5 WSS Process through LTI Systems Random processes have limited usefulness until we can apply operations to them. In this sectionwediscusshowWSSprocessesrespondtoalineartime-invariant(LTI)system.This technique is most useful in signal processing, communication, speech analysis, and imaging. We will be brief here since you can find most of this information in any standard textbook on signals and systems. 10.5.1 Review of linear time-invariant systems When we say a “system”, we mean that there exists an input-output relationship as shown in Figure 10.21. Figure 10.21: A system can be viewed as a black box that takes an input X(t) and turns it into an output Y(t). Linear time-invariant (LTI) systems are the simplest systems we use in engineering problems. An LTI system has two properties. • Linearity.Linearitymeansthatwhentwoinputrandomprocessesareaddedand scaled,theoutputrandomprocesseswillalsobeaddedandscaledinexactlythe same way. Mathematically, linearity says that if X (t) → Y (t) and X (t) → 1 1 2 Y (t), then 2 aX (t)+bX (t)→aY (t)+bY (t). 1 2 1 2 • Time-invariant:Timeinvariancemeansthatifweshifttheinputrandomprocess by a certain time period, the output will be shifted in the same way. Mathemat- ically, time invariance means that if X(t)→Y(t), then X(t+τ)→Y(t+τ). Ifasystemislineartime-invariant,theinput-to-outputrelationisgivenbyconvolution: The convolution between two functions X(t) and h(t) is defined as (cid:90) ∞ Y(t)=h(t)∗X(t)= h(τ)X(t−τ)dτ, −∞ in which we call h(t) the system response or impulse response. 646
10.5. WSS PROCESS THROUGH LTI SYSTEMS The function h(t) is called the impulse response because if X(t) = δ(t), then according to the convolution equation we have (cid:90) ∞ Y(t)= h(τ)δ(t−τ)dτ =h(t). −∞ Therefore, if we send an impulse to the system, the output will be h(t). Convolution is commutative, meaning that h(t)∗X(t)=X(t)∗h(t). Written as inte- grations, we have (cid:90) ∞ (cid:90) ∞ h(τ)X(t−τ)dτ = h(t−τ)X(τ)dτ. (10.24) −∞ −∞ For LTI systems, Y(t) can be determined through the Fourier transforms. The Fourier transform of a (squared-integrable) function X(t) is (cid:90) ∞ X(ω)=F{X(t)}= X(τ)e−jωτ dτ. (10.25) −∞ A basic property of convolution is that convolution in the time domain is equivalent to multiplication in the Fourier domain. Therefore Y(ω)=H(ω)X(ω), (10.26) where H(ω)=F{h(t)} is the Fourier transform of h(t), and Y(ω)=F(Y(t)) is the Fourier transform of Y(t). Intherestofthissectionwestudythepairofinputandoutputrandomprocessesthat are defined as follows • X(t) = input. It is a WSS random process. • Y(t) = output. It is constructed by sending X(t) through an LTI system with impulse response h(t). Therefore, Y(t)=h(t)∗X(t). 10.5.2 Mean and autocorrelation through LTI Systems SinceX(t)isWSS,themeanfunctionofX(t)staysconstant,i.e.,µ (t)=µ .Thefollowing X X theorem gives the mean function of the output. Theorem10.3. IfX(t)passesthroughanLTIsystemtoyieldY(t),themeanfunction of Y(t) is (cid:90) ∞ E[Y(t)]=µ h(τ)dτ. (10.27) X −∞ 647
CHAPTER 10. RANDOM PROCESSES Proof. Suppose that Y(t)=h(t)∗X(t). Then, (cid:20)(cid:90) ∞ (cid:21) µ (t)=E[Y(t)]=E h(τ)X(t−τ)dτ Y −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(τ)E[X(t−τ)]dτ = h(τ)µ dτ =µ h(τ)dτ, X X −∞ −∞ −∞ where the second to last equality is valid because E[X(t−τ)]=µ . (cid:3) X The theorem suggests that if the input X(t) has a constant mean, the output Y(t) should also have a constant mean. This should not be a surprise because if the system is linear, a constant input will give a constant output. Example 10.15. Consider a WSS random process X(t) such that each sample is an i.i.d.Gaussianrandomvariablewithzeromeanandunitvariance.Wesendthisprocess through an LTI system with impulse response h(t), where (cid:40) 10(1−|t|), −1≤t≤1, h(t)= 0, otherwise. The mean function of X(t) is µ (t)=0, and that of Y(t) is µ (t)=0. Figure 10.22 X Y illustrates a numerical example, in which we see that the random processes X(t) and Y(t) have different shapes but the mean functions remain constant. 4 0.2 X(t) (t) R X(t) X R (t) 2 Y(t) 0.15 Y (t) Y 0.1 0 0.05 -2 0 -4 -0.05 -10 -5 0 5 10 -2 -1 0 1 2 (a) µ (t) and µ (t) (b) R (t) and R (t) X Y X Y Figure 10.22: When sending a WSS random process through an LTI system, the mean and the autocorrelation functions are changed. Next, we derive the autocorrelation function of a random process when sent through an LTI system. Theorem 10.4. If X(t) passes through an LTI system to yield Y(t), the autocorre- lation function of Y(t) is (cid:90) ∞ (cid:90) ∞ R (τ)= h(s)h(r)R (τ +s−r)dsdr. (10.28) Y X −∞ −∞ 648
10.5. WSS PROCESS THROUGH LTI SYSTEMS Proof. We start with the definition of Y(t): R (τ)=E[Y(t)Y(t+τ)] Y (cid:20)(cid:90) ∞ (cid:90) ∞ (cid:21) =E h(s)X(t−s)ds h(r)X(t+τ −r)dr −∞ −∞ (cid:90) ∞ (cid:90) ∞ ( =a) h(s)h(r)E[X(t−s)X(t+τ −r)dsdr] −∞ −∞ (cid:90) ∞ (cid:90) ∞ = h(s)h(r)R (τ +s−r)dsdr, X −∞ −∞ where in (a) we assume that integration and expectation are interchangeable. (cid:3) AshorthandnotationoftheaboveformulaisR (t)=[h(cid:126)(h∗R )](t),where∗denotes Y X the convolution and (cid:126) denotes the correlation. Figure 10.22(b) shows the autocorrelation functions R and R . In this example R is a delta function because for i.i.d. Gaussian X Y X noise the power spectral density is a constant. After convolving with the system response, the autocorrelation R has a different shape. Y 10.5.3 Power spectral density through LTI systems Denoting the Fourier transform of the impulse response by H(ω) = F(h(t)), we derive the power spectral density of the output. Theorem 10.5. If X(t) passes through an LTI system to yield Y(t), the power spec- tral density of Y(t) is S (ω)=|H(ω)|2S (ω). (10.29) Y X Proof. By definition, the power spectral density S (ω) is the Fourier transform of the Y autocorrelation function R (ω). Therefore, Y (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ Y Y −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(s)h(r)R (τ +s−r)dsdre−jωτ dτ. X −∞ −∞ −∞ Letting u=τ +s−r, we have (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ S (ω)= h(s)h(r)R (u)e−jω(u−s+r) dsdr du Y X −∞ −∞ −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(s)ejωs ds h(r)e−jωr dr R (u)e−jωu du X −∞ −∞ −∞ =H(ω)H(ω)S (ω), X where H(ω) is the complex conjugate of H(ω). (cid:3) ItistemptingtothinkthatsinceY(t)=h(t)∗X(t),thepowerspectraldensityshould also be S (ω) = H(ω)X(ω), but this is not true. The above result shows that we need an Y 649
CHAPTER 10. RANDOM PROCESSES additional complex conjugate H(ω) because S (ω) is the power, which means the square Y of the signal. Note that R is “squared” because we have convolved it with itself, and R X Y is also squared. Therefore, to match R and R , the impulse response h also needs to be X Y squared in the Fourier domain. Example 10.16. A WSS process X(t) has a correlation function R (τ)=sinc(πτ). X Suppose that X(t) passes through an LTI system with input/output relationship d2 d d2 d 2 Y(t)+2 Y(t)+4Y(t)=3 X(t)−3 X(t)+6X(t). dt2 dt dt2 dt Find R (τ). Y Solution: The sinc function has a Fourier transform given by π (cid:16) ω (cid:17) sinc(Wt)←→ rect . F W 2W Therefore, the autocorrelation function is π (cid:16) ω (cid:17) R (τ)=sinc(πτ) ←→ rect . X F π 2π By taking the Fourier transform on both sides, we have (cid:40) 1, −π ≤ω ≤π, S (ω)= X 0, elsewhere. The system response is found from the differential equation: 3(jω)2−3(jω)+6 H(ω)= 2(jω)2+2(jω)+4 3(cid:2) (2−ω2)−jω(cid:3) = . 2[(2−ω2)+jω] Taking the magnitude square yields 3(cid:2) (2−ω2)−jω(cid:3) 3(cid:2) (2−ω2)+jω(cid:3) |H(ω)|2 = 2[(2−ω2)+jω] 2[(2−ω2)−jω] 9(2−ω2)2+ω2 9 = = . 4(2−ω2)2+ω2 4 Therefore, the output power spectral density is 9 S (ω)=|H(ω)|2S (ω)= S (ω). Y X 4 X 650
10.5. WSS PROCESS THROUGH LTI SYSTEMS Taking the inverse Fourier transform, we have 9 R (τ)= sinc(πτ). Y 4 Example 10.17. A random process X(t) has zero mean and R (t,s) = min(t,s). X Consider a new process Y(t)=etX(e−2t). 1. Is Y(t) WSS? 2. Suppose Y(t) passes through a LTI system to yield an output Z(t) according to d d Z(t)+2Z(t)= Y(t)+Y(t). dt dt Find R (τ). Z Solution: 1. InordertoverifywhetherY(t)isWSS,weneedtocheckthemeanfunctionand the autocorrelation function. The mean function is E[Y(t)]=E(cid:2) etX(e−2t)(cid:3) =etE(cid:2) X(e−2t)(cid:3) . Since X(t) has zero mean, E[X(t)] = 0 for all t. This implies that if u = e−2t, then E[X(u)]=0 because u is just another time instant. Thus E[X(e−2t)]=0, and hence E[Y(t)]=0. The autocorrelation is (cid:104) (cid:105) E[Y(t+τ)Y(t)]=E et+τX(e−2(t+τ))etX(e−2t) (cid:104) (cid:105) =e2t+τE X(e−2(t+τ))X(e−2t) =e2t+τR (e−2(t+τ),e−2t). X Substituting R (t,s)=min(t,s), we have that X e2t+τR (e−2(t+τ),e−2t)=e2t+τmin(e−2(t+τ),e−2t) X (cid:26) e−2(t+τ), τ ≥0 =e2t+τ e−2t, τ <0 (cid:26) e−τ, τ ≥0 = eτ, τ <0 =e−|τ|. So R (τ)=e−|τ|. Since R (τ) is a function of τ, Y(t) is WSS. Y Y 651
CHAPTER 10. RANDOM PROCESSES 2. The system response is given by 1+jω H(ω)= . 2+jω The magnitude is therefore 1+ω2 |H(ω)|2 = . 4+ω2 Hence, the output autocorrelation function is 2 R (τ)=e−|τ| ←→S (ω)= , Y Y 1+ω2 and S (ω)=|H(ω)|2S (ω) Z Y 1+ω2 2 2 = = . 4+ω21+ω2 4+ω2 Therefore 1 R (τ)= e−2|τ|. Z 2 10.5.4 Cross-correlation through LTI Systems Theaboveanalysesaredevelopedfortheautocorrelationfunction.Ifweconsiderthecross- correlationbetweentworandomprocesses,sayX(t)andY(t),thentheaboveresultsdonot hold. In this section, we discuss the cross-correlation through LTI systems. To begin with, we need to define WSS for a pair of random processes. Definition 10.11. Two random processes X(t) and Y(t) are jointly WSS if 1. X(t) is WSS and Y(t) is WSS, and 2. R (t ,t )=E[X(t )Y(t )] is a function of t −t . X,Y 1 2 1 2 1 2 If X(t) and Y(t) are jointly WSS, we write R (t ,t )=R (τ)d =efE[X(t+τ)Y(τ)]. X,Y 1 2 X,Y Thedefinitionof“jointlyWSS”isnecessaryherebecauseR isdefinedbyX andY.Just X,Y knowing that X(t) and Y(t) are WSS does not allow one to say that R (t ,t ) can be X,Y 1 2 written as the time difference. If we flip the order of X and Y to consider R (τ) and not R (τ), then we need Y,X X,Y to flip the argument. The following lemma explains why. 652
10.5. WSS PROCESS THROUGH LTI SYSTEMS Lemma10.3. ForanyrandomprocessesX(t)andY(t),the cross-correlationR (τ) X,Y is related to R (τ) as Y,X R (τ)=R (−τ). (10.30) X,Y Y,X Proof. Recall the definition of R (−τ) = E[Y(t−τ)X(t)]. This can be simplified as Y,X follows: R (−τ)=E[Y(t−τ)X(t)] Y,X =E[X(t)Y(t−τ)] =E[X(t(cid:48)+τ)Y(t(cid:48))] =R (τ), X,Y where we substituted t(cid:48) =t−τ. (cid:3) Example 10.18.LetX(t)andN(t)betwoindependentWSSrandomprocesseswith expectationsE[X(t)]=µ andE[N(t)]=0,respectively.LetY(t)=X(t)+N(t).We X want to show that X(t) and Y(t) are jointly WSS, and we want to find R (τ). X,Y Solution. Before we show the joint WSS property of X(t) and Y(t), we first show that Y(t) is WSS: E[Y(t)]=E[X(t)+N(t)]=µ . X R (t ,t )=E[(X(t )+N(t ))(X(t )+N(t ))] Y 1 2 1 1 2 2 =E[(X(t )X(t )]+E[(N(t )N(t )] 1 2 1 2 =R (t −t )+R (t −t ). X 1 2 N 1 2 Thus, Y(t) is WSS. To show that X(t) and Y(t) are jointly WSS, we need to check the cross- correlation function: R (t ,t )=E[X(t )Y(t )] X,Y 1 2 1 2 =E[X(t )(X(t )+N(t ))] 1 2 2 =E[X(t )(X(t )]+E[X(t )N(t )] 1 2 1 2 =R (t ,t )+E[X(t )]E[N(t )] X 1 2 1 2 =R (t ,t ). X 1 2 Since R (t ,t ) is a function of t −t , and since X(t) and Y(t) are WSS, X(t) and X,Y 1 2 1 2 Y(t) must be jointly WSS. Finally,tofindR (τ),wesubstituteτ =t −t andobtainR (τ)=R (τ). X,Y 1 2 X,Y X KnowingthedefinitionofjointlyWSS,weconsiderthecross-correlationbetweenX(t) and Y(t). Note that here we are asking about the cross-correlation between the input and the output of the same LTI system, as illustrated in Figure 10.23. The pair X(t) and Y(t)=h(t)∗X(t) are special because Y(t) is the convolved version of X(t). 653
CHAPTER 10. RANDOM PROCESSES Figure 10.23: The source of the signals when defining R (τ), R (τ), R (τ) and R (τ). X X,Y Y,X Y Theorem 10.6. Let X(t) and Y(t) be jointly WSS processes, and let Y(t) = h(t)∗ X(t). Then the cross-correlation R (τ) is Y,X R (τ)=h(τ)∗R (τ). (10.31) Y,X X Proof. Recalling the definition of cross-correlation, we have R (τ)=E[Y(t+τ)X(t)] Y,X (cid:20) (cid:90) ∞ (cid:21) =E X(t) X(t+τ −r)h(r)dr −∞ (cid:90) ∞ (cid:90) ∞ = E[X(t)X(t+τ −r)]h(r)dr = R (τ −r)h(r)dr, X −∞ −∞ which is the convolution R (τ)=h(τ)∗R (τ). Y,X X (cid:3) We next define the cross power spectral density of two jointly WSS processes as the Fourier transform of the cross-correlation function. Definition 10.12. The cross power spectral density of two jointly WSS processes X(t) and Y(t) is defined as S (ω)=F[R (τ)], X,Y X,Y S (ω)=F[R (τ)]. Y,X Y,X The relationship between S and S can be seen from the following theorem. X,Y Y,X Theorem 10.7. For two jointly WSS random processes X(t) and Y(t), the cross power spectral density satisfies the property that S (ω)=S (ω), (10.32) X,Y Y,X where (·) denotes the complex conjugate. 654
10.5. WSS PROCESS THROUGH LTI SYSTEMS Proof. Since S (ω)=F[R (τ)] by definition, it follows that X,Y X,Y (cid:90) ∞ F[R (τ)]= R (τ)e−jωτ dτ X,Y X,Y −∞ (cid:90) ∞ (cid:90) ∞ = R (−τ)e−jωτ dτ = R (τ(cid:48))ejωτ(cid:48) dτ(cid:48), Y,X X,Y −∞ −∞ which is exactly the conjugate S (ω). Y,X (cid:3) WhensendingtherandomprocessthroughanLTIsystem,thecross-correlationpower spectral density is given by the theorem below. Theorem 10.8. If X(t) passes through an LTI system to yield Y(t), then the cross power spectral density is S (ω)=H(ω)S (ω), Y,X X S (ω)=H(ω)S (ω). X,Y X Proof. By taking the Fourier transform on R (τ) we have that S (ω) = H(ω)S (ω). Y,X Y,X X Since R (τ)=R (−τ), it holds that S (ω)=H(ω)S (ω). X,Y Y,X X,Y X (cid:3) Example 10.19. Let X(t) be a WSS random process with R (τ)=e−τ2/2, H(ω)=e−ω2/2. X Find S (ω), R (τ), S (ω) and R (τ). X,Y X,Y Y Y Solution. First, by the Fourier transform table we know that √ S (ω)= 2πe−ω2/2. X Since H(ω)=e−ω2/2, we have S (ω)=H(ω)S (ω) X,Y X √ = 2πe−ω2 . The cross-correlation function is (cid:104)√ (cid:105) R (ω)=F−1 2πe−ω2 X,Y = √1 e−τ 42 . 2 655
CHAPTER 10. RANDOM PROCESSES The power spectral density of Y(t) is S (ω)=|H(ω)|2S (ω) Y X √ = 2πe−3ω 22 . Therefore, the autocorrelation function of Y(t) is R Y(τ)=F−1(cid:104)√ 2πe−3ω 22(cid:105) 1 = √ e−τ2/6. 3 10.6 Optimal Linear Filter In the previous sections, we have built many tools to analyze random processes. Our next goal is to apply these techniques. To that end, we will discuss optimal linear filter design, whichisasetofestimationtechniquesforpredictingandrecoveringinformationfromatime series. 10.6.1 Discrete-time random processes We begin by introducing some notations. In the previous sections, we have been using continuous-time random processes to study statistics. In this section, we mainly focus on discrete-timerandomprocesses.Theshiftfromcontinuous-timetodiscrete-timeisstraight- forward as far as the theories are concerned — we switch the continuous-time index t to a discrete-time index n. However, shifting to discrete-time random processes can simplify manydifficultproblemsbecausemanydiscrete-timeproblemscanbesolvedbymatricesand vectors. This will make the computations and implementations much easier. To make this transition easier, we provide a few definitions and results without proof. Notations for discrete-time random processes • Wedenotethediscrete-timeindicesbymandn,correspondingtothecontinuous- time indices t and t , respectively. 1 2 • A discrete-time random process is denoted by X[n]. • Its mean function and the autocorrelation function are µ [n]=E[X[n]], X R [m,n]=E[X[m]X[n]]. X • We say that X[n] is WSS if µ [n] = constant, and R [m,n] is a function of X X m−n. 656
10.6. OPTIMAL LINEAR FILTER • If X[n] is WSS, we write R [m,n] as X R [m,n]=R [m−n]=R [k], X X X where k =m−n is the interval. • If X[n] is WSS, we define the power spectral density as S (ejω)=F{R [k]}, X X where S (ejω) denotes the discrete-time Fourier transform. X When a random process X[n] is sent through an LTI system with an impulse response h[n], the output is ∞ (cid:88) Y[n]=h[n]∗X[n]= h[k]X[n−k]. (10.33) k=−∞ WhenaWSSprocessX[n]passesthroughanLTIsystemh[n]toyieldanoutputY[n], the auto- and cross-correlation function and power spectral densities are • R [k]=E[Y[n+k]Y[n]], S (ejω)=F{R [k]}=|H(ejω)|2S (ejω). Y Y Y X • R [k]=E[X[n+k]Y[n]], S (ejω)=F{R [k]}=H(ejω)S (ejω). XY XY XY X • R [k]=E[Y[n+k]X[n]], S (ejω)=F{R [k]}=H(ejω)S (ejω). YX YX YX X 10.6.2 Problem formulation Theproblemwestudyhereisknownastheoptimal linear filter design.Supposethatthere is a WSS process X[n] that we want to process. For example, if X[n] is a corrupted version of some clean time-series, we may want to remove the noise by filtering (also known as averaging) X[n]. Conceptualizing the denoising process as a linear time-invariant system with an impulse response h[n], our goal is to determine the optimal h[n] such that the estimated time series Y(cid:98)[n] is as close to the true time series Y[n] as possible. Referring to Figure 10.24, we refer to X[n] as the input function and to Y[n] as the target function. X[n] and Y[n] are related according to the equation K−1 (cid:88) Y[n]= h[k]X[n−k]+E[n], (10.34) k=0 (cid:124) (cid:123)(cid:122) (cid:125) Y(cid:98)[n] where E[n] is a noise random process to model the error. The linear part of the equation is known as the prediction and is constructed by sending X[n] through the system. For simplicity we assume that X[n] is WSS. Thus, it follows that Y[n] is also WSS. We may also assume that we can estimate R [k], R [k], R [k] and R [k]. X YX XY Y 657
CHAPTER 10. RANDOM PROCESSES Figure10.24:Aschematicdiagramillustratingtheoptimallinearfilterproblem:Givenaninputfunction X[n],wewanttodesignafilterh[n]suchthatthepredictionY(cid:98)[n]isclosetothetargetfunctionY[n]. Example 10.20. If we let K =3, Equation (10.34) gives us Y[n]=h[0]X[n]+h[1]X[n−1]+h[2]X[n−2]+E[n]. Thatis,thecurrentsampleY[n]isalinearcombinationoftheprevioussamplesX[n], X[n−1] and X[n−2]. Given X[n] and Y[n], what would be the best guess of the impulse response h[n] so that the prediction is as close to the true values as possible? From our discussions of linear regression, we know that this is equivalent to solving the optimization problem (cid:32) K−1 (cid:33)2 (cid:88) minimize Y[n]− h[k]X[n−k] . (10.35) {h[k]}K−1 k=0 k=0 The choice of the squared error is more or less arbitrary, depending on how we want to model E[n]. By using the square norm, we implicitly assume that the error is Gaussian. This may not be true, but it is commonly used because the squared norm is differentiable. We will follow this tradition. The challenge associated with the minimization is that in most of the practical set- tings the random processes X[n] and Y[n] are changing rapidly because they are random processes. Therefore, even if we solve the optimization problem, the estimates h[k] will be randomvariablessincewearesolvingarandomequation.Toeliminatethisrandomness,we take the expectation over all the possible choices of X[n] and Y[n], yielding (cid:32) K−1 (cid:33)2 (cid:88) minimize Y[n]− h[k]X[n−k] , {h[k]}K−1 k=0 k=0 ⇓ (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k] . {h[k]}K−1 k=0 k=0 The resulting impulse responses h[k], derived by solving the above minimization, is known as the optimal linear filter. It is the best linear model for describing the input- output relationships between X[n] and Y[n]. 658
10.6. OPTIMAL LINEAR FILTER What is the optimal linear filter? The optimal linear filter is the solution to the optimization problem (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k] . (10.36) {h[k]}K−1 k=0 k=0 10.6.3 Yule-Walker equation To solve the optimal linear filter problem, we first perform some (slightly tedious) algebra to obtain the following results: Lemma 10.4. Let Y(cid:98)[n]=(cid:80)K−1h[k]X[n−k] be the prediction of Y[n]. The squared- k=0 norm error can be written as (cid:20)(cid:16) (cid:17)2(cid:21) E X,Y Y[n]−Y(cid:98)[n] K−1 K−1K−1 (cid:88) (cid:88) (cid:88) =R [0]−2 h[k]R [k]+ h[k]h[j]R [j−k]. (10.37) Y YX X k=0 k=0 j=0 Thus we can express the error in terms of R [k], R [k] and R [k]. YX X Y Proof. We expand the error as follows: E X,Y (cid:20)(cid:16) Y[n]−Y(cid:98)[n](cid:17)2(cid:21) =E Y (cid:2) (Y[n])2(cid:3) −2E X,Y (cid:104) Y[n]Y(cid:98)[n](cid:105) +E X(cid:104) (Y(cid:98)[n])2(cid:105) . The first term is the autocorrelation of Y[n]: E (cid:2) (Y[n])2(cid:3) =E[Y[n+0]Y[n]]=R [0]. (10.38) Y Y The second term is (cid:34) K−1 (cid:35) (cid:104) (cid:105) (cid:88) E X,Y Y[n]Y(cid:98)[n] =E X,Y Y[n] h[k]X[n−k] k=0 K−1 (cid:88) = h[k]E [Y[n]X[n−k]] X,Y k=0 K−1 (cid:88) = h[k]R [k]. (10.39) YX k=0 659
CHAPTER 10. RANDOM PROCESSES The third term is (cid:32)K−1 (cid:33) K−1  (cid:104) (cid:105) (cid:88) (cid:88) E X (Y(cid:98)[n])2 =E X h[k]X[n−k]  h[j]X[n−j] k=0 j=0   K−1K−1 (cid:88) (cid:88) =E X h[k]h[j]X[n−k]X[n−j] k=0 j=0 K−1K−1 (cid:88) (cid:88) = h[k]h[j]E [X[n−k]X[n−j]] X k=0 j=0 K−1K−1 (cid:88) (cid:88) = h[k]h[j]R [j−k]. (10.40) X k=0 j=0 This completes the proof. (cid:3) ThesignificanceofthistheoremisthatitallowsustowritetheerrorintermsofR [k], YX R [k] and R [k]. As we have mentioned, while we can solve the randomized optimization X Y Equation(10.35),theresultingsolutionwillbearandomvectordependingontheparticular realizationsX[n]andY[n].SwitchingfromEquation(10.35)toEquation(10.36)eliminates therandomnessbecausewehavetakentheexpectation.Theresultingoptimizationaccording to the theorem is also convenient. Instead of seeking individual realizations, we only need to know the overall statistical description of the data through R [k], R [k] and R [k]. YX X Y These can be estimated through modeling or pseudorandom signals. The solution to the optimal linear filter problem is summarized by the Yule-Walker equation: Theorem 10.9. The solution {h[0],...,h[K−1]} to the optimal linear filter problem (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k]  (10.41) {h[k]}K−1 k=0 k=0 is given by the following matrix equation:      R RY YX X . . . [ [0 1] ]     =       R RX X . . .[ [0 1] ] R RX X . . .[ [1 0] ] · · ..· · .· · RX[K . . . . . . −1]            h h[ [ . . .0 1] ]     , (10.42) RYX[K−1] RX[K−1] RX[k−2] ··· RX[0] h[K−1] which is known as the Yule-Walker equation. Therefore, by solving the simple linear problem given by the Yule-Walker equation, we will find the optimal linear filter solution. Proof. Since the error is a squared norm, the optimal solution is obtained by taking the 660
10.6. OPTIMAL LINEAR FILTER derivative: d (cid:20)(cid:16) (cid:17)2(cid:21) dh[i]E X,Y Y[n]−Y(cid:98)[n]   d  K (cid:88)−1 K (cid:88)−1K (cid:88)−1  = R [0]−2 h[k]R [k]+ h[k]h[j]R [j−k] dh[i] Y YX X   k=0 k=0 j=0 K−1 (cid:88) =0−2R [i]+2 h[k]R [i−k], YX X k=0 in which the derivative of the last term is computed by noting that K−1K−1 d (cid:88) (cid:88) h[k]h[j]R [j−k] dh[i] X k=0 j=0 K−1 K−1 d (cid:88) d (cid:88) (cid:88) = h[j]2R [0]+ h[k]h[j]R [j−k] dh[i] X dh[i] X j=0 k=0 j(cid:54)=k K−1 (cid:88) =2 h[k]R [i−k]. X k=0 Equating the derivative to zero yields K−1 (cid:88) R [i]= h[k]R [i−k], i=0,...,K−1, YX X k=0 and putting the above equations into the matrix-vector form we complete the proof. (cid:3) The matrix in the Yule-Walker equation is a Toeplitz matrix, in which each row is a shifted version of the preceding row. This matrix structure is a consequence of a WSS process so that the autocorrelation function is determined by the time difference k and not by the starting and end times. Remark. If we take the derivative of the loss w.r.t. h[i], we have that d (cid:20)(cid:16) (cid:17)2(cid:21) (cid:104)(cid:16) (cid:17) (cid:105) 0= dh[i]E X,Y Y[n]−Y(cid:98)[n] =−2E Y[n]−Y(cid:98)[n] X[n−i] . Thisconditionisknownastheorthogonality condition,asitsaysthattheerrorY[n]−Y(cid:98)[n] is orthogonal to the signal X[n−i]. 10.6.4 Linear prediction We now demonstrate how to use the Yule-Walker equation in modeling an autoregressive process. The procedure in this simple example can be used in speech processing and time- series forecasting. SupposethatwehaveaWSSrandomprocessY[n].Wewouldliketopredictthefuture samples by using the most recent K samples through an autoregressive model. Since the 661
CHAPTER 10. RANDOM PROCESSES model is linear, we can write K (cid:88) Y(cid:98)[n]= h[k]Y[n−k]+E[n]. (10.43) k=1 In this model, we say that the predicted value Y(cid:98)[n] is a linear combination of the past K samples, albeit to approximation error E[n]. The problem we need to solve is (cid:20)(cid:16) (cid:17)2(cid:21) minimize E Y[n]−Y(cid:98)[n] . h[k] SinceY(cid:98)[n]iswrittenintermsofthepastsamplesofY[n]inthisproblem,intheYule-Walker equation we can replace X with Y. Consequently, we can write the matrix equation from  RYX[0]   RX[0] RX[1] ··· RX[K−1]  h[0]   RYX[1]   RX[1] RX[0] ··· RX[K−2]  h[1]     . . .   =   . . . . . . ... . . .      . . .   , RYX[K−1] RX[K−1] RX[k−2] ··· RX[0] h[K−1] to  RY[1]   RY[0] RY[1] ··· RY[K−1]  h[0]   RY[2]   RY[1] RY[0] ··· RY[K−2]  h[1]     . . .   =   . . . . . . ... . . .      . . .   . (10.44) RY[K] RY[K−1] RY[k−2] ··· RY[0] h[K−1] (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) r R On a computer, solving the Yule-Walker equation requires a few steps. First, we need to estimate the correlation N 1 (cid:88) R [k]=E[Y[n+k]Y[n]]≈ Y[n+k]Y[n]. Y N n=1 Theaveragingontheright-handsideisoftendoneusingxcorrinMATLABornp.correlate in Python. A graphical illustration of the input and the autocorrelation function is shown in Figure 10.25. After we have found R [n], we need to construct the Yule-Walker equation. For this Y linear prediction problem, the left-hand side of the Yule-Walker equation is the vector r, definedaccordingtoEquation(10.44).TheYule-WalkerequationalsorequiresthematrixR. This R can be constructed via the Toeplitz matrix as (cid:26) (cid:27) R=Toeplitz R [0],R [1],...,R [K−1] . Y Y Y In MATLAB, we can call Toeplitz to construct the matrix. In Python, the command is lin.Toeplitz. To solve the Yule-Walker equation, we need to invert the matrix R. There are built-in commands for such an operation. In MATLAB, the command is \ (the backslash), whereas in Python the command is np.linalg.lstsq. 662
10.6. OPTIMAL LINEAR FILTER 0.2 0.4 Y[n] R [k] Y 0.3 0.1 0.2 0 0.1 0 -0.1 -0.1 -0.2 -0.2 0 50 100 150 200 250 300 -300 -200 -100 0 100 200 300 (a) Y[n] (b) R [k] Y Figure 10.25: An example time-series and its autocorrelation function. % MATLAB code to solve the Yule Walker Equation y = load(’data_ch10.txt’); K = 10; N = 320; y_corr = xcorr(y); R = Toeplitz(y_corr(N+[0:K-1])); lhs = y_corr(N+[1:K]); h = R\lhs; # Python code to solve the Yule Walker Equation y = np.loadtxt(’./data_ch10.txt’) K = 10 N = 320 y_corr = np.correlate(y,y,mode=’full’) R = lin.Toeplitz(y_corr[N-1:N+K-1]) #call scipy.linalg lhs = y_corr[N:N+K] h = np.linalg.lstsq(R,lhs,rcond = None)[0] Note that in both the MATLAB and Python codes the Toeplitz matrix R starts with the index N. This is because, as you can see from Figure 10.25, the origin of the autocor- relation function is the middle index of the computed autocorrelation function. For r, the starting index is N +1 because the vector starts with R [1]. Y To predict the future samples, we recall the autoregressive model for this problem: K−1 (cid:88) Y(cid:98)[n]= h[k]Y[n−k]. k=0 Therefore, given Y[n−1],Y[n−2],...,Y[n−K], we can predict Y(cid:98)[n]. Then we insert this predicted Y(cid:98)[n] into the sequence and increment the estimation problem to the next time index. By repeating the process, we will be able to predict the future samples of Y[n]. 663
CHAPTER 10. RANDOM PROCESSES Figure 10.26illustratesthepredictionresultsoftheYule-Walkerequation.Asyoucan see, the predictions are reasonably meaningful since the patterns follow the trend. 0.2 0.1 0 -0.1 -0.2 Prediction Input -0.3 0 50 100 150 200 250 300 350 Figure 10.26: An example of the predictions made by the autoregressive model. The MATLAB and Python codes are shown below. % MATLAB code to predict the samples z = y(311:320); yhat = zeros(340,1); yhat(1:320) = y; for t = 1:20 predict = z’*h; z = [z(2:10); predict]; yhat(320+t) = predict; end plot(yhat, ’r’, ’LineWidth’, 3); hold on; plot(y, ’k’, ’LineWidth’, 4); # Python code to predict the samples z = y[310:320] yhat = np.zeros((340,1)) yhat[0:320,0] = y for t in range(20): predict = np.inner(np.reshape(z,(1,10)),h) z = np.concatenate((z[1:10], predict)) yhat[320+t,0] = predict plt.plot(yhat,’r’) plt.plot(y,’k’) plt.show() 664
10.6. OPTIMAL LINEAR FILTER 10.6.5 Wiener filter In the previous formulation, we notice that the impulse response has a finite length. There are, however, problems in which the impulse response is infinite. For example, a recur- sive filter h[n] will be infinitely long. The extension from finite length to infinite length is straightforward. We can model the problem as ∞ (cid:88) Y[n]= h[k]X[n−k]+E[n]. k=−∞ However, when h[n] is infinitely long the Yule-Walker equation does not hold because the matrix R will be infinitely large. Nevertheless, the building block equation for Yule-Walker is still valid: ∞ (cid:88) R [i]= h[k]R [i−k]. (10.45) YX X k=−∞ To maintain the spirit of the Yule-Walker equation while enabling computation, we recognize that the infinite sum on the right-hand side is, in fact, a convolution. Thus we can take the (discrete-time) Fourier transform of both sides to obtain S (ejω)=H(ejω)S (ejω). (10.46) YX X Therefore, the corresponding optimal linear filter (in the Fourier domain) is S (ejω) H(ejω)= YX , (10.47) S (ejω) X and (cid:26) S (e−jω)(cid:27) h[n]=F−1 YX . S (e−jω) X The filter obtained in this way is known as the Wiener filter. Example 10.21. (Denoising) Suppose X[n]=Y[n]+W[n], where W[n] is the noise term that is independent of Y[n], as shown in Figure 10.27. Figure10.27:DesignofaWienerfilterthattakesaninputfunctionX[n]andoutputsanestimate Y(cid:98)[n] that is close to the true function Y[n]. Now, given the input function X[n], can we construct the Wiener filter h[n] such that the predicted function Y(cid:98)[n] is as close to Y[n] as possible? The Wiener filter for this problem is also the optimal denoising filter. 665
CHAPTER 10. RANDOM PROCESSES Solution. The following correlation functions can easily be seen: R [k]=E[X[n+k]X[n]] X =E[(Y[n+k]+W[n+k])(Y[n]+W[n])] =E[Y[n+k]Y[n]]+E[Y[n+k]W[n]] +E[W[n+k]Y[n]]+E[W[n+k]W[n]] =E[Y[n+k]Y[n]]+0+0+E[W[n+k]W[n]] =R [k]+R [k]. Y W Similarly, we have R [k]=E[Y[n+k]X[n]] YX =E[Y[n](Y[n+k]+W[n+k])]=R [k]. Y Consequently, the optimal linear filter is S (ejω) H(ejω)= YX S (ejω) X F{R [k]} = YX F{R [k]} X S (ejω) = Y . S (ejω)+S (ejω) Y W What is the Wiener filter for a denoising problem? • SupposethecorruptedfunctionX[n]isrelatedtothecleanfunctionY[n]through X[n]=Y[n]+W[n], for some noise function W[n]. • The Wiener filter is S (ejω) H(ejω)= Y . (10.48) S (ejω)+S (ejω) Y W • To perform the filtering, the denoised function Y(cid:98)[n] is Y(cid:98)[n]=F−1(cid:8) H(ejω)X(ejω)(cid:9) . Figure 10.28 shows an example of applying the Wiener filter to a noise removal prob- lem. In this example we let W[n] be an i.i.d. Gaussian process with standard deviation σ = 0.05 and mean µ = 0. The noisy samples of random process X[n] are defined as X[n]=Y[n]+W[n],whereY[n]isthecleanfunction.AsyoucanseefromFigure 10.28(a), the Wiener filter is able to denoise the function reasonably well. Theoptimallinearfilterusedforthisdenoisingtaskisinfinitelylong.Thiscanbeseen in Figure 10.28(b), where the filter length is the same as the length of the observed time series X[n]. If X[n] is longer, the filter h[n] will also become longer. Therefore, finite-length approaches such as the Yule-Walker equation do not apply here. 666
10.6. OPTIMAL LINEAR FILTER 0.2 0.25 h[n] 0.1 0.2 0.15 0 0.1 -0.1 0.05 Noisy Input X[n] Wiener Filtered Yhat[n] 0 -0.2 Ground Truth Y[n] -0.05 0 50 100 150 200 250 300 -300 -200 -100 0 100 200 300 (a) Noise removal by Wiener filtering (b) Wiener filter Figure 10.28: (a) Applying a Wiener filter to denoise a function. (b) The Wiener filter used for the denoising task. The MATLAB / Python codes used to generate Figure 10.28(a) are shown below. Themaincommandsherearescipy.fftandscipy.ifft,whichareavailableinthescipy library. The commands Yhat = H.*fft(x, 639) in MATLAB execute the Wiener filtering step. Here, we resample the function x to 639 samples so that it matches with the Wiener filter H. Similar commands in Python are H * fft(x, 639). % MATLAB code for Wiener filtering w = 0.05*randn(320,1); x = y + w; Ry = xcorr(y); Rw = xcorr(w); Sy = fft(Ry); Sw = fft(Rw); H = Sy./(Sy + Sw); Yhat = H.*fft(x, 639); yhat = real(ifft(Yhat)); plot(x, ’LineWidth’, 4, ’Color’, [0.7, 0.7, 0.7]); hold on; plot(yhat(1:320), ’r’, ’LineWidth’, 2); plot(y, ’k:’, ’LineWidth’, 2); # Python code for Wiener filtering from scipy.fft import fft, ifft w = 0.05*np.random.randn(320) x = y + w Ry = np.correlate(y,y,mode=’full’) Rw = np.correlate(w,w,mode=’full’) Sy = fft(Ry) Sw = fft(Rw) H = Sy / (Sy+Sw) Yhat = H * fft(x, 639) 667
CHAPTER 10. RANDOM PROCESSES yhat = np.real(ifft(Yhat)) plt.plot(x,color=’gray’) plt.plot(yhat[0:320],’r’) plt.plot(y,’k:’) Example 10.22. (Deconvolution) Suppose that the corrupted function is generated according to a linear process given by ∞ (cid:88) X[n]= g[(cid:96)]Y[n−(cid:96)]+W[n], (cid:96)=−∞ where g[n] is the impulse response of some kind of degradation process and W[n] is theGaussiannoiseterm,asshowninFigure 10.29.Findtheoptimallinearfilter(i.e., the Wiener filter) to estimate Y(cid:98)[n]. Figure10.29:DesignofaWienerfilterthattakesaninputfunctionX[n]andoutputsanestimate Y(cid:98)[n] that is close to the true function Y[n]. Solution.ToconstructtheWienerfilter,wefirstdeterminethecross-correlationfunc- tion: (cid:34) ∞ (cid:35) (cid:88) R [k]=E[Y[n+k]X[n]]=E Y[n+k] g[(cid:96)]Y[n−(cid:96)]+W[n] . YX (cid:96)=−∞ Using algebra, it follows that (cid:34) ∞ (cid:35) (cid:88) E Y[n+k] g[(cid:96)]Y[n−(cid:96)]+W[n] (cid:96)=−∞ ∞ (cid:88) = g[(cid:96)]E[Y[n+k]Y[n−(cid:96)]]+E[Y[n+k]W[n]] (cid:96)=−∞ ∞ (cid:88) = g[(cid:96)]R [k+(cid:96)]+0=(g(cid:126)R )[k], Y Y (cid:96)=−∞ whichisthecorrelationbetweeng andR .Therefore,thecrosspowerspectraldensity Y S (ejω) is YX S (ejω)=G(ejω)S (ejω). YX Y 668
10.6. OPTIMAL LINEAR FILTER The autocorrelation of this problem is R [k]=E[X[n+k]X[n]] X =E[((g∗Y)[n+k]+W[n+k])((g∗Y)[n]+W[n])] =E[(g∗Y)[n+k](g∗Y)[n]]+E[W[n+k]W[n]] =(g(cid:126)(g∗R ))[k]+R [k], Y W where,accordingtotheprevioussection,thefirstpartisthecorrelation(cid:126)followedby a convolution ∗. Therefore, the power spectral density of X is S (ejω)=|G(ejω)|2S (ejω)+S (ejω). X Y W Combining the results, the Wiener filter is S (ejω) G(ejω)S (ejω) H(ejω)= YX = Y . S (ejω) |G(ejω)|2S (ejω)+S (ejω) X Y W What is the Wiener filter for a deconvolution problem? • Suppose that the corrupted function X[n] is related to the clean function Y[n] through X[n]=(g∗Y)[n]+W[n], for some degradation g[n] and noise W[n]. • The Wiener filter is G(ejω)S (ejω) H(ejω)= Y . (10.49) |G(ejω)|2S (ejω)+S (ejω) Y W • To perform the filtering, the estimated function Y(cid:98)[n] is Y(cid:98)[n]=F−1(cid:8) H(ejω)X(ejω)(cid:9) . As an example of the deconvolution problem, we show a WSS function Y[n] in Fig- ure 10.30.ThiscleanfunctionY[n]isconstructedbypassingani.i.d.noiseprocessthrough an arbitrary LTI system so that the WSS property is guaranteed. Given this Y[n], we con- structadegradationprocessinwhichtheimpulseresponseisgivenbyg[n].Inthisexample, we assume that g[n] is a uniform function. We then add noise W[n] to the time series to obtain the corrupted observation X[n]. The reconstruction by the Wiener filter is shown in Figure 10.30. The MATLAB and Python codes used to generate Figure 10.30 are shown below. % MATLAB code to solve the Wiener deconvolution problem load(’ch10_wiener_deblur_data’); g = ones(32,1)/32; w = 0.02*randn(320,1); x = conv(y,g,’same’) + w; Ry = xcorr(y); 669
CHAPTER 10. RANDOM PROCESSES 0.6 0.4 0.2 0 -0.2 -0.4 Noisy Input X[n] -0.6 Wiener Filtered Yhat[n] Ground Truth Y[n] -0.8 50 100 150 200 250 300 Figure 10.30: Reconstructing time series from degraded observations using a Wiener filter. Rw = xcorr(w); Sy = fft(Ry); Sw = fft(Rw); G = fft(g,639); H = (conj(G).*Sy)./(abs(G).^2.*Sy + Sw); Yhat = H.*fft(x, 639); yhat = real(ifft(Yhat)); figure; plot(x, ’LineWidth’, 4, ’Color’, [0.5, 0.5, 0.5]); hold on; plot(16:320+15, yhat(1:320), ’r’, ’LineWidth’, 2); plot(1:320, y, ’k:’, ’LineWidth’, 2); # Python code to solve the Wiener deconvolution problem y = np.loadtxt(’./ch10_wiener_deblur_data.txt’) g = np.ones(64)/64 w = 0.02*np.random.randn(320) x = np.convolve(y,g,mode=’same’) + w Ry = np.correlate(y,y,mode=’full’) Rw = np.correlate(w,w,mode=’full’) Sy = fft(Ry) Sw = fft(Rw) G = fft(g,639) H = (np.conj(G)*Sy)/( np.power(np.abs(G),2)*Sy + Sw ) Yhat = H * fft(x, 639) yhat = np.real(ifft(Yhat)) plt.plot(x,color=’gray’) 670
10.6. OPTIMAL LINEAR FILTER plt.plot(np.arange(32,320+32),yhat[0:320],’r’) plt.plot(y,’k:’) Caveat to Wiener filtering. In practice, the above Wiener filter needs to be modified because S (ejω) and S (ejω) cannot be estimated from the data via the temporal corre- Y W lation (as we did in the MATLAB/Python programs). The reason is that we never have accesstoY[n]andW[n].Inthiscase,onehastoguess thepowerspectraldensitiesS (ejω) Y and S (ejω). The noise power S (ejω) is usually not difficult to estimate. For example, W W in the program we showed above, the noise power spectral density is Sw = 0.02^2*320 (MATLAB), which is the noise standard deviation times the number of samples. The signal S (ejω) is often the hard part. In the absence of any knowledge about the Y groundtruth’spowerspectraldensity,theWienerfilterdoesnotwork.However,forcertain problems in which S (ejω) can be predetermined by prior knowledge, the Wiener filter is Y guaranteed to be optimal — optimal in the mean-squared-error sense over the entire time axis. Wiener filter versus ridge regression. The Wiener filter equation can be interpreted as a ridge regression. Denoting the forward observation model by x=Gy+w, the corresponding ridge regression minimization is y =argmin (cid:107)x−Gy(cid:107)2+λ(cid:107)y(cid:107)2 (cid:98) y =(GTG+λI)−1GTx. If G is a convolutional matrix, the above solution can be written in the Fourier domain (by using the Fourier transform as the eigenvectors): (cid:34) (cid:35) G(ejω) Y(cid:98)(ejω)= X(ejω). |G(ejω)|2+λ (cid:124) (cid:123)(cid:122) (cid:125) H(ejω) Comparing this “optimal linear filter” with the Wiener filter, we observe that the Wiener filter has slightly more generality: (cid:34) (cid:35) G(ejω)S (ejω) Y(cid:98)(ejω)= Y X(ejω). |G(ejω)|2S (ejω)+S (ejω) Y W Therefore, in the absence of S (ejω) and assuming that S (ejω) is a constant (e.g., for Y W Gaussian noise), the Wiener filter is exactly a ridge regression. 671
CHAPTER 10. RANDOM PROCESSES 10.7 Summary Random processes are very useful tools for analyzing random variables over time. In this chapter, we have introduced some of the most basic mechanisms: • Statistical versus temporal analysis: The statistical analysis of a random process looksattherandomprocessvertically.IttreatsX(t)asarandomvariableandstudies the randomness across different realizations. The temporal analysis is the horizontal perspective.IttreatsX(t)asafunctionintimewithafixedrandomindex.Ingeneral, statistical average (cid:54)= temporal average. • Mean function µ (t): The mean function is the expectation of the random process. X At every time t, we take the expectation to obtain the expected value E[X(t)]. • Autocorrelation function R (t ,t ). This is the joint expectation of the random pro- X 1 2 cessattwodifferenttimeinstantst andt .ThecorrespondingvaluesX(t )andX(t ) 1 2 1 2 are two random variables, and so the joint expectation measures how correlated these two variables are. • Wide-sense stationary (WSS): This is a special class of random processes in which µ (t)isaconstantandR (t ,t )isafunctionoft −t .Whenthishappens,theauto- X X 1 2 1 2 correlation function (which is originally a 2D function) will have a Toeplitz structure. We write R (t ,t ) as R (τ), where τ =t −t . X 1 2 X 1 2 • Power spectral density (PSD): This is the Fourier transform of the autocorrelation function R (τ), according to the Einstein-Wiener-Khinchin theorem. It is called the X power spectral density because we can integrate it in the Fourier space to retrieve the power.Thisprovidesuswithsomeconvenientcomputationaltoolsforanalyzingdata. • Random process through a linear time-invariant (LTI) system: This tells us how a randomprocessbehavesaftergoingthroughanLTIsystem.Theanalysiscanbedone at the realization level, where we look at each random process, or at the statistical level, where we look at the autocorrelation function and the PSD. • Optimal linear filter: A set of techniques that can be used to retrieve signals by using the statistical information of the data and the system. We introduced two specific approaches: the Yule-Walker equation for a finite-length filter and the Wiener filter for an infinite-length filter. We demonstrated how these techniques could be applied to forecast a time series and recover a time series from corrupted measurements. While we have covered some of the most basic ideas in random processes, there are also several topics we have not discussed. These include, but are not limited to: strictly stationary process, a more restrictive class of random process than WSS; Poisson process, a useful model for arrival analysis; Markov chain, a discrete-time random process where the current state only depends on the previous state. Readers interested in these materials should consult the references listed at the end of this chapter. 672
10.8. APPENDIX 10.8 Appendix The Einstein-Wiener-Khinchin theorem TheEinstein-Wiener-Khinchintheoremisafundamentalresult.Itstatesthatforanywide- sense stationary process, the power spectral density S (ω) is the Fourier transform of the X autocorrelation function. Theorem 10.10 (TheEinstein-Wiener-Khinchintheorem). ForaWSSrandompro- cess X(t), S (ω)=F{R (τ)}, (10.50) X X whenever the Fourier transform of R (τ) exists. X Proof. First, let’s recall the definition of S (ω): X 1 (cid:104) (cid:105) S X(ω)d =ef Tl →im ∞2TE |X(cid:101)T(ω)|2 . (10.51) By expanding the expectation, we have (cid:34)(cid:32) (cid:33)(cid:32) (cid:33)∗(cid:35) (cid:90) T (cid:90) T E[|X(cid:101)T(ω)|2]=E X(t)e−jωt dt X(θ)e−jωθ dθ −T −T (cid:90) T (cid:90) T (cid:90) T (cid:90) T = E[X(t)X(θ)]e−jω(t−θ) dtdθ = R (t−θ)e−jω(t−θ) dtdθ. X −T −T −T −T (10.52) Our next step is to analyze R (t−θ). Define X Q (v)=F{R (τ)}. (10.53) X X Then, by inverse Fourier transform 1 (cid:90) ∞ R (τ)= Q (v)ejvτ dv, X 2π X −∞ and therefore 1 (cid:90) ∞ R (t−θ)= Q (v)ejv(t−θ) dv. X 2π X −∞ Substituting this into Equation (10.52) yields (cid:90) T (cid:90) T (cid:18) 1 (cid:90) ∞ (cid:19) E[|X(cid:101)T(ω)|2 = 2π Q X(v)ejv(t−θ) dv e−jω(t−θ) dtdθ −T −T −∞ (cid:32) (cid:33)(cid:32) (cid:33) 1 (cid:90) ∞ (cid:90) T (cid:90) T = Q (v) ejt(v−ω) dt ejθ(ω−v) dθ dv. 2π X −∞ −T −T 673
CHAPTER 10. RANDOM PROCESSES We now need to simplify the two inner integrals. Recall by Fourier pair that (cid:18) (cid:19) (cid:18) (cid:19) t ωT rect F Tsinc . T ←→ 2 This implies that (cid:90) T (cid:90) T ejt(v−ω) dt= e−j(ω−v)t dt −T −T (cid:90) ∞ t sin((ω−v)T) = rect( )e−j(ω−v)t dt=2T sinc((ω−v)T)=2T . 2T (ω−v)T −∞ Hence, we have (cid:104) (cid:105) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 E |X(cid:101)T(ω)|2 = 2π Q X(v) 2T (ω−v)T dv. (10.54) −∞ and so 1 2T (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 2TE[|X(cid:101)T(ω)|2 = 2π Q X(v) (ω−v)T dv. (10.55) −∞ As T →∞ (see Lemma 10.5 below), we have (cid:18) sin((ω−v)T)(cid:19)2 2T −→ 2πδ(ω−v). (ω−v)T Therefore, 1 (cid:104) (cid:105) 1 (cid:90) ∞ (cid:34) (cid:18) sin((ω−v)T)(cid:19)2(cid:35) Tl →im ∞2TE |X(cid:101)T(ω)|2 = 2π −∞Q X(v) Tl →im ∞2T (ω−v)T dv (cid:90) ∞ = Q (v)δ(ω−v)dv =Q (ω). X X −∞ Since Q (ω)=F[R (τ)], we conclude that X X 1 S X(ω)= Tl →im ∞2TE[|X(cid:101)T(ω)|2]=Q X(ω)=F[R X(τ)]. Lemma 10.5. 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 lim Q (v)2T dv =Q (ω). (10.56) T→∞2π −∞ X (ω−v)T X To prove this lemma, we first define δ (ω)=2T(sin(ωT))2. It is sufficient to show that T ωT (cid:12) (cid:12) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 (cid:12) (cid:12) (cid:12) (cid:12)Tl →im ∞2π −∞Q X(v)2T (ω−v)T dv−Q X(ω)(cid:12) (cid:12)→0 as T →∞. (10.57) We will proceed by demonstrating the following three facts about δ (ω): T 674
10.8. APPENDIX 1. 1 (cid:90) ∞ δ (ω)dω =1 2π T −∞ . 2. For any (cid:52)>0, (cid:90) δ (ω)dω →0 as T →∞ T {ω:|ω|>(cid:52)} . 3. For any |ω|≥(cid:52)>0, we have |δ (ω)|≤ 2 . T T(cid:52)2 Proof of Fact 1. 1 (cid:90) ∞ 1 (cid:90) ∞ (cid:18) sin(ωT)(cid:19)2 δ (ω)dω = 2T dω. 2π T 2π ωT −∞ −∞ (cid:124) (cid:123)(cid:122) (cid:125) sinc2(ωT) Note that (cid:18) (cid:19) t Λ ←→2Tsinc2(ωT). 4T Therefore, 1 (cid:90) ∞ 1 (cid:90) ∞ 2Tsinc2(ωT)dω = 2Tsinc2(ωT)ejω0 dω 2π 2π −∞ −∞ (cid:18) (cid:19) 0 =Λ =1. 4T Proof of Fact 2. δ (ω) is symmetric, so, it is sufficient to check only one side: T (cid:90) ∞ (cid:90) ∞ (cid:18) sin(ωt)(cid:19)2 δ (ω)dω = 2T dω T ωT (cid:52) (cid:52) 2T (cid:90) ∞ sin2(ωt) = dω T2 ω2 (cid:52) 2 (cid:90) ∞ 1 ≤ dω |sin(.)|2 ≤1 T ω2 (cid:52) 2 (cid:20) 1(cid:21)∞ 2 = − = →0 as T →∞. T ω T(cid:52) (cid:52) Proof of Fact 3. (cid:18) sin(ωT)(cid:19)2 (cid:18) 1 (cid:19) 2 2 |δ (ω)|=2T ≤2T = ≤ . T ωT (ωT)2 ω2T T(cid:52)2 Proof of Lemma. Consider Q (ω). By Property 1, X 1 (cid:90) ∞ 1 (cid:90) ∞ Q (ω)=Q (ω). δ (ω−v)dv = Q (ω)δ (ω−v)dv. X X 2π T 2π X T −∞ −∞ 675
CHAPTER 10. RANDOM PROCESSES Therefore, (cid:12) (cid:12) 1 (cid:90) ∞ (cid:12) (cid:12) (cid:12) (cid:12)2π Q X(v)δ T(ω−v)dv−Q X(ω)(cid:12) (cid:12) −∞ (cid:12) (cid:12) 1 (cid:90) ∞ 1 (cid:90) ∞ (cid:12) (cid:12) =(cid:12) (cid:12)2π Q X(v)δ T(ω−v)dv− 2π Q X(ω)δ T(ω−v)dv(cid:12) (cid:12) −∞ −∞ 1 (cid:12) (cid:12)(cid:90) ∞ (cid:12) (cid:12) 1 (cid:90) ∞ (cid:12) (cid:12) = 2π(cid:12) (cid:12) (Q X(v)−Q X(ω))δ T(ω−v)dv(cid:12) (cid:12)≤ 2π (cid:12)Q X(v)−Q X(ω)(cid:12)δ T(ω−v)dv. −∞ −∞ For any (cid:15)>0, let (cid:52) be a constant such that |ω−v|<(cid:52) whenever |Q (v)−Q (ω)|<(cid:15). X X Then we can partition the above integral into 1 (cid:90) ∞ (cid:12) (cid:12) 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv = 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv (1) −∞ ω−(cid:52) 1 (cid:90) ∞ (cid:12) (cid:12) + 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv (2) ω+(cid:52) 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) + 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv. (3) −∞ Partition (1) above can be evaluated as follows: 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω−(cid:52) 1 (cid:90) ω+(cid:52) ≤ (cid:15)δ (ω−v)dv 2π T ω−(cid:52) (cid:15) (cid:90) ω+(cid:52) = δ (ω−v)dv 2π T ω−(cid:52) (cid:15) (cid:90) ∞ ≤ δ (ω−v)dv =(cid:15), 2π T −∞ where the last inequality holds because δ (ω−v)≥0. Since (cid:15) can be arbitrarily small, the T only possibility for 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω−(cid:52) for all (cid:15) is that the integral is 0. Partition (2) above can be evaluated as follows: 1 (cid:90) ∞ (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω+(cid:52) 1 (cid:90) ∞ (cid:0)(cid:12) (cid:12) (cid:12) (cid:12)(cid:1) ≤ 2π (cid:12)Q X(ω)(cid:12)+(cid:12)Q X(v)(cid:12) δ T(ω−v)dv ω+(cid:52) 1 (cid:90) ∞ 1 (cid:90) ∞ =Q (ω) δ (ω−v)dv+ Q (v)δ (ω−v)dv. X 2π T 2π X T ω+(cid:52) ω+(cid:52) 676
10.8. APPENDIX By Property 2, 1 (cid:82)∞ δ (ω−v)dv →0 as T →∞. By Property 3, 2π ω+(cid:52) T 1 (cid:90) ∞ 1 2 (cid:90) ∞ Q (v)δ (ω−v)dv ≤ Q (v)dv →0. 2π X T 2πT(cid:52)2 X ω+(cid:52) ω+(cid:52) (cid:124) (cid:123)(cid:122) (cid:125) <∞becauseQX(v)=F[RX(τ)] Therefore, we conclude that 1 (cid:90) ∞ Q (v)δ (ω−v)dv →0 as T →∞. 2π X T ω+(cid:52) and hence (1), (2) and (3) all →0 as T →∞. So we have (cid:12) (cid:12) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 (cid:12) (cid:12) (cid:12) (cid:12)Tl →im ∞2π −∞Q X(v)2T (ω−v)T dv−Q X(ω)(cid:12) (cid:12)→0 as T →∞, which completes the proof. 10.8.1 The Mean-Square Ergodic Theorem The mean-square ergodic theorem states that for any WSS random process, the statistical average is the same as the temporal average. This provides an important tool in practice because finding the statistical average is typically very difficult. With the mean ergodic theorem, one can easily estimate the statistical average using the temporal average. Theorem 10.11 (Mean-Square Ergodic Theorem). Let Y(t) be a WSS process, with mean E[Y(t)]=m and autocorrelation function R (τ). Assume that the Fourier Y transform of R (τ) exists. Define Y 1 (cid:90) T def M = Y(t)dt. (10.58) T 2T −T Then E(cid:104)(cid:12) (cid:12)M T −m(cid:12) (cid:12)2(cid:105) →0 as T →∞. Proof of Mean Ergodic Theorem. Let X(t)=Y(t)−m. It follows that 1 (cid:90) T 1 (cid:90) T M −m= Y(t)dt−m= X(t)dt. T 2T 2T −T −T We define the finite-window approximation of X(t): (cid:26) X(t), −T ≤t≤T, X (t)= T 0, elsewhere. Then the difference M −m can be computed as T M T −m= 21 T (cid:90) T X(t)dt= 21 T (cid:90) ∞ X(t)e−j0t dt= 21 TX(cid:101)T(ω)(cid:12) (cid:12) ω=0 = X(cid:101) 2T T(0) . −T −∞ 677
CHAPTER 10. RANDOM PROCESSES Taking the expectation of the squares yields E(cid:2) |M −m|2(cid:3) = E(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) . T 4T2 Recall from the Einstein-Wiener-Khinchin theorem, 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(ω)(cid:12) (cid:12)2(cid:105) = 21 π (cid:90) ∞ S X(v)2T (cid:18) sin (( ω(ω −− v)v T)T)(cid:19)2 dv. −∞ Putting the limit T →∞, if we have that 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 lim S (v)2T dv =S (ω), T→∞2π −∞ X (ω−v)T X then we will have 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(ω)(cid:12) (cid:12)2(cid:105) →S X(ω) and 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) →S X(0). Hence, Tl →im ∞E(cid:104)(cid:12) (cid:12)M T −m(cid:12) (cid:12)2(cid:105) = Tl →im ∞21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) = Tl →im ∞21 TS X(0)=0. This completes the proof. 10.9 References Basic texts The following textbooks are basic texts about random processes. They offer many comple- mentary materials to our book. For example, we omitted the topics of straightly stationary processes and memoryless properties. We have also omitted a few classical examples, such as the random telegraph signal, the incremental independence of Poisson processes, and Markov chains. These materials can be found in the texts below. 10-1 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, Illustrated edition, 2006. 10-2 AlbertoLeon-Garcia,Probability, Statistics, and Random Processes For Electrical En- gineering, Pearson, 3rd Edition, 2007. 10-3 AthanasiosPapoulis,S.UnnikrishnaPillai,Probability,RandomVariablesandStochas- tic Processes, McGraw-Hill, 4th Edition, 2012. 10-4 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. 678
10.10. PROBLEMS 10-5 EugeneWongandBruceHajek,StochasticProcessesinEngineeringSystems,Springer- Verlag, 1985. 10-6 Bruce Hajek, Random Processes for Engineers, Cambridge University Press, 2015. 10-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. 10-8 Robert G. Gallager, Stochastic Processes: Theory for Applications, Cambridge Uni- versity Press, 1st Edition, 2014. Signal and systems / Fourier transforms The following references are classic references on signal and systems. 10-9 Alan Oppenheim and Ronald Schafer, Discrete-Time Signal Processing, 2nd Edition, Prentice Hall 1999. 10-10 Alan Oppenheim and Alan Willsky, Signals and Systems, Pearson, 2nd Edition, 1996. 10-11 Martin Vetterli, Jelena Kovacevic, and Vivek K. Goyal, Foundations of Signal Pro- cessing, Cambridge University Press, 3rd Edition, 2014. 10-12 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Engineering applications 10-13 John G. Proakis and Masoud Salehi, Communication Systems Engineering, Pearson, 2nd Edition, 2001. 10-14 Rodger E. Ziemer, William H. Tranter, Principles of Communications, Wiley, 7th Edition, 2014. 10-15 Joseph W. Goodman, Statistical Optics, Wiley, 2015. 10.10 Problems Exercise 1. (Video Solution) Consider the random process X(t)=2Acos(t)+(B−1)sin(t), where A and B are two independent random variables with E[A]=E[B]=0, and E[A2]= E[B2]=1. (a) Find µ (t). X 679
CHAPTER 10. RANDOM PROCESSES (b) Find R (t ,t ). X 1 2 (c) Find C (t ,t ). X 1 2 Exercise 2. (Video Solution) Let X[n] be a discrete-time random process with mean function m [n] = E{X[n]} and X correlation function R [n,m]=E{X[n]X[m]}. Suppose that X ∞ (cid:88) Y[n]= h[n−i]X[i]. (10.59) i=−∞ (a) Find µ [n]. Y (b) Find R [n,m]. XY Exercise 3. (Video Solution) Let Y(t)=X(t)−X(t−d). (a) Find R (τ) and S (ω). X,Y X,Y (b) Find R (τ). Y (c) Find S (ω). Y Exercise 4. (Video Solution) Let X(t) be a zero-mean WSS process with autocorrelation function R (τ). Let Y(t) = X X(t)cos(ωt+Θ), where Θ∼uniform(−π,π) and Θ is independent of the process X(t). (a) Find the autocorrelation function R (τ). Y (b) Find the cross-correlation function of X(t) and Y(t). (c) Is Y(t) WSS? Why or why not? Exercise 5. (Video Solution) A WSS process X(t) with autocorrelation function R (τ)=1/(1+τ2) X is passed through an LTI system with impulse response h(t)=3sin(πt)/(πt). Let Y(t) be the system output. Find S (ω) and sketch S (ω). Y Y Exercise 6. (Video Solution) A white noise X(t) with power spectral density S (ω)=N /2 is applied to a lowpass filter X 0 h(t) with impulse response 1 h(t)= e−t/RC, t>0. (10.60) RC Find the followings. 680
10.10. PROBLEMS (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y Exercise 7. (Video Solution) Consider a WSS process X(t) with autocorrelation function R (τ)=sinc(πτ). X The process is sent to an LTI system with input-output relationship d2 d d2 d 2 Y(t)+2 Y(t)+4Y(t)=3 X(t)−3 X(t)+6X(t). dt2 dt dt2 dt Find the autocorrelation function R (τ). Y Exercise 8. (Video Solution) Given the functions a(t), b(t) and c(t), let g(t,1)=a(t), g(t,2)=b(t), g(t,3)=c(t). Let X(t) = g(t,Z), where Z is a discrete random variable with PMF P[Z = 1] = p , 1 P[Z =2]=p and P[Z =3]=p . Find, in terms of the p , p , p , a(t), b(t) and c(t), 2 3 1 2 3 (a) µ (t). X (b) R (t ,t ). X 1 2 Exercise 9. In the previous problem, let a(t)=e−λ|t|, b(t)=sin(πt) and c(t)=−1. (a) Choose p , p , p so that X(t) is WSS. 1 2 3 (b) Choose p , p , p so that X(t) is not WSS. 1 2 3 Exercise 10. (Video Solution) FindtheautocorrelationfunctionR (τ)correspondingtoeachofthefollowingpowerspec- X tral densities: (a) δ(ω−ω )+δ(ω+ω ). 0 0 (b) e−ω2/2. (c) e−|ω|. 681
CHAPTER 10. RANDOM PROCESSES Exercise 11. (Video Solution) A WSS process X(t) with autocorrelation function R X(τ) = e−τ2/(2σ T2) is passed through an LTI system with transfer function H(ω) = e−ω2/(2σ H2). Denote the system output by Y(t). Find the followings. (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y Exercise 12. (Video Solution) A white noise X(t) with power spectral density S (ω)=N /2 is applied to a lowpass filter X 0 h(t) with (cid:40) 1−ω2, if |ω|≤π, H(ω)= 0, otherwise. Find E[|Y(t)|2], where Y(t) is the output of the filter. Exercise 13. (Video Solution) Let X(t) be a WSS process with correlation function (cid:40) 1−|τ|, if −1≤τ ≤1, R (τ)= (10.61) X 0, otherwise. It is known that when X(t) is input to a system with transfer function H(ω), the system output Y(t) has a correlation function sinπτ R (τ)= . (10.62) Y πτ Find the transfer function H(ω). Exercise 14. Consider the system (cid:90) t Y(t)=e−t eτX(τ)dτ. −∞ Assume that X(t) is zero-mean white noise with power spectral density S (ω) = N /2. X 0 Find the followings: (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y 682
Chapter A Appendix Useful Identities ∞ 1. (cid:80) rk =1+r+r2+···= 1 1−r k=0 n 2. (cid:80) k =1+2+3+···+n= n(n+1) 2 k=1 ∞ 3. ex = (cid:80) xk =1+ x + x2 +··· k! 1! 2! k=0 ∞ 4. (cid:80) krk−1 =1+2r+3r2+···= 1 (1−r)2 k=1 n 5. (cid:80) k2 =12+22+33+···+n2 = n3 + n2 + n 3 2 6 k=1 n 6. (a+b)n = (cid:80) (cid:0)n(cid:1) akbn−k k k=0 Common Distributions Distribution PMF/PDF E[X] Var[X] M (s) X Bernoulli p (1)=pandp (0)=1−p p p(1−p) 1−p+pes X X Binomial p (k)=(cid:0)n(cid:1) pk(1−p)n−k np np(1−p) (1−p+pes)n X k 1 1−p pes Geometric p (k)=p(1−p)k−1 X p p2 1−(1−p)es λke−λ Poisson p (k)= λ λ eλ(es−1) X k! 1 (cid:26) (x−µ)2(cid:27) (cid:26) σ2s2(cid:27) Gaussian f (x)= √ exp − µ σ2 exp µs+ X 2πσ2 2σ2 2 1 1 λ Exponential f (x)=λexp{−λx} X λ λ2 λ−s a+b (b−a)2 esb−esa Uniform f (x)= 1 X b−a 2 12 s(b−a) 683
CHAPTER A. APPENDIX Sum of Two Random Variables X X Sum X +X 1 2 1 2 Bernoulli(p) Bernoulli(p) Binomial(2,p) Binomial(n,p) Binomial(m,p) Binomial(m+n,p) Poisson(λ ) Poisson(λ ) Poisson(λ +λ ) 1 2 1 2 Exponential(λ) Exponential(λ) Erlang(2,λ) Gaussian(µ ,σ2) Gaussian(µ ,σ2) Gaussian(µ +µ , σ2+σ2) 1 1 2 2 1 2 1 2 Fourier Transform Table (cid:90) ∞ F(ω)= f(t)e−jωt dt. −∞ f(t)←→F(ω) f(t)←→F(ω) 1 (cid:18)Wt(cid:19) 2π (cid:16) ω (cid:17) 1. e−atu(t)←→ ,a>0 10. sinc2 ←→ ∆ a+jω 2 W 2W 1 ω 2. eatu(−t)←→ ,a>0 11. e−atsin(ω t)u(t)←→ 0 ,a>0 a−jω 0 (a+jω)2+ω2 0 2a a+jω 3. e−a|t|←→ ,a>0 12. e−atcos(ω t)u(t)←→ ,a>0 a2+ω2 0 (a+jω)2+ω2 0 a2 (cid:110) (cid:111) √ (cid:110) (cid:111) 4. ←→πae−a|ω|,a>0 13. exp − t2 ←→ 2πσexp −σ2ω2 a2+t2 2σ2 2 1 5. te−atu(t)←→ ,a>0 14. δ(t)←→1 (a+jω)2 n! 6. tne−atu(t)←→ ,a>0 15. 1←→2πδ(ω) (a+jω)n+1 (cid:18)t(cid:19) (cid:16)ωτ(cid:17) 7. rect τ ←→τsinc 2 16. δ(t−t 0)←→e−jwt0 π (cid:16) ω (cid:17) 8. sinc(Wt)←→ Wrect 2W 17. ejω0t←→2πδ(ω−ω 0) (cid:18)t(cid:19) τ (cid:16)ωτ(cid:17) 9. ∆ τ ←→ 2sinc2 4 18. f(t)ejω0t←→F(ω−ω 0) Some definitions: sin(t) sinc(t)= t (cid:40) 1, −0.5≤t≤0.5, rect(t)= 0, otherwise. (cid:40) 1−2|t|, −0.5≤t≤0.5, ∆(t)= 0, otherwise. 684
Basic Trigonometric Identities ejθ =cosθ+jsinθ sin2θ =2sinθcosθ cos2θ =2cos2θ−1 1 cosAcosB = (cos(A+B)+cos(A−B)) 2 1 sinAsinB =− (cos(A+B)−cos(A−B)) 2 1 sinAcosB = (sin(A+B)+sin(A−B)) 2 1 cosAsinB = (sin(A+B)−sin(A−B)) 2 cos(A+B)=cosAcosB−sinAsinB cos(A−B)=cosAcosB+sinAsinB sin(A+B)=sinAcosB+cosAsinB sin(A−B)=sinAcosB−cosAsinB 685
Index absolutely integrable, 183 definition, 143 almost sure convergence, 362 MATLAB and Python, 144 autocorrelation function properties, 146 2D visualization, 624 binomial series, 6 interpretation, 625, 635 binomial theorem, 6 LTI system, 647 proof, 9 properties, 634 birthday paradox, 31, 321 temporal average, 638 bootstrapping, 561 definition, 620 bootstrapped distribution, 564 MATLAB and Python, 628 confidence interval, 561 autocovariance function definition, 561 definition, 620 distribution of samples, 562 relationtoautocorrelationfunction,630 interpretation, 566 autoregressive model, 406, 661 MATLAB and Python, 567 linear prediction, 661 procedure, 563 MATLAB and Python, 407, 662 standard error, 567 prediction, 663 when to use, 562 Toeplitz, 662 Yule-Walker equation, 661 Cauchy distribution, 331, 360 Cauchy-Schwarz inequality, 261, 335 Basel problem, 5 Central Limit Theorem, 323, 367, 372, 381 basis functions, 405 Berry-Esseen Theorem, 375 Bayes’ theorem, 89 examples, 376 conditional probability, 81 interpretation, 375 law of total probability, 90 limitations, 379 Bayesian, 43 proof, 374 Bernoulli random variable characteristic function, 329 definition, 137 alternative definition, 329 MATLAB and Python, 137 Fourier transform, 330 maximum variance, 140 Chebyshev’s inequality, 341 properties, 138 proof, 342 bias-variance Chernoff’s bound, 343 average predictor, 433 compare with Chebyshev, 344 MATLAB and Python, 434 Chernoff, Herman, 343 noise-free case, 430 combination, 35 noisy case, 433 concave function, 336 trade off, 429 conditional distribution binomial random variable conditional expectation, 275 alternative definition, 148 conditional PDF, 272 686
INDEX conditional PMF, 267 examples, 653 conditional probability, 81 through LTI systems, 652 Bayes’ theorem, 89 cross-covariance function, 631 definition, 81 cross-correlation function, 631 independence, 85 cumulative distribution function properties, 84 continuous, 186 ratio, 81 discrete, 121 confidence interval, 543 left- and right-continuous, 190 bootstrapping, 561 MATLAB and Python, 186 critical value, 553 properties, 188 definition, 548 distribution of estimator, 546 delta function, 178 estimator, 545 discrete cosine transform (DCT), 23 examples, 549 eigenvalues and eigenvectors, 295 how to construct, 549 Gaussian, 296 interpretation, 547 MATLAB and Python, 296 margin of error, 554 Erd˝os-R´enyi graph, 140 MATLAB and Python, 552 MATLAB and Python, 480 number of samples, 555 even functions, 15 properties, 553 event, 61 standard error, 553 event space, 61 Student’s t-distribution, 556 expectation, 104 conjugate prior, 513 continuous, 180 convergence in distribution, 368 properties, 130, 182 convergence in probability, 356 transformation, 182 convex function, 336 center of mass, 127 convex optimization discrete, 125 CVXPY, 451 existence, 130, 183 convolution, 220, 641 exponential random variables correlation, 641 definition, 205 filtering, 641 MATLAB and Python, 205 correlation, 635 origin, 207, 209 autocorrelation function, 620 properties, 206 autocovariance function, 620 exponential series, 12 cross-correlation function, 652 convolution, 641 field, 64 correlation coefficient σ-field, 65 MATLAB and Python, 265 Borel σ-field, 65 properties, 263 Fourier transform, 647 definition, 263 table, 330 cosine angle, 26 characteristic function, 330 covariance, 262 frequentist, 43 covariance matrix, 289 Fundamental Theorem of Calculus, 17 independent, 289 chain rule, 19 cross power spectral density, 654 proof, 18 cross-correlation function cross-covariance function, 631 Gaussian random variables definition, 631 CDF, 214 687
INDEX definition, 211 joint PDF, 247 MATLAB and Python, 212 joint PMF, 245 origin, 220 joint expectation, 257 properties, 212 cosine angle, 258 standard Gaussian, 213 geometric random variable kurtosis, 216 definition, 149 MATLAB and Python, 217 MATLAB and Python, 150 Laplace transform, 324 properties, 151 law of large numbers, 323, 351, 381 geometric sequence strong law of large numbers, 361 finite, 4 weak law of large numbers, 354 infinite, 4 learning curve, 427 geometric series, 3 MATLAB and Python, 427 finite, 4 Legendre polynomial, 403 infinite, 4 MATLAB and Python, 404 harmonic series, 5 likelihood, 466, 468, 503 histogram, 2, 113 log-likelihood, 469 Hoeffding’s inequality, 348 linear algebra Hoeffding lemma, 348 basis vector, 23 proof, 348 representation, 23 hypothesis testing span, 22 p-value test, 569, 573 standard basis vector, 22 T-test, 576 linear combination, 21 Z-test, 576 linear model, 21 alternative hypothesis, 568 linear prediction, 661 critical level, 571 linear programming, 414 critical-value test, 569 linear regression definition, 568 MATLAB and Python, 30 MATLAB and Python, 570 linear time-invariant (LTI) null hypothesis, 568 convolution, 641 definition, 646 impulse response, 646 system, 646 independence, 85 conditional probability, 88 marginal distribution, 250 versus disjoint, 86 Markov’s inequality, 339 independent proof, 339 random variables, 252 tight, 341 independentandidenticallydistributed(i.i.d.),matrix calculus, 28 253 maximum-a-posteriori (MAP), 503 indicator function, 182 choosing prior, 505 inner product, 24 conjugate prior, 513 MATLAB and Python, 24 MAP versus LASSO, 519 MAP versus ML, 504 Jensen’s inequality, 336 MAP versus regression, 517 proof, 338 MAP versus ridge, 519 joint distribution posterior, 503, 512 definition, 241 prior, 503 joint CDF, 255 solution, 506 688
INDEX maximum-likelihood MATLAB and Python, 291 1D Gaussian, 484 covariance, 293 consistent estimator, 494 transformation, 293 estimation, 468 whitening, 299 estimator, 491 high-dimensional Gaussian, 486 Neyman-Pearson test, 579 image reconstruction, 481 decision rule, 584 independent observations, 469 likelihood ratio, 586 invariance principle, 500 rejection zone, 580 MATLAB and Python, 472 likelihood ratio test, 580 number of training samples, 474 norm, 24, 26 Poisson, 485 (cid:96) 1, 27 regression versus ML, 488 (cid:96) ∞, 27 social networks, 478 MATLAB and Python, 26 unbiased estimator, 492 weighted, 27 visualization, 471 normalization property, 112 mean, 199 odd functions, 15 mean function open and closed intervals, 45 LTI system, 647 optimal linear filter, 656 definition, 620 deconvolution, 668 MATLAB and Python, 623 denoising, 665 mean squared error (MSE), 521, 522 orthogonality condition, 661 measure, 68 Wiener filter, 664 almost surely, 73 Yule-Walker equation, 659 finite sets, 68 input function, 657 intervals, 68 prediction, 657 Lebesgue integration, 71 target function, 657 measure zero sets, 71 orthogonality condition, 661 definition, 72 overdetermined system, 409 examples, 72 overfitting, 418 regions, 68 factors, 420 size, 69 LASSO, 454 median, 196 linear analysis, 425 minimum mean-square estimation (MMSE), source, 429 521 conditional expectation, 524 parameter estimation, 165, 465 Gaussian, 530 Pascal triangle, 8 minimum-norm least squares, 411 Pascal’s identity, 7 mode, 198 performance guarantee model selection, 165 average case, 321 moment, 133 worst case, 321 continuous case, 184 permutation, 33 moment-generating function, 322, 324 Poisson random variable common distributions, 326 applications, 154 derivative, 325 definition, 152 existence, 331 origin, 157 sum of random variables, 327 photon arrivals, 161 multidimensional Gaussian, 290 Poisson approximation of binomial, 159 689
INDEX properties, 155 random process MATLAB and Python, 152 discrete time, 656 positive semi-definite, 297 definition, 614 posterior, 466, 503 example power spectral density, 639 random amplitude, 614 Einstein-Wiener-KhinchinTheorem,639 random phase, 615 through LTI systems, 649 function, 614 cross power spectral density, 642, 654 independent, 631 eigendecomposition, 641 index, 614 Fourier transform, 642 sample space, 616 origin, 643 statistical average, 616 wide-sense stationary, 642 temporal average, 616 PR (precision-recall) curve uncorrelated, 632 definition, 603 random variable, 104, 105 MATLAB and Python, 605 function of, 223 precision, 603 transformation of, 223 recall, 603 random vector, 286 principal-component analysis, 303 expectation, 288 limitations, 311 independent, 286 main idea, 303 regression, 391, 394 MATLAB and Python, 306 loss, 394 prior, 466, 503 MATLAB and Python, 400 probability, 43, 45 outliers, 412 measure of a set, 43 prediction model, 394 probability axioms, 74 solution, 397 additivity, 75 linear model, 395 corollaries, 77 outliers, 417 countable additivity, 75 squared error, 396 measure, 76 regularization, 440 non-negativity, 75 LASSO, 449 normalization, 75 MATLAB and Python, 442 probability density function, 172 parameter, 445 definition, 175 ridge, 440 discrete cases, 178 sparse solution, 449 properties, 174 robust linear regression, 412 intuition, 172 MATLAB and Python, 416 per unit length, 173 linear programming, 414 probability inequality, 323, 333 ROC probability law, 66 comparing performance, 599 definition, 66 computation, 594 examples, 66 definition, 591 measure, 67 MATLAB and Python, 595 probability mass function, 104, 110 properties, 593 probability space Receiver operating characteristic, 591 (Ω,F,P), 58 sample average, 320, 351 Rademacher random variable, 140 sample space, 59 random number generator, 228 continuous outcomes, 59 690
INDEX counterexamples, 61 Taylor approximation, 10 discrete outcomes, 59 first-order, 11 examples, 59 second-order, 11 exclusive, 61 exponential, 12 exhaustive, 61 logarithmic, 13 functions, 59 testing error, 420 set, 45 analysis, 424 associative, 56 testing set, 420 commutative, 56 Three Prisoners problem, 92 complement, 52 Toeplitz, 407, 633 countable, 45 training error, 420 De Morgan’s Law, 57 analysis, 421 difference, 53 training set, 420 disjoint, 54 type 1 error distributive, 56 definition, 581 empty set, 48 false alarm, 582 finite, 45 false positive, 581 improper subset, 47 power of test, 583 infinite, 45 type 2 error intersection, 50 definition, 581 finite, 50 false negative, 581 infinite, 51 miss, 582 of functions, 46 partition, 55 underdetermined system, 409 proper subset, 47 uniform random variables, 202 subset, 47 MATLAB and Python, 203 uncountable, 45 union bound, 333 union, 48 validation, 165 finite, 48 variance, 134 infinite, 49 properties, 135 universal set, 48 continuous case, 184 simplex method, 414 skewness, 216 white noise, 641 MATLAB and Python, 217 wide-sense stationary, 632 statistic, 320 jointly, 652 Student’s t-distribution Wiener filter, 664 definition, 556 deconvolution, 668 degrees of freedom, 557 definition, 664 MATLAB and Python, 558 denoising, 665 relation to Gaussian, 557 MATLAB and Python, 664 sum of random variables, 280 power spectral density, 665 Bernoulli, 327 recursive filter, 664 binomial, 328 Gaussian, 283, 329 Yule-Walker equation, 659 Poisson, 328 MATLAB and Python, 662 common distributions, 282 convolution, 281 symmetric matrices, 296 691
--- End of content from file: 2021 Chan, Stanley ~ Introduction to Probability for Data Science [Michigan Publishing] _.pdf ---


--- Start of content from file: The Science of Nutrition Debunk the Diet Myths and Learn How to Eat Responsibly for Health and Happiness (DK Science of).pdf ---
Art Editor Alison Gardner Editors Andrea Page, Holly Kyte, Salima Hirani Project Designer Louise Brigenshaw Senior Editor Alastair Laing US Editor Jennette ElNaggar Design Manager Marianne Markham Managing Editor Dawn Henderson DTP and Design Coordinator Heather Blagden Science Production Editor David Almond Production Controller Luca Bazzoli Jacket Coordinator Lucy Philpott Art Director Maxine Pedliham Publishing Director Katie Cowan Photographer Stephanie McLeod Nutrition Illustrators Nelli Velichko, Sally Caulwell First American Edition, 2021 Published in the United States by DK Publishing 1450 Broadway, Suite 801, New York, NY 10018 Text copyright © Rhitrition Limited 2021 Copyright © 2021 Dorling Kindersley Limited DK, a Division of Penguin Random House LLC 21 22 23 24 25 10 9 8 7 6 5 4 3 2 1 001–324057–Dec/2021 All rights reserved. Without limiting the rights under the copyright reserved above, no part of this publication may be reproduced, stored in or introduced into a retrieval system, or transmitted, in any form, or by any means (electronic, mechanical, photocopying, recording, or otherwise), without the prior written permission of the copyright owner. Published in Great Britain by Dorling Kindersley Limited. A catalog record for this book is available from the Library of Congress. ISBN 978-0-7440-3989-4 Printed and bound in China www.dk.com This book was made with Forest Stewardship Council ™ certified paper – one small step in DK’s commitment to a sustainable future. For more information go to www.dk.com/our-green-pledge UUSS__000022--000033__ttiittllee..iinndddd 22 2244//0088//22002211 1166::3333
RHIANNON LAMBERT Art Editor Alison Gardner Editors Andrea Page, Holly Kyte, Salima Hirani Project Designer Louise Brigenshaw Senior Editor Alastair Laing US Editor Jennette ElNaggar Design Manager Marianne Markham The Managing Editor Dawn Henderson DTP and Design Coordinator Heather Blagden SScciieennccee Production Editor David Almond Production Controller Luca Bazzoli Jacket Coordinator Lucy Philpott Art Director Maxine Pedliham Publishing Director Katie Cowan of Photographer Stephanie McLeod NNuuttrriittiioonn Illustrators Nelli Velichko, Sally Caulwell First American Edition, 2021 Published in the United States by DK Publishing 1450 Broadway, Suite 801, New York, NY 10018 Text copyright © Rhitrition Limited 2021 Copyright © 2021 Dorling Kindersley Limited DK, a Division of Penguin Random House LLC 21 22 23 24 25 10 9 8 7 6 5 4 3 2 1 001–324057–Dec/2021 All rights reserved. Without limiting the rights under the copyright reserved above, no part of this publication may be reproduced, stored in or introduced into a retrieval system, or transmitted, in any form, or by any means (electronic, mechanical, photocopying, recording, or otherwise), without the prior written permission of the copyright owner. Published in Great Britain by Dorling Kindersley Limited. A catalog record for this book is available from the Library of Congress. ISBN 978-0-7440-3989-4 Printed and bound in China www.dk.com This book was made with Forest Stewardship Council ™ certified paper – one small step in DK’s commitment to a sustainable future. For more information go to www.dk.com/our-green-pledge UUSS__000022--000033__ttiittllee..iinndddd 22 2244//0088//22002211 1166::3333 UUSS__000022--000033__ttiittllee..iinndddd 33 2244//0088//22002211 1166::3333
UUSS__000044--000055__ccoonntteennttss..iinndddd 44 2244//0088//22002211 1166::2222
CONTENTS Introduction 6 What Is Nutrition? 8 How Can We Eat Well? 34 Should I Go On a Diet? 88 Should I Choose Plant-based Nutrition? 116 Can We Eat for Better Health Outcomes? 134 How Does Nutrition Affect Child Development? 176 Can We Eat to Support Mental Health? 200 Index 212 Acknowledgments 223 About the Author 224 UUSS__000044--000055__ccoonntteennttss..iinndddd 44 2244//0088//22002211 1166::2222 UUSS__000044--000055__ccoonntteennttss..iinndddd 55 2244//0088//22002211 1166::2222
INTRODUCTION Enrolling to study nutrition at the University of But after four years suffering the pressures of the Roehampton was a life-changing decision for me. music industry, compounded by fueling myself It was one that, in part, I was driven to from with passing dietary fads, I looked at my career firsthand experience of how easily we can fall into and thought, I don’t want this anymore. While dysfunctional relationships with food, how pressures a complete career change is rare, it is one of the to look a certain way can lead to a pursuit of dietary best moves I ever made. quick fixes that are a world away from a relaxed and Following four grueling yet thrilling years, healthy enjoyment of food. I obtained undergraduate and master’s degrees At the age of 17, I was an aspiring soprano singer, in nutrition and started a new life as a fledgling thrust into the limelight having won Classic FM’s nutritionist. Having founded Rhitrition in 2016, young musician of the year. Schooled at The Royal a private Harley Street clinic, my specialist team Academy of Music and singing on stage at the likes and I now work with individuals and leading brands of the Royal Albert Hall and Paris Fashion Week, to support their health and well-being. Our ethos is I appeared to be living the most thrilling life. simply that we believe in empowering everyone to UUSS__000066--000077__iinnttrroodduuccttiioonn..iinndddd 66 2244//0088//22002211 1166::2222
INTRODUCTION Enrolling to study nutrition at the University of But after four years suffering the pressures of the embrace a healthy way of living through the food we Covering every conceivable topic—from gut bacteria Roehampton was a life-changing decision for me. music industry, compounded by fueling myself enjoy and the life we lead. Our bodies are as unique to weight management to heart health and immune It was one that, in part, I was driven to from with passing dietary fads, I looked at my career as our personalities, so each of us should strive to support to vegan diets and intermittent fasting firsthand experience of how easily we can fall into and thought, I don’t want this anymore. While find a way of eating that works for us individually. (and everything in between)—The Science of Nutrition dysfunctional relationships with food, how pressures a complete career change is rare, it is one of the It is through my experience in clinic that I have offers clear answers with informative graphics, to look a certain way can lead to a pursuit of dietary best moves I ever made. come to learn just how widespread pseudoscience is. making it easy to understand. I know that in quick fixes that are a world away from a relaxed and Following four grueling yet thrilling years, It’s everywhere, from the labels in the supermarket, debunking popular myths and diets, this book will healthy enjoyment of food. I obtained undergraduate and master’s degrees on the ads that pop up in your Instagram feed, and enable you to make informed decisions that are best At the age of 17, I was an aspiring soprano singer, in nutrition and started a new life as a fledgling most of all in magazines—bold statements in jargon suited to you, about what, when, and how to eat thrust into the limelight having won Classic FM’s nutritionist. Having founded Rhitrition in 2016, that give the false impression they’re supported by responsibly for health and happiness. young musician of the year. Schooled at The Royal a private Harley Street clinic, my specialist team laboratory research. With this book, I’m betting on Academy of Music and singing on stage at the likes and I now work with individuals and leading brands the opposite approach—cold, hard facts that will of the Royal Albert Hall and Paris Fashion Week, to support their health and well-being. Our ethos is teach you and help shape the way you think about I appeared to be living the most thrilling life. simply that we believe in empowering everyone to food like never before. UUSS__000066--000077__iinnttrroodduuccttiioonn..iinndddd 66 2244//0088//22002211 1166::2222 UUSS__000066--000077__iinnttrroodduuccttiioonn..iinndddd 77 2244//0088//22002211 1166::2222
UUSS__000088--000099__cchhaapptteerr__11..iinndddd 88 2244//0088//22002211 1166::2222
WHAT IS NUTRITION? What Do We Mean by Nutrition? 10 Is Hydration Part of Nutrition? 24 What Are Carbohydrates? 12 What Is Digestion? 26 What Is Protein? 14 How Does the Body Absorb Nutrients What Is Fat? 16 During Digestion? 28 What Is Fiber? 18 What Is Metabolism? 30 What Are Vitamins? 20 Am I Malnourished? 32 What Are Minerals? 22 UUSS__000088--000099__cchhaapptteerr__11..iinndddd 88 2244//0088//22002211 1166::2222 UUSS__000088--000099__cchhaapptteerr__11..iinndddd 99 2255//0088//22002211 1199::1144
10 WHAT IS NUTRITION? 11 WHAT DO WE MEAN BY NUTRITION? Nutrition is the process by which we provide the body with the nutrients necessary for good health and growth. Essentially, nutrition is nourishment for the body via the foods you eat. Eating nutritiously enables you to enjoy the Because we need macronutrients in much larger sense of well-being that comes with good health. quantities than micronutrients, it’s easy to Adequate nutrition is vital if you want your body underestimate the importance of the latter and to be healthy and well maintained and have the best focus more on including the former in the diet. chance at fighting disease and operating optimally. But the absence of micronutrients can lead to severe We get our nourishment mostly from consequences. The World Health Organization macronutrients, which are the dietary main players, suggests that micronutrient deficiency is responsible but micronutrients are no less important. A balanced for some of the most common nutritional deficiencies, diet contains many different types of both. such as anemia (iron), rickets, and osteoporosis (vitamin D), all of which can have a debilitating MACRONUTRIENTS effect on the body’s well-being and performance. There are three macronutrients: carbohydrates You can get most of your vitamins and minerals (see pages 12–13), protein (see pages 14–15), and from plants. Plant foods come in various colors and fat (see pages 16–17). These three macronutrients shades, and their color is linked to the nutrients they give your body the energy it needs to operate. contain. For instance, orange often indicates the The body takes care of many processes that presence of vitamin A; purples indicate antioxidants; require energy without our conscious control greens contain vitamin K and iron; and red (such as breathing, temperature regulation, vegetables contain lots of vitamin C. So a colorful digestion, and cell repair). And, of course, diet will give you a well-varied nutrient intake. your body requires energy for movement. Each The daily requirement of each micronutrient of the macronutrients is required in relatively varies between individuals, but if your diet is healthy large amounts every day to support many of and balanced, including foods from both plants and your body’s vital functions. animal sources, you are likely to be ingesting all the micronutrients your body needs without the need MICRONUTRIENTS for supplements. For those who don’t eat animal We need vitamins and minerals, known as products, a well-thought-through diet alongside micronutrients, in much smaller quantities than targeted supplementation will provide the essential macronutrients, but they are vital for the body to nutrients you need (see pages 128–131). However, do carry out its functions. In children, they are also see a registered dietitian nutritionist if you want to essential for healthy growth and development. enhance your diet for optimal performance. UUSS__001100--001111__wwhhaatt__iiss__nnuuttrriittiioonn..iinndddd 1100 2244//0088//22002211 1166::2222
1100 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 1111 WHAT DO WE MEAN Unique nutritional needs BY NUTRITION? THERE IS NO ONE-SIZE-FITS-ALL IN NUTRITION. Your optimal intake of each macronutrient and Nutrition is the process by which we provide the body with the micronutrient depends on numerous factors. Your age, nutrients necessary for good health and growth. Essentially, gender, genetics, metabolism, level of physical activity, nutrition is nourishment for the body via the foods you eat. and personal preferences all have a part to play in establishing the best foods to keep working optimally. Learn to listen to your body and notice how you feel in response to your diet. See your primary care physician if at all concerned. Eating nutritiously enables you to enjoy the Because we need macronutrients in much larger sense of well-being that comes with good health. quantities than micronutrients, it’s easy to Adequate nutrition is vital if you want your body underestimate the importance of the latter and to be healthy and well maintained and have the best focus more on including the former in the diet. chance at fighting disease and operating optimally. But the absence of micronutrients can lead to severe VEGGIES We get our nourishment mostly from consequences. The World Health Organization macronutrients, which are the dietary main players, suggests that micronutrient deficiency is responsible PACKED FULL OF MICRONUTRIENTS AND but micronutrients are no less important. A balanced for some of the most common nutritional deficiencies, FLAVOR, VEGGIES diet contains many different types of both. such as anemia (iron), rickets, and osteoporosis PROVIDE VARIETY AND (vitamin D), all of which can have a debilitating NOURISHMENT MACRONUTRIENTS effect on the body’s well-being and performance. There are three macronutrients: carbohydrates You can get most of your vitamins and minerals (see pages 12–13), protein (see pages 14–15), and from plants. Plant foods come in various colors and fat (see pages 16–17). These three macronutrients shades, and their color is linked to the nutrients they give your body the energy it needs to operate. contain. For instance, orange often indicates the CARBS The body takes care of many processes that presence of vitamin A; purples indicate antioxidants; CARBOHYDRATES ARE A require energy without our conscious control greens contain vitamin K and iron; and red MAJOR ENERGY SOURCE (such as breathing, temperature regulation, vegetables contain lots of vitamin C. So a colorful FOR THE BODY AND ALSO PROVIDE FIBER digestion, and cell repair). And, of course, diet will give you a well-varied nutrient intake. FOR HEALTHY your body requires energy for movement. Each The daily requirement of each micronutrient DIGESTION of the macronutrients is required in relatively varies between individuals, but if your diet is healthy large amounts every day to support many of and balanced, including foods from both plants and your body’s vital functions. animal sources, you are likely to be ingesting all the micronutrients your body needs without the need MICRONUTRIENTS for supplements. For those who don’t eat animal We need vitamins and minerals, known as products, a well-thought-through diet alongside micronutrients, in much smaller quantities than targeted supplementation will provide the essential macronutrients, but they are vital for the body to nutrients you need (see pages 128–131). However, do carry out its functions. In children, they are also see a registered dietitian nutritionist if you want to essential for healthy growth and development. enhance your diet for optimal performance. UUSS__001100--001111__wwhhaatt__iiss__nnuuttrriittiioonn..iinndddd 1100 2244//0088//22002211 1166::2222 UUSS__001100--001111__wwhhaatt__iiss__nnuuttrriittiioonn..iinndddd 1111 2244//0088//22002211 1166::2222
12 WHAT IS NUTRITION? 13 WHAT ARE CARBOHYDRATES? Carbohydrates are the body’s greatest energy source. They provide the body with glucose to use for energy, which can also be stored (as glycogen) for future use. Carbs also play a valuable role in gut health, by providing useful fiber to the digestive tract. Glucose is the preferred energy source for muscles consuming higher quantities of carbs or protein-rich during strenuous exercise. It is only when the foods containing tryptophan supports mood body’s glucose supply is depleted that it turns to improvement in humans. However, it may be the fat for energy. The body also requires glucose to case that low-carb consumption causes low mood. fuel multiple unconscious biological processes. If you’ve ever been on a diet that involves avoiding Glucose is essential fuel for the brain, aiding carbs, you may have experienced mood swings and in concentration. Carbohydrates play an important found it hard to concentrate. You may have also felt role in generating the brain’s serotonin supply. fatigued. Serotonin is converted to melatonin, a This mood-regulating hormone is made with hormone that helps regulate circadian rhythm. tryptophan, an amino acid (see page 15) obtained through protein in the diet. Carbs help convert DIGESTING CARBS tryptophan into serotonin, so eating carbs may In the small intestine (see page 28), the less starchy help enhance mood. This might explain why carbs complex carbs are broken down into simple carbs and sweet foods are often treated as comfort foods. (see below). All non-glucose monosaccharides are There isn’t enough research to show that converted into glucose in the liver, which is released Simple and SINGLE TWO MULTIPLE MOLECULE MOLECULES MOLECULES complex carbs Monosaccharides (sugar Monosaccharides Disaccharides Polysaccharide in its simplest molecular "Mono" means "one"; When two monosaccharides Carbs can be made up of form) and disaccharides "saccharide" means bond chemically, they hundreds, even thousands, are simple carbohydrates. "sugar." These are carbs form a disaccharide. of monosaccharides. These Complex carbs comprise in their most basic form. are called complex carbs. many monosaccharides and also contain starch in varying levels. Less GLUCOSE LACTOSE starchy complex carbs grains, pasta DAIRY include broccoli, zucchini, tomato, and eggplant. Examples of FRUCTOSE SUCROSE starchier complex carbs fruit, honey sugar beet, cane sugar are potatoes, beans, and corn. many vegetables, GALACTOSE MALTOSE beans, lentils, dairy molasses, beer and whole grains UUSS__001122--001133__ccaarrbbss..iinndddd 1122 2244//0088//22002211 1166::2222
1122 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 1133 WHAT ARE CARBOHYDRATES? into the bloodstream. It is either used immediately or converted into glycogen (a polysaccharide of What does GI mean? glucose), which is stored in the liver and muscles Carbohydrates are the body’s greatest energy source. They provide the body with for later use (see pages 110–111). GLUCOSE FROM SIMPLE CARBS IS ABSORBED glucose to use for energy, which can also be stored (as glycogen) for future use. Carbs Fiber (see pages 18–19) refers to any complex INTO THE BLOODSTREAM QUICKLY, WHILE also play a valuable role in gut health, by providing useful fiber to the digestive tract. COMPLEX CARBS TAKE LONGER TO BREAK DOWN. carbohydrates that cannot be broken down by Glycemic index (GI) is a measure of how quickly the digestive enzymes in the small intestine a carbohydrate food will make your blood glucose (see pages 28–29). This fibrous matter moves into levels rise after ingesting it. The higher the GI, the Glucose is the preferred energy source for muscles consuming higher quantities of carbs or protein-rich the large intestine, where it helps produce highly faster the impact. Glycemic load (GL) is a slightly different unit of measurement. It takes into during strenuous exercise. It is only when the foods containing tryptophan supports mood useful short-chain fatty acids and nourishes the account both the GI and the amount of body’s glucose supply is depleted that it turns to improvement in humans. However, it may be the lining of the gut. carbohydrate in the food. So pasta has a lower GI fat for energy. The body also requires glucose to case that low-carb consumption causes low mood. than watermelon, yet pasta has more carbs and, fuel multiple unconscious biological processes. If you’ve ever been on a diet that involves avoiding "GOOD" AND "BAD" CARBS therefore, a higher GL. If you eat similar amounts of both, the pasta will have the greater effect Glucose is essential fuel for the brain, aiding carbs, you may have experienced mood swings and There is no such thing as an inherently good or on your blood sugar levels. in concentration. Carbohydrates play an important found it hard to concentrate. You may have also felt bad carb. All foods have a place—it’s about finding role in generating the brain’s serotonin supply. fatigued. Serotonin is converted to melatonin, a the right balance for your own body. As a general This mood-regulating hormone is made with hormone that helps regulate circadian rhythm. rule, carbohydrates in their natural, fiber-rich form tryptophan, an amino acid (see page 15) obtained are more nutritious than those that have been through protein in the diet. Carbs help convert DIGESTING CARBS stripped of their fiber content. Fruits and vegetables QUICK tryptophan into serotonin, so eating carbs may In the small intestine (see page 28), the less starchy are excellent sources of carbohydrates. help enhance mood. This might explain why carbs complex carbs are broken down into simple carbs There’s a strong case for reducing refined carbs, ENERGY and sweet foods are often treated as comfort foods. (see below). All non-glucose monosaccharides are like white bread, and opting for complex carbs, like CONSISTING MOSTLY There isn’t enough research to show that converted into glucose in the liver, which is released whole grains (see p.45), which release energy slowly. OF WHITE RICE AND AIR, RICE CAKES PROVIDE A While refined carbs are great for providing energy LITTLE BIT OF ENERGY quickly, they usually lack essential nutrients. Many QUICKLY grain-based processed foods in the US are fortified Simple and SINGLE TWO MULTIPLE or enriched with vitamins and minerals. MOLECULE MOLECULES MOLECULES complex carbs Monosaccharides (sugar Monosaccharides Disaccharides Polysaccharide in its simplest molecular "Mono" means "one"; When two monosaccharides Carbs can be made up of form) and disaccharides "saccharide" means bond chemically, they hundreds, even thousands, are simple carbohydrates. "sugar." These are carbs form a disaccharide. of monosaccharides. These NUTRIENT-LITE Complex carbs comprise in their most basic form. are called complex carbs. many monosaccharides FOR A MORE NUTRIENT- and also contain starch RICH OPTION, CHOOSE in varying levels. Less GLUCOSE LACTOSE BROWN RICE CAKES OR starchy complex carbs grains, pasta DAIRY WHOLE GRAIN CRACKERS include broccoli, zucchini, tomato, and eggplant. Examples of FRUCTOSE SUCROSE starchier complex carbs fruit, honey sugar beet, cane sugar Rice cakes Many people are potatoes, beans, presume these popular low- and corn. many vegetables, calorie snacks are healthy GALACTOSE MALTOSE beans, lentils, when, in fact, they are also dairy molasses, beer and whole grains low in nutrients. UUSS__001122--001133__ccaarrbbss..iinndddd 1122 2244//0088//22002211 1166::2222 UUSS__001122--001133__ccaarrbbss..iinndddd 1133 2244//0088//22002211 1166::2222
14 WHAT IS NUTRITION? 15 20 AMINO ACIDS Essential amino acids Our bodies can make all but nine of the 20 amino acids it needs to function. It is essential we source these nine amino acids from diet. It’s easy to meet your protein needs through foods by eating a variety of plant foods on a daily basis. 9 ESSENTIAL AMINO ACIDS COMPLETE INCOMPLETE PROTEINS PROTEINS CONTAIN ALL CONTAIN FEWER 9 ESSENTIAL THAN 9; COMBINE TO AMINO ACIDS FORM COMPLETE PROTEINS ANIMAL- PLANT-BASED PLANT-BASED BASED NUTS, SEEDS, HEMP, QUINOA, DAIRY PRODUCTS LEGUMES, GRAINS, SOY (MILK, YOGURT, MANY VEGETABLES CHEESE), EGGS, FISH, MEAT POPULAR PAIRINGS OF INCOMPLETE PROTEINS: LENTILS + RICE PEANUT BUTTER + WHOLE GRAIN BREAD NOODLES IN A PEANUT SAUCE OATS + NUTS HUMMUS + BREAD OR CRACKERS BEAN BURGERS ON A BUN BROWN RICE + BLACK BEANS BAKED BEANS ON WHOLE GRAIN TOAST LENTILS OR BEANS WITH PASTA UUSS__001144--001155__wwhhttaatt__iiss__pprrootteeiinn..iinndddd 1144 2244//0088//22002211 1166::2222
1144 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 1155 20 WHAT IS PROTEIN? AMINO ACIDS Essential amino acids Our bodies can The macronutrient protein is a major player in the diet. It is the body’s building make all but nine of the 20 amino acids it needs to function. It is essential we block, used to form and repair muscles, skin, hair, and nails, for instance. source these nine amino acids from diet. It’s easy to meet your protein needs Protein also enables many of the body’s vital metabolic functions. through foods by eating a variety of plant foods on a daily basis. 9 Every single cell in the body contains protein, and “incomplete,” they are no less valuable than complete there are thousands of types found in the body. proteins as protein needs are met across the whole ESSENTIAL AMINO Proteins form the structure of tissues. They also day, not just at each individual meal. ACIDS carry molecules around the body to where they are Vegans and vegetarians are advised to eat a wide COMPLETE INCOMPLETE needed, playing an important role in many chemical variety of protein-rich and fortified foods to ensure PROTEINS PROTEINS reactions that take place, including immune response they consume all nine of the essential amino acids CONTAIN ALL CONTAIN FEWER and the production and deployment of hormones. each day from both complete and incomplete sources 9 ESSENTIAL THAN 9; COMBINE TO Proteins are made up of amino acids. Short chains (see pages 128–129). AMINO ACIDS FORM COMPLETE of amino acids are called peptides (they connect to PROTEINS each other with peptide bonds) while longer chains PROTEIN INTAKE are called polypeptides or proteins. Protein chains Scientists agree that age, gender, and level of can become complex in structure as more and more physical activity all determine how much protein to chains join and fold in on themselves. The body consume. In the US, adults are advised to consume breaks down protein chains into peptides to use 0.8g protein per kg (2lb) of bodyweight per day. for specific purposes as required. For instance, the (Particularly active people should increase this intake ANIMAL- hormone insulin (see pages 172–173) is a peptide. to 1g.) Based on average weights and activity levels, PLANT-BASED BASED PLANT-BASED The body can produce many of the amino acids daily intake should be 56g for men and 46g for NUTS, SEEDS, it needs to make peptides and proteins, but nine women. That’s about two palm-size portions of meat, HEMP, QUINOA, DAIRY PRODUCTS SOY (MILK, YOGURT, LEGUMES, GRAINS, of them, known as essential amino acids, must be fish, tofu, nuts, or beans. Elderly people need up to MANY VEGETABLES CHEESE), EGGS, sourced from the diet (see opposite). Because the 50 percent more than the recommended daily intake. FISH, MEAT body doesn’t store proteins in the same way it does As we grow older, our bodies become less efficient at other macronutrients, it needs to consume proteins using protein. Increasing protein intake makes it every day. Numerous studies demonstrate that a more likely we’ll meet daily requirements. diet with adequate protein has major health benefits. I advise my clients to get as many complete protein sources in as possible throughout the day FOOD SOURCES and/or focus on consuming a wide variety of POPULAR PAIRINGS OF INCOMPLETE PROTEINS: Food sources containing all nine essential amino incomplete sources. acids in sufficient quantities are known as complete When choosing what protein to eat, ensure the LENTILS + RICE PEANUT BUTTER + WHOLE GRAIN BREAD NOODLES IN A proteins. They are found in animal products and a foods you pick give you the best all-around nutrition PEANUT SAUCE few plant sources. possible. Consider what other nutrients you are OATS + NUTS HUMMUS + BREAD Incomplete proteins are plant-based sources of receiving with your protein sources. OR CRACKERS BEAN BURGERS ON A BUN protein that don’t contain all nine essential amino And don’t worry about getting everything in at BROWN RICE + acids, or don’t have sufficient quantities of them one sitting—your protein input can add up BLACK BEANS BAKED BEANS ON WHOLE GRAIN TOAST LENTILS OR BEANS WITH PASTA to meet the body’s daily requirements. Although throughout the day. UUSS__001144--001155__wwhhttaatt__iiss__pprrootteeiinn..iinndddd 1144 2244//0088//22002211 1166::2222 UUSS__001144--001155__wwhhttaatt__iiss__pprrootteeiinn..iinndddd 1155 2244//0088//22002211 1166::2222
16 WHAT IS NUTRITION? 17 WHAT IS FAT? Fat is a macronutrient found in many food sources. Consuming fats is vital to the body’s function, including processes such as brain activity, hormone production, and the body’s absorption of other nutrients from the diet. We should aim to get one-third of our calories POLYUNSATURATED FATS from fat. The fat we eat is broken down into This type of unsaturated fat contains two or more triglycerides (fatty acid cells combined with double bonds in its structure. Polyunsaturated fatty glycerol, a type of glucose) that travel in the acids (PUFAs) are found in sunflower seeds, oily blood to wherever they will be used or stored. fish, walnuts, flaxseeds, and vegetable oils, including There are two main types of dietary fats: safflower, sunflower, and corn oils. saturated and unsaturated. Unsaturated fats Omega-3 and omega-6 are polyunsaturated fats. are either monounsaturated or polyunsaturated. Omega-3 plays a crucial role in the production of Most foods containing fat naturally contain a hormones, in the immune system, blood clotting, mixture of different types of fats, so it’s difficult and cellular growth. Studies show that consuming to exclude one type in favor of another. However, omega-3 fats is linked to reduced incidence of we should aim to cut down on saturated fats health conditions, including neurodegenerative and opt for more monounsaturated and disease, heart disease, and diabetes. We need to polyunsaturated fats. consider increasing our intake of omega-3 fats, Avoid the artificial additive transfat as these are usually eaten in small quantities and (see pages 58–59) due to its links to inflammation, found in less commonly consumed foods, like oily unhealthy cholesterol levels, impaired artery fish, flaxseeds, or walnuts. function, insulin resistance (see page 172). SATURATED FATS MONOUNSATURATED FATS These come mainly from animal sources and tend This type of unsaturated fat contains only to be solid at room temperature. Saturated fats are one double bond in its molecular structure. very stable at high temperatures and therefore less Monounsaturated fats are linked to several likely to be damaged during cooking. That’s why, for health benefits, including a reduced risk of instance, butter is used traditionally for baking cakes. serious diseases such as heart disease and diabetes. Milk, cheese, fatty meats such as lamb, processed Monounsaturated fatty acids (MUFAs) are meat (sausages, burgers, and bacon), coconut oil, typically liquid at room temperature and are cakes, and cookies are sources of saturated fats. fairly stable for cooking purposes (see pages 66–67). While saturated fat plays a part in nutrition, too The most common MUFA is oleic acid, which is much in the diet is linked to heart disease. We still present in olive oil in high amounts. Excellent eat too much saturated fat in the US. No more than sources of this healthy fat are avocados, nuts, 5–6 percent of our total intake of calories should seeds, canola oil, fish oils, and nut oils. come from saturated fats. UUSS__001166--001177__wwhhaatt__iiss__ffaatt..iinndddd 1166 2244//0088//22002211 1166::2222
1166 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 1177 WHAT IS FAT? Cholesterol Fat is a macronutrient found in many food sources. Consuming fats is vital Cholesterol is a lipid (fatty to the body’s function, including processes such as brain activity, hormone substance) the body uses to build HDL LDL cells, among other things. Some production, and the body’s absorption of other nutrients from the diet. High-density lipoproteins Low-density lipoproteins (LDL) are bad cholesterol is obtained from diet, (HDL) are good cholesterol. cholesterol. They have a lower proportion but much of the body’s supply is These particles contain a high of protein to cholesterol. These types of made in the liver. It attaches to a proportion of protein to fatty acids carry cholesterol to the cells. protein to form tiny spheres cholesterol. They protect the Too much LDL can be harmful because it We should aim to get one-third of our calories POLYUNSATURATED FATS body by carrying LDL cholesterol sticks to the inside walls of your arteries, (lipoproteins) that are carried in from fat. The fat we eat is broken down into This type of unsaturated fat contains two or more away from arteries to the causing buildup of fatty material, blood to wherever they are needed. liver, and they have anti- limiting blood flow and leading to triglycerides (fatty acid cells combined with double bonds in its structure. Polyunsaturated fatty There are two types: HDL and LDL. inflammatory properties. heart conditions and stroke. glycerol, a type of glucose) that travel in the acids (PUFAs) are found in sunflower seeds, oily blood to wherever they will be used or stored. fish, walnuts, flaxseeds, and vegetable oils, including to the liver to the liver to the liver There are two main types of dietary fats: safflower, sunflower, and corn oils. saturated and unsaturated. Unsaturated fats Omega-3 and omega-6 are polyunsaturated fats. are either monounsaturated or polyunsaturated. Omega-3 plays a crucial role in the production of Most foods containing fat naturally contain a hormones, in the immune system, blood clotting, mixture of different types of fats, so it’s difficult and cellular growth. Studies show that consuming to exclude one type in favor of another. However, omega-3 fats is linked to reduced incidence of we should aim to cut down on saturated fats health conditions, including neurodegenerative and opt for more monounsaturated and disease, heart disease, and diabetes. We need to polyunsaturated fats. consider increasing our intake of omega-3 fats, LDL CHOLESTEROL BEGINS TO LDL BUILDUP BUILDUP OF PLAQUE NOW Avoid the artificial additive transfat as these are usually eaten in small quantities and ACCUMULATE IN ARTERY BY INCREASES OVER TIME IMPEDES THE JOURNEY OF (see pages 58–59) due to its links to inflammation, found in less commonly consumed foods, like oily STICKING TO THE WALLS PROTECTIVE HDL TO THE LIVER unhealthy cholesterol levels, impaired artery fish, flaxseeds, or walnuts. function, insulin resistance (see page 172). SATURATED FATS MONOUNSATURATED FATS These come mainly from animal sources and tend Increasing HDL and reducing LDL cholesterol This type of unsaturated fat contains only to be solid at room temperature. Saturated fats are one double bond in its molecular structure. very stable at high temperatures and therefore less We want to aim for a preferential ratio between HDL and Monounsaturated fats are linked to several likely to be damaged during cooking. That’s why, for LDL cholesterol. There are measures we can take to increase health benefits, including a reduced risk of instance, butter is used traditionally for baking cakes. our HDL cholesterol levels and lower LDL. Nondietary Purple fruits and Oily fish 1–2 actions include exercising regularly and stopping smoking. vegetables are rich in times a week may serious diseases such as heart disease and diabetes. Milk, cheese, fatty meats such as lamb, processed anthocyanins, which help increase HDL Avoid transfats completely, and include the following foods Monounsaturated fatty acids (MUFAs) are meat (sausages, burgers, and bacon), coconut oil, may help increase HDL cholesterol levels and regularly in your diet. cholesterol levels. benefit heart health. typically liquid at room temperature and are cakes, and cookies are sources of saturated fats. fairly stable for cooking purposes (see pages 66–67). While saturated fat plays a part in nutrition, too The most common MUFA is oleic acid, which is much in the diet is linked to heart disease. We still present in olive oil in high amounts. Excellent eat too much saturated fat in the US. No more than sources of this healthy fat are avocados, nuts, 5–6 percent of our total intake of calories should seeds, canola oil, fish oils, and nut oils. come from saturated fats. Olive oil increases Whole grains are Nuts are rich in Avocados contain Legumes such HDL levels in healthy linked to a lower risk of cholesterol-lowering monounsaturated as beans, peas, and people, the elderly, heart disease. Oats and fats and fiber, as well fatty acids and fiber, lentils help lower LDL and those with high barley contain beta- as minerals that are both of which lower levels and are a good LDL cholesterol. glucan, which lowers linked to improved LDL cholesterol. source of plant-based LDL cholesterol. heart health. proteins. UUSS__001166--001177__wwhhaatt__iiss__ffaatt..iinndddd 1166 2244//0088//22002211 1166::2222 UUSS__001166--001177__wwhhaatt__iiss__ffaatt..iinndddd 1177 2255//0088//22002211 1199::1144
18 WHAT IS NUTRITION? 19 WHAT IS FIBER? Fiber is in part made of long chains of glucose molecules (polysaccharides, see pages 12–13). The small intestine is unable to break down the fiber content of carbs we consume. Fiber’s passage through the digestive system is very beneficial for health. SOLUBLE FIBER IS FOUND IN THE FLESH AND PULP OF MANY FRUITS AND VEGETABLES, IN GRAINS, INSOLUBLE SEEDS, AND FIBER IS FOUND IN THE LEGUMES SKINS AND SEEDS OF MANY FRUITS AND VEGETABLES, IN NUTS, SEEDS, WHOLE GRAINS, RICE, AND WHEAT Different types of fiber BRAN are usually contained within one food. Fiber keeps the digestive system healthy and has another great benefit; in the small intestine, its in good working order. It slows down digestion, presence reduces the absorption of cholesterol into which regulates blood sugar levels, and helps the bloodstream, which has the positive effect of us feel fuller longer, reducing the likelihood reducing the levels of LDL cholesterol (the bad of weight gain. Most people don’t eat enough type of cholesterol; p.17) circulating in the blood. of it, despite the positive health outcomes associated with consuming more of it. For each INSOLUBLE FIBER 8g increase in daily intake, we reduce the risk of As the name suggests, insoluble fiber does not type 2 diabetes by 15 percent, heart disease by dissolve in water, so it can be only partially broken 19 percent, and colon cancer by 80 percent. down by digestion. It helps to push things along in your digestive tract, preventing digestive problems. SOLUBLE FIBER Consuming enough insoluble fiber promotes This type of fiber dissolves in water to form a regularity of bowel movements and helps gel-like substance that helps make stools soft. regulate blood sugar levels. This enables them to progress through the gut with ease, preventing constipation. Soluble fiber RESISTANT STARCH (in oats, for instance) slows down digestion, Carbohydrates made up of high proportions of which helps with satiety (see page 104) and also beta-glucose monomers (see right), such as cellulose, with regulating blood sugar levels. Soluble fiber are referred to as resistant starches because they UUSS__001188--001199__ffiibbrree..iinndddd 1188 2244//0088//22002211 1166::2222
1188 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 1199 WHAT IS FIBER? cannot be broken down by the small intestine. grains may reduce your risk of developing heart They pass into the large intestine, where they are disease, diabetes, and several types of cancers. fermented by gut bacteria. Bear in mind that suddenly increasing fiber can Fiber is in part made of long chains of glucose molecules (polysaccharides, see pages This fermentation process produces short-chain cause problems, such as bloating and loose bowel 12–13). The small intestine is unable to break down the fiber content of carbs we fatty acids (SCFAs), which stimulate the immune movements. Increase levels of fiber gradually and, consume. Fiber’s passage through the digestive system is very beneficial for health. system and can impact mental health. if necessary, under the supervision of a doctor, Cooked and cooled potatoes and rice contain dietitian, or nutritionist. resistant starch, which is also found in whole grains like barley, oats, and sorghum, green bananas, and SOLUBLE beans and legumes. Cooked and cooled FIBER IS FOUND IN THE COOLED COOKED WHITE RICE, POTATOES, FLESH AND PULP OF INCLUDING ENOUGH FIBER SWEET POTATOES, OR PASTA PROVIDE MORE MANY FRUITS AND Our average daily fiber intake is 10–15g, and RESISTANT STARCH THAN IF CONSUMED HOT. VEGETABLES, IN GRAINS, INSOLUBLE SEEDS, AND we should aim for 28g. A food high in fiber is Even if you reheat them later, the resistant starch content (see left) remains increased. FIBER IS FOUND IN THE LEGUMES considered to have 5g fiber per serving. So enjoy potato, pasta, and rice salads, and cook SKINS AND SEEDS OF Aim to include a mix of different sources of fiber. up extra batches to refrigerate and reheat MANY FRUITS AND To ensure you get enough, include lots of whole for tomorrow’s lunch or dinner. Care should be taken VEGETABLES, IN NUTS, SEEDS, WHOLE GRAINS, grains, vegetables, fruits, beans, lentils, nuts, and seeds when storing cooked rice, which can contain bacterial spores that cause food poisoning. These RICE, AND WHEAT Different types of fiber in your diet. aim for as wide a variety as possible. BRAN are usually contained spores flourish when rice is left to stand at room within one food. Foods may contain a mixture of types of fiber. temperature, so cool and refrigerate rice within an For instance, whole grain products are good hour of cooking, refrigerate for no longer than a day, and reheat until piping hot before consuming. sources of both insoluble fiber and resistant starch. Consuming a diet that includes fiber from whole Fiber keeps the digestive system healthy and has another great benefit; in the small intestine, its in good working order. It slows down digestion, presence reduces the absorption of cholesterol into weak alpha-bonds strong beta-bonds which regulates blood sugar levels, and helps the bloodstream, which has the positive effect of us feel fuller longer, reducing the likelihood reducing the levels of LDL cholesterol (the bad of weight gain. Most people don’t eat enough type of cholesterol; p.17) circulating in the blood. Starch and fiber of it, despite the positive health outcomes associated with consuming more of it. For each INSOLUBLE FIBER These are often confused, and 8g increase in daily intake, we reduce the risk of As the name suggests, insoluble fiber does not easily so, since both are contained type 2 diabetes by 15 percent, heart disease by dissolve in water, so it can be only partially broken together in the same foods, and ALPHA-GLUCOSE BETA-GLUCOSE MONOMERS MONOMERS 19 percent, and colon cancer by 80 percent. down by digestion. It helps to push things along in both are carbohydrates, made up of polysaccharides. But in starch, your digestive tract, preventing digestive problems. chains of glucose monomers SOLUBLE FIBER Consuming enough insoluble fiber promotes (monosaccharides, see p.12) STARCH FIBER This type of fiber dissolves in water to form a regularity of bowel movements and helps are joined by alpha bonds, which Polysaccharides like amylopectin (found in Polysaccharides like cellulose (found gel-like substance that helps make stools soft. regulate blood sugar levels. can be broken down in the small rice, potato, white bread and pasta, wheat, in plants) are made with beta-glucose This enables them to progress through the gut intestine. In fiber, glucose and barley) are made with alpha-glucose monomers. The bond angles that form with ease, preventing constipation. Soluble fiber RESISTANT STARCH monomers are joined by beta monomers. The bond angles formed when when these units of glucose join build into bonds, which can’t be broken alpha-glucose momomers join builds a stable parallel chains that are strongly (in oats, for instance) slows down digestion, Carbohydrates made up of high proportions of down. Instead the chains remain branched and/or spiral structure. These interlinked. There are no enzymes in the which helps with satiety (see page 104) and also beta-glucose monomers (see right), such as cellulose, intact throughout their passage bonds can be broken down by enzymes in small intestine that are able to break with regulating blood sugar levels. Soluble fiber are referred to as resistant starches because they through the small intestine. the digestive system down these beta-bonds. UUSS__001188--001199__ffiibbrree..iinndddd 1188 2244//0088//22002211 1166::2222 UUSS__001188--001199__ffiibbrree..iinndddd 1199 2244//0088//22002211 1166::2222
20 WHAT IS NUTRITION? 21 BUTTERNUT SQUASH CONTAIN VITAMINS A, B1, B2, B3, B7, B9 (FOLATE), C, AND E, AS WELL AS CHOLINE AND MAGNESIUM EGGPLANT CONTAIN VITAMINS B1, B2, B3, B5, B6, B9 (FOLATE), C, E, K, AS WELL AS CHOLINE, MAGNESIUM, POTASSIUM, AND DIETARY FIBER. UUSS__002200--002211__vviittaammiinnss..iinndddd 2200 2244//0088//22002211 1166::2222
2200 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 2211 WHAT ARE VITAMINS? Vitamins are nutrients we need to obtain from the foods we eat in order to ensure optimal health and prevent nutritional deficiencies. Aim to include a colorful variety of veggies and fruits in your diet to pack in all the vitamins you need. Our bodies cannot make all the micronutrients Vitamin C is often hailed as the cure to colds BUTTERNUT (see page 10) we need to function, so this and flu because it contributes to healing. It also SQUASH CONTAIN requirement is met solely by diet. Most people maintains healthy skin, blood vessels, and cartilage, VITAMINS A, B1, B2, B3, enjoying a healthy, balanced diet will easily obtain and plays a role in the production of collagen, which B7, B9 (FOLATE), C, AND E, AS WELL AS CHOLINE enough vitamins. If you don’t eat animal products, maintains our skin’s elasticity and strength. Sources AND MAGNESIUM though, there are measures you can take to prevent include oranges, peppers, broccoli, bananas. potential nutritional deficiencies (see pp.130–131). Vitamins come in two main forms, water-soluble FAT SOLUBLE and fat-soluble. Fat-soluble vitamins tend to accumulate within the body so are not needed in the diet on a daily basis. WATER SOLUBLE Water-soluble vitamins are easily lost through bodily Vitamins A and E are powerful antioxidants, fluids (and in cooking) and must be replaced each day. helping to protect cells from free radicals and aging. Vitamin A contributes to cell renewal and B vitamins play an important role in keeping repair, but note that excesses during pregnancy the nervous system healthy and helping may harm the baby (see page 183). Vitamin E our bodies release energy from the foods we eat. reduces the effects of skin aging and the risk of Folate (vitamin B9) helps with brain and spinal skin cancer. Sources of vitamin A include carrots cord development in unborn babies (see page 178). and sweet potatoes. Sources of vitamin E include Sources of B vitamins are: almonds and avocados. EGGPLANT B1 Peas, bananas, nuts, whole grains CONTAIN VITAMINS B1, B2, B2 Milk, eggs, fortified cereals, mushrooms Vitamin D is unique because it is a hormone that B3, B5, B6, B9 (FOLATE), C, B3 Meat, fish, wheat flour, eggs we can produce in our own bodies with exposure to E, K, AS WELL AS CHOLINE, MAGNESIUM, POTASSIUM, B5 Chicken, beef, eggs, avocado sunlight. This means we don’t need to get it from AND DIETARY B6 Pork, soybeans, peanuts, oats, bananas, milk diet if there is enough exposure. However, with the FIBER. B7 is needed in trace quantities, and is available use of sunscreen and the lack of sunlight in parts of from many food sources, so will be present in a the US and during winter, we are advised to varied and balanced diet supplement with vitamin D if our blood levels are B9 Leafy greens, chickpeas, edamame beans, low (see pages 138–139). Dietary sources include broccoli, liver, foods fortified with folate egg yolks, oily fish, fortified foods. B12 is found only in animal products, such as eggs, meat, or fish, or in fortified plant foods, Vitamin K is important for wound healing such as some nutritional yeast products. If you (we need it for blood clotting), and some evidence are vegan, I highly advise reviewing whether you links it to bone health. Sources include green leafy are getting enough B12. vegetables, some cereal grains, and vegetable oil. UUSS__002200--002211__vviittaammiinnss..iinndddd 2200 2244//0088//22002211 1166::2222 UUSS__002200--002211__vviittaammiinnss..iinndddd 2211 2244//0088//22002211 1166::2222
22 WHAT IS NUTRITION? 23 WHAT ARE MINERALS? Our bodies need certain minerals to function well. Many foods contain both vitamins and minerals, so a varied diet will help you meet your mineral requirements. Unlike vitamins, which are organic compounds selenium, and zinc are needed in trace quantities. (made by plants or animals), minerals are inorganic Calcium is a vital component of bone and teeth, chemical elements that come from soil, rock, or and a key nutrient for the nervous system, muscles, water. They are absorbed from the environment by and heart. Sources include milk, yogurt, spinach. plants as they grow, and by animals that eat those plants. There are many minerals, each with benefits. Iodine deficiency affects nearly one-third of Try to include each type in your diet regularly. You the world’s population. This mineral is essential need some minerals in greater quantities, such as for normal thyroid function and the production calcium, chloride, magnesium, phosphorous, of thyroid hormones, which are involved in many potassium, sodium. Others, like iodine, iron, processes in the body such as growth, brain GOJI BERRIES CONTAIN IRON, WHICH MANGO IS ESSENTIAL TO THE CONTAINS CALCIUM ABILITY OF BLOOD TO FOR BONES AND TEETH, TRANSFER OXYGEN IRON FOR IMMUNITY, TO TISSUES AND POTASSIUM FOR NERVE FUNCTION Dried fruit tend to be a more concentrated source of minerals than fresh equivalents, but also of fruit sugars so take care not to overconsume. UUSS__002222--002233__mmiinneerraallss..iinndddd 2222 2255//0088//22002211 1199::1144
2222 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 2233 WHAT ARE MINERALS? Our bodies need certain minerals to function well. Many foods contain both vitamins and minerals, so a varied diet will help you meet your mineral requirements. Unlike vitamins, which are organic compounds selenium, and zinc are needed in trace quantities. development, and bone maintenance. Thyroid and promotes the health of the reproductive system. (made by plants or animals), minerals are inorganic Calcium is a vital component of bone and teeth, hormones also regulate the metabolic rate. Sources include Brazil nuts, eggs, meat, fish. chemical elements that come from soil, rock, or and a key nutrient for the nervous system, muscles, Sources include fish, dairy products, eggs, water. They are absorbed from the environment by and heart. Sources include milk, yogurt, spinach. seaweed. Zinc supports the immune system, hormone plants as they grow, and by animals that eat those production, and fertility. It can help reduce skin plants. There are many minerals, each with benefits. Iodine deficiency affects nearly one-third of Iron deficiency is the most common nutritional inflammation and support wound healing and Try to include each type in your diet regularly. You the world’s population. This mineral is essential deficiency in the world and the only one that is protects against UV damage from the sun. need some minerals in greater quantities, such as for normal thyroid function and the production prevalent in developed countries. More than 30 Sources include shellfish, red meat, eggs, chickpeas. calcium, chloride, magnesium, phosphorous, of thyroid hormones, which are involved in many percent of the world’s population has anemia. potassium, sodium. Others, like iodine, iron, processes in the body such as growth, brain Lack of iron lowers the ability of the blood to SHOULD I TAKE SUPPLEMENTS? carry oxygen. Iron has many benefits, including Food is the preferable way for the body to source improved immune and brain function. Sources nutrients, which are often better absorbed from foods include shellfish, broccoli, red meat, tofu. than pills. It’s possible for supplements to interact in ways that affect one another, or to contain one or more GOJI BERRIES Magnesium plays a role in more than 600 cellular of the same nutrients, potentially leading to toxic CONTAIN IRON, WHICH MANGO processes, including energy production, nervous buildups. Generally, water-soluble vitamins are less IS ESSENTIAL TO THE CONTAINS CALCIUM system function, and muscle contraction. Sources likely to cause harm than fat-soluble since they are ABILITY OF BLOOD TO FOR BONES AND TEETH, include avocados, nuts, leafy greens. excreted easily in your urine and have less chance of TRANSFER OXYGEN IRON FOR IMMUNITY, TO TISSUES building up in the body. Yet be cautious. Too much AND POTASSIUM FOR Manganese helps make and activate some of the vitamin C or zinc (a water-soluble mineral) may cause NERVE FUNCTION enzymes in the body that carry out chemical reactions nausea, diarrhea, and stomach cramps. Too much such as breaking down food. Sources include bread, selenium could lead to hair loss, gastrointestinal upset, nuts, breakfast cereals, green vegetables. fatigue, and mild nerve damage. Dried fruit tend to be a As the consequences of taking supplements can more concentrated source Potassium is important for blood pressure be serious, and supplements can be costly, it’s not of minerals than fresh equivalents, but also of control, fluid balance, and muscles and nerve worth taking supplements unless advised to do so fruit sugars so take care function. Sources include bananas, spinach, by a health care professional. not to overconsume. potatoes, apricots. There are sometimes valid reasons for taking supplements, for instance, during preconception Phosphorus helps the body build strong bones and pregnancy (see pages 178–181). Also, iron or and also release energy from food. Sources include vitamin B12 deficiency can lead to anemia. If you red meat, dairy, fish, poultry, oats, bread. believe you have a nutritional deficiency, speak to your doctor. Blood tests can help diagnose Selenium helps the immune system work deficiencies, and supplements can then be correctly, prevents damage to cells and tissues, prescribed to help correct them. UUSS__002222--002233__mmiinneerraallss..iinndddd 2222 2255//0088//22002211 1199::1144 UUSS__002222--002233__mmiinneerraallss..iinndddd 2233 2244//0088//22002211 1166::2222
24 WHAT IS NUTRITION? 25 IS HYDRATION PART OF NUTRITION? Hydration is an incredibly important part of nutrition. Water is essential for all of the body’s processes to work. In fact, we would survive a good deal longer without food than without water. It is not easy to find even one system of the Each kidney filters more than 100ml of blood per minute. body that does not require water. Water enables Waste and excess water are the circulatory system to carry essential oxygen separated from useful Blood in substances. and nutrients to cells. Our kidneys need water to filter out waste products (see right). It helps us cool off via sweat when we are too hot. It Blood out helps the digestive system do its job. And the Each kidney list goes on! contains roughly 1 million tiny When it comes to the brain, 75 percent of its filtering units mass is water, so alongside the bodily processes, called nephrons hydration also plays a crucial role in regulating mood, productivity, and concentration. Your body uses a lot of water each day. You need to drink enough water to replace what Urine is directed is lost so the body continues to function along the ureter, a well and you feel your best. So, drink up! fine, muscular tube, to the bladder for storage until urination DAILY TARGETS According to guidelines, most of us Nephrons return useful substances to the blood supply should aim to drink 2.7 –3.7 liters of water every day. A typical mug or glass Nephrons create urine, a has a capacity of 7oz (200ml), so you’d mix of waste substances and excess water need to have 11.5–15.5 drinks a day. Bear in mind that these are the recommended daily intake amounts and some people may need Make adjustments to these targets based on Ureter more. Aim for 15.5 cups (3.7 liters) daily if your lifestyle. If you sweat a lot (say, if you’re you easily get to 11.5 cups (2.7 liters). Get very active) you need to replenish that lost water yourself a BPA-free reusable water bottle frequently. On vacation in a hot climate, you to help you keep track of your water intake. may sweat more than usual and need to increase Children and infants require less fluids your water intake. Also, if you’re breastfeeding, than adults. Introduce tap water from the you require a lot of additional fluid. As we age, age of 6 months. With children, aim for hydration is still important. Elderly people often 7–10 glasses a day. struggle to get enough hydration due to issues UUSS__002244--002255__hhyyddrraattiioonn..iinndddd 2244 2255//0088//22002211 1199::1144
2244 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 2255 IS HYDRATION PART OF NUTRITION? Urine color check Hydration is an incredibly important part of nutrition. Water is Urine should be quite clear in color. essential for all of the body’s processes to work. In fact, we would The darker yellow survive a good deal longer without food than without water. it is, the more dehydrated you are, and the more you WELL DRINK A MODERATELY VERY SERIOUSLY need to drink to HYDRATED LITTLE MORE DEHYDRATED DEHYDRATED DEHYDRATED hydrate yourself. It is not easy to find even one system of the Each kidney filters more than 100ml of blood per minute. body that does not require water. Water enables Waste and excess water are the circulatory system to carry essential oxygen separated from useful Blood in KIDNEYS with mobility and memory loss, for instance. to your body. The best way to hydrate is simply substances. and nutrients to cells. Our kidneys need water It’s important to note that people with these with water. In the US, tap water is safe to drink. to filter out waste products (see right). It helps issues can feel more comfortable day to day us cool off via sweat when we are too hot. It by drinking plenty of water. SIGNS OF DEHYDRATION Blood out helps the digestive system do its job. And the Signs of dehydration include dry mouth, dark Each kidney list goes on! contains roughly WHAT TO DRINK yellow urine (see above), feeling tired, thirsty, 1 million tiny When it comes to the brain, 75 percent of its filtering units About 20 percent of our total fluid intake comes and dizzy, and urinating fewer than four times mass is water, so alongside the bodily processes, called nephrons from the food we eat. When it comes to the rest, per day. Studies show that at about 1 percent hydration also plays a crucial role in regulating research suggests that many people prefer to dehydration (equivalent to 1 percent of body mood, productivity, and concentration. take in fluids in the form of sugary drinks, tea, weight water loss) there are negative effects Your body uses a lot of water each day. You coffee, and juices. In one survey, 23 percent of on mental and physical function, which become need to drink enough water to replace what responders claimed they chose carbonated more severe as dehydration increases. Urine is directed is lost so the body continues to function along the ureter, a beverages to stay hydrated. Although these Regularly underhydrating often leads to well and you feel your best. So, drink up! fine, muscular tube, drinks supply you with necessary fluids, you constipation. If you find that you are to the bladder for storage until urination also end up adding caffeine, sugar, and constipated frequently, try increasing your DAILY TARGETS sweeteners (and their negative health effects) fluid consumption (see page 155). According to guidelines, most of us Nephrons return useful substances to the blood supply should aim to drink 2.7 –3.7 liters of water every day. A typical mug or glass Nephrons create urine, a Calculate your water needs has a capacity of 7oz (200ml), so you’d mix of waste substances and excess water need to have 11.5–15.5 drinks a day. Bear in Use this calculation as a rule-of-thumb mind that these are the recommended daily guide to help you calculate your daily water intake amounts and some people may need Make adjustments to these targets based on Ureter requirement. Make adjustments to factor in your sweat levels (based on the amount of more. Aim for 15.5 cups (3.7 liters) daily if your lifestyle. If you sweat a lot (say, if you’re physical activity you undertake). you easily get to 11.5 cups (2.7 liters). Get very active) you need to replenish that lost water yourself a BPA-free reusable water bottle frequently. On vacation in a hot climate, you to help you keep track of your water intake. may sweat more than usual and need to increase Children and infants require less fluids your water intake. Also, if you’re breastfeeding, 0.033 Liters Example: you weigh 60kg, than adults. Introduce tap water from the you require a lot of additional fluid. As we age, 60 x 0.003 = 1.98 liters YOUR WEIGHT PER DAY age of 6 months. With children, aim for hydration is still important. Elderly people often BLADDER (KG) 7–10 glasses a day. struggle to get enough hydration due to issues UUSS__002244--002255__hhyyddrraattiioonn..iinndddd 2244 2255//0088//22002211 1199::1144 UUSS__002244--002255__hhyyddrraattiioonn..iinndddd 2255 2244//0088//22002211 1166::2222
26 WHAT IS NUTRITION? 27 WHAT IS DIGESTION? Digestion is the process by which our bodies absorb the nutrients needed to stay alive from the foods we consume. Nutrients are passed into the bloodstream to be taken to wherever in the body they are needed, and waste matter is formed and excreted. The digestive system is around 30ft (9m) long in SMALL INTESTINE adults, and an awful lot is achieved along this length. The majority of nutrient absorption takes place Each section of it plays an important role. in this 23ft (7m) long section of the digestive tract. It is covered with multiple tiny villi and microvilli MOUTH that absorb nutrients from chyme through a process Food enters the mouth and is initially broken down called diffusion (see pages 28–29) and pass them by teeth as we chew it into smaller pieces. Digestive into the bloodstream. enzymes in saliva begin the process of chemically Food spends 2–6 hours in the small intestine, breaking down the food. The bolus (ball of masticated being broken down by digestive enzymes so food) is swallowed and enters the esophagus. diffusion can take place. Some of these enzymes are supplied by the pancreas, which also releases ESOPHAGUS hormones to help regulate blood sugar levels as At the back of the throat, behind the tongue, is the they rise in response to eating a meal (pp.30–31). epiglottis, a flap of cartilage that covers the larynx The gallbladder also secretes bile into the small (windpipe) as swallowed food passes it to enter the intestine to further aid in digestion. esophagus (food pipe). The esophagus is large The remaining material consists mostly of muscular tube that extends from the epiglottis to water, bacteria, dead cells from the gut lining, the stomach. Its muscles move the bolus down the and indigestible fiber (see page 18). It moves pipe to the stomach. The lower esophageal sphincter along the small intestine to the ileocecal valve is a muscular ring acting as the stomach’s gateway. —the gateway to the large intestine. Acid reflux results if it doesn’t close properly. LARGE INTESTINE STOMACH The large intestine plays a key role in finishing The stomach releases digestive enzymes and acid that the job and creating feces. During the 12–30 hours break down the food that enters. Gut muscles contract it spends in here, the initially liquid mixture that to churn the food, to help the chemicals break it up, arrives is turned into stools as its water content is and the acid kills off unwanted microbes in it. This slowly absorbed by the large intestine. process also informs the satiety hormone, leptin, to The majority of the trillions of microbes living in be released (p.105). (The corresponding hunger the gut are in the large intestine. These gut bacteria hormone, ghrelin, is released by the stomach when seem to play an important role in the synthesis of it is empty, to stimulate hunger pangs.) The stomach key nutrients. They also communicate with our turns the food into chyme, which has a souplike immune cells and help prevent inflammation. They consistency that the small intestine is able to process. ferment the indigestible fiber in the large intestine, UUSS__002266--002277__ddiiggeessttiioonn..iinndddd 2266 2244//0088//22002211 1166::2222
2266 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 2277 WHAT IS DIGESTION? Digestion is the process by which our bodies absorb the nutrients needed to stay alive Mouth from the foods we consume. Nutrients are passed into the bloodstream to be taken to wherever in the body they are needed, and waste matter is formed and excreted. Esophagus The digestive system is around 30ft (9m) long in SMALL INTESTINE which results in the release of valuable short- adults, and an awful lot is achieved along this length. The majority of nutrient absorption takes place chain fatty acids (p.18). Another product of Each section of it plays an important role. in this 23ft (7m) long section of the digestive tract. these bacteria is gas (see p.154). Giving our gut It is covered with multiple tiny villi and microvilli microbes plenty of different types of fiber to MOUTH that absorb nutrients from chyme through a process feed on aids the digestive processes and helps Food enters the mouth and is initially broken down called diffusion (see pages 28–29) and pass them support beneficial gut bacteria (pp.48–51). by teeth as we chew it into smaller pieces. Digestive into the bloodstream. enzymes in saliva begin the process of chemically Food spends 2–6 hours in the small intestine, RECTUM AND ANUS breaking down the food. The bolus (ball of masticated being broken down by digestive enzymes so The end product is semi-solid waste material food) is swallowed and enters the esophagus. diffusion can take place. Some of these enzymes Stomach known as feces. This is collected together in are supplied by the pancreas, which also releases the rectum, which is located at the far end of ESOPHAGUS hormones to help regulate blood sugar levels as Gallbladder the large intestine, just before the two anal At the back of the throat, behind the tongue, is the they rise in response to eating a meal (pp.30–31). sphincters at the exit. Contraction and Pancreas epiglottis, a flap of cartilage that covers the larynx The gallbladder also secretes bile into the small relaxation of the anal muscles pushes the stool (windpipe) as swallowed food passes it to enter the intestine to further aid in digestion. through the sphincters and out of the anus. esophagus (food pipe). The esophagus is large The remaining material consists mostly of muscular tube that extends from the epiglottis to water, bacteria, dead cells from the gut lining, Small the stomach. Its muscles move the bolus down the and indigestible fiber (see page 18). It moves intestine pipe to the stomach. The lower esophageal sphincter along the small intestine to the ileocecal valve is a muscular ring acting as the stomach’s gateway. —the gateway to the large intestine. Large Acid reflux results if it doesn’t close properly. Intestine Type 1: separate hard lumps LARGE INTESTINE Type 2: sausage shape but lumpy STOMACH The large intestine plays a key role in finishing Type 3: sausage shaped, but with cracks The stomach releases digestive enzymes and acid that the job and creating feces. During the 12–30 hours Type 4: sausage shaped, smooth break down the food that enters. Gut muscles contract it spends in here, the initially liquid mixture that to churn the food, to help the chemicals break it up, arrives is turned into stools as its water content is Rectum Type 5: soft blobs and the acid kills off unwanted microbes in it. This slowly absorbed by the large intestine. Type 6: fluffy pieces process also informs the satiety hormone, leptin, to The majority of the trillions of microbes living in Type 7: runny be released (p.105). (The corresponding hunger the gut are in the large intestine. These gut bacteria hormone, ghrelin, is released by the stomach when seem to play an important role in the synthesis of Bristol stool chart These guidelines help you assess it is empty, to stimulate hunger pangs.) The stomach key nutrients. They also communicate with our the health of your digestive system by the shape and turns the food into chyme, which has a souplike immune cells and help prevent inflammation. They consistency of the stools it produces. Types 1 and 2 indicate constipation; types 3, 4, and 5 are considered consistency that the small intestine is able to process. ferment the indigestible fiber in the large intestine, normal stools; types 6 and 7 illustrate diarrhea. UUSS__002266--002277__ddiiggeessttiioonn..iinndddd 2266 2244//0088//22002211 1166::2222 UUSS__002266--002277__ddiiggeessttiioonn..iinndddd 2277 2244//0088//22002211 1166::2222
28 HOW DOES THE BODY ABSORB NUTRIENTS DURING DIGESTION? Once nutrients have been unlocked from the foods we eat by digestion (see page 26), they must be transferred into the bloodstream in order for the body to be able to use them. This process takes place in the stomach and the large and small intestines. Stomach Duodenum Small intestine Jejunum Ileum Large intestine Sites of absorption Different sections of the digestive tract are better suited to absorbing specific nutrients. Most nutrients are absorbed in the small intestine. UUSS__002288--002299__nnuuttrriieennttss..iinndddd 2288 2255//0088//22002211 1199::1144
2288 STOMACH water ethyl alcohol HOW DOES THE BODY copper fluoride iodide ABSORB NUTRIENTS molybdenum DURING DIGESTION? DUODENUM calcium biotin The process by which nutrients are Each villi contains a mini-network of Once nutrients have been unlocked from the foods we eat by digestion (see page 26), copper absorbed by the body is known as diffusion. lymphatic vessels (lacteals) and capillaries folate they must be transferred into the bloodstream in order for the body to be able to use iron The inner wall of the small intestine contains that essentially connect the intestine to the magnesium them. This process takes place in the stomach and the large and small intestines. niacin multiple tiny projections, called villi. body’s circulatory and lymphatic systems. phosphorous These dramatically increase the surface Proteins (broken down into amino acids) riboflavin selenium area available for absorption to take place; and carbohydrates (broken down into thiamine the average small intestine has 2,690 sq ft glucose) pass into the blood vessels. Fats vitamins A, D, E, K (250 sq m) of surface area! (broken down into lipids) pass into the JEJUNUM Villi are themselves covered in microscopic lymphatic vessels. Stomach lipids projections, known as microvilli, and these These blood and lymphatic vessels then monosaccharides amino acids are responsible for diffusion. Nutrient transport those nutrients to different parts small peptides particles released in the small intestine pass of the body to be used as needed or stored biotin Duodenum calcium through these fine projections into the villi. for later use. chromium folate iron phosphorous magnesium Cell of the lining manganese of the intestinal molydenum Small wall (enterocyte) Microvilli niacin intestine pantothenate riboflavin thiamine vitamins A, B6, C, D, E, K Jejunum zinc ILEUM Ileum bile salts acids folate Large magnesium intestine vitamins B12, C vitamins D, K Microvilli The surface of each villi is covered in microscopic filaments. It is these extensions of the epithelial (outer) cell walls Enterocyte that absorb nutrients from the Blood capillaries Lymph lacteals small intestine into the villi. absorb amino acids absorb lipids LARGE INTESTINE Sites of absorption Different Villi These tiny fingerlike projections of the sections of the digestive tract are wall of the small intestine greatly expand its water better suited to absorbing specific surface area. Each houses blood and lymphatic short-chain fatty acids nutrients. Most nutrients are vessels that transport the nutrients absorbed biotin absorbed in the small intestine. by the villi to other parts of the body. potassium sodium chloride vitamin K UUSS__002288--002299__nnuuttrriieennttss..iinndddd 2288 2255//0088//22002211 1199::1144 UUSS__002288--002299__nnuuttrriieennttss..iinndddd 2299 2255//0088//22002211 1199::1144
30 WHAT IS NUTRITION? 31 WHAT IS METABOLISM? Metabolism is the term used to collectively refer to every single chemical reaction that takes place in your body in order to keep it alive. Metabolism is closely linked to nutrition, because the food we eat provides the energy required to fuel metabolism. On average, we use 10 percent of energy intake on FASTED (POSTABSORPTIVE) STATE digestion itself, 20 percent on physical activity, and Several hours after eating, blood glucose levels a whopping 70 percent by organs and tissues to keep drop, which triggers the pancreas to release glucagon the body alive. Every process in the body, from (see opposite). This stimulates the liver and adipose breathing to thinking, uses energy. A person’s basal tissue to metabolize glycogen stores, which releases metabolic rate (BMR) is the number of calories glucose into the bloodstream to make it available needed to sustain their life while they are sitting still. for the body to use as energy. How the body metabolizes the energy it consumes After prolonged fasting, fat stores in adipose is played out in a balancing act between two states— tissue are broken down into glycerol and fatty fed and fasted. acids in the liver. Ketone bodies are a by- product of this reaction (see page 170). FED (ABSORPTIVE) STATE Protein is used for fuel only as a last During or after eating a meal, food is broken down resort (see page 33). and glucose (see pages 12–13) is released into the pancreas blood for cells to absorb and use as fuel. When the body has obtained more glucose from food than is insulin released needed by cells, they stop absorbing it. The resulting from beta cells increase in blood glucose levels triggers the release Pancreatic cells of insulin. Insulin stimulates liver and muscle cells to reverse the conversion of glycogen into glucose The pancreatic islets, (see pages 110–111) that takes place during the also referred to as the fasted state (see opposite) and, instead, absorb the islets of Langerhans, surplus glucose in the bloodstream, convert it to are clusters of cells glycogen granules, and store it for future use. that secrete the Insulin also triggers the conversion of glucose to hormones responsible for balancing triglycerides (fats) in adipose tissue. Surplus fatty metabolism. These acids from the diet are also stored in adipose tissue. hormones work to How much energy gets stored in part depends on balance blood sugar your BMR, which is influenced by factors such as levels as the body oscillates between the genetics, age, sex, and body composition. To maintain fed and fasted states. a healthy and stable weight, we must (to simplify) put energy into the body that matches the energy expelled glucagon released from alpha cells to stay alive, plus energy used in physical activity. If blood vessel we consistently put in more calories than we burn, the excess is efficiently stored as fat. islet of Langerhans UUSS__003300--003311__mmeettaabboolliissmm..iinndddd 3300 2244//0088//22002211 1166::2222
3300 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 3311 WHAT IS METABOLISM? What is a calorie? 1 Calorie is another word for Metabolism is the term used to collectively refer to every single chemical reaction kilocalorie (kcal), and 1 calorie that takes place in your body in order to keep it alive. Metabolism is closely linked to is the amount of energy it takes nutrition, because the food we eat provides the energy required to fuel metabolism. to raise the temperature of 1g ENERGY 1G WATER 1G WATER CALORIE of water by 1.8oF (1oC). 0OF 1.8OF (4.18KJ) On average, we use 10 percent of energy intake on FASTED (POSTABSORPTIVE) STATE WHAT ARE CALORIES? may absorb different levels of nutrients from the digestion itself, 20 percent on physical activity, and Several hours after eating, blood glucose levels This unit of measurement is used to estimate how same quantity of the same foods. Gut health and a whopping 70 percent by organs and tissues to keep drop, which triggers the pancreas to release glucagon much energy there is stored in the chemical bonds the length of the intestines play a role in how much the body alive. Every process in the body, from (see opposite). This stimulates the liver and adipose of the foods we eat. Outdated guidelines state the energy your body is able to absorb from foods. breathing to thinking, uses energy. A person’s basal tissue to metabolize glycogen stores, which releases average man needs about 2,500kcal a day to maintain It’s important to remember that calories are not metabolic rate (BMR) is the number of calories glucose into the bloodstream to make it available a healthy weight, and for women, it’s 2,000kcal. But everything! A number definitely does not dictate needed to sustain their life while they are sitting still. for the body to use as energy. these figures should be adjusted depending on how healthy you are or the quality of nutrition you How the body metabolizes the energy it consumes After prolonged fasting, fat stores in adipose factors such as age, size, and level of physical activity. consume. You could be in your body’s ideal calorie- is played out in a balancing act between two states— tissue are broken down into glycerol and fatty Your body may not gain the full amount of energy intake range by eating a chocolate brownie for fed and fasted. acids in the liver. Ketone bodies are a by- that is released from foods. Foods full of fiber, like breakfast, lunch, and dinner, but this won’t provide product of this reaction (see page 170). nuts, for instance, take more energy to digest, and you with all the key macronutrients, micronutrients, FED (ABSORPTIVE) STATE Protein is used for fuel only as a last you absorb the remaining energy. Also, two people and fiber you need to be healthy and happy. During or after eating a meal, food is broken down resort (see page 33). and glucose (see pages 12–13) is released into the pancreas blood for cells to absorb and use as fuel. When the In response to high The presence As glucose in the body has obtained more glucose from food than is blood sugar levels, of insulin enables bloodstream is depleted, insulin released needed by cells, they stop absorbing it. The resulting from beta cells pancreatic beta cells glucose to enter the blood sugar levels begin release the hormone body’s cells so that it to fall, stimulating increase in blood glucose levels triggers the release HIGH BLOOD insulin into the can be used by pancreatic alpha cells of insulin. Insulin stimulates liver and muscle cells Pancreatic cells SUGAR RESPONSE bloodstream. them as energy. to release glucagon. to reverse the conversion of glycogen into glucose The pancreatic islets, (see pages 110–111) that takes place during the also referred to as the BETA CELL INSULIN BLOOD SUGAR fasted state (see opposite) and, instead, absorb the islets of Langerhans, surplus glucose in the bloodstream, convert it to are clusters of cells glycogen granules, and store it for future use. that secrete the Insulin also triggers the conversion of glucose to hormones responsible for balancing triglycerides (fats) in adipose tissue. Surplus fatty metabolism. These acids from the diet are also stored in adipose tissue. hormones work to How much energy gets stored in part depends on balance blood sugar ALPHA CELL GLUCAGON BLOOD SUGAR your BMR, which is influenced by factors such as levels as the body oscillates between the genetics, age, sex, and body composition. To maintain fed and fasted states. a healthy and stable weight, we must (to simplify) put LOW BLOOD In response to low Glucagon helps to The released glucose energy into the body that matches the energy expelled glucagon released SUGAR RESPONSE blood-sugar levels, regulate blood sugar by enters the bloodstream pancreatic alpha cells stimulating the release and is now available for from alpha cells to stay alive, plus energy used in physical activity. If secrete the hormone of glucose from stores in use as energy. The resulting blood vessel glucagon. the liver and muscles. rise in blood sugar levels we consistently put in more calories than we burn, eventually stimulates the excess is efficiently stored as fat. islet of Langerhans pancreatic beta cells to release insulin. UUSS__003300--003311__mmeettaabboolliissmm..iinndddd 3300 2244//0088//22002211 1166::2222 UUSS__003300--003311__mmeettaabboolliissmm..iinndddd 3311 2244//0088//22002211 1166::2222
32 WHAT IS NUTRITION? 33 AM I MALNOURISHED? Malnutrition is not something to worry about if you are receiving adequate nutrition from your diet. Sadly, this isn’t the case for around 42 million people who are food insecure. Social causes of malnutrition include poverty and MALNUTRITION IN CHILDHOOD lack of knowledge about nutrition. Physical causes Children with diseases and illness will often make include chronic or acute illness. For instance, an up a large number of reported cases of childhood estimated 65–75 percent of people with Crohn’s malnourishment. Food intolerances or allergies may disease and up to 62 percent of those with ulcerative prevent children from receiving adequate nutrition. colitis are malnourished (see pages 164–165). Also, young children have small stomachs, so they Eating disorders may also be a cause (see page 210). need to eat more frequently than adults do in order Malnutrition is most common in children and to get in all the nutrients they need across the day. the elderly, and women are more impacted than men. This can be difficult for parents to juggle, leading As women have smaller musculature, they require to malnourishment. Not growing or putting on 25 percent less energy per day than men but the weight as expected are key signs to look out for. same quantities of nutrients so must prioritize It’s important to be mindful when discussing nutrient-rich foods, which is expensive. Pregnancy healthy weight with children. Even weighing and breastfeeding also increase nutrient requirements. children may be linked to future disordered relationships with food. Signs of malnutrition IDENTIFYING MALNUTRITION Many people erroneously believe that only BEING MALNOURISHED HAS A BIG IMPACT ON undereating causes malnutrition. In fact, the HEALTH AND LIFE QUALITY. SIGNS INCLUDE: term refers to any severe imbalance of energy and increased incidence of illness nutrients in the body. This can describe a variety and infection of malnutritional scenarios. Most obviously, the body slower wound healing may lack certain nutrients it needs to function in a increased incidence of falls healthy way. Toward the other end of the spectrum, low mood energy intake (food) exceeds energy expenditure reduced energy levels (physical activity). In the case of malnourishment reduced muscle strength and due to obesity, the micronutrients needed for good muscle mass (see opposite) health are often undersupplied due to a diet high in reduced quality of life refined carbohydrates, snack foods, sugar, junk foods, reduced independence and ability to carry out daily activities and highly processed foods. Both undernutrition and deficiencies of multiple vital overnutrition are forms of malnutrition, and both are micronutrients (see pp.20–23). detrimental to health. If you have any concerns about malnutrition According to the World Health Organization, in you or a family member, see your doctor. 1.9 billion adults are overweight or obese, while 462 million are underweight. UUSS__003322--003333__mmaallnnuuttrriittiioonn..iinndddd 3322 2244//0088//22002211 1166::2222
3322 WWHHAATT IISS NNUUTTRRIITTIIOONN?? 3333 AM I MALNOURISHED? Muscle wasting Malnutrition is not something to worry about if you are receiving If the amount of energy coming from the diet is adequate nutrition from your diet. Sadly, this isn’t the case for around inadequate for the body to perform basic functions, the body resorts to unlocking protein from muscles to 42 million people who are food insecure. use as energy. Over time, this reduces muscle mass, causing weakness and increasing the risk of injury. Social causes of malnutrition include poverty and MALNUTRITION IN CHILDHOOD lack of knowledge about nutrition. Physical causes Children with diseases and illness will often make include chronic or acute illness. For instance, an up a large number of reported cases of childhood estimated 65–75 percent of people with Crohn’s malnourishment. Food intolerances or allergies may disease and up to 62 percent of those with ulcerative prevent children from receiving adequate nutrition. colitis are malnourished (see pages 164–165). Also, young children have small stomachs, so they Eating disorders may also be a cause (see page 210). need to eat more frequently than adults do in order Malnutrition is most common in children and to get in all the nutrients they need across the day. the elderly, and women are more impacted than men. This can be difficult for parents to juggle, leading As women have smaller musculature, they require to malnourishment. Not growing or putting on 25 percent less energy per day than men but the weight as expected are key signs to look out for. same quantities of nutrients so must prioritize It’s important to be mindful when discussing nutrient-rich foods, which is expensive. Pregnancy healthy weight with children. Even weighing NUTRIENT NUTRIENT and breastfeeding also increase nutrient requirements. children may be linked to future disordered RICH DIET DEFICIENT DIET relationships with food. Signs of malnutrition IDENTIFYING MALNUTRITION Many people erroneously believe that only PHYSICAL IMPACT BEING MALNOURISHED HAS A BIG IMPACT ON undereating causes malnutrition. In fact, the When malnourished, vitamin and mineral deficiencies HEALTH AND LIFE QUALITY. SIGNS INCLUDE: Malnutrition in later life term refers to any severe imbalance of energy and are more likely, which can impact negatively on many increased incidence of illness nutrients in the body. This can describe a variety bodily processes. This could eventually affect a UP TO ONE IN TWO ADULTS AGE 65 AND OLDER and infection of malnutritional scenarios. Most obviously, the body person’s ability to undertake everyday tasks. AND AS MANY AS 39 PERCENT OF OLDER ADULT slower wound healing PATIENTS MAY BE MALNOURISHED OR AT RISK. may lack certain nutrients it needs to function in a In the case of extreme undereating, once the body In many cases, the overall energy intake increased incidence of falls healthy way. Toward the other end of the spectrum, has depleted its glycogen stores (see page 110), it then is adequate, but there is too much fat, sugar, low mood energy intake (food) exceeds energy expenditure turns to fat and protein stores for sources of energy. and salt in the diet and not enough fruit, reduced energy levels vegetables, and oily fish, for instance. This may be (physical activity). In the case of malnourishment Reduced fat stores can lead to hormonal disruption. reduced muscle strength and due to entrenched habits, a reduced ability to cook, due to obesity, the micronutrients needed for good Women may lose their periods, and men can lose their muscle mass (see opposite) or increased pickiness; elderly people often lose health are often undersupplied due to a diet high in morning erection. Once fat stores are depleted, breaking their sense of smell and taste, which reduces reduced quality of life refined carbohydrates, snack foods, sugar, junk foods, down protein to release amino acids to use as energy is appetite. Psychological factors, such as reduced independence and ability memory loss or dementia, can also contribute to carry out daily activities and highly processed foods. Both undernutrition and the body’s last resort. This leads to muscle wastage (see to reduced energy intake. A decline in deficiencies of multiple vital overnutrition are forms of malnutrition, and both are above). The mass of individual organs can also become gastrointestinal health means fewer nutrients micronutrients (see pp.20–23). detrimental to health. depleted over time, affecting their function. are absorbed. Other physical issues, such as If you have any concerns about malnutrition According to the World Health Organization, Malnutrition in childhood can stunt growth, lead to dysphagia (problems with swallowing) can in you or a family member, see your doctor. limit energy intake. 1.9 billion adults are overweight or obese, while poor mental development, life-limiting behavioral 462 million are underweight. abnormalities, and even life-threatening conditions. UUSS__003322--003333__mmaallnnuuttrriittiioonn..iinndddd 3322 2244//0088//22002211 1166::2222 UUSS__003322--003333__mmaallnnuuttrriittiioonn..iinndddd 3333 2244//0088//22002211 1166::2222
UUSS__003344--003355__cchhaapptteerr__22..iinndddd 3344 2244//0088//22002211 1166::2222
HOW CAN WE EAT WELL? What Can I Learn from Mediterranean, Should I Pay Attention to Food Labels? 60 Nordic, and Japanese Diets? 36 Can I Trust Marketing Buzzwords? 62 How Can I Eat a More Mediterranean, Is Sugar the Enemy? 64 Nordic, or Japanese Diet? 38 Is Fat Still Bad for Me? 66 What Are the Principles of Healthy Eating? 40 Should I Cut Out Red Meat? 68 Should I Eat Less Meat and More Fish? 42 Will Salt Give Me a Heart Attack? 70 Why Are Legumes and Pulses So Good for You? 44 Surely Caffeine Can’t Be Good for Me? 72 Why Are Whole Grains So Good for You? 45 Does Alcohol Have Any Benefits? 73 Should I Cook Meals from Scratch More Often? 46 Will Superfoods Save Me? 74 What Is Good Gut Health, and Is Organic Better for You? 76 Why Is It Important? 48 Should I Change My Diet if I’m Exercising? 78 How Can I Increase Gut Microbiome Diversity? 50 How Important Is Hydration? 79 What Is the Difference Between Do I Need Sports Supplements? 80 Pre- and Probiotics? 52 When Should I Eat Before Exercise? 82 What Are the Elements of a Poor Diet? 54 Is It Better to Refuel Right After Exercise? 83 Why Do So Many of Us Have Poor Diets? 56 Will I Lose Weight Faster if I Exercise? 84 Should I Avoid Processed Food? 58 Which Exercise Is Better at Fat Burning? 86 UUSS__003344--003355__cchhaapptteerr__22..iinndddd 3344 2244//0088//22002211 1166::2222 UUSS__003344--003355__cchhaapptteerr__22..iinndddd 3355 2255//0088//22002211 1199::1144
HOW CAN WE EAT WELL? 37 MEDITERRANEAN NORDIC Olive oil is comprised of Whole grain rye mainly unsaturated fats bread is high in fiber WHAT CAN I LEARN FROM MEDITERRANEAN, NORDIC, AND JAPANESE DIETS? Noncommunicable diseases are now the leading cause of death globally, but in some parts of the world, people seem to live longer and healthier lives, which may be partly due to how they eat. The Mediterranean diet concept developed comes from oily fish and poultry; and there is when health organizations identified a possible low intake of red meat, eggs, yogurt, and cheese. link between traditional ways of eating in olive- The Nordic diet was developed in response growing areas like southern Italy and Greece to rising obesity rates and to encourage people and low levels of chronic disease among older in the region to eat locally produced, sustainable people. The core focus is fresh, seasonal produce; foods. Most calories come from plant-based plant-based eating; and healthy unsaturated fats, foods, making it fiber rich. Sea and lake fish especially from olive oil. Vegetables, legumes and lean game meats are key protein sources, (beans and lentils), whole grains, and nuts form and canola oil, which is high in healthier the bulk of meals; moderate animal protein monounsaturated fat, is used for cooking. UUSS__003366--003377__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3366 2244//0088//22002211 1166::2222
HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 3377 Sweet potatoes for life OKINAWA, SOUTH OF MAINLAND JAPAN, HAS ONE OF THE WORLD’S LONGEST LIFE EXPECTANCIES, WITH A REPORTED 68 CENTENARIANS PER 100,000 INHABITANTS IN 2019. While genetics, social habits, and exercise play a part, the islands’ traditional diet relies heavily on purple sweet potatoes. These are high in fiber and a source of nutrients, including vitamins A and C and potassium, as well as the antioxidant anthocyanin, which various studies indicate helps protect against illnesses, including cardiovascular disease. Okinawans are also said to eat 18 different foods, MEDITERRANEAN NORDIC JAPANESE including seven vegetables and fruits, every day. Olive oil is comprised of Whole grain rye Purple sweet potatoes mainly unsaturated fats bread is high in fiber contain antioxidants WHAT CAN I LEARN FROM Japan has among the highest life expectancy and countries; it may be that high consumption of lowest obesity rates globally. Its diet generally soy-based foods and oily fish increases “good” MEDITERRANEAN, NORDIC, emphasizes plant-based eating of leafy vegetables, HDL (high-density lipoprotein) cholesterol, which soybean-based foods like tofu and miso, and grains helps clear other types of cholesterol from the blood. AND JAPANESE DIETS? like rice and noodles, with animal protein from fish Cancer and diabetes: Studies suggest that and pork. The Okinawa prefecture is particularly closely following a Mediterranean diet long term Noncommunicable diseases are now the leading cause of death well known for longevity; its traditional diet is may reduce the onset of various cancers (including globally, but in some parts of the world, people seem to live longer broadly similar, although high-fiber root vegetables breast, prostate, and colorectal). It’s also been linked and healthier lives, which may be partly due to how they eat. are staples rather than rice, and very small amounts to improved control of blood glucose, which can help of fish and pork are eaten—not beef, eggs, or dairy. in managing type 2 diabetes. A strict Nordic diet has been associated with a lower risk of developing type ARE THESE DIETS HEALTHY? 2 diabetes, although further research is needed. The Mediterranean diet concept developed comes from oily fish and poultry; and there is Heart health: A variety of evidence suggests Cognitive health: Japan has low rates of age- when health organizations identified a possible low intake of red meat, eggs, yogurt, and cheese. that the Mediterranean diet can reduce the risk of related diseases, although research hasn’t yet link between traditional ways of eating in olive- The Nordic diet was developed in response cardiovascular disease (CVD), meaning conditions identified the precise role of its diets. Most evidence growing areas like southern Italy and Greece to rising obesity rates and to encourage people affecting the heart or blood vessels. A landmark focuses on Mediterranean eating, which has been and low levels of chronic disease among older in the region to eat locally produced, sustainable study found that the diet reduced both the number linked to a slower rate of decline in memory and people. The core focus is fresh, seasonal produce; foods. Most calories come from plant-based of heart attacks and strokes and all deaths from cognitive ability; high levels of plant antioxidants plant-based eating; and healthy unsaturated fats, foods, making it fiber rich. Sea and lake fish cardiovascular causes after five years. Research has may reduce inflammation associated with diseases especially from olive oil. Vegetables, legumes and lean game meats are key protein sources, linked Nordic eating to a reduction in major risk like Alzheimer’s. (beans and lentils), whole grains, and nuts form and canola oil, which is high in healthier factors for CVD, including high blood pressure. To find out how you could apply these diets to the bulk of meals; moderate animal protein monounsaturated fat, is used for cooking. Japan has lower rates of CVD than other developed your daily eating, turn to pages 38–39. UUSS__003366--003377__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3366 2244//0088//22002211 1166::2222 UUSS__003366--003377__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3377 2244//0088//22002211 1166::2222
38 HOW CAN I EAT A MORE MEDITERRANEAN, NORDIC, OR JAPANESE DIET? Making small but sustainable changes that work with your personal preferences and budget is often the most effective way to eat a healthier diet. So how can you apply some of the key features of Mediterranean, Nordic, and Japanese eating? DO I NEED TO EAT SWEET POTATOES? SHOULD I EAT MORE NUTS AND SEEDS? Consuming purple sweet potatoes (or foods like Nuts and seeds are featured in both seaweed or bitter melon, which are also featured in Mediterranean and Nordic eating; they are high the Japanese diet) can provide positive nutritional in monounsaturated fat and protein and are sources benefits; however, it’s better to eat them as part of of fiber and vitamins, making them a highly a varied diet. Also, foods like these may need to be nutritious addition to a healthy and balanced diet. imported, increasing their cost and environmental Their nutritional profile varies; for example, pecans footprint. You can find similar benefits to purple are good for B vitamins and almonds for calcium, sweet potatoes in locally grown foods. For example, while Brazil, macadamia, and cashew nuts have a bit berries and cabbage are high in fiber, and red more saturated fat. Overall, they make a healthy cabbage, blueberries, and blackberries are more snack or can be sprinkled on breakfast cereal or accessible, and sustainable, sources of anthocyanins. salad and vegetables. However, due to naturally high fat content, a palm-sized serving is one portion. IS CANOLA OR OLIVE OIL HEALTHIER? Choose raw and unsalted varieties, if possible. Olive oil has a superior reputation to canola oil because the Mediterranean diet is well researched. KEEP IT VARIED However, both contain monounsaturated fatty While evidence points to some diets being more acids, known to be beneficial for heart health and protective of health, it’s not clear whether this is due cholesterol. Canola oil has less saturated fat and to particular elements or the diet as a whole. This contains omegas 3, 6, and 9, which support brain, suggests there isn’t one “perfect” diet; variety and a heart, and joint function, whereas olive oil, especially balance of healthy foods is probably more important. extra-virgin, is higher in antioxidant substances called polyphenols. Canola oil retains antioxidant properties and its neutral flavor at higher temperatures. Both oils are high in calories— around 120 calories per tablespoon for olive oil. UUSS__003388--003399__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3388 2244//0088//22002211 1166::2222
3388 HOW CAN I EAT A MORE What these diets share MEDITERRANEAN, NORDIC, Eating a wide range of different vegetables as the bulk of meals is the basis of all three OR JAPANESE DIET? diets, while other types of foods are common to at least two of them. Making small but sustainable changes that work with your personal preferences and budget is often the most effective way to eat a healthier diet. So how can you apply some of the key features of Mediterranean, Nordic, and Japanese eating? DO I NEED TO EAT SWEET POTATOES? SHOULD I EAT MORE NUTS AND SEEDS? VEGETABLES WHOLE LEGUMES LEAN AND LEAN MEAT HEALTHY AND FRUIT GRAINS AND PULSES OILY FISH AND POULTRY OIL Consuming purple sweet potatoes (or foods like Nuts and seeds are featured in both seaweed or bitter melon, which are also featured in Mediterranean and Nordic eating; they are high the Japanese diet) can provide positive nutritional in monounsaturated fat and protein and are sources benefits; however, it’s better to eat them as part of of fiber and vitamins, making them a highly a varied diet. Also, foods like these may need to be nutritious addition to a healthy and balanced diet. imported, increasing their cost and environmental Their nutritional profile varies; for example, pecans footprint. You can find similar benefits to purple are good for B vitamins and almonds for calcium, sweet potatoes in locally grown foods. For example, while Brazil, macadamia, and cashew nuts have a bit MEDITERRANEAN NORDIC JAPANESE (OKINAWA) berries and cabbage are high in fiber, and red more saturated fat. Overall, they make a healthy EAT MOST EAT MOST EAT MOST cabbage, blueberries, and blackberries are more snack or can be sprinkled on breakfast cereal or Vegetables; legumes and pulses Local vegetables, especially Local vegetables accessible, and sustainable, sources of anthocyanins. salad and vegetables. However, due to naturally high (including chickpeas, kidney cabbage, peas, and root (including purple and orange sweet fat content, a palm-sized serving is one portion. beans, fava beans, and lentils); vegetables; local fruits and potato, Chinese okra, bitter melon, whole grains (including whole berries; whole grains (rye bread, cabbage, seaweed, bamboo shoots) IS CANOLA OR OLIVE OIL HEALTHIER? Choose raw and unsalted varieties, if possible. wheat pasta); nuts and seeds oats, and barley); nuts and seeds Olive oil has a superior reputation to canola oil because the Mediterranean diet is well researched. KEEP IT VARIED EAT MODERATELY EAT MODERATELY EAT MODERATELY Fish (especially oily); Sea/lake fish, including oily fish Rice; noodles However, both contain monounsaturated fatty While evidence points to some diets being more poultry; olive oil; yogurt, (herring, mackerel, salmon); Soybean foods acids, known to be beneficial for heart health and protective of health, it’s not clear whether this is due cheese; fruit low-fat dairy foods; (including tofu and miso) canola oil cholesterol. Canola oil has less saturated fat and to particular elements or the diet as a whole. This contains omegas 3, 6, and 9, which support brain, suggests there isn’t one “perfect” diet; variety and a EAT A LITTLE EAT A LITTLE EAT A LITTLE Lean red meat; Lean red meat/game; poultry; Fish and seafood; heart, and joint function, whereas olive oil, especially balance of healthy foods is probably more important. processed meat eggs; cheese pork extra-virgin, is higher in antioxidant substances KEY FEATURE KEY FEATURE KEY FEATURE called polyphenols. Canola oil retains antioxidant Olive oil Canola oil Purple sweet potatoes properties and its neutral flavor at higher (extra-virgin is highest quality) (cold pressed is highest quality) temperatures. Both oils are high in calories— around 120 calories per tablespoon for olive oil. TAKE NOTE TAKE NOTE TAKE NOTE It is easy to follow, and the Some root veggies, like potatoes, The diet is high fiber and low protein; combination of foods have turnips, and parsnips, contain a the traditional version restricts some proven health benefits. lot of starchy carbs (pp. 12–13). nutritious foods, like fruits and eggs. UUSS__003388--003399__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3388 2244//0088//22002211 1166::2222 UUSS__003388--003399__mmeedd__nnoorrddiicc__jjaappaann..iinndddd 3399 2255//0088//22002211 1199::1144
40 HOW CAN WE EAT WELL? 41 WHAT ARE THE PRINCIPLES OF HEALTHY EATING? There is so much information available and so many theories about the best way to eat well, you can easily end up feeling confused and frustrated. But the principles are simple—variety and balance on the plate. There is no one way to achieve a healthy, balanced adult’s diet should contain carbohydrates, protein, diet; it has to reflect your body’s energy needs and and fat—in five key food groups. Hydration is also your lifestyle, beliefs, and preferences. The “balanced important; it helps you absorb nutrients and feel full. plate” concept is a useful guide to the types and proportions of foods you should try to consume at WHAT’S ON A BALANCED PLATE? mealtimes; apply it when shopping, cooking, or Starchy carbohydrates like rice, pasta, eating out to help you eat a varied, nutritious potatoes, quinoa, and oats should form the base range of food. of the meal and about one-third of your daily It isn’t essential to achieve the balance of food intake. Choose higher-fiber whole grain versions groups outlined here at every meal; just aim for it of these foods wherever possible, with reduced over the course of your day or week. A healthy or no salt or sugar. A good day’s eating Try to eat a range of foods 5+ 3–4 from these groups daily and a small amount of PORTIONS OF FRUITS PORTIONS OF STARCHY high-fat, sugar, or salt AND VEGETABLES CARBOHYDRATES foods only occasionally. 1 PORTION 1 PORTION Portion amounts shown = 1 handful/3oz/3–4 heaped teaspoons = 2 handfuls dried rice/pasta/couscous here are based on an cooked spinach/green beans (less for 4 portions) average woman, so = 1 medium tomato = 1 fist-size baked potato they can vary; use your = 1 medium apple/orange/banana = 2 slices bread hands as a rough guide. = 5oz of fruit juice (maximum per day) UUSS__004400--004411__hheeaalltthhyy__eeaattiinngg..iinndddd 4400 2244//0088//22002211 1166::2222
4400 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 4411 WHAT ARE THE PRINCIPLES Vegetables and fruits are just as important, Unsaturated oils and fats like olive OF HEALTHY EATING? if not more so; aim to eat at least five portions oil or canola oil should be used in small every day and exceed this wherever possible. amounts for cooking or to enhance flavor. Try to eat as wide a range as you can—this There is so much information available and so many theories about the best can include fresh, frozen, dried, and canned VEGETARIANS AND VEGANS way to eat well, you can easily end up feeling confused and frustrated. But the (in water or juice without salt or sugar). 6-8 The same broad principles apply, principles are simple—variety and balance on the plate. Include a higher ratio of vegetables to fruits but eating a wide variety of protein GLASSES OF and regularly change your combinations. FLUIDS/DAY sources is even more important because Protein-rich foods include pulses most plant-based protein is incomplete, Ideally water or There is no one way to achieve a healthy, balanced adult’s diet should contain carbohydrates, protein, (such as kidney beans, lentils, and chickpeas), meaning it lacks certain essential amino sugar-free drinks; diet; it has to reflect your body’s energy needs and and fat—in five key food groups. Hydration is also quinoa, soybean products like tofu and tea and coffee count acids (see pages 128–129). your lifestyle, beliefs, and preferences. The “balanced important; it helps you absorb nutrients and feel full. tempeh, nuts, eggs, fish, and meat. It’s best Choose dairy alternatives like soy plate” concept is a useful guide to the types and to limit the amount of red and processed meat milk that are unsweetened and fortified proportions of foods you should try to consume at WHAT’S ON A BALANCED PLATE? (see pages 68–69). with calcium, plus other sources of healthy mealtimes; apply it when shopping, cooking, or Starchy carbohydrates like rice, pasta, Dairy is a good source of many nutrients, omega-3 fatty acids like walnuts and ground eating out to help you eat a varied, nutritious potatoes, quinoa, and oats should form the base including calcium and phosphorus; it includes flaxseed. Vegans may still need to supplement range of food. of the meal and about one-third of your daily hard and soft cheese, yogurt, and cow’s milk. certain nutrients, such as vitamins D and B12, It isn’t essential to achieve the balance of food intake. Choose higher-fiber whole grain versions and iron (see pages 130–131). groups outlined here at every meal; just aim for it of these foods wherever possible, with reduced over the course of your day or week. A healthy or no salt or sugar. AA ggoooodd ddaayy’’ss eeaattiinngg Try to eat a range of foods 5+ 3–4 2–3 2–3 < 1 from these groups daily and a small amount of PORTIONS OF FRUITS PORTIONS OF STARCHY PORTIONS PORTIONS OF DAIRY SMALL AMOUNT high-fat, sugar, or salt AND VEGETABLES CARBOHYDRATES OF PROTEIN AND ALTERNATIVE OF FAT foods only occasionally. 1 PORTION 1 PORTION 1 PORTION 1 PORTION = 1 teaspoon to Portion amounts shown = 1 handful/3oz/3–4 heaped teaspoons = 2 handfuls dried rice/pasta/couscous = half a handful salmon/chicken/steak = 1oz/2 thumbs cheese cook a meal here are based on an cooked spinach/green beans (less for 4 portions) = 4oz cooked beans/lentils = 7oz low-fat cow’s milk or unsweetened average woman, so = 1 medium tomato = 1 fist-size baked potato = 1oz/palm-size nuts or seeds dairy alternative (4oz on cereal) they can vary; use your = 1 medium apple/orange/banana = 2 slices bread = 3oz tofu = 4oz low-fat yogurt hands as a rough guide. = 5oz of fruit juice (maximum per day) UUSS__004400--004411__hheeaalltthhyy__eeaattiinngg..iinndddd 4400 2244//0088//22002211 1166::2222 UUSS__004400--004411__hheeaalltthhyy__eeaattiinngg..iinndddd 4411 2244//0088//22002211 1166::2222
42 HOW CAN WE EAT WELL? 43 SHOULD I EAT LESS MEAT AND MORE FISH? Most adults choose meat over fish as their main source of protein, and most Americans eat less fish than recommended. But it’s worth adding more into your diet because whether it’s fresh, canned, or frozen, fish offers plenty of benefits. A great source of protein, with less fat than many contain low levels of omega-3s), but it’s mostly meats, fish and shellfish typically provide 15g–20g healthy polyunsaturated fat. EPA and DHA are of protein per 100g, roughly a third of the daily associated with improved cardiovascular and recommended amount for many adults. cognitive health; for example, research indicates Eating fish—especially oily types like sardines, that oily fish eaters have more gray matter—the salmon, trout, and mackerel—is also a good way to brain’s major functional tissue linked to memory. obtain beneficial amounts of the omega-3 Many people don’t eat enough omega-3s, polyunsaturated fatty acids EPA (eicosapentaenoic which you can obtain only from diet. Some oily acid) and DHA (docosahexaenoic acid). Oily fish is fish are also among the few dietary sources of fattier than white fish and shellfish (some of which Vitamin D (see pages 138–139). ONE STUDY FOUND THAT FARMED SALMON HAD ONLY 25% OF THE VITAMIN D CONTENT OF WILD SALMON UUSS__004422--004433__mmeeaatt__ffiisshh__vveegg..iinndddd 4422 2255//0088//22002211 1199::1144
4422 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 4433 Dietary patterns that include regular fish fresh. Choose canned fish in water; brine contains SHOULD I EAT LESS MEAT consumption, like the Mediterranean diet, are a lot of salt, and omega-3s may seep into oil. Fish AND MORE FISH? generally associated with a lower risk of becoming will absorb fats used during cooking, especially lean overweight and obese. In a US-based study of more varieties. High-temperature methods can break than 40,000 men, those who consumed more than down omega-3s—baking, grilling, and steaming Most adults choose meat over fish as their main source of protein, and most one portion of any fish every week had a 15 percent will help preserve nutritional content. Sustainability Americans eat less fish than recommended. But it’s worth adding more into your diet lower risk of developing heart disease. The is also a consideration (see pages 124–127). because whether it’s fresh, canned, or frozen, fish offers plenty of benefits. recommended portion for seafood is 8–10oz per week for adults 19 and older, 2–10oz for those 2–18 WHAT ABOUT SUPPLEMENTS? years old, and 2–3oz for children aged 6–23 months. While it’s not a substitute for fish within a balanced A great source of protein, with less fat than many contain low levels of omega-3s), but it’s mostly US FDA and EPA guidelines suggest consuming diet, fish oil can be beneficial for those who don’t meats, fish and shellfish typically provide 15g–20g healthy polyunsaturated fat. EPA and DHA are seafood choices higher in EPA and DHA, such as eat fish. However, it can contain heavy metals, and of protein per 100g, roughly a third of the daily associated with improved cardiovascular and salmon, anchovies, sardines, Pacific oysters, and as fish derive omega-3 by eating algae, it may be recommended amount for many adults. cognitive health; for example, research indicates trout, to limit mercury exposure. better to opt for algae oil. Krill oil is extracted from Eating fish—especially oily types like sardines, that oily fish eaters have more gray matter—the crustaceans and is rich in both EPA and DHA. Fish salmon, trout, and mackerel—is also a good way to brain’s major functional tissue linked to memory. BUYING AND COOKING liver oil contains a high amount of vitamin A; this obtain beneficial amounts of the omega-3 Many people don’t eat enough omega-3s, Unlike processed fish products, frozen may be harmful during pregnancy, and research polyunsaturated fatty acids EPA (eicosapentaenoic which you can obtain only from diet. Some oily and canned fish and seafood can suggests long-term use can weaken bones. acid) and DHA (docosahexaenoic acid). Oily fish is fish are also among the few dietary sources of be just as nutritious as fattier than white fish and shellfish (some of which Vitamin D (see pages 138–139). HOW MUCH IS A PORTION? ONE STUDY THE AMOUNT OF FISH FOUND THAT FARMED IN A PORTION WILL SALMON HAD ONLY 25% VARY WITH AGE. OF THE VITAMIN D CONTENT OF WILD AGE 2–3 SALMON 1oz RESEARCH SUGGESTS COLLAGEN AGE 4–7 PEPTIDES IN SALMON SKIN COULD HELP 2oz MANAGE TYPE 2 DIABETES, BUT FURTHER STUDY IS AGE 8–10 NEEDED 3oz AGE 11+ and ADULTS 4oz UUSS__004422--004433__mmeeaatt__ffiisshh__vveegg..iinndddd 4422 2255//0088//22002211 1199::1144 UUSS__004422--004433__mmeeaatt__ffiisshh__vveegg..iinndddd 4433 2244//0088//22002211 1166::2222
44 HOW CAN WE EAT WELL? 45 WHY ARE LEGUMES AND PULSES SO GOOD FOR YOU? Legumes and pulses provide variety of both flavor to the palate and nutrition to the body. They are packed full of micronutrients and fiber, and pulses specifically offer an affordable way to add nonmeat protein to your diet. A legume is the leaves, stem, pod, or seeds of any that make up protein (see pges 14–15). A 3.5oz plant from the Fabaceae family. We tend to eat the (100g) serving of red lentils, chickpeas, or kidney seed pods (like green beans and sugar snap peas) beans provides 7.5g–8.5g of protein, which makes or the seeds from within them (either fresh, like peas up a nice chunk of your daily requirement. You can and fava beans, or dried, like “pulses”). Both contain save much money by getting more of your protein zero saturated fat and contain valuable nutrients. from beans rather than meat, adding lots of valuable There is strong evidence associating beans and fiber to your diet at the same time. Being starchy lentils with a lower risk of cardiovascular disease, carbs (see pages 12–13), they typically contain obesity, diabetes, and cancer. They are loaded with around 8g fiber per 100g—that’s almost one-third of prebiotics (see pages 52–53) as well as fiber (see your daily requirement. pages 18–19). Clinical trials show improved health outcomes in people eating 25–29g of fiber daily. LEGUMES Eating leguminous vegetables like green beans PULSES is also important, because they contain other The dried seeds of legumes are called pulses. important nutrients. There is a lot of research to They include lentils, chickpeas, black or pinto beans, suggest we should aim to include more leguminous soybeans, and kidney beans. These robust seeds vegetables in our diet. Aim to include more green contain varying amounts of the essential amino acids beans, butter beans, and soybeans in your diet. LEGUMES PULSES CONTAIN PROTEIN, CONTAIN NUTRIENTS FIBER, FOLATE, FOUND IN MEATS AND PHOSPHOROUS, IRON, FISH, LIKE ZINC, AS WELL AND MONO- AND THOSE FOUND IN VEGGIES, POLYUNSATURATED LIKE POTASSIUM AND FATTY ACIDS FOLATE Heart healthy Research shows that eating legumes roughly four times weekly is associated with a 14 percent reduced risk of coronary artery disease. Substituting plant proteins for red meat reduces the risk factors, such as blood cholesterol, triglycerides, and blood pressure. UUSS__004444--004455__ppuullsseess__bbeeaannss..iinndddd 4444 2266//0088//22002211 1111::3377
4444 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 4455 WHY ARE LEGUMES AND WHY ARE WHOLE GRAINS PULSES SO GOOD FOR YOU? SO GOOD FOR YOU? Legumes and pulses provide variety of both flavor to the palate and nutrition Humans across the globe consume plenty of cereal crops, such as barley, oats, rice, to the body. They are packed full of micronutrients and fiber, and pulses and wheat. It is the seed of these crops, known as the grain, that we eat. Whole grains specifically offer an affordable way to add nonmeat protein to your diet. are those left intact when prepared for use as an ingredient. A legume is the leaves, stem, pod, or seeds of any that make up protein (see pges 14–15). A 3.5oz plant from the Fabaceae family. We tend to eat the (100g) serving of red lentils, chickpeas, or kidney seed pods (like green beans and sugar snap peas) beans provides 7.5g–8.5g of protein, which makes Bran or the seeds from within them (either fresh, like peas up a nice chunk of your daily requirement. You can and fava beans, or dried, like “pulses”). Both contain save much money by getting more of your protein Multi-layered edible bran zero saturated fat and contain valuable nutrients. from beans rather than meat, adding lots of valuable Germ There is strong evidence associating beans and fiber to your diet at the same time. Being starchy lentils with a lower risk of cardiovascular disease, carbs (see pages 12–13), they typically contain Endosperm obesity, diabetes, and cancer. They are loaded with around 8g fiber per 100g—that’s almost one-third of prebiotics (see pages 52–53) as well as fiber (see your daily requirement. pages 18–19). Clinical trials show improved health When they are being processed, refined USING MORE WHOLE GRAINS outcomes in people eating 25–29g of fiber daily. LEGUMES carbohydrates have the bran and germ In many cases, you can swap out refined Eating leguminous vegetables like green beans removed, which results in light flours for products for whole grain ones—substitute PULSES is also important, because they contain other making white bread or fluffy cakes, or whole wheat bread for white bread, or The dried seeds of legumes are called pulses. important nutrients. There is a lot of research to enables the production of white rice and brown rice for white rice. Try serving a They include lentils, chickpeas, black or pinto beans, suggest we should aim to include more leguminous white pasta. However, the bran and germ curry with a new and whole grain, like soybeans, and kidney beans. These robust seeds vegetables in our diet. Aim to include more green are valuable sources of fiber and nutrients. buckwheat, spelt, or pot barley, for instance. contain varying amounts of the essential amino acids beans, butter beans, and soybeans in your diet. While refined carbs provide energy Whole oats, for example, are also (see page 12), whole grains are 75 WHOLE GRAINS very nutritious. Try having oatmeal percent more nutritious. for breakfast more frequently, and INCLUDE WILD AND LEGUMES PULSES Whole grains provide fiber, B BROWN RICE, WHOLE choose granola bars for snacks. vitamins, omega-3 fatty acids, OATS AND RYE, QUINOA, From time to time, experiment CONTAIN PROTEIN, CONTAIN NUTRIENTS CORN, FREEKEH, protein, as well as many antioxidants, with new whole grains to consider FIBER, FOLATE, FOUND IN MEATS AND AMARANTH, SORGHUM, PHOSPHOROUS, IRON, FISH, LIKE ZINC, AS WELL micronutrients, and phytochemicals. AND TEFF how you might add more types AND MONO- AND THOSE FOUND IN VEGGIES, Regular consumption is linked to gut to your culinary repertoire. Some POLYUNSATURATED LIKE POTASSIUM AND and heart health and the prevention of are sweeter, like farro. Some make great FATTY ACIDS FOLATE cancers, diabetes, and obesity. Surveys accompaniments to a main dish, like quinoa show that 95 percent of adults or bulgur. There’s tons of fantastic advice don’t eat enough whole grains, and inspiration on social media Heart healthy Research shows that eating and almost one in three to help you get started with legumes roughly four times weekly is associated with a 14 percent reduced risk of coronary adults get none at all. new flavors. artery disease. Substituting plant proteins for red meat reduces the risk factors, such as blood cholesterol, triglycerides, and blood pressure. UUSS__004444--004455__ppuullsseess__bbeeaannss..iinndddd 4444 2266//0088//22002211 1111::3377 UUSS__004444--004455__ppuullsseess__bbeeaannss..iinndddd 4455 2244//0088//22002211 1166::2222
46 HOW CAN WE EAT WELL? 47 SHOULD I COOK MEALS FROM SCRATCH MORE OFTEN? Cooking a meal using fresh, canned, or frozen ingredients can seem like a chore, especially when life is already busy. Is it worth the extra time and effort— and does it actually work out more expensive, as many people assume? Preparing and cooking meals yourself can have not to mention the addition of sugar to entice real health benefits because you are in charge of people into eating more of them (see pages 58–59). how much sugar, fat, and salt goes into your meal Cooking your own meals also means you can choose (see pages 64, 66, and 70). Many highly processed more nutrient-dense ingredients. For example, try foods, in particular ready-made meals and takeout, using whole grains instead of refined carbohydrates, have added salt for flavor and preservation and are such as brown rice rather than white or whole wheat cooked in a significant amount of oil or butter, pasta or couscous (see pages 12–13). How you cook matters Potatoes are a rich source of micronutrients like potassium, vitamin B6, and vitamin C. How you prepare and cook them, like other ingredients, can significantly alter their nutritional content (shown per 3.5oz) and affect nutrient loss. DEEP FRYING PAN FRYING ROASTING IN SUNFLOWER OIL IN OLIVE OIL IN OLIVE OIL PROTEIN: 3.2g PROTEIN: 2.6g PROTEIN: 3g CARBS: 36.6g CARBS: 23.3g CARBS: 26g FAT: 14.5g FAT: 7g FAT: 4.5g During deep frying, fat penetrates the Quick frying has little impact on protein or This flavorful method provides flesh; deep-fried potatoes can have 2–3 mineral content, retains vitamin C, and more calories than most others but is times the calories of boiled or baked. boosts fiber by forming resistant starch. healthier than deep-fat frying. UUSS__004466--004477__ccooookkiinngg__ffrroomm__ssccrraattcchh..iinndddd 4466 2244//0088//22002211 1166::2222
4466 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 4477 SHOULD I COOK MEALS FROM HOME COOKING AND HEALTH and takeout meals. Keep these simple tips in mind: SCRATCH MORE OFTEN? A body of research supports the benefits of cooking Use less meat and bulk up stews, soups, from scratch. One 2017 study found that people who and curries with protein-rich canned beans, ate home-cooked meals more than five times a week whole wheat pasta, and brown rice. Cooking a meal using fresh, canned, or frozen ingredients can seem like a chore, were 28 percent less likely to have an overweight Divide your usual portion of meat into two especially when life is already busy. Is it worth the extra time and effort— BMI (body mass index) score compared with people meals, for instance, by adding grated carrot, and does it actually work out more expensive, as many people assume? eating this way less than three times weekly. They zucchini, or chopped mushrooms. Loose also ate more fruits and vegetables. Research also vegetables are often less expensive. suggests that cooking improves self-esteem and Switch fresh fish for cheaper frozen or canned, Preparing and cooking meals yourself can have not to mention the addition of sugar to entice mood, although there are many factors to consider ideally in plain water. Frozen fish has just as many, real health benefits because you are in charge of people into eating more of them (see pages 58–59). when isolating studies. Creating meals addresses or even more, nutrients as fresh. This is true for how much sugar, fat, and salt goes into your meal Cooking your own meals also means you can choose different aspects of psychological well-being by many other ingredients. (see pages 64, 66, and 70). Many highly processed more nutrient-dense ingredients. For example, try providing a sense of autonomy and confidence and Shop with a friend to split bulk-buy discounts foods, in particular ready-made meals and takeout, using whole grains instead of refined carbohydrates, opportunities to socialize and build relationships. and bargains nearing sell-by dates. Separate large have added salt for flavor and preservation and are such as brown rice rather than white or whole wheat packs of meat or fish into portions and freeze ones cooked in a significant amount of oil or butter, pasta or couscous (see pages 12–13). COST-EFFECTIVE COOKING you won’t cook before the use-by date. Cooking with fresh ingredients like meat and fish Before shopping, plan your meals and check can still work out cheaper than buying preprepared which ingredients are already in your pantry. How you cook matters Potatoes are a rich source of micronutrients like potassium, vitamin B6, and vitamin C. How you prepare and cook them, like other ingredients, can significantly alter their nutritional content (shown per 3.5oz) and affect nutrient loss. DEEP FRYING PAN FRYING ROASTING IN SUNFLOWER OIL IN OLIVE OIL IN OLIVE OIL BOILING BAKING STEAMING PROTEIN: 3.2g PROTEIN: 2.6g PROTEIN: 3g PROTEIN: 2.2g PROTEIN: 2g PROTEIN: 3g CARBS: 36.6g CARBS: 23.3g CARBS: 26g CARBS: 20.4g CARBS: 21g CARBS: 26g FAT: 14.5g FAT: 7g FAT: 4.5g FAT: 0.1g FAT: 0.1g FAT: 0.1g During deep frying, fat penetrates the Quick frying has little impact on protein or This flavorful method provides Boiling with skins on greatly reduces the Baked potatoes are virtually fat-free, Steaming retains the most nutrients; it is flesh; deep-fried potatoes can have 2–3 mineral content, retains vitamin C, and more calories than most others but is loss of water-soluble micronutrients, like and baking in skin is one of the best particularly useful for new potatoes, which times the calories of boiled or baked. boosts fiber by forming resistant starch. healthier than deep-fat frying. vitamins B6 and B12, into cooking water. methods for retaining nutrients. are high in water-soluble vitamin C. UUSS__004466--004477__ccooookkiinngg__ffrroomm__ssccrraattcchh..iinndddd 4466 2244//0088//22002211 1166::2222 UUSS__004466--004477__ccooookkiinngg__ffrroomm__ssccrraattcchh..iinndddd 4477 2244//0088//22002211 1166::2222
48 HOW CAN WE EAT WELL? 49 WHAT IS GOOD GUT HEALTH, AND WHY IS IT IMPORTANT? A healthy gut isn’t just necessary for good digestion. Scientists are uncovering more about all the ways the bacteria in your gut may be involved in your overall health—even your mental well-being. We tend to associate bacteria with infection, but around 70 percent of immunity-related cells most bacteria in the gut are actually beneficial. are located there. Friendly bacteria interact Along with yeasts, fungi, and viruses, bacteria with the intestinal lining to prevent harmful make up the gut microbiome—the ecosystem molecules from leaking into the body and help of around 100 trillion microorganisms mainly activate new immune cells (see pages 136–137). found in the colon. There are around 1,000 species of gut bacteria; the specific combination HEALTHY GUT, HEALTHY MIND? is unique to each of us and changes over our life We know the brain affects the gut because we span. Research suggests maintaining a balance often feel stress or excitement in our stomach, between more and less helpful bacteria is key but diet could also have an impact on our brain. to a healthy gut. Scientists have only recently started exploring In addition to digesting food, gut bacteria the mechanisms by which gut bacteria may perform many other important tasks. For influence the brain—and therefore how gut example, they help absorb minerals from food, health could affect mood. Several studies have synthesize vitamins like vitamin K (which helps found the microbes of people with depression with blood clotting), and digest dietary fiber, differ from others; in 2019, scientists identified releasing molecules, including butyrate, which two specific types of gut bacteria (Coprococcus contributes to a stronger gut barrier, and and Dialister) that were consistently lacking in propionate, which helps the liver regulate the microbiomes of the depressed subjects. blood sugar levels and appetite. WIDER HEALTH Fermenting discomfort People with the inflammatory bowel conditions Crohn’s disease and ulcerative colitis have FERMENTATION IS WHEN THE GUT BACTERIA been found to have fewer species and a lower BREAK DOWN DIETARY FIBER. This creates hydrogen and methane gas, which proportion of beneficial gut bacteria. Studies is normal and in fact shows that the gut is have also observed lower diversity in people working effectively. However, those with a with obesity, diabetes, and certain types of functional gut disorder like irritable bowel eczema and arthritis. Dysbiosis—an imbalance syndrome are more susceptible to the effects, due to increased sensitivity of their in the gut microbiome—has also been shown intestines—so they can feel bloating or to contribute to metabolic syndrome, allergies, abdominal pain. colorectal cancer, and Alzheimer’s disease. A healthy gut supports the immune system— UUSS__004488--004499__ggoooodd__gguutt__hheeaalltthh..iinndddd 4488 2244//0088//22002211 1166::2222
4488 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 4499 WHAT IS GOOD GUT HEALTH, Brain nuclei produce serotonin to affect mood The vagus nerve links organs to the medulla AND WHY IS IT IMPORTANT? A healthy gut isn’t just necessary for good digestion. Scientists are uncovering more about all the ways the bacteria in your gut may be involved in your overall health—even your mental well-being. We tend to associate bacteria with infection, but around 70 percent of immunity-related cells VAGUS IMMUNE NEURO- most bacteria in the gut are actually beneficial. are located there. Friendly bacteria interact NERVE SYSTEM TRANSMITTERS Along with yeasts, fungi, and viruses, bacteria with the intestinal lining to prevent harmful THIS NERVE DIRECTLY THE GUT IS THE MAIN make up the gut microbiome—the ecosystem molecules from leaking into the body and help CHEMICAL MESSAGES CONNECTS THE BRAIN LOCATION FOR LET THE BRAIN AND of around 100 trillion microorganisms mainly activate new immune cells (see pages 136–137). TO THE GUT’S OWN EXPOSURE TO MANY BODY COMMUNICATE found in the colon. There are around 1,000 AND PROMPT OR NERVE CELLS AND FOREIGN PATHOGENS SENDS SIGNALS LIKE TOXINS AND species of gut bacteria; the specific combination HEALTHY GUT, HEALTHY MIND? INHIBIT FEELINGS BOTH WAYS VIRUSES is unique to each of us and changes over our life We know the brain affects the gut because we span. Research suggests maintaining a balance often feel stress or excitement in our stomach, Serotonin is a neurotransmitter As a main communication Gut bacteria stimulate the between more and less helpful bacteria is key but diet could also have an impact on our brain. mostly made in the gut to highway between the brain and production of specific cells that to a healthy gut. Scientists have only recently started exploring promote feelings of fullness and numerous organs, its functions fight infection; it is thought that control appetite; it may also include regulating heart rate, these could travel via the blood or In addition to digesting food, gut bacteria the mechanisms by which gut bacteria may reach the brain via the blood and digestion, fighting inflammation, lymph and interact with the perform many other important tasks. For influence the brain—and therefore how gut influence levels of happiness. and relaxation. central nervous system. example, they help absorb minerals from food, health could affect mood. Several studies have synthesize vitamins like vitamin K (which helps found the microbes of people with depression with blood clotting), and digest dietary fiber, differ from others; in 2019, scientists identified releasing molecules, including butyrate, which two specific types of gut bacteria (Coprococcus contributes to a stronger gut barrier, and and Dialister) that were consistently lacking in propionate, which helps the liver regulate the microbiomes of the depressed subjects. blood sugar levels and appetite. Nerve cells in the gut WIDER HEALTH lining control digestion/ Fermenting discomfort elimination People with the inflammatory bowel conditions Crohn’s disease and ulcerative colitis have FERMENTATION IS WHEN THE GUT BACTERIA The gut–brain axis been found to have fewer species and a lower BREAK DOWN DIETARY FIBER. Gut mucous membranes This creates hydrogen and methane gas, which are an important site of proportion of beneficial gut bacteria. Studies is normal and in fact shows that the gut is Scientists now know there immune activity have also observed lower diversity in people working effectively. However, those with a is a constant two-way with obesity, diabetes, and certain types of functional gut disorder like irritable bowel communication between the eczema and arthritis. Dysbiosis—an imbalance syndrome are more susceptible to the effects, gut and brain that happens due to increased sensitivity of their via different pathways— in the gut microbiome—has also been shown intestines—so they can feel bloating or including the vagus nerve, The vagus nerve’s other to contribute to metabolic syndrome, allergies, abdominal pain. end point is in the colon cells of the immune system, colorectal cancer, and Alzheimer’s disease. A and chemicals released into healthy gut supports the immune system— the bloodstream. UUSS__004488--004499__ggoooodd__gguutt__hheeaalltthh..iinndddd 4488 2244//0088//22002211 1166::2222 UUSS__004488--004499__ggoooodd__gguutt__hheeaalltthh..iinndddd 4499 2244//0088//22002211 1166::2222
50 HOW CAN WE EAT WELL? 51 VEGETABLES RED ORANGE BEANS & FRUITS YELLOW LENTILS INCLUDE EAT A RAINBOW 30+ DIFFERENT COLORS WHITE OF PLANTS PROVIDE PLANTS DIFFERENT NATURAL A WEEK CHEMICALS HERBS NUTS & & SPICES SEEDS GREEN BLUE/ WHOLE PURPLE BLACK GRAINS YOUR GUT MICROBIOME Feeding your microbiome HIGHLY PROCESSED ALCOHOL Research shows that FOODS eating 30-plus different plant foods each week supports AVOID/ ARTIFICIAL the gut’s bacterial RESTRICT SWEETENERS* diversity and barrier strength, by providing varying combinations ADDED of vitamins, minerals, SALT and phyto (plant) SUGARS chemicals. * Animal studies suggest that these may disrupt gut bacteria UUSS__005500--005511__gguutt__ddiivveerrssiittyy..iinndddd 5500 2244//0088//22002211 1166::2222
5500 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 5511 HOW CAN I INCREASE GUT VEGETABLES RED MICROBIOME DIVERSITY? ORANGE The key to a healthy, functioning gut is the diversity of the bacteria living in it. BEANS & Although our gut microbiome is established in infancy, as adults, we may still be FRUITS YELLOW LENTILS able to boost our good gut bacteria through the foods we choose. INCLUDE EAT A RAINBOW 30+ DIFFERENT COLORS WHITE OF PLANTS PROVIDE Our gut microbiome is shaped from birth to around In contrast, several studies have found that a typical PLANTS DIFFERENT NATURAL three years old. Most microbes come from the Western diet, which is high in animal protein and A WEEK CHEMICALS mother’s birth canal during vaginal birth, or from fat and low in fiber, led to a marked decrease in total HERBS NUTS & the hospital environment in a Cesarean section, and numbers of bacteria and beneficial Bifidobacteria. & SPICES SEEDS GREEN also vary with breast or bottle feeding. Infants then Some have also noted that while a change in diet acquire bacteria from their environment, nearby can rapidly alter microbiome balance, it takes a BLUE/ people, and diet. Once established, our lifestyle, long-term change in eating habits to create WHOLE PURPLE stress levels, and diet can still cause shifts in our significant shifts in bacterial diversity. BLACK GRAINS microbiome that may support or undermine health. For example, some strains from the Bifidobacteria MEDICATIONS and Lactobacillus families can prevent potentially Overuse of medications, particularly antibiotics, harmful gut bacteria from getting out of control. can decrease the amount of good bacteria in the gut. In one study, more than a quarter of 900 YOUR GUT DIVERSITY MATTERS antibiotic medications tested were found to be MICROBIOME Bacterial diversity is associated with how many potentially damaging to the growth of gut unique plant species we eat, according to an microbes. Probiotics can help increase good bacteria international gut microbiome project analyzing data in the gut; the most evidence we have for their from thousands of volunteers. Researchers identified effectiveness is taking them after a course of that eating 30-plus types of plants a week is linked antibiotics to prevent antibiotic-associated diarrhea. to the production of various short-chain fatty acids; Feeding your these help protect both gut health and immunity. microbiome HIGHLY Eating plenty of indigestible fiber has also been PROCESSED ALCOHOL shown to boost the richness of the gut microbiome, Fecal transplant Research shows that eating 30-plus FOODS while a low level in the diet reduces overall bacterial THIS PROCEDURE IS A FORM OF BACTERIOTHERAPY different plant foods diversity. As well as many vegetables, whole grains THAT USES A STOOL FROM A HEALTHY DONOR. each week supports AVOID/ ARTIFICIAL like oats, brown rice, beans and lentils, and nuts and It is placed in the colon of someone who is unwell in order to rebalance their gut bacteria. Although this is the gut’s bacterial RESTRICT SWEETENERS* seeds are good fiber sources—US health guidelines still a developing area, a 2016 review found its diversity and barrier advise healthy adults eat 28g of fiber on a success rate varied from a third to three-quarters of strength, by providing varying combinations 2,000-calorie diet. Many have a “prebiotic” effect cases in trials involving irritable bowel syndrome ADDED patients. While this is a developing area and research of vitamins, minerals, by feeding beneficial bacteria. Naturally fermented SALT is ongoing, several studies indicate it may be an and phyto (plant) SUGARS “probiotic” foods like kefir can also be helpful; for effective treatment for recurring C. difficile infections. chemicals. example, most species of Lactobacillus are found in * Animal studies suggest that these may disrupt gut bacteria fermented foods (see pages 52–53). UUSS__005500--005511__gguutt__ddiivveerrssiittyy..iinndddd 5500 2244//0088//22002211 1166::2222 UUSS__005500--005511__gguutt__ddiivveerrssiittyy..iinndddd 5511 2244//0088//22002211 1166::2222
52 WHAT IS THE DIFFERENCE BETWEEN PRE- AND PROBIOTICS? The concept of consuming a daily prebiotic or probiotic to keep your gut balanced and healthy is hugely popular—sales of probiotic supplements alone have been projected to hit $65 billion by 2024. But is it really that straightforward? Probiotics are live strains of bacteria consumed to WHAT ARE THE BENEFITS? directly increase the population of “good” bacteria Research suggests probiotics are mainly of benefit in the gut. Prebiotic foods feed existing gut bacteria when your gut microbiome is out of balance, for so they can thrive and work effectively. instance, to relieve diarrhea caused by infection Probiotics are found in several fermented foods or after a course of antibiotics. Although studies and drinks—specially formulated probiotic drinks, are limited, certain types of bacterial strains have yogurts, and supplements contain specific “friendly” been found to reduce symptoms of irritable bowel bacterial strains, often from the Lactobacillus and syndrome, particularly bloating. Otherwise, a study Bifidobacteria species. found that eating fermented foods 1–5 times weekly One concern around probiotic foods is how many is associated with a subtle change in gut bacteria. microbes can survive the acidic conditions in the In general, though, healthy people should not stomach in order to reach your colon intact and require probiotic supplements. colonize there. Live strains will also be destroyed by A key benefit of prebiotic foods is that different heat-based processes like canning or pasteurizing. types of the nondigestible fibers within the food are There are no clear guidelines on how much you broken down by gut bacteria, which then produces need to eat to gain a benefit from probiotics. In gut protective short-chain fatty acids. Some studies the US, the probiotic supplement market is largely show that eating a type of dietary fiber called unregulated. Bacteria content of supplements inulin can help maintain the gut’s mucus barrier is specified on the label as “CFU” (colony and prevent inflammation. It’s best to increase forming units). prebiotic food intake gradually, to avoid bloating. Prebiotics FRUITS VEGETABLES OTHERS APPLES LEEKS WHEAT BRAN MANY NATURALLY PREBIOTIC FOODS AND DATES GARLIC CASHEWS DRINKS WILL ALSO PRUNES LEGUMES/PULSES PISTACHIOS PROVIDE VARIOUS DRIED MANGO (BEANS & LENTILS) CHAI TEA VITAMINS, MINERALS, PEARS JERUSALEM ARTICHOKE FENNEL TEA AND PHYTOCHEMICALS. GRAPEFRUIT CHICORY ROOT FIBER APRICOT ASPARAGUS UUSS__005522--005533__pprree--pprroobbiioottiiccss..iinndddd 5522 2255//0088//22002211 1199::1144
5522 WHAT IS THE DIFFERENCE Probiotics ONIONS CONTAIN PROBIOTICS ARISE FROM BETWEEN PRE- AND PROBIOTICS? DIFFERENT TYPES OF LACTOFERMENTATION, WHERE NONFERMENTABLE CULTURES FEED ON STARCH FIBERS THAT CONVERT OR SUGARS IN THE FOOD AND The concept of consuming a daily prebiotic or probiotic to keep your gut balanced and TO BENEFICIAL SHORT- CREATE LACTIC ACID. healthy is hugely popular—sales of probiotic supplements alone have been projected CHAIN FATTY ACIDS IN THE COLON to hit $65 billion by 2024. But is it really that straightforward? Probiotics are live strains of bacteria consumed to WHAT ARE THE BENEFITS? directly increase the population of “good” bacteria Research suggests probiotics are mainly of benefit BACTERIA in the gut. Prebiotic foods feed existing gut bacteria when your gut microbiome is out of balance, for so they can thrive and work effectively. instance, to relieve diarrhea caused by infection YOGURT Probiotics are found in several fermented foods or after a course of antibiotics. Although studies YOGURT IS MADE FROM MILK and drinks—specially formulated probiotic drinks, are limited, certain types of bacterial strains have FERMENTED BY BACTERIA; FOR yogurts, and supplements contain specific “friendly” been found to reduce symptoms of irritable bowel PROBIOTIC EFFECT, CHOOSE ONE bacterial strains, often from the Lactobacillus and syndrome, particularly bloating. Otherwise, a study WITH LIVE OR ACTIVE CULTURES AND NO ADDED SUGAR. Bifidobacteria species. found that eating fermented foods 1–5 times weekly One concern around probiotic foods is how many is associated with a subtle change in gut bacteria. KEFIR microbes can survive the acidic conditions in the In general, though, healthy people should not THIS DRINK IS A POTENT stomach in order to reach your colon intact and require probiotic supplements. PROBIOTIC THAT CAN BE MADE AT colonize there. Live strains will also be destroyed by A key benefit of prebiotic foods is that different HOME FROM MILK OR WATER AND heat-based processes like canning or pasteurizing. types of the nondigestible fibers within the food are REUSABLE "GRAINS" COMBINING BACTERIA, YEAST, AND ENZYMES. There are no clear guidelines on how much you broken down by gut bacteria, which then produces need to eat to gain a benefit from probiotics. In gut protective short-chain fatty acids. Some studies SAUERKRAUT the US, the probiotic supplement market is largely show that eating a type of dietary fiber called unregulated. Bacteria content of supplements inulin can help maintain the gut’s mucus barrier MADE FROM CABBAGE THAT HAS BEEN FERMENTED USING is specified on the label as “CFU” (colony and prevent inflammation. It’s best to increase NATURALLY PRESENT LACTIC ACID forming units). prebiotic food intake gradually, to avoid bloating. BACTERIA, SAUERKRAUT IS ALSO A GOOD SOURCE OF FIBER. KIMCHI KIMCHI IS A WIDELY USED CONDIMENT IN KOREA THAT Prebiotics FRUITS VEGETABLES OTHERS CONSISTS OF CABBAGE OR OTHER ALLIUMS APPLES LEEKS WHEAT BRAN VEGETABLES FERMENTED BY MANY NATURALLY PREBIOTIC FOODS AND DATES GARLIC CASHEWS AS WELL AS PREBIOTIC BACTERIA, OFTEN FLAVORED DRINKS WILL ALSO PRUNES LEGUMES/PULSES PISTACHIOS Raw benefits: To obtain PROPERTIES, ONIONS, WITH CHILE AND GARLIC VIP TAR MOV INID SE , MVA INR EIO RU AS LS , DRIED MANGO (BEANS & LENTILS) CHAI TEA m ono ir oe n o af n a d p gr ae rb lii co t ci ac n b ben e e fit, LEE AK RS I, C A HN SD O G UA RR CL EIC O A FR E KKOOMMBBUUCCHHAA AND PHYTOCHEMICALS. PEARS JERUSALEM ARTICHOKE FENNEL TEA eaten uncooked, if possible. ANTIOXIDANTS MADE FROM SWEET TEA GRAPEFRUIT CHICORY ROOT FERMENTED WITH A CULTURE OF FIBER APRICOT ASPARAGUS BACTERIA AND YEAST, RESEARCH ON ITS BENEFITS ISN’T CLEAR. UUSS__005522--005533__pprree--pprroobbiioottiiccss..iinndddd 5522 2255//0088//22002211 1199::1144 UUSS__005522--005533__pprree--pprroobbiioottiiccss..iinndddd 5533 2266//0088//22002211 0077::4477
54 HOW CAN WE EAT WELL? 55 WHAT ARE THE ELEMENTS OF A POOR DIET? A poor diet is one that isn’t providing enough crucial nutrients to keep you in optimal health, or your body’s energy needs and your appetite in balance. Over the long term, it could also have life-limiting effects. A key way of defining a poor diet is one that is high with cardiovascular disease (including heart disease, in salt, sugar, and saturated fat. US health guidelines heart failure, and stroke) the leading global cause recommend a daily intake of <2,300mg of sodium, of diet-related deaths. <10% total calories of added sugar, and <10% total Poor diet is mainly associated with weight gain. calories of saturated fat (pp. 64, 66, and 70). In the US, three-quarters of adults are classed as Preprepared food is responsible for much of the overweight or obese, and more than half have at excess; three-quarters of the salt in our diet comes least one preventable chronic disease related to diet. from processed foods, including staples like bread, While excess body fat is often not a choice, people while three large slices of takeout vegetable pizza with obesity are three times more likely to develop could contain around 12g of salt. But fast food is colon cancer and five times more likely to develop only one factor—adding a tablespoon of soy sauce type 2 diabetes. The World Health Organization to a healthy vegetable stir-fry could increase its salt (WHO) wants to reduce the adult global content by up to 3g. population’s intake of salt by 30 percent to halve Poor diet is not only about eating too much of cases of diabetes and stroke by 2025, due to the the “wrong” foods; it can also mean not consuming association between excess salt and raised blood enough nutrient-dense foods. In 2017, a global study pressure (see pages 70–71). of diet and mortality found that eating low amounts of healthy foods—including whole grains, fruits and SHOULD I EAT FEWER CALORIES? vegetables, and nuts and seeds—was associated with In terms of overall calories, recommended daily more deaths than a diet with too much salt, sugar- adult intakes in the US are 2,000 for women and sweetened drinks, red and processed meats, and 2,500 for men. Chronic overconsumption may lead trans fats. A low intake of whole grains was the to unhealthy body fat levels and increase the risk single dietary factor associated with most deaths of developing type 2 diabetes, heart disease, and in the US and western Europe. Of course, multiple some cancers. However, occasionally eating more lifestyle factors can contribute to poor health, but calories than you need is unlikely to impact your diet is an area where people can take some control. health. Instead of focusing simply on the number of calories, consider whether you have enough HEALTH IMPLICATIONS variety in your daily diet and its quality as a whole. The same study estimated that one in five deaths worldwide were associated with eating a poor diet, UUSS__005544--005555__ppoooorr__ddiieett..iinndddd 5544 2244//0088//22002211 1166::2222
5544 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 5555 WHAT ARE THE ELEMENTS Healthier meal hacks OF A POOR DIET? Don’t automatically rule out meals you enjoy, but be aware they could be high in salt, sugar, and fat and low in fiber A poor diet is one that isn’t providing enough crucial nutrients to keep you in or protein. Some simple changes can optimal health, or your body’s energy needs and your appetite in balance. improve a meal’s potential nutritional Over the long term, it could also have life-limiting effects. value and keep you feeling satisfied. A key way of defining a poor diet is one that is high with cardiovascular disease (including heart disease, in salt, sugar, and saturated fat. US health guidelines heart failure, and stroke) the leading global cause recommend a daily intake of <2,300mg of sodium, of diet-related deaths. <10% total calories of added sugar, and <10% total Poor diet is mainly associated with weight gain. calories of saturated fat (pp. 64, 66, and 70). In the US, three-quarters of adults are classed as Preprepared food is responsible for much of the overweight or obese, and more than half have at excess; three-quarters of the salt in our diet comes least one preventable chronic disease related to diet. from processed foods, including staples like bread, While excess body fat is often not a choice, people while three large slices of takeout vegetable pizza with obesity are three times more likely to develop could contain around 12g of salt. But fast food is colon cancer and five times more likely to develop only one factor—adding a tablespoon of soy sauce type 2 diabetes. The World Health Organization to a healthy vegetable stir-fry could increase its salt (WHO) wants to reduce the adult global content by up to 3g. population’s intake of salt by 30 percent to halve Poor diet is not only about eating too much of cases of diabetes and stroke by 2025, due to the BUN (fiber) SAUCES (sugar/serving) the “wrong” foods; it can also mean not consuming association between excess salt and raised blood ✔ WHOLE WHEAT BUN, 3.4g ✔ HOMEMADE SALSA, 0.1g enough nutrient-dense foods. In 2017, a global study pressure (see pages 70–71). ✘ WHITE BUN, 1.1g ✘ KETCHUP, 3.4g of diet and mortality found that eating low amounts of healthy foods—including whole grains, fruits and SHOULD I EAT FEWER CALORIES? BURGER (saturated fat) EXTRAS (micronutrients) vegetables, and nuts and seeds—was associated with In terms of overall calories, recommended daily ✔ VEGETARIAN BEAN BURGER, 1.2g ✔ SPINACH, ONION & more deaths than a diet with too much salt, sugar- adult intakes in the US are 2,000 for women and ✘ BEEF BURGER, 11.6g TOMATO > PLAIN LETTUCE sweetened drinks, red and processed meats, and 2,500 for men. Chronic overconsumption may lead trans fats. A low intake of whole grains was the to unhealthy body fat levels and increase the risk FRIES (fat/serving) DRINK (sugar/3.5oz) single dietary factor associated with most deaths of developing type 2 diabetes, heart disease, and ✔ BAKED SWEET POTATOES, 3.7g ✔ SPARKLING WATER WITH SPLASH OF in the US and western Europe. Of course, multiple some cancers. However, occasionally eating more ✘ FRENCH FRIES, 6.6g JUICE, 4.7g lifestyle factors can contribute to poor health, but calories than you need is unlikely to impact your ✘ SUGAR-SWEETENED BEVERAGE, 10.6g diet is an area where people can take some control. health. Instead of focusing simply on the number of calories, consider whether you have enough HEALTH IMPLICATIONS variety in your daily diet and its quality as a whole. The same study estimated that one in five deaths worldwide were associated with eating a poor diet, EXPANDING THE PORTION SIZE ON THE PACK OF 85g OVER A 130g DECADE TO AS PORTIONS AN AVERAGE MUFFIN ROSE FROM MUCH AS UUSS__005544--005555__ppoooorr__ddiieett..iinndddd 5544 2244//0088//22002211 1166::2222 UUSS__005544--005555__ppoooorr__ddiieett..iinndddd 5555 2244//0088//22002211 1166::2222
56 HOW CAN WE EAT WELL? 57 WHY DO SO MANY OF US HAVE POOR DIETS? The reasons people don’t eat a healthy, balanced diet are complex and often interlinked. They can range from access to a wide choice of different foods to psychological influences they may not even be aware of. Lack of education about food, and an unhealthy relationship with it, are key factors contributing to Learn to say “No” poor diets in developed countries. Giving sweets HAVE YOU EVER ACCEPTED AN EXTRA HELPING to an upset child arguably creates an emotional EVEN THOUGH YOU WERE FULL? association; as an adult, that person is more likely We seem to be susceptible to eating more with to reach for sugary carbohydrates to self-soothe others than alone, although the underlying reasons aren’t clear. Some "social facilitation" studies show against stress or anxiety. Animal and human studies eating with friends and family in particular can show that certain foods, especially those high in increase intake; this could be because chatting fat, carbohydrates, and salt, stimulate the brain’s with others distracts attention from food, or eating reward centers. The Yale Food Addiction Scale was a greater amount is more acceptable in larger groups. A research review suggests meals eaten developed to identify markers relating to certain socially could be a third to a half bigger. foods, although this is a controversial area with conflicting research. But evidence clearly shows that the more highly palatable the food, the more likely it is to be consumed for pleasure—called “hedonic” low-income individuals and those using public eating (see pages 58–59). Manufacturers design the programs like SNAP spend and intake more sweets specific flavors and textures of foods like chips and and salty snacks. Millions also live in “food deserts,” ice cream to ensure people will want to eat more. or areas with limited access to supermarkets or grocery stores that can make it harder to eat a ACCESS AND AFFORDABILITY healthy diet. Easier access to fast food, including One report found that the lowest income populations takeout delivery, can be more tempting than cooking spent 36 percent more of their income on food when at home (see pages 46–47). Obesity prevalence is compared to the highest income populations who much higher in low-income children and adolescents spent just 8 percent on food. US consumers spend than in higher income groups with nearly 19 percent an average of 8.6 percent of their income on food, and 11 percent, respectively. With higher rates of but as incomes rise, money spent on food represents obesity comes diet-related disease that may impact a smaller portion of the budget. Studies show that their quality of life. UUSS__005566--005577__ppoooorr__ddiieett..iinndddd 5566 2244//0088//22002211 1166::2233
5566 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 5577 WHY DO SO MANY OF US HAVE POOR DIETS? The reasons people don’t eat a healthy, balanced diet are complex and often interlinked. They can range from access to a wide choice of different foods to psychological influences they may not even be aware of. Lack of education about food, and an unhealthy relationship with it, are key factors contributing to Learn to say “No” poor diets in developed countries. Giving sweets HAVE YOU EVER ACCEPTED AN EXTRA HELPING to an upset child arguably creates an emotional EVEN THOUGH YOU WERE FULL? association; as an adult, that person is more likely We seem to be susceptible to eating more with to reach for sugary carbohydrates to self-soothe others than alone, although the underlying reasons aren’t clear. Some "social facilitation" studies show against stress or anxiety. Animal and human studies eating with friends and family in particular can show that certain foods, especially those high in increase intake; this could be because chatting fat, carbohydrates, and salt, stimulate the brain’s with others distracts attention from food, or eating reward centers. The Yale Food Addiction Scale was a greater amount is more acceptable in larger groups. A research review suggests meals eaten developed to identify markers relating to certain socially could be a third to a half bigger. foods, although this is a controversial area with conflicting research. But evidence clearly shows that the more highly palatable the food, the more likely it is to be consumed for pleasure—called “hedonic” low-income individuals and those using public eating (see pages 58–59). Manufacturers design the programs like SNAP spend and intake more sweets specific flavors and textures of foods like chips and and salty snacks. Millions also live in “food deserts,” ice cream to ensure people will want to eat more. or areas with limited access to supermarkets or grocery stores that can make it harder to eat a ACCESS AND AFFORDABILITY healthy diet. Easier access to fast food, including One report found that the lowest income populations takeout delivery, can be more tempting than cooking spent 36 percent more of their income on food when at home (see pages 46–47). Obesity prevalence is compared to the highest income populations who much higher in low-income children and adolescents spent just 8 percent on food. US consumers spend than in higher income groups with nearly 19 percent an average of 8.6 percent of their income on food, and 11 percent, respectively. With higher rates of but as incomes rise, money spent on food represents obesity comes diet-related disease that may impact a smaller portion of the budget. Studies show that their quality of life. UUSS__005566--005577__ppoooorr__ddiieett..iinndddd 5566 2244//0088//22002211 1166::2233 PLEASURE AFFORDABILITY HUNGER VS N O M I C PR A O C EC TI C A L SUPER I FN OC OO DM PE ORI F FCI EN RG S T SA CK KOE ILOO LKU SINT/ GA D C EC LE IM VS EA S RR YK ET Q U PRA IL CIT EY VS NTI SM T RE AIN T S C O S O S FOOD DECISIONS C DE FAMILY PREFERENCES EACH TIME WE CHOOSE FRIEND INFLUENCES I A L U IT TRYING TO BE HEALTHY TO EAT A PARTICULAR FOOD OR WORK/SCHOOL OPTIONS P R T DRINK, IT REFLECTS A WEB OF E T S A ADS SOCIAL, EDUCATIONAL, AND FAMILY S U OTHER INFLUENCES R M WO ET II GV A HT TI O C N O, NE. T G R., O L E OAT TI HN EG R W SIT H E SELF-IS MT AR GEM S EO SOD P R CT E UA F LS E TT R AUE E NR K N DA N H LC O EHE V AWAS LA B L TL I E HU T D YE S GS E E A TO IF N G S E C P N S Y C RIE H OLOGY PAS T E X P E UUSS__005566--005577__ppoooorr__ddiieett..iinndddd 5577 2244//0088//22002211 1166::2233
58 HOW CAN WE EAT WELL? 59 SHOULD I AVOID PROCESSED FOOD? There are good reasons why processed food is a large part of the modern diet— it’s often cheaper than fresh, allows for a huge amount of choice, and is convenient. But not all processed food is equal, and it’s important to understand the differences. Processed food isn’t automatically inferior or HIGHLY PROCESSED FOODS unhealthy compared to fresh. Some nutrient-rich These should be limited in a healthy, balanced diet. whole foods like vegetables, fish, milk, or whole They typically combine already modified ingredients grains are processed simply for storage or and additives and are either ready to eat or need preservation; for example, freezing happens minimal preparation; examples include many sweets, straight after harvest, so freshly picked foods chips, baked goods, and ready meals. They lack much arrive in stores at near peak nutritional value. of the original whole food’s fiber, making them easily Likewise, canning is processing but retains digestible, while added salt, sugar, and fats also make much of the protein of fish and the fiber of fruits them highly palatable. The mechanisms behind their and vegetables. However, skinless fruit in syrup has suggested addictive nature include sugar’s ability to less fiber and more sugar than whole fruit; choose stimulate the brain’s reward centers. fruit in water, or discard the syrup, or beans in In the US, nearly 60 percent of average daily water rather than sauce or brine. energy intake is now said to come from highly Certain processed foods have been altered for processed foods; evidence suggests rising sales and flavor or texture or have additives to extend shelf obesity rates are associated (although obesity has life; even so, foods like whole wheat bread, rolled many causes). For example, high-fructose corn oats, sauerkraut, and tomato paste still fit easily syrup is a type of sugar widely added to highly into healthy eating. Some are fortified to replace processed foods in the US; since the body has no vitamins or minerals lost during processing or to biochemical reactions that use it, excess is turned boost nutrients for plant-based eaters. into fat in the liver. Degrees of processing Any food you buy in a supermarket will have undergone some form of processing, but the impact on its MINIMALLY nutritional value can vary widely. UNPROCESSED PROCESSED Corn, for example, can be eaten in Fresh sweet corn Canned sweet corn a range of different forms, including This is still in its natural Canned vegetables usually as a refined ingredient in highly state, although it may have retain fiber; salt may be processed foods. been cleaned prior to sale. added for preservation. UUSS__005588--005599__pprroocceesssseedd__ffoooodd..iinndddd 5588 2244//0088//22002211 1166::2233
5588 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 5599 SHOULD I AVOID PROCESSED FOOD? THE SALT CONTENT IN FLAVORED CORN CHIPS CAN BE AS There are good reasons why processed food is a large part of the modern diet— MUCH AS FOUR TIMES it’s often cheaper than fresh, allows for a huge amount of choice, and is convenient. HIGHER THAN PLAIN But not all processed food is equal, and it’s important to understand the differences. VARIETIES Processed food isn’t automatically inferior or HIGHLY PROCESSED FOODS unhealthy compared to fresh. Some nutrient-rich These should be limited in a healthy, balanced diet. whole foods like vegetables, fish, milk, or whole They typically combine already modified ingredients grains are processed simply for storage or and additives and are either ready to eat or need preservation; for example, freezing happens minimal preparation; examples include many sweets, straight after harvest, so freshly picked foods chips, baked goods, and ready meals. They lack much arrive in stores at near peak nutritional value. of the original whole food’s fiber, making them easily Likewise, canning is processing but retains digestible, while added salt, sugar, and fats also make much of the protein of fish and the fiber of fruits them highly palatable. The mechanisms behind their and vegetables. However, skinless fruit in syrup has suggested addictive nature include sugar’s ability to A 3.5oz SERVING OF PLAIN less fiber and more sugar than whole fruit; choose stimulate the brain’s reward centers. CORN CHIPS CAN fruit in water, or discard the syrup, or beans in In the US, nearly 60 percent of average daily CONTAIN NEARLY 30g water rather than sauce or brine. energy intake is now said to come from highly OF FAT—MORE THAN A Certain processed foods have been altered for processed foods; evidence suggests rising sales and THIRD OF THE MAXIMUM DAILY AMOUNT flavor or texture or have additives to extend shelf obesity rates are associated (although obesity has life; even so, foods like whole wheat bread, rolled many causes). For example, high-fructose corn Corn chips: Some are more oats, sauerkraut, and tomato paste still fit easily syrup is a type of sugar widely added to highly highly processed than others, into healthy eating. Some are fortified to replace processed foods in the US; since the body has no having been cooked in refined oils, with artificial flavors and vitamins or minerals lost during processing or to biochemical reactions that use it, excess is turned preservatives added. boost nutrients for plant-based eaters. into fat in the liver. Are additives bad? ADDITIVES ARE OFTEN USED IN PROCESSED FOODS TO IMITATE NATURAL FLAVORS, MAKE FOOD FEEL BETTER IN THE MOUTH, OR AID WITH PROCESSING. They can be synthetic or naturally derived, like Degrees of processing ascorbic acid, which is vitamin C. Monosodium glutamate (MSG) is a widely used flavor enhancer. While there is a lack of evidence on its long-term Any food you buy in a supermarket effects on health, some studies have suggested a will have undergone some form of link with obesity, central nervous system disorders, processing, but the impact on its MINIMALLY HIGHLY and liver damage. Additives are rigorously tested and nutritional value can vary widely. UNPROCESSED PROCESSED PROCESSED PROCESSED deemed safe for use; if you have concerns, check Corn, for example, can be eaten in labels and reduce your intake. Fresh sweet corn Canned sweet corn Popcorn High-fructose corn syrup a range of different forms, including This is still in its natural Canned vegetables usually It has been altered but may be This is made from cornstarch as a refined ingredient in highly state, although it may have retain fiber; salt may be freshly made simply by heat, or may and is often used as a processed foods. been cleaned prior to sale. added for preservation. contain additives, fat, salt, or sugar. sweetener in the US. UUSS__005588--005599__pprroocceesssseedd__ffoooodd..iinndddd 5588 2244//0088//22002211 1166::2233 UUSS__005588--005599__pprroocceesssseedd__ffoooodd..iinndddd 5599 2244//0088//22002211 1166::2233
60 HOW CAN WE EAT WELL? 61 SHOULD I PAY ATTENTION TO FOOD LABELS? Nutritional information on food packaging can appear confusing at first. But it’s not too difficult to decode the data, and understanding it will let you make more informed choices about your daily diet. Checking the nutrition panel on food packaging information per entire package. Manufacturers can can, for example, help you spot that a low-fat choose whether to show information about vitamins, yogurt has had extra sugar added, making a minerals, dietary fiber, and certain nutrients. higher-fat option with less sugar a smarter choice. Rules around nutritional labeling vary in different HEALTH CLAIMS countries; in the US, most prepackaged foods must Health, nutrient content, and structure/function show the following per serving: claims are useful for making a quick decision about a product and comparing similar items. They aren’t Energy in kilocalories or calories mandatory, and some show only the amount of Total and saturated fats energy (calories), but they visually display whether Protein the food is low, medium, or high in salt, sugar, and Total carbohydrates from sugars that are saturated fat. The numbers tell you how much of added and naturally present each is in a portion, both in grams and as a share of Salt, which may also be listed as “sodium” the total daily recommended amount. Food suppliers can decide on the portion size shown on the label, The nutrition label must show the number of which could be less than what’s inside the package— servings per package and may also show nutritional so if you eat extra, you’ll consume more than it says. Facts Up Front PER 1 CUP SERVING THIS US VOLUNTARY LABEL IDEA WAS DEVELOPED BY FOOD INDUSTRY BODIES. 140 1g 410mg 5g 1,000mg VITAMIN Labels show the amount of calories and nutrients A CALORIES SAT FAT SODIUM SUGARS POTASSIUM to limit in a standardized serving amount, like a 5% DV 17% DV 29% DV 20% DV cup. The label can highlight up to two ‘‘positive’’ components, like a specific vitamin. Percentage of daily allowance UUSS__006600--006611__ffoooodd__llaabbeellss..iinndddd 6600 2244//0088//22002211 1166::2233
6600 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 6611 SHOULD I PAY ATTENTION TO FOOD LABELS? Nutritional information on food packaging can appear confusing at first. But it’s not too difficult to decode the data, and understanding it will let you make more informed choices about your daily diet. 1. KNOW THE DAILY MAXIMUM These are the maximum daily intake Checking the nutrition panel on food packaging information per entire package. Manufacturers can amounts in US health guidelines. can, for example, help you spot that a low-fat choose whether to show information about vitamins, (They are based on an average woman, in terms of body size and yogurt has had extra sugar added, making a minerals, dietary fiber, and certain nutrients. activity levels, so they will vary, for instance, for a highly active man.) higher-fat option with less sugar a smarter choice. Rules around nutritional labeling vary in different HEALTH CLAIMS countries; in the US, most prepackaged foods must Health, nutrient content, and structure/function show the following per serving: claims are useful for making a quick decision about 2. a product and comparing similar items. They aren’t CALCULATE BY WEIGHT Energy in kilocalories or calories mandatory, and some show only the amount of The nutrition panel on packaging 30g Total and saturated fats energy (calories), but they visually display whether lists sugar, fat, and salt per serving; figure out the total amounts in Protein the food is low, medium, or high in salt, sugar, and the food and check whether they Total carbohydrates from sugars that are saturated fat. The numbers tell you how much of fit into your daily amounts. added and naturally present each is in a portion, both in grams and as a share of 20g Salt, which may also be listed as “sodium” the total daily recommended amount. Food suppliers can decide on the portion size shown on the label, The nutrition label must show the number of which could be less than what’s inside the package— servings per package and may also show nutritional so if you eat extra, you’ll consume more than it says. 10g 5g Facts Up Front 3. PER 1 CUP SERVING THIS US VOLUNTARY LABEL IDEA WAS CHECK BY SERVING DEVELOPED BY FOOD INDUSTRY BODIES. 140 1g 410mg 5g 1,000mg VITAMIN Labels help you assess the sugar, Labels show the amount of calories and nutrients CALORIES SAT FAT SODIUM SUGARS POTASSIUM A salt, and fat in one serving/ to limit in a standardized serving amount, like a portion and track how your intake cup. The label can highlight up to two ‘‘positive’’ 5% DV 17% DV 29% DV 20% DV is adding up. components, like a specific vitamin. Percentage of daily allowance UUSS__006600--006611__ffoooodd__llaabbeellss..iinndddd 6600 2244//0088//22002211 1166::2233 SRAGUS DEDDA TAF LATOT TAF DETARUTAS TLAS Monitor your sugar, fat, and salt Sugar, saturated fat, and salt intake should be controlled as part of healthy eating. Use these steps to become more aware of how much you eat in each meal or snack and over a whole day. CALORIES ADDED TOTAL FAT SATURATED SALT SUGAR FAT < 2,000 2,000 < 50g < 77g < 22g mg KEY High Medium Low MED LOW MED HIGH MED CALORIES SUGAR FAT SAT FAT SALT 220 0.8g 13g 5.9g 0.7g 11% <1% 19% 30% 12% Amber: More of these Green: The more of Red: You may want to Check %daily values: These indicate the food can be these there are on the eat a small amount, show how much a serving eaten regularly label, the healthier have it occasionally, or contributes to the maximum a choice it is choose an alternative daily amount UUSS__006600--006611__ffoooodd__llaabbeellss..iinndddd 6611 2244//0088//22002211 1166::2233
62 HOW CAN WE EAT WELL? 63 IT CONTAINS MAY BE FRUIT FRESH FRUIT CONCENTRATE JUICE AND IS WITH ADDED LOW IN “NATURALLY FRUIT SUGAR CALORIES FLAVORED” ALL FRUITS AND THERE IS NO REAL NUTS/SEEDS ARE “CONTAINS DEFINITION FOR HIGHLY SUPERFOODS” THIS TERM NUTRITIOUS THIS DOES NOT AT LEAST 25% “REDUCED NECESSARILY FEWER CALORIES CALORIE” MEANS IT’S A THAN OTHER LOW-CALORIE PRODUCT FOOD COULD BENEFIT A THIN LAYER GUT HEALTH AND OF YOGURT “YOGURT COATED” STRENGTHEN PROVIDES LITTLE BONES AND PROTEIN, WITH TEETH ADDED SUGAR “ALL NATURAL” MADE WITH IT MAY ENTIRELY NATURAL CONTAIN INGREDIENTS ADDITIVES AND AND NO PROCESSED ADDITIVES INGREDIENTS WHAT WE MAY ASSUME THIS MEANS WHAT THIS MAY ACTUALLY MEAN Messages on foods often emphasize them as a healthy choice; while some ingredients may be, the overall nutritional value may not match our perceptions. UUSS__006622--006633__bbuuzzzz__wwoorrddss..iinndddd 6622 2244//0088//22002211 1166::2233
6622 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 6633 CAN I TRUST MARKETING BUZZWORDS? Food manufacturers and suppliers spend huge amounts on crafting messages that influence our perceptions about nutritional value, to entice us to choose their particular product. Don’t take these at face value. IT CONTAINS MAY BE FRUIT FRESH FRUIT CONCENTRATE JUICE AND IS WITH ADDED LOW IN “NATURALLY FRUIT CALORIES SUGAR Claims on food packaging can be about health, HIGH FIBER FLAVORED” nutritional content, or structure/function and are Food products that contain at least 10% DV or 2.5g regulated by the FDA. To be FDA approved, the of fiber per serving can claim they are a “good ALL FRUITS AND claim must be supported by the totality of publicly source of fiber,” and those containing at least 20% THERE IS NO REAL NUTS/SEEDS ARE “CONTAINS available scientific evidence. A US study found that DV of fiber or 5g or more of fiber per serving can DEFINITION FOR HIGHLY SUPERFOODS” THIS TERM highlighting added vitamins in snacks made label the product with a high fiber claim. NUTRITIOUS consumers more likely to view them as healthier and less likely to check the nutrition label. NATURAL/ORGANIC THIS DOES NOT AT LEAST 25% “REDUCED The FDA defines “natural” as having nothing NECESSARILY FEWER CALORIES CALORIE” MEANS IT’S A NO ADDED SUGAR artificial or synthetic (including all color additives) THAN OTHER LOW-CALORIE “Sugar free” means one serving contains less than included in or added to the food. The claim “100% PRODUCT FOOD 0.5g of sugar, both natural and added. “No added organic” can appear on any product that contains COULD BENEFIT A THIN LAYER sugar” refers only to sugars added during 100% organic ingredients (excluding salt and water). GUT HEALTH AND OF YOGURT processing, not naturally occurring sugar, such as “YOGURT COATED” STRENGTHEN PROVIDES LITTLE fructose in fruit. A no-added-sugar fruit smoothie, GOOD SOURCE/EXCELLENT SOURCE BONES AND PROTEIN, WITH for example, could potentially contain more sugar If a food contains 10–19% DV of a certain nutrient, TEETH ADDED SUGAR than a can of soda. it’s considered a “good source.” An “excellent source” “ALL NATURAL” has at least 20% DV of a certain nutrient. MADE WITH IT MAY LOW/LIGHT ENTIRELY NATURAL CONTAIN The claim “low” refers to the following amounts INGREDIENTS ADDITIVES AND AND NO PROCESSED compared to a given reference amount: <40 calories; Unhealthy influence ADDITIVES INGREDIENTS <3g total fat; <1g saturated fat (with no more than 15% of calories coming from saturated fat); <140mg SOCIAL MEDIA IS A MATRIX OF MISINFORMATION sodium; and <20mg cholesterol (and only when a AND PSEUDOSCIENTIFIC CLAIMS. For example, nearly half of social media users WHAT WE MAY ASSUME THIS MEANS WHAT THIS MAY ACTUALLY MEAN food contains <2g of saturated fat per serving). reportedly think cutting out a whole food group is Food that derives less than 50% of its calories from healthy. A study of nine popular UK influencers fat can be labeled “light” or “lite” if its total amount blogging about weight management found that only two had relevant qualifications, and five didn’t of calories is decreased by at least 33.3% or its fat cite evidence-based references for nutrition claims. content is reduced by at least 50% compared to a Messages on foods often emphasize Influencers often have a commercial agenda—be them as a healthy choice; while some standard/original version. wary of their recommendations around food. ingredients may be, the overall nutritional value may not match our perceptions. UUSS__006622--006633__bbuuzzzz__wwoorrddss..iinndddd 6622 2244//0088//22002211 1166::2233 UUSS__006622--006633__bbuuzzzz__wwoorrddss..iinndddd 6633 2244//0088//22002211 1166::2233
64 HOW CAN WE EAT WELL? 65 IS SUGAR THE ENEMY? Although sugar has been regarded the enemy for a long while, the truth is that not all sugars are bad for us. Sugar has its place in the diet—in the right quantities and in a form your body can handle comfortably. Sugar is a carbohydrate found in many foods to as “total sugars,” which includes naturally (see pages 12–13). Foods containing natural sugars, occurring and added sugars. Sugar comes in many such as fruits and dairy products, are good sources forms—sucrose, glucose, fructose, maltose, fruit of many nutrients and the energy provided by sugar. juice, molasses, corn and other syrups, honey, and Despite its bad reputation, scientists have not fruit juice concentrate. proved that sugar affects health when it is not part Avoid high-fructose corn syrup (HFCS), which of a diet too high in calories. is packaged into many foods in the US. Countries consuming HFCS the most show higher levels of ADDED SUGARS diabetes. It has been shown that a diet of more than Added sugar might be sugar added to an oat bar 150g per day of HFCS reduces insulin sensitivity during production or a spoonful of honey in (see page 172), increasing the risk of developing herbal tea. high blood pressure and high cholesterol levels. Foods containing added sugars often have little or no nutritional benefit. The World Health Organization THE RIGHT BALANCE (WHO) recommends no more than 5 percent of Science confirms what we all know—sugar affects daily energy intake comes from added sugars. the feel-good reward center in the brain. It triggers a Sugars, often chemically produced, are added pleasure response that’s similar to how we feel when to products to enhance palatability. Check sugar watching cute puppies or receiving love. Perhaps content on labels (see pages 60–61). It may be referred that explains why sugar provides more calories in Added and "free" sugars While not an added sugar, fruit juice is an example of a "free" sugar. Once the sugar it contains is separated 16oz from the fruit’s pulp, the fiber (and 1 1 bulk) is lost. You wouldn’t eat four ORANGE JUICE oranges in one sitting, but you might ORANGE (4 ORANGES) CAN SODA drink their juice in one large glass of orange juice—which is more sugar FIBER 1.8g FIBER 0.2g FIBER 0g than you’d find in a can of soda! SUGAR 12.3g SUGAR 44g SUGAR 30g UUSS__006644--006655__ssuuggaarr..iinndddd 6644 2244//0088//22002211 1166::2233
6644 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 6655 IS SUGAR THE ENEMY? our diets than is advisable. But there’s no need to demonize it. Psychology plays a major role in the Sugar alternatives nutrition choices we make. Going teetotal on any Although sugar has been regarded the enemy for a long while, the truth is that food may lead to binging; moderation is best. IF YOU DRINK MANY SUGAR-SWEETENED not all sugars are bad for us. Sugar has its place in the diet—in the right BEVERAGES, CONSIDER USING SWEETENERS quantities and in a form your body can handle comfortably. TO HELP YOU GRADUALLY WEAN YOURSELF REDUCING SUGAR OFF ADDING SUGAR TO TEA OR COFFEE. Ensure you’re aware of how much sugar you are Molasses, maple syrup, and honey come consuming, so you can make appropriate choices from natural sources. Artificial sweeteners, such as sucralose, aspartame, and saccharine, Sugar is a carbohydrate found in many foods to as “total sugars,” which includes naturally to help you stay within the recommended allowance. are chemically manufactured. Both natural and (see pages 12–13). Foods containing natural sugars, occurring and added sugars. Sugar comes in many Swap out sugary foods for alternatives. Substitute artificial sweeteners are safe to use in such as fruits and dairy products, are good sources forms—sucrose, glucose, fructose, maltose, fruit cookies, and add fruit instead of sugar to breakfast moderation. There is some research suggesting of many nutrients and the energy provided by sugar. juice, molasses, corn and other syrups, honey, and cereals, for instance. that the latter are not beneficial to the gut microbiome, but as this is a relatively new field Despite its bad reputation, scientists have not fruit juice concentrate. Have smaller portions. Share dessert with a loved of research, the evidence is not conclusive. proved that sugar affects health when it is not part Avoid high-fructose corn syrup (HFCS), which one. Ask for sugar-laden dressings on the side so of a diet too high in calories. is packaged into many foods in the US. Countries you can use less. Condiments like ketchup can have consuming HFCS the most show higher levels of 23g of sugar per 100g; use 1 teaspoon per serving. ADDED SUGARS diabetes. It has been shown that a diet of more than When cooking, don’t add sugar to savory foods. Added sugar might be sugar added to an oat bar 150g per day of HFCS reduces insulin sensitivity Nearly a quarter of the added sugar in our diets during production or a spoonful of honey in (see page 172), increasing the risk of developing comes from sugar-sweetened beverages (fizzy drinks, herbal tea. high blood pressure and high cholesterol levels. sweetened juices, and fruit-flavored beverages). Foods containing added sugars often have little or Swap sugary drinks for water. no nutritional benefit. The World Health Organization THE RIGHT BALANCE It may take time to settle into your new habits, (WHO) recommends no more than 5 percent of Science confirms what we all know—sugar affects but there is no conclusive research linking sugar daily energy intake comes from added sugars. the feel-good reward center in the brain. It triggers a with physical addiction in humans. You can’t Sugars, often chemically produced, are added pleasure response that’s similar to how we feel when experience withdrawal symptoms as you would to products to enhance palatability. Check sugar watching cute puppies or receiving love. Perhaps with a dependency on alcohol or drugs. content on labels (see pages 60–61). It may be referred that explains why sugar provides more calories in Added and "free" sugars Recommended allowance 6–9 The American Heart Association recommends limiting sugar to 6 While not an added sugar, fruit juice teaspoons or 100 calories per day SUGAR CUBES OR is an example of a "free" sugar. Once for women, and about 9 teaspoons TEASPOONS IS THE or 150 calories per day for men. RECOMMENDED DAILY the sugar it contains is separated 16oz ALLOWANCE OF from the fruit’s pulp, the fiber (and 1 1 SUGARS bulk) is lost. You wouldn’t eat four ORANGE JUICE oranges in one sitting, but you might ORANGE (4 ORANGES) CAN SODA drink their juice in one large glass of orange juice—which is more sugar FIBER 1.8g FIBER 0.2g FIBER 0g than you’d find in a can of soda! SUGAR 12.3g SUGAR 44g SUGAR 30g UUSS__006644--006655__ssuuggaarr..iinndddd 6644 2244//0088//22002211 1166::2233 UUSS__006644--006655__ssuuggaarr..iinndddd 6655 2244//0088//22002211 1166::2233
66 HOW CAN WE EAT WELL? 67 IS FAT STILL BAD FOR ME? Scientific thinking on fat has shifted dramatically in recent decades, and it’s a complex area that is still developing today. However, there is general agreement that we should all enjoy some healthier fats within our diet. Fat is actually crucial to our well-being. It gives us of heart disease and stroke. They are used rarely energy, delivers flavor to food, and helps us feel full in UK foods, at very low levels—look for “partially and satisfied. The body requires fat to absorb certain hydrogenated” on labels. Saturated fat is found vitamins. We should try to include both mono- and mainly in animal-derived foods and cereal products polyunsaturated fats, particularly omega-3 fatty like pizza and cookies. There is evidence that a high acids, which come only from diet and are proven intake of saturated fat raises LDL cholesterol. to help lower LDL (low-density lipoprotein) Recently, scientific debate has emerged over the cholesterol and support heart health and cognition. link between saturated fat and heart disease, with one research review suggesting that saturated fat ARE THERE "BAD" FATS? doesn’t raise the risk of heart disease. However, the Research has discredited the broad “fat is bad” American and British Heart associations advise that viewpoint that led to a proliferation of low-fat diet replacing saturated fat with unsaturated lowers the plans and foods. A 2017 study explored reduced-fat risk, and health bodies, including the World Health diets over eight years and found no noticeable Organization, advise limiting saturated fat to no health benefits over diets that didn’t restrict fat; more than 10%–11% of daily calories. Most experts one possible factor may be eating the extra sugar agree that a healthy diet will do more to reduce often added by manufacturers to replace lost flavor. heart disease risk than focusing on saturated fat. Trans fats, or artificially hardened vegetable oils, The key is to eat less food that’s high in saturated are banned in several countries, including the US, fat and replace it with healthier fats from fish and due to concerns they raise LDL cholesterol, a cause plants—not added sugars or refined carbohydrates. Which fat is healthiest? Oils and fats contain different combinations of EXTRA-VIRGIN OLIVE OIL CANOLA OIL unsaturated and saturated fatty acids per 100g, making SATURATED FAT: 15.5g SATURATED FAT: 6g some a healthier choice. POLYUNSATURATED FAT: 10.7g POLYUNSATURATED FAT: 27g Their versatility for cooking MONOUNSATURATED FAT: 65g MONOUNSATURATED FAT: 54g depends on their smoke SMOKE POINT: 374OF–405OF SMOKE POINT: 400OF–446OF points—the temperature Good for heart health, this oil Its combination of unsaturated fats range at which they degrade is more suited to lower-temperature make this is a good choice for a and may release potentially cooking, dressing, or marinating. heart-healthy daily cooking oil; it can harmful compounds. be used for frying and roasting. UUSS__006666--006677__ffaatt..iinndddd 6666 2244//0088//22002211 1166::2233
6666 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 6677 IS FAT STILL BAD FOR ME? CHOLESTEROL Scientific thinking on fat has shifted dramatically in recent decades, and it’s a complex STUDIES SHOW AVOCADOS CAN RAISE area that is still developing today. However, there is general agreement that we "GOOD" (HDL) AND should all enjoy some healthier fats within our diet. LOWER "BAD" (LDL) CHOLESTEROL Fat is actually crucial to our well-being. It gives us of heart disease and stroke. They are used rarely energy, delivers flavor to food, and helps us feel full in UK foods, at very low levels—look for “partially POTASSIUM and satisfied. The body requires fat to absorb certain hydrogenated” on labels. Saturated fat is found AVOCADOS ARE EVEN vitamins. We should try to include both mono- and mainly in animal-derived foods and cereal products HIGHER THAN BANANAS polyunsaturated fats, particularly omega-3 fatty like pizza and cookies. There is evidence that a high IN THIS MINERAL, WHICH IS LINKED TO VARIOUS acids, which come only from diet and are proven intake of saturated fat raises LDL cholesterol. HEALTH BENEFITS to help lower LDL (low-density lipoprotein) Recently, scientific debate has emerged over the cholesterol and support heart health and cognition. link between saturated fat and heart disease, with one research review suggesting that saturated fat ENERGY ARE THERE "BAD" FATS? doesn’t raise the risk of heart disease. However, the 77% OF THE CALORIES Research has discredited the broad “fat is bad” American and British Heart associations advise that IN AVOCADOS COME viewpoint that led to a proliferation of low-fat diet replacing saturated fat with unsaturated lowers the FROM FAT; IT HAS A SIMILAR FAT PROFILE plans and foods. A 2017 study explored reduced-fat risk, and health bodies, including the World Health TO EXTRA VIRGIN diets over eight years and found no noticeable Organization, advise limiting saturated fat to no OLIVE OIL health benefits over diets that didn’t restrict fat; more than 10%–11% of daily calories. Most experts one possible factor may be eating the extra sugar agree that a healthy diet will do more to reduce often added by manufacturers to replace lost flavor. heart disease risk than focusing on saturated fat. Trans fats, or artificially hardened vegetable oils, The key is to eat less food that’s high in saturated Plant-based fat: Avocados and unrefined avocado oil are banned in several countries, including the US, fat and replace it with healthier fats from fish and are both rich sources of due to concerns they raise LDL cholesterol, a cause plants—not added sugars or refined carbohydrates. monounsaturated fatty acids. Which fat is healthiest? Oils and fats contain different combinations of EXTRA-VIRGIN OLIVE OIL CANOLA OIL SUNFLOWER OIL COCONUT OIL BUTTER unsaturated and saturated fatty acids per 100g, making SATURATED FAT: 15.5g SATURATED FAT: 6g SATURATED FAT: 10g SATURATED FAT: 86.5g SATURATED FAT: 67g some a healthier choice. POLYUNSATURATED FAT: 10.7g POLYUNSATURATED FAT: 27g POLYUNSATURATED FAT: 56g POLYUNSATURATED FAT: 1.8g POLYUNSATURATED FAT: 5g Their versatility for cooking MONOUNSATURATED FAT: 65g MONOUNSATURATED FAT: 54g MONOUNSATURATED FAT: 25.8g MONOUNSATURATED FAT: 5.8g MONOUNSATURATED FAT: 28g depends on their smoke SMOKE POINT: 374OF–405OF SMOKE POINT: 400OF–446OF SMOKE POINT: 446OF SMOKE POINT: 347OF–385OF SMOKE POINT: 300OF–347OF points—the temperature Good for heart health, this oil Its combination of unsaturated fats Widely used for cooking, it contains a This plant oil contains mostly Butter is primarily a saturated fat range at which they degrade is more suited to lower-temperature make this is a good choice for a high level of omega-6 fatty acids, which saturated fatty acids and has been but is also a source of vitamins and may release potentially cooking, dressing, or marinating. heart-healthy daily cooking oil; it can can help reduce cholesterol but have found to raise LDL cholesterol so is A and D and contains calcium. harmful compounds. be used for frying and roasting. also been linked to inflammation. best used occasionally. UUSS__006666--006677__ffaatt..iinndddd 6666 2244//0088//22002211 1166::2233 UUSS__006666--006677__ffaatt..iinndddd 6677 2244//0088//22002211 1166::2233
68 HOW CAN WE EAT WELL? 69 SHOULD I CUT OUT RED MEAT? We know that consuming different kinds of protein—including plants, fish, and poultry—is a sensible step toward healthy eating. But if you really enjoy things like steaks and hot dogs, consider their nutritional and health implications. Red meat is red when raw and includes lamb, beef, contain nitrites; for example, traditionally venison, pork, and veal. It’s a rich source of muscle- produced prosciutto di Parma is cured with salt. building protein and important micronutrients, One recent US study concluded that evidence notably vitamins B3 and B12 (not available from didn’t support eating less red or processed meat, plants), iron, zinc, and selenium. While it’s nutritious, but this was challenged by several health bodies. it can be high in saturated fat, especially fattier cuts; The World Cancer Research Fund advises adults 3.5oz of prime rib contains around 34g of total fat. to eat little or no processed meat and up to One limited study found organic meat contains 12oz–18oz cooked weight of red meat weekly, while 50 percent more omega-3 fatty acids, known to some guidelines suggest regular consumers eat no support the heart and immune system, possibly more than 3oz in total a day. A more recent finding because livestock eat a more grass-based diet. concluded that eating 3oz daily—about three Processed meats like ham, bacon, and salami slices of ham—could still elevate cancer risk, have been cured, salted, smoked, or otherwise although overall diet and lifestyle matters, too. altered to improve flavor and for longer life and If you eat red meat, opt for unprocessed, lean often contain a lot of salt. A diet high in salt and cuts with plenty of vegetables. saturated fat has been shown to raise blood pressure and LDL cholesterol, a risk factor for cardiovascular disease. CANCER RISK What’s in your sausage? The pigment in fresh red meat, called heme, and NOT ALL SAUSAGES ARE NUTRITIONALLY EQUAL nitrite or nitrate preservatives used in processing IN TERMS OF MEAT, SALT, AND FAT CONTENT. have both been linked to increased bowel cancer For example, a survey found salt in premade sausages varied from 0.75g to 2.3g per 3.5oz. Freshly risk. Although nitrites also occur naturally in green made sausages or burgers may be a healthier choice vegetables, the way the body digests them in meat and may not be a cancer risk unless they are further can create toxic nitrosamines. In 2015, World Health modified—ask your butcher what’s in them. One research review noted the link between colorectal Organization cancer experts identified red meat as cancers and sodium nitrite but claimed this isn’t a probable cause of cancers and processed meats a added to most traditional sausages. definite cause (although not how many cases they cause). However, not all processed meat products UUSS__006688--006699__rreedd__mmeeaatt..iinndddd 6688 2244//0088//22002211 1166::2233
6688 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 6699 SHOULD I CUT Cooking fresh or processed meat OUT RED MEAT? Cooking can create substances called HCAs and PAHs that are mutagenic; they can be activated by enzymes We know that consuming different kinds of protein—including plants, fish, and FRESH PROCESSED during digestion and cause DNA poultry—is a sensible step toward healthy eating. But if you really enjoy things like MEAT changes that may increase MEAT cancer risk. steaks and hot dogs, consider their nutritional and health implications. Red meat is red when raw and includes lamb, beef, contain nitrites; for example, traditionally venison, pork, and veal. It’s a rich source of muscle- produced prosciutto di Parma is cured with salt. building protein and important micronutrients, One recent US study concluded that evidence HIGH HEAT notably vitamins B3 and B12 (not available from didn’t support eating less red or processed meat, 3900F+ plants), iron, zinc, and selenium. While it’s nutritious, but this was challenged by several health bodies. it can be high in saturated fat, especially fattier cuts; The World Cancer Research Fund advises adults PAN FRYING, 3.5oz of prime rib contains around 34g of total fat. to eat little or no processed meat and up to One limited study found organic meat contains 12oz–18oz cooked weight of red meat weekly, while GRILLING & ROASTING 50 percent more omega-3 fatty acids, known to some guidelines suggest regular consumers eat no support the heart and immune system, possibly more than 3oz in total a day. A more recent finding BARBECUING because livestock eat a more grass-based diet. concluded that eating 3oz daily—about three Processed meats like ham, bacon, and salami slices of ham—could still elevate cancer risk, have been cured, salted, smoked, or otherwise although overall diet and lifestyle matters, too. HCAs NITROSAMINES altered to improve flavor and for longer life and If you eat red meat, opt for unprocessed, lean When meat, poultry, or fish Nitrosamine molecules are often contain a lot of salt. A diet high in salt and cuts with plenty of vegetables. cooks at high temperature, present in some processed especially if it’s overcooked, meats and are probable saturated fat has been shown to raise blood creatine in muscle and human carcinogens; pressure and LDL cholesterol, a risk factor for amino acids can react and research has found higher form HCAs (heterocyclic PAHs levels in bacon that has cardiovascular disease. amines). been fried. Fat from meat grilled at high heat can drip onto an CANCER RISK What’s in your sausage? open fire, creating smoke containing PAHs (polycyclic The pigment in fresh red meat, called heme, and NOT ALL SAUSAGES ARE NUTRITIONALLY EQUAL aromatic hydrocarbons) nitrite or nitrate preservatives used in processing IN TERMS OF MEAT, SALT, AND FAT CONTENT. MARINATING MEAT that can stick to the meat’s surface; PAHs can also have both been linked to increased bowel cancer For example, a survey found salt in premade IN OLIVE OIL, LEMON form when meat is JUICE, OR RED WINE CAN sausages varied from 0.75g to 2.3g per 3.5oz. Freshly smoked. risk. Although nitrites also occur naturally in green REDUCE HCAs BY UP TO made sausages or burgers may be a healthier choice vegetables, the way the body digests them in meat and may not be a cancer risk unless they are further 90% can create toxic nitrosamines. In 2015, World Health modified—ask your butcher what’s in them. One research review noted the link between colorectal CHARRING Organization cancer experts identified red meat as cancers and sodium nitrite but claimed this isn’t INDICATES THAT PAHs KEY a probable cause of cancers and processed meats a MAY BE PRESENT; added to most traditional sausages. AVOID EATING THE Fresh meat definite cause (although not how many cases they MEAT OR SCRAPE Processed meat cause). However, not all processed meat products BURNT AREAS UUSS__006688--006699__rreedd__mmeeaatt..iinndddd 6688 2244//0088//22002211 1166::2233 UUSS__006688--006699__rreedd__mmeeaatt..iinndddd 6699 2244//0088//22002211 1166::2233
70 HOW CAN WE EAT WELL? 71 WILL SALT GIVE ME A HEART ATTACK? If you like to add salt to your meals but you’ve never thought much about the salt content that’s in so many everyday foods, from baked beans to cookies, there is a chance your eating habits could be harming your health. Salt is partly made of sodium, and our bodies need In 2018, an international study gained headlines some sodium for biological processes like fluid for concluding that this risk would require a regulation and nerve impulse transmission. But significantly higher average salt intake than most research estimates that US adults consume an people actually eat (China being an exception), average 3,400mg of salt a day; the US guideline daily but the basis of the research was challenged. limit is 2,300mg—about a teaspoon. The WHO continues to advise adults to restrict For decades, the consensus among leading salt intake to 5g (5,000mg) a day. health bodies and experts, including the World Health Organization (WHO), has ARE PREMIUM SALTS HEALTHIER? been that eating too much sodium can Sea salt crystals and flakes and coarse lead to high blood pressure, a major mined rock salt like Himalayan pink salt risk factor for stroke, kidney disease, are less refined than table salt, which has and cardiovascular disease, which minerals removed as it is finely ground can lead to heart attack. and anti-caking agents are added. Brain Heart Kidneys Salt and hypertension NORMAL BLOOD PRESSURE High blood pressure (hypertension) caused by a long-term diet high in salt could potentially Artery walls expand and damage the heart, brain, contract as the heart pumps to circulate blood. and other organs. UUSS__007700--007711__ssaalltt..iinndddd 7700 2255//0088//22002211 1199::1144
7700 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 7711 WILL SALT GIVE ME Although one survey found that nearly two- can be high in salt when canned in brine—if SALT INTAKE thirds of people think sea salt contains less you can’t buy fresh veggies, it may be better US GUIDELINES A HEART ATTACK? sodium, the American Heart Association says to choose frozen because salt isn’t used SUGGEST DAILY table salt and most sea salts are around 40 during freezing. MAXIMUMS OF: percent sodium by weight, and there are no Salt content in similar products can vary, If you like to add salt to your meals but you’ve never thought much about the AGE 0–6 mths health reasons for choosing sea salt. so check labels. These may list salt or sodium; salt content that’s in so many everyday foods, from baked beans to cookies, <110mg multiply sodium by 2.5 to get the salt content. there is a chance your eating habits could be harming your health. "HIDDEN" SALT AGE 7–12 mths Many people don’t realize they are eating too REDUCING SALT: <370mg much salt because around 75 percent of the Aim to reduce salt gradually—our taste Salt is partly made of sodium, and our bodies need In 2018, an international study gained headlines salt in our diet is already in processed and buds get used to salt intake, so it should get AGE 1–3 years some sodium for biological processes like fluid for concluding that this risk would require a preprepared foods. It’s no surprise foods like easier over time. Check food labels and opt for <1,500mg regulation and nerve impulse transmission. But significantly higher average salt intake than most soy sauce, processed meats, and many snacks products containing less salt where possible. research estimates that US adults consume an people actually eat (China being an exception), and ready-made meals are high in salt. But it’s Experiment with herbs and spices for AGE 4–8 years average 3,400mg of salt a day; the US guideline daily but the basis of the research was challenged. also present in foods you might not expect, flavor; keep plenty of dried options handy. <1,900mg limit is 2,300mg—about a teaspoon. The WHO continues to advise adults to restrict like cookies and cakes, and those you may eat In restaurants, ask for your meal to be For decades, the consensus among leading salt intake to 5g (5,000mg) a day. regularly, like bread and cereals. Even vegetables prepared without added salt. AGE 14+ health bodies and experts, including the <2,300mg World Health Organization (WHO), has ARE PREMIUM SALTS HEALTHIER? been that eating too much sodium can Sea salt crystals and flakes and coarse lead to high blood pressure, a major mined rock salt like Himalayan pink salt KIDNEYS BRAIN risk factor for stroke, kidney disease, are less refined than table salt, which has THEY BECOME LESS ABLE IT CAN BE DEPRIVED and cardiovascular disease, which minerals removed as it is finely ground TO REMOVE WASTE AND OF OXYGEN BY BLOOD can lead to heart attack. and anti-caking agents are added. FLUIDS, FURTHER RAISING CLOTS IN NARROW BLOOD PRESSURE ARTERIES RISKS: RISK: KIDNEY STROKE, T.I.A. (MINI DISEASE STROKE) Tear Brain HEART IT CAN BE ENLARGED BY Heart OVERWORK OR AREAS CAN BECOME DAMAGED RISKS: HEART ATTACK, Kidneys HEART FAILURE Salt and hypertension NORMAL BLOOD EXCESS ARTERY PRESSURE SALT DAMAGE RESTRICTION High blood pressure (hypertension) caused by a long-term diet high in salt could potentially Artery walls expand and The body reacts to excess salt Pressure can eventually Artery damage makes it easier damage the heart, brain, contract as the heart pumps by retaining water in the blood, cause artery walls to narrow, for cholesterol to collect there, to circulate blood. increasing blood volume and tear, and stiffen, reducing leading to blockages that can and other organs. pressure on arteries. blood flow. restrict oxygen. UUSS__007700--007711__ssaalltt..iinndddd 7700 2255//0088//22002211 1199::1144 UUSS__007700--007711__ssaalltt..iinndddd 7711 2244//0088//22002211 1166::2233
72 HOW CAN WE EAT WELL? 73 SURELY CAFFEINE CAFFEINE SOURCES CAN’T BE GOOD FOR ME? CAFFEINE IS FOUND IN MANY PRODUCTS. THESE Caffeine is the world’s most commonly consumed psychoactive ARE SOME TYPICAL substance, but is it okay to have as much as you like? AMOUNTS IN MILLIGRAMS (mg). Caffeine occurs naturally in coffee, cocoa, and antioxidants and potassium, a mineral that has tea plants and is added to some drinks, food, and been shown to help lower blood pressure. medications. It boosts alertness by stimulating the central nervous system and blocking CAN I HAVE TOO MUCH? adenosine, a molecule that reduces heart rate More than 600mg of caffeine daily can cause 95-125 and promotes sleep. anxiety, upset stomach, and raised blood Coffee Moderate caffeine intake—around 300mg– pressure; frequent consumption may worsen 8oz (240ml) 400mg per day, or 3–4 cups of coffee—can have irritable bowel syndrome symptoms. If taken beneficial effects, including a reduced risk of less than six hours before bed, caffeine can heart disease and enhanced concentration. disrupt sleep; it may also stimulate a need to Many studies show that caffeine aids athletic urinate. Pregnant women should limit intake to performance, for instance, by boosting metabolic 200mg daily, due to an association with low 91 rate and glycogen (fuel) storage if taken prior to birth weight and miscarriage. Despite some Energy drink high-intensity interval training. Research (albeit scientific debate on the issue, the World Health 8oz (240ml) not as strong) also suggests caffeine may help Organization recognizes caffeine addiction as a relieve headaches and migraines. Most studies clinical disorder. Reactions to caffeine vary, but are based on coffee, which also contains many can safely enjoy moderate consumption. Adrenaline rush: Consuming caffeine 42 increases brain activity, prompting the release of adrenaline and a series Diet cola of “fight-or-flight” effects. B BS 11oz (330ml) L O M O LI H D P E C WS E A R T R A T U M P S T RESSUR ACH SLO 26 AIR WAYS OPE N WI DERE I N C R E A S E S O M U S C L E S BLOOD P BLOOD TO STOM O O D S U G A R R ELE A S E D 8B ozla (c 2k 4 0te ma l ) O R E B L PUPILS DILATE M MUSCLES TENSE 16 Dark chocolate 1oz (20g) UUSS__007722--007733__ccooffffeeee__aallccoohhooll..iinndddd 7722 2244//0088//22002211 1166::2233
7722 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 7733 SURELY CAFFEINE CAFFEINE DOES ALCOHOL HAVE SOURCES CAN’T BE GOOD FOR ME? ANY BENEFITS? CAFFEINE IS FOUND IN MANY PRODUCTS. THESE Caffeine is the world’s most commonly consumed psychoactive While moderate alcohol intake may have some positive health impacts, ARE SOME TYPICAL substance, but is it okay to have as much as you like? the risks of drinking in excess are well established. AMOUNTS IN MILLIGRAMS (mg). Caffeine occurs naturally in coffee, cocoa, and antioxidants and potassium, a mineral that has Alcohol has no nutritional benefits that warrant MAXIMUM AMOUNTS tea plants and is added to some drinks, food, and been shown to help lower blood pressure. introducing it. Several studies have associated The US’s recommended alcohol intake is one medications. It boosts alertness by stimulating moderate consumption of any alcohol with drink or less per day for women and two drinks the central nervous system and blocking CAN I HAVE TOO MUCH? lower risk of coronary heart disease; however, or less per day for men (on days when alcohol is adenosine, a molecule that reduces heart rate More than 600mg of caffeine daily can cause 95-125 definitions of “moderate” vary. consumed). One alcohol drink is equivalent to and promotes sleep. anxiety, upset stomach, and raised blood Red wine can contain organic compounds 12oz of regular beer, 5oz of wine, or 1.5oz of Coffee Moderate caffeine intake—around 300mg– pressure; frequent consumption may worsen 8oz (240ml) called polyphenols; their antioxidant and anti- 80-proof distilled spirits. 400mg per day, or 3–4 cups of coffee—can have irritable bowel syndrome symptoms. If taken inflammatory properties may have preventive Heavier, and prolonged, drinking can lead beneficial effects, including a reduced risk of less than six hours before bed, caffeine can and/or therapeutic effects for cardiovascular to health problems, including increased blood heart disease and enhanced concentration. disrupt sleep; it may also stimulate a need to disease, neurodegenerative disorders, cancer, pressure, heart disease, liver disease, and Many studies show that caffeine aids athletic urinate. Pregnant women should limit intake to and obesity. Red wine has around 10 times depression. Alcohol also stimulates appetite, and performance, for instance, by boosting metabolic 200mg daily, due to an association with low 91 more polyphenols than white. Comparisons one survey suggested that 80 percent of people rate and glycogen (fuel) storage if taken prior to birth weight and miscarriage. Despite some show lower incidences of heart disease in wine- are not aware of the calories in a glass of wine. Energy drink high-intensity interval training. Research (albeit scientific debate on the issue, the World Health 8oz (240ml) drinking countries like France compared to Drinking patterns also matter. A study found not as strong) also suggests caffeine may help Organization recognizes caffeine addiction as a those with a higher intake of beer or spirits, consuming around one drink daily across four or relieve headaches and migraines. Most studies clinical disorder. Reactions to caffeine vary, but such as Germany and Russia (although wine- more days a week resulted in lower death rates are based on coffee, which also contains many can safely enjoy moderate consumption. drinking countries often have healthier diets, than the same amount over 1–2 days. To reduce and this is based on moderate consumption). intake, aim for several alcohol-free days a week. Adrenaline rush: Consuming caffeine 42 increases brain activity, prompting the release of adrenaline and a series Diet cola of “fight-or-flight” effects. B BS 11oz (330ml) L O M Calories in alcohol: Spirits O LI tend to have fewest calories H D P E C WS per serving, without mixers. E A R T R A T U M P S T RESSUR ACH SLO 26 AIR WAYS OPE N WI DERE I N C R E A S E S O M U S C L E S BLOOD P BLOOD TO STOM O O D S U G A R R ELE A S E D 8B ozla (c 2k 4 0te ma l ) 61 110 61+93 160 182 216 O R E B L PUPILS DILATE M MUSCLES TENSE 16 SR inu gm le V So ind gk la e SinG gin le & (1o t zo /n 2i 5c m l R 1e d gl w asin s e Beer 1/ pp ia nl te ale C 1 pid ine tr Dark chocolate (1oz/25ml) (1oz/25ml) + 5oz/150ml) (6oz/175ml) (16oz) (16oz) 1oz (20g) UUSS__007722--007733__ccooffffeeee__aallccoohhooll..iinndddd 7722 2244//0088//22002211 1166::2233 UUSS__007722--007733__ccooffffeeee__aallccoohhooll..iinndddd 7733 2244//0088//22002211 1166::2233
74 HOW CAN WE EAT WELL? 75 WILL SUPERFOODS SAVE ME? So-called "superfoods" seem to be everywhere, from social media to supermarket shelves, with new arrivals constantly being talked about. Are these foods all hype or healthy enough to earn our attention—and their price? No single food can compensate for an unhealthy turmeric and curcumin can increase bile secretion. diet or prevent illness. So-called “superfoods” Check with your doctor before consuming. supposedly offer a particular health benefit or promote wellness—in fact, this is a marketing term COST OF "SUPERFOODS" to entice us into buying more of various exotic- These foods are often more expensive than common sounding foods. It appears to work: in 2015 alone, fruits and vegetables with similar nutritional the number of new products labeled “superfood,” benefits. For example, 3.5oz of papaya contains 61mg “supergrain,” or “superfruit” rose by a third. of vitamin C, compared to 77.6mg in bell peppers, Foods often called “super,” like spirulina and goji while a cup of whole milk has significantly more berries, can pack a nutritional punch—3.5oz of dried calcium than 3.5oz of spirulina. Although goji berries goji berries contains 4mg of vitamin A, compared to have more vitamin A than carrots by weight, over a 2.5mg in fresh carrots. Research suggests their mix week, you’re likely to eat more carrots—and so take of phytochemicals and high nutrient levels can help in as much, if not more, vitamin A at less cost. At the improve overall health, support the immune system, time of writing, in one supermarket, goji berries regulate mood by increasing serotonin, and more. were 40 times more expensive than carrots per 3.5oz. However, a lot of this comes from animal studies. In addition to expense, foods like goji berries, açai, It’s also worth noting that we can absorb only a spirulina, and chia may be hard to find. Buying more certain amount of any nutrient. Even supposed readily available alternatives could help offset some superfoods are beneficial only when consumed in of the environmental concerns around importing moderation, as part of a balanced diet; for example, foods from around the globe. CHIA SEEDS SPIRULINA AÇAI GINGER If eaten in the right amount Including omega-3 and -6 fatty Usually sold frozen or Ginger is thought to have and within a healthy diet, acids and iron, this algae is powdered due to their short life anti-inflammatory and these are a source of fiber, said to have antioxidant and span, these berries contain antioxidant effects, although calcium, and phosphorus. anti-inflammatory properties. antioxidants and fiber. research is limited. ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE FLAXSEEDS WHOLE MILK BLUEBERRIES N/A SESAME SEEDS CRANBERRIES UUSS__007744--007755__ssuuppeerrffooooddss..iinndddd 7744 2244//0088//22002211 1166::2233
7744 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 7755 WILL SUPERFOODS SAVE ME? RESEARCH ON THE So-called "superfoods" seem to be everywhere, from social media to supermarket ANTI-INFLAMMATORY BENEFITS OF TURMERIC shelves, with new arrivals constantly being talked about. Are these foods all hype VARIES; ONE REVIEW or healthy enough to earn our attention—and their price? FOUND NO EFFECT ON SEVERAL MARKERS OF INFLAMMATION No single food can compensate for an unhealthy turmeric and curcumin can increase bile secretion. diet or prevent illness. So-called “superfoods” Check with your doctor before consuming. supposedly offer a particular health benefit or promote wellness—in fact, this is a marketing term COST OF "SUPERFOODS" to entice us into buying more of various exotic- These foods are often more expensive than common sounding foods. It appears to work: in 2015 alone, fruits and vegetables with similar nutritional the number of new products labeled “superfood,” benefits. For example, 3.5oz of papaya contains 61mg “supergrain,” or “superfruit” rose by a third. of vitamin C, compared to 77.6mg in bell peppers, Foods often called “super,” like spirulina and goji while a cup of whole milk has significantly more berries, can pack a nutritional punch—3.5oz of dried calcium than 3.5oz of spirulina. Although goji berries goji berries contains 4mg of vitamin A, compared to have more vitamin A than carrots by weight, over a 2.5mg in fresh carrots. Research suggests their mix week, you’re likely to eat more carrots—and so take of phytochemicals and high nutrient levels can help in as much, if not more, vitamin A at less cost. At the improve overall health, support the immune system, time of writing, in one supermarket, goji berries regulate mood by increasing serotonin, and more. were 40 times more expensive than carrots per 3.5oz. STUDIES However, a lot of this comes from animal studies. In addition to expense, foods like goji berries, açai, SUGGEST CONSUMING TURMERIC It’s also worth noting that we can absorb only a spirulina, and chia may be hard to find. Buying more WITH BLACK PEPPER MAY certain amount of any nutrient. Even supposed readily available alternatives could help offset some SIGNIFICANTLY IMPROVE Turmeric root’s active ingredient superfoods are beneficial only when consumed in of the environmental concerns around importing ABSORPTION OF THE curcumin is known to have poor moderation, as part of a balanced diet; for example, foods from around the globe. POLYPHENOL bioavailability, which may limit CURCUMIN its effectiveness. CHIA SEEDS SPIRULINA AÇAI GINGER GOJI BERRIES AVOCADO PAPAYA TURMERIC If eaten in the right amount Including omega-3 and -6 fatty Usually sold frozen or Ginger is thought to have These are rich in carotenoids, This fruit is a source of Research suggests its Often consumed as powder, it and within a healthy diet, acids and iron, this algae is powdered due to their short life anti-inflammatory and which act as antioxidants, and monounsaturated fatty acids antioxidant carotenoids, in contains the polyphenol these are a source of fiber, said to have antioxidant and span, these berries contain antioxidant effects, although have benefits relating to vision (linked to blood cholesterol particular lycopene, are curcumin, which may have calcium, and phosphorus. anti-inflammatory properties. antioxidants and fiber. research is limited. and macular degeneration. benefits), vitamin E, and fiber. particularly well absorbed. anti-inflammatory effects. ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE ALTERNATIVE FLAXSEEDS WHOLE MILK BLUEBERRIES N/A SWEET POTATO BANANAS BELL PEPPERS N/A SESAME SEEDS CRANBERRIES CARROTS UUSS__007744--007755__ssuuppeerrffooooddss..iinndddd 7744 2244//0088//22002211 1166::2233 UUSS__007744--007755__ssuuppeerrffooooddss..iinndddd 7755 2255//0088//22002211 1199::1144
76 HOW CAN WE EAT WELL? 77 IS ORGANIC BETTER FOR YOU? Many people now think organic food is safer, healthier, and tastier than regular food. Others say it’s better for the environment and improves animal welfare. You can pay up to 200 percent more for an organic label, but is it all marketing hype? The focus of organic production is usually based on significantly more nutritious than conventionally environmental sustainability and human well-being. farmed foods. This was also the conclusion of the It is easy to see how organic is perceived as healthier, UK Food Standards Agency (FSA), though their the image of wildlife and nature untouched by research considered just 11 studies. (The FSA mankind, it can paint a very vivid picture in our publicly supports consumer choice and is not pro- imaginations of the way we want to see the food or anti-organic food.) we eat in harmony with the world. There are some small nutritional differences in People are willing to pay more for this ideal. organic foods, but they are marginal and won’t play a Organic food comes with a price tag, so while the significant difference in overall health. Some organic more privileged among us may be able to shop products have been found to be a bit higher in organic, for many people, it is simply not an option. phosphorus but lower in protein. Organic cow’s milk may contain higher levels of omega-3 fatty acids, THE EVIDENCE ON ORGANIC iron, and vitamin E than nonorganic but, again, While there may be respected studies that find more less of other nutrients such as selenium and iodine. nutrients in organic foods, a great many others have Agricultural research is renowned for varying found insufficient evidence to recommend organic results. The nutrient content of food depends on over nonorganic for health or safety. One review so many factors, including soil quality, weather of 233 studies concluded that there is no strong conditions, and when the crops are harvested (which evidence to suggest that organic foods are differs throughout the world). The composition of Organic certification TO MAKE AN ORGANIC CLAIM, A PRODUCT MUST FOLLOW STRICT PRODUCTION, HANDLING, AND LABELING STANDARDS, AND GO THROUGH A CERTIFICATION PROCESS. The four distinct labeling categories for organic products are: 100 percent organic—made with 100 percent certified organic ingredients; organic— certified organic, except where specified on the National List of Allowed and Prohibited Substance; made with organic ingredients—at least 70 percent of the product is made with certified organic ingredients; and specific organic ingredients—less than 70 percent certified organic content, doesn’t need to be certified. UUSS__007766--007777__oorrggaanniicc..iinndddd 7766 2244//0088//22002211 1166::2233
7766 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 7777 IS ORGANIC BETTER FOR YOU? Many people now think organic food is safer, healthier, and tastier than regular food. Others say it’s better for the environment and improves animal welfare. You can pay up to 200 percent more for an organic label, but is it all marketing hype? The focus of organic production is usually based on significantly more nutritious than conventionally dairy products and meat can also be affected by environmental sustainability and human well-being. farmed foods. This was also the conclusion of the differences in genetics and what diet animals are It is easy to see how organic is perceived as healthier, UK Food Standards Agency (FSA), though their fed. Even the natural variations in the production the image of wildlife and nature untouched by research considered just 11 studies. (The FSA and handling of foods make comparisons difficult. mankind, it can paint a very vivid picture in our publicly supports consumer choice and is not pro- So the results of these studies should be interpreted imaginations of the way we want to see the food or anti-organic food.) with caution. we eat in harmony with the world. There are some small nutritional differences in Ultimately, there is not enough strong evidence People are willing to pay more for this ideal. organic foods, but they are marginal and won’t play a available to prove that eating organic foods provides Organic food comes with a price tag, so while the significant difference in overall health. Some organic added health benefits when compared to eating more privileged among us may be able to shop products have been found to be a bit higher in conventionally farmed foods. organic, for many people, it is simply not an option. phosphorus but lower in protein. Organic cow’s milk may contain higher levels of omega-3 fatty acids, PESTICIDES THE EVIDENCE ON ORGANIC iron, and vitamin E than nonorganic but, again, Conventional farming relies on the use of chemical While there may be respected studies that find more less of other nutrients such as selenium and iodine. pesticides. Although overall they are safe, we are nutrients in organic foods, a great many others have Agricultural research is renowned for varying advised to wash our produce before we eat it found insufficient evidence to recommend organic results. The nutrient content of food depends on to remove pesticide residues. Some research over nonorganic for health or safety. One review so many factors, including soil quality, weather suggests that high exposure to pesticides BEET of 233 studies concluded that there is no strong conditions, and when the crops are harvested (which in early life may harm cognitive GREENS evidence to suggest that organic foods are differs throughout the world). The composition of development, but findings are mixed. CONTAIN CALCIUM, MAGNESIUM, AND IRON Organic certification TO MAKE AN ORGANIC CLAIM, A PRODUCT MUST FOLLOW STRICT PRODUCTION, HANDLING, AND LABELING STANDARDS, AND GO THROUGH A CERTIFICATION PROCESS. The four distinct labeling categories for organic BEETS products are: 100 percent organic—made with 100 THE ROOT CONTAINS percent certified organic ingredients; organic— FOLATE AND certified organic, except where specified on the CAROTENOIDS National List of Allowed and Prohibited Substance; Full of nutrition Whether produced by conventional made with organic ingredients—at least 70 methods or organic farming, percent of the product is made with certified foods that are unprocessed organic ingredients; and specific organic and ready to be cooked ingredients—less than 70 percent certified organic from scratch are bursting content, doesn’t need to be certified. with nutrients. UUSS__007766--007777__oorrggaanniicc..iinndddd 7766 2244//0088//22002211 1166::2233 UUSS__007766--007777__oorrggaanniicc..iinndddd 7777 2244//0088//22002211 1166::2233
78 HOW CAN WE EAT WELL? 79 SHOULD I CHANGE MY DIET IF I’M EXERCISING? Anyone who’s getting more serious about fitness can learn from sports nutrition for elite athletes—eat the right balance of food groups for your personal training goals. Carbohydrate is the main fuel for any exercise; it carbohydrate, protein will be used for energy promotes strength and endurance, delays muscle instead. Stepping up training can increase appetite, fatigue, and speeds up recovery, meaning fewer so it’s easy to eat more than your body really needs. injuries. It is converted to glucose, and excess is stored as glycogen in the liver and muscles, FAT providing instantly available energy. The longer Dietary fat must be converted into fatty acids and/or more intense the training, the faster glycogen before being taken up by muscles; it acts as support depletes and fatigue can set in. Resistance training fuel during low-intensity endurance exercise like plus protein builds muscle, but without enough long-distance running, when glycogen runs low. Slower availability is also why you should avoid eating fats just before training. At least 15–20 percent of overall calorie intake should come LIGHT from healthy fats. LOW INTENSITY 3g–5g PROTEIN Protein’s primary role is to build, and rebuild, muscle. Aim to eat 0.8g–1g per kg/2lb of body weight daily—in the lower range for endurance and higher MODERATE for strength training. Opt for lean or low-fat foods AROUND 5g–7g like skinless chicken and yogurt. Protein is more 1 HOUR effectively absorbed from food than supplements. ENERGY BALANCE HIGH Signs that your diet isn’t meeting your energy needs include fatigue, poor sleep quality, and 1–3 HOURS, 6g–10g MODERATE–HIGH irregular bowel movements. INTENSITY VERY HIGH Daily carbohydrate needs: 4–5 HOURS, 8g–12g This is measured in grams by MODERATE–HIGH 1kg (2lb) of body weight, based INTENSITY on your individual activity level. UUSS__007788--007799__hhyyddrraattiioonn..iinndddd 7788 2244//0088//22002211 1166::2233
7788 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 7799 SHOULD I CHANGE MY DIET HOW IMPORTANT IS IF I’M EXERCISING? HYDRATION? Anyone who’s getting more serious about fitness can learn from sports nutrition for Proper hydration is essential before, during, and after exercise; elite athletes—eat the right balance of food groups for your personal training goals. otherwise, physical and mental performance can be compromised. Carbohydrate is the main fuel for any exercise; it carbohydrate, protein will be used for energy When the body doesn’t get enough fluids, the liters while training. Weigh yourself before and promotes strength and endurance, delays muscle instead. Stepping up training can increase appetite, blood thickens, making the heart work less after to see how much you drop through fluid fatigue, and speeds up recovery, meaning fewer so it’s easy to eat more than your body really needs. efficiently and increasing heart rate. While loss, taking into account any fluid consumed injuries. It is converted to glucose, and excess is some sweat can be lost without affecting your during exercise. Aim to stay within 2 percent of KIDNEYS stored as glycogen in the liver and muscles, FAT workout, after a point, fluid and sodium loss may your body weight; if you weigh 143lb at the start providing instantly available energy. The longer Dietary fat must be converted into fatty acids impact performance by up to 20 percent. You of your workout, you should weigh no less than Hydration and kidneys: Staying and/or more intense the training, the faster glycogen before being taken up by muscles; it acts as support can feel tired and everything becomes harder. 140lb at the end. This is your “hydration zone.” hydrated helps blood deliver nutrients to depletes and fatigue can set in. Resistance training fuel during low-intensity endurance exercise like After exercise, for every 2lb (1kg) of water kidneys and enables plus protein builds muscle, but without enough long-distance running, when glycogen runs low. HYDRATION ZONE weight lost, gradually drink 5–6 cups of water them to expel waste as urine. Slower availability is also why you should avoid Research shows being properly hydrated over about an hour (the extra 1–2 cups eating fats just before training. At least 15–20 when you start training gives the best chance compensates for increased urination). A little percent of overall calorie intake should come of optimum performance. Drinking 13oz–20oz sodium (salt) in food or drink helps retain fluid. LIGHT from healthy fats. about two hours beforehand gives the body Drinking too much may cause hyponatraemia, LOW INTENSITY 3g–5g time to excrete what you don’t need and makes or abnormally low sodium in the blood, which PROTEIN up for any previous fluid deficit. can lead to blackouts and fits. Be aware that Protein’s primary role is to build, and rebuild, Men should drink around 15.5 cups symptoms (including lethargy, dizziness, and muscle. Aim to eat 0.8g–1g per kg/2lb of body weight (3.7 liters) of water daily and women around nausea) can mimic those of dehydration; if daily—in the lower range for endurance and higher 11.5 cups (2.7 liters), but you can lose 1.5–4 these persist, seek medical attention. MODERATE for strength training. Opt for lean or low-fat foods AROUND 5g–7g like skinless chicken and yogurt. Protein is more 1 HOUR effectively absorbed from food than supplements. Sodium Water Blood cells ENERGY BALANCE Hydration HIGH Signs that your diet isn’t meeting your energy and blood needs include fatigue, poor sleep quality, and 1–3 HOURS, 6g–10g Water in the blood MODERATE–HIGH irregular bowel movements. moves between INTENSITY blood cells and NORMAL SODIUM HYPERNATRAEMIA HYPONATRAEMIA blood plasma in a process called VERY HIGH osmosis—toward When we are well hydrated, If we don’t drink enough, If we drink too much too Daily carbohydrate needs: the highest sodium levels in both sodium levels in plasma rise; quickly, blood sodium 4–5 HOURS, 8g–12g This is measured in grams by concentration of plasma and cells are in response, water moves decreases and osmotic MODERATE–HIGH 1kg (2lb) of body weight, based sodium, in order balanced, ensuring an even from cells into plasma, pressure moves water from INTENSITY on your individual activity level. pressure between them and causing cells to shrink. plasma into cells. These to equalize their supporting normal blood Symptoms include excessive swell, leading to "water sodium levels. volume and supply. thirst and fatigue. intoxication." BLADDER UUSS__007788--007799__hhyyddrraattiioonn..iinndddd 7788 2244//0088//22002211 1166::2233 UUSS__007788--007799__hhyyddrraattiioonn..iinndddd 7799 2244//0088//22002211 1166::2233
80 HOW DO WE EAT WELL? 81 AROUND 1oz (35ml) OF CONCENTRATED BEET JUICE IS EQUIVALENT TO 7oz (200g) OF BEETS EVIDENCE INDICATES THAT BEET JUICE HAS GREATER BENEFITS FOR UNTRAINED, RATHER THAN ELITE, Beet juice: This is a more ATHLETES convenient option than eating the amount of beets required to benefit from nitrates—which is around 7oz (200g). 15 Muscle supporter: 10 This chart shows leucine as a percentage of total protein; you may need to 5 eat more overall calories to obtain it from some foods with lower levels. 0 YEHW KLIM FEEB SGGE HSIF NROC ECIR STAO TAEHW SLITNEL DIETARY SOURCES OF LEUCINE (BRANCHED-CHAIN AMINO ACID) KEY Animal-based leucine Plant-based leucine UUSS__008800--008811__ssuupppplleemmeennttss..iinndddd 8800 2244//0088//22002211 1166::2233
80 HOW DO WE EAT WELL? 81 AROUND 1oz (35ml) OF CONCENTRATED BEET JUICE IS EQUIVALENT TO 7oz (200g) OF BEETS EVIDENCE INDICATES THAT BEET JUICE HAS GREATER BENEFITS FOR UNTRAINED, RATHER THAN ELITE, Beet juice: This is a more ATHLETES convenient option than eating the amount of beets required to benefit from nitrates—which is around 7oz (200g). 15 Muscle supporter: 10 This chart shows leucine as a percentage of total protein; you may need to 5 eat more overall calories to obtain it from some foods with lower levels. 0 YEHW KLIM FEEB SGGE HSIF NROC ECIR STAO TAEHW SLITNEL 80 HOW DO WE EAT WELL? 81 DO I NEED SPORTS SUPPLEMENTS? If you regularly run or cycle long distances, build muscle strength at the gym, or otherwise train hard, supplementing a healthy, balanced diet can be a good way to fill in nutritional gaps, get extra fuel on workout days, or boost performance. For most people who visit the gym or play sports sources should be varied (see pages 128–129). The recreationally, supplements aren’t necessary; they body can use 1g–3g of leucine, or around 20g–40g of may have a place only if you’re on a low-calorie protein, per meal for muscle synthesis (from 0.25g to diet or unable to meet your nutritional needs. More 0.4g of protein per kg/2lb of bodyweight, based on intense exercise increases the body’s need for certain activity levels). While supplements support muscle nutrients, but the International Olympic Committee growth over time, it’s cheaper to eat more protein. says even elite athletes should for the most part be able meet their needs by eating a balanced diet. PROTEIN POWDER Supplements can be a useful addition to enhance Only those with high energy requirements need to performance, although an excess can potentially supplement protein. Whey protein is derived from cause stomach pain, nausea, and constipation. cow’s milk; evidence suggests it’s the best form of protein to take after exercise, being absorbed by the BEET JUICE body more quickly than others such as casein or soy. Beets increase nitrate levels in the blood, which (Plant-based supplements combine soy, pea, and then helps dilate blood vessels and regulate blood rice.) While it’s also a good source of leucine, studies pressure so more nutrients and oxygen can reach show no evidence of greater muscle growth over muscles during exercise, allowing you to sustain 24 hours from taking whey protein, as opposed to higher levels of power for longer. Drink beet juice eating a normal, balanced diet. 2–3 hours before training. Nitrate is also found in veggies like spinach, arugula, broccoli, and cabbage. CREATINE Creatine is found in muscle cells, and supplements BRANCHED-CHAIN AMINO ACIDS (BCAAs) have proven effectiveness in increasing strength and DIETARY SOURCES OF LEUCINE (BRANCHED-CHAIN AMINO ACID) These are found in protein; leucine and isoleucine power, especially for activities involving explosive support muscle growth and repair and glucose movements. Red meat, fish, and poultry contain only uptake into cells, to fuel body and brain functions. small amounts of creatine, so supplementing is an Active people require 0.8g–1g of protein per kg/2lb option for boosting performance, and for vegetarians of body weight per day; there are many food sources and vegans. Of the many types available, creatine of BCAAs, including meat and eggs, although plant monohydrate appears to be an effective option. KEY Animal-based leucine Plant-based leucine UUSS__008800--008811__ssuupppplleemmeennttss..iinndddd 8800 2244//0088//22002211 1166::2233 UUSS__008800--008811__ssuupppplleemmeennttss..iinndddd 8811 2244//0088//22002211 1166::2233
82 HOW CAN WE EAT WELL? 83 WHEN SHOULD I EAT BEFORE EXERCISE? Just as you wouldn’t set off on a long drive very low on fuel, you may need to top off your personal tank before training to get the most from a workout. Your goal is to have enough stored glycogen from carbohydrates when you start training to sustain 30g performance throughout. If you exercise first thing, glycogen in the liver has been depleted overnight, CARBS although some is still present in muscles if your diet = contains enough carbohydrates (see pages 78–79). Exercising on a full stomach can be uncomfortable, as blood is directed away from your digestive system. 1 LARGE BANANA BEFORE EXERCISING The consensus is to eat a meal 2–4 hours before working out, where possible. An ideal preworkout 1 CEREAL BAR meal is mainly carbohydrate with some protein and a little fat, for example, salmon, white rice, and vegetables roasted in olive oil. If you train early and 1 CARB GEL PACK don’t have the time or appetite, try a more carb- heavy meal the night before. If you’re exercising sooner, or need to top off, a snack (like toast and 16oz ISOTONIC SPORTS DRINK honey or fruit salad) 1–2 hours beforehand gives a burst of energy for fuel and is quickly absorbed. With under an hour, stick to liquids like smoothies or sports drinks. Experiment to find the optimal BANANAS timing for your activity, schedule, and digestion. ARE A RICH SOURCE OF POTASSIUM, WHICH DURING EXERCISE HELPS THE BODY STORE CARBOHYDRATES, BUT Water should suffice for 45–75-minute sessions. IS LOST THROUGH After an hour, you may want to consume around SWEAT 30g of carbs per hour for two hours, increasing to 60g per hour for the next 2–2.5 hours. Gels and sports drinks can help maintain blood sugar levels but can also contribute to stomach upset during endurance activities like distance running; practice what works for you. UUSS__008822--008833__eeaattiinngg__eexxeerrcciissee..iinndddd 8822 2244//0088//22002211 1166::2233
8822 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 8833 WHEN SHOULD I EAT IS IT BETTER TO REFUEL BEFORE EXERCISE? RIGHT AFTER EXERCISE? Just as you wouldn’t set off on a long drive very low on fuel, you may need to The more frequently and intensively you train, the more important it is to replenish top off your personal tank before training to get the most from a workout. fluid and fuel afterward to avoid aching muscles, fatigue, and underperformance. Your goal is to have enough stored glycogen from While experts no longer believe refueling must POST carbohydrates when you start training to sustain 30g happen within a 30-minute “anabolic window,” it WORKOUT performance throughout. If you exercise first thing, can be a good idea for recreational athletes to refuel FOR CARBS AND glycogen in the liver has been depleted overnight, CARBS within a couple of hours of exercise. Glycogen levels PROTEIN, A FRUIT although some is still present in muscles if your diet = SMOOTHIE WITH OATS, replenish at 150 percent of the normal rate during contains enough carbohydrates (see pages 78–79). YOGURT, AND MILK this time, and muscle cell membranes are more Exercising on a full stomach can be uncomfortable, IS IDEAL permeable so they can absorb more glucose to as blood is directed away from your digestive system. 1 LARGE restore glycogen levels faster. BANANA BEFORE EXERCISING CARB–PROTEIN BALANCE The consensus is to eat a meal 2–4 hours before Recovery foods should contain quality carbohydrates working out, where possible. An ideal preworkout 1 CEREAL BAR to replenish glycogen and fluid and electrolytes to meal is mainly carbohydrate with some protein rehydrate effectively. In addition, combining a small and a little fat, for example, salmon, white rice, and amount of protein with carbs postworkout has been vegetables roasted in olive oil. If you train early and 1 CARB GEL PACK shown to more effectively promote glycogen recovery don’t have the time or appetite, try a more carb- than carbohydrates alone. Flavored milks, smoothies, heavy meal the night before. If you’re exercising and fruit yogurts all check these boxes. Choose the sooner, or need to top off, a snack (like toast and 16oz ISOTONIC fat and sugar content based on your individual body SPORTS DRINK honey or fruit salad) 1–2 hours beforehand gives composition and energy needs. a burst of energy for fuel and is quickly absorbed. If your training is mainly strength based, or if With under an hour, stick to liquids like smoothies you’re training at a high intensity, there is evidence or sports drinks. Experiment to find the optimal that adding 15g–25g of protein to a postworkout BANANAS timing for your activity, schedule, and digestion. meal or snack can reduce muscle soreness and ARE A RICH SOURCE OF POTASSIUM, WHICH promote muscle repair. (A 150g serving of edamame DURING EXERCISE HELPS THE BODY STORE would provide this.) CARBOHYDRATES, BUT Water should suffice for 45–75-minute sessions. Otherwise, follow your food preferences, appetite, IS LOST THROUGH After an hour, you may want to consume around SWEAT and what sits comfortably in your stomach after 30g of carbs per hour for two hours, increasing to exercise, and eat when you feel hungry. Overall daily 60g per hour for the next 2–2.5 hours. Gels and energy and macronutrient intake is the priority; sports drinks can help maintain blood sugar levels provided you consume enough calories, carbs, and but can also contribute to stomach upset during protein over a 24-hour period, your muscles should endurance activities like distance running; practice recover before exercising again (see pages 78–79). what works for you. UUSS__008822--008833__eeaattiinngg__eexxeerrcciissee..iinndddd 8822 2244//0088//22002211 1166::2233 UUSS__008822--008833__eeaattiinngg__eexxeerrcciissee..iinndddd 8833 2244//0088//22002211 1166::2233
84 HOW CAN WE EAT WELL? 85 WILL I LOSE WEIGHT FASTER IF I EXERCISE? Rapid weight loss is a popular goal, although generally the faster this happens, the less sustainable it is. If you’re already restricting calories, is exercise the missing piece in the puzzle of how to successfully reduce weight long term? Exercise burns extra calories, and expending more some weight-based exercise to build muscle may energy than we consume creates an energy deficit help offset this because muscle uses more energy—if that leads to weight loss. In practice, though, you don’t consume extra calories. Sleep quality and exercising doesn’t guarantee less body fat. One stress also play a role in appetite and activity levels. reason may be unrealistic expectations; a study found people overestimated calories burned CAN I JUST EXERCISE? during exercise by 2–3 times the actual amount. Studies indicate that while aerobic (cardio) exercise burns fat, diet has a bigger impact on weight loss EXERCISE AND METABOLISM than exercise. Combining exercise with diet appears Several investigations have shown huge variations in to be more effective over both the short and long individual responses to exercise—this is because its term. However, exercise is beneficial for maintaining effect on appetite depends on your hormones, body lower weight; people who have done so for five years fat levels, and metabolism. The largest proportion report exercising daily. of energy intake is used for basic physiological What’s not in doubt are the many long-term functions while doing nothing—your resting health benefits of regular exercise. These include metabolic rate (RMR). Physical activity, including significantly reduced risk of heart disease, stroke, exercise, uses 10–30 percent, so its impact is limited. colon cancer, depression, and early death. Rapid weight loss can cause loss of muscle and One study defines successful weight loss as slow your metabolic rate (see pages 92–93). Doing losing at least 10 percent of initial body weight Energy balance equation Weight loss requires a negative energy balance, with fewer calories eaten than used by resting metabolic rate, exercise, other non-exercise activity (NEAT), and the thermic effect of food (TEF), or energy used to digest it. = WEIGHT GAIN NEUTRAL (WEIGHT MANAGEMENT) UUSS__008844--008855__lloossee__wweeiigghhtt__eexxeerrcciissee..iinndddd 8844 2244//0088//22002211 1166::2233
8844 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 8855 1.2 WILL I LOSE WEIGHT Calculate your calorie needs Sedentary Little or 1.3 FASTER IF I EXERCISE? no exercise To get a rough idea of your body’s daily energy needs, Light activity first figure out your resting metabolic rate, then multiply Exercise Rapid weight loss is a popular goal, although generally the faster this happens, the by the appropriate activity factor. 1–3 days/week less sustainable it is. If you’re already restricting calories, is exercise the missing piece in the puzzle of how to successfully reduce weight long term? 1.55 MEN 4.536x 15.88x 5x Moderate activity Exercise weight (lb) Height (in) age WOMEN 3–5 days/week Exercise burns extra calories, and expending more some weight-based exercise to build muscle may energy than we consume creates an energy deficit help offset this because muscle uses more energy—if Example: Male 165lb, 72in, age 40, moderately active: that leads to weight loss. In practice, though, you don’t consume extra calories. Sleep quality and 1.7 (4.536 x 165) + (15.88 x 72) – (5 x 40) + 5 = 1,697kcals exercising doesn’t guarantee less body fat. One stress also play a role in appetite and activity levels. 1,697kcals x 1.55 = 2,630 calories Very active reason may be unrealistic expectations; a study * Based on the Harris-Benedict equation 1.9 Hard exercise found people overestimated calories burned CAN I JUST EXERCISE? 6–7 days/week Extra active during exercise by 2–3 times the actual amount. Studies indicate that while aerobic (cardio) exercise Very hard burns fat, diet has a bigger impact on weight loss exercise daily EXERCISE AND METABOLISM than exercise. Combining exercise with diet appears Several investigations have shown huge variations in to be more effective over both the short and long individual responses to exercise—this is because its term. However, exercise is beneficial for maintaining effect on appetite depends on your hormones, body lower weight; people who have done so for five years and keeping it off for one year. This suggests the eating, or exercising. Small daily changes fat levels, and metabolism. The largest proportion report exercising daily. best approach is slowly increasing the amount and in behavior, like standing up while reading of energy intake is used for basic physiological What’s not in doubt are the many long-term intensity of exercise so it becomes an enjoyable or taking stairs, can all boost NEAT. One functions while doing nothing—your resting health benefits of regular exercise. These include lifestyle habit and your body has time to adapt. research review even estimated higher-level metabolic rate (RMR). Physical activity, including significantly reduced risk of heart disease, stroke, NEAT activities (like washing windows) exercise, uses 10–30 percent, so its impact is limited. colon cancer, depression, and early death. ACTIVE LIVING could burn up to 2,000 calories extra daily. Rapid weight loss can cause loss of muscle and One study defines successful weight loss as Non-exercise activity thermogenesis (NEAT) is But fatigue resulting from exercise can also slow your metabolic rate (see pages 92–93). Doing losing at least 10 percent of initial body weight the energy expended when we aren’t sleeping, reduce NEAT. KEY Energy balance equation Weight loss requires a negative Calorie intake energy balance, with fewer calories eaten than used by NEAT and TEF resting metabolic rate, exercise, other non-exercise activity Physical activity (exercise) (NEAT), and the thermic effect of food (TEF), or Resting metabolic rate energy used to digest it. = WEIGHT GAIN NEUTRAL (WEIGHT = WEIGHT LOSS MANAGEMENT) UUSS__008844--008855__lloossee__wweeiigghhtt__eexxeerrcciissee..iinndddd 8844 2244//0088//22002211 1166::2233 UUSS__008844--008855__lloossee__wweeiigghhtt__eexxeerrcciissee..iinndddd 8855 2244//0088//22002211 1166::2233
86 HOW CAN WE EAT WELL? 87 WHICH EXERCISE IS BETTER AT FAT BURNING? Body fat is distributed within muscles, around organs, and beneath skin, which means you can’t target specific areas of the body. However, you may have heard that some types of exercise are better for burning fat—do such shortcuts really exist? When working out at lower intensities, like walking FASTED CARDIO or steady running, your body will predominantly Cardio exercise, like cycling, power-walking, running, burn fat for fuel. In the gym, the “fat-burning” zone and aerobics, raises your heart rate. Some studies is usually indicated on a heart rate chart on cardio have shown doing cardio on an empty stomach equipment like rowing machines and stair climbers. increases the rate of fat burning, because glycogen Staying within this zone keeps your heart rate, and stores are depleted so the body takes more energy your intensity, relatively low. This burns more from fat. However, this may also cause breakdown calories from stored body fat rather than glycogen of muscle tissue and increased appetite. in the muscles and liver, and glucose in blood (both derived from carbohydrates). However, you’ll burn INTERVAL TRAINING more calories overall—and therefore lose more Combining bursts of high-intensity exercise like weight—by working at a higher intensity for as long sprinting with intervals of less intense activity is and as often as you can safely manage (supported by often promoted as more effective for fat burning diet so you’re in an energy deficit; see pages 84–85). than maintaining a continuous moderate intensity, Target heart rate To ensure you’re working at your target intensity, place your index and middle fingers on your pulse, count for 30 seconds, and multiply by two for your heart rate in BPM (beats per minute). RESTING MAXIMUM MODERATE VIGOROUS HEART RATE HEART RATE EXERCISE EXERCISE ADULTS 220 MINUS AGE BPM AT 65%–75% BPM AT 77%–93% = 60–100 BPM = MAXIMUM BPM OF MAXIMUM OF MAXIMUM EXAMPLE: Age 40 = 180 BPM EXAMPLE = 117–135 BPM EXAMPLE = 139–167 BPM AGE UUSS__008866--008877__ffaatt__bbuurrnniinngg__eexxeerrcciissee..iinndddd 8866 2244//0088//22002211 1166::2233
8866 HHOOWW CCAANN WWEE EEAATT WWEELLLL?? 8877 WHICH EXERCISE IS 100 BETTER AT FAT BURNING? 90 80 Body fat is distributed within muscles, around organs, and beneath skin, which Fat vs glycogen use: 70 means you can’t target specific areas of the body. However, you may have heard that While actual usage depends on the intensity some types of exercise are better for burning fat—do such shortcuts really exist? 60 at which you do any exercise, and individual 50 fitness, broadly speaking, some activities will tend 40 to burn more fuel from When working out at lower intensities, like walking FASTED CARDIO fat stores than glycogen 30 in muscles. or steady running, your body will predominantly Cardio exercise, like cycling, power-walking, running, 20 burn fat for fuel. In the gym, the “fat-burning” zone and aerobics, raises your heart rate. Some studies is usually indicated on a heart rate chart on cardio have shown doing cardio on an empty stomach 10 equipment like rowing machines and stair climbers. increases the rate of fat burning, because glycogen Staying within this zone keeps your heart rate, and stores are depleted so the body takes more energy your intensity, relatively low. This burns more from fat. However, this may also cause breakdown calories from stored body fat rather than glycogen of muscle tissue and increased appetite. in the muscles and liver, and glucose in blood (both derived from carbohydrates). However, you’ll burn INTERVAL TRAINING more calories overall—and therefore lose more Combining bursts of high-intensity exercise like weight—by working at a higher intensity for as long sprinting with intervals of less intense activity is and as often as you can safely manage (supported by often promoted as more effective for fat burning diet so you’re in an energy deficit; see pages 84–85). than maintaining a continuous moderate intensity, Target heart rate To ensure you’re working at your target intensity, place your index and middle fingers on your pulse, count for 30 seconds, and multiply by two for your heart rate in BPM (beats per minute). RESTING MAXIMUM MODERATE VIGOROUS HEART RATE HEART RATE EXERCISE EXERCISE ADULTS 220 MINUS AGE BPM AT 65%–75% BPM AT 77%–93% = 60–100 BPM = MAXIMUM BPM OF MAXIMUM OF MAXIMUM EXAMPLE: Age 40 = 180 BPM EXAMPLE = 117–135 BPM EXAMPLE = 139–167 BPM AGE UUSS__008866--008877__ffaatt__bbuurrnniinngg__eexxeerrcciissee..iinndddd 8866 2244//0088//22002211 1166::2233 YLPPUS LEUF % KLAW REWOP GNINIART DESAB-THGIEW NUR ECNATSID-GNOL ELCYC RUOH-1 DRAH TNIRPS m001 Target heart rate Choose intensity: The ideal range for fat 200 burning is working at around 65%–85% of 180 maximum heart rate. 160 140 120 100 80 25 30 35 40 45 50 55 60 65 70 )MPB( ETAR TRAEH for instance, by jogging. However, a 2017 analysis of 31 studies found high-intensity interval training resulted in similar body fat reduction as continuous moderate-intensity training—and maintaining a moderate pace for longer was more beneficial than shorter periods of interval training. Overall, evidence suggests interval training is better for improving overall fitness and respiratory and cardiovascular health than burning fat—unless you can sustain it for KEY long enough. Glycogen/glucose SWITCH INTENSITIES Fat It’s sensible to mix up different intensities in your workout, to keep it interesting and to avoid injury. While weight-based resistance training is used to build muscle rather than burn fat, having more muscle can promote better fat burning when you’re not exercising (see pages 84–85). By keeping your heart rate within this range, you will predominantly be using your fat stores for energy KEY Anaerobic (muscle building) Aerobic (fat burning) Endurance building Warm up AGE UUSS__008866--008877__ffaatt__bbuurrnniinngg__eexxeerrcciissee..iinndddd 8877 2244//0088//22002211 1166::2233
UUSS__008888--008899__cchhaapptteerr__33..iinndddd 8888 2244//0088//22002211 1166::2233
SHOULD I GO ON A DIET? Is It Good to Be Slim? 90 Are Mealtimes Important? 102 Is There a Limit to How Much Weight I Can Lose? 92 Is Snacking Bad for You? 103 If Diets Don’t Work, What Does? 94 Why Do I Feel Hungry All the Time? 104 How Do I Find a Weight-Loss Can My Gut Bacteria Help Me Lose Weight? 106 Program That Works for Me? 96 Can Liquid Foods Help Me Lose Weight? 108 Do I Need to Gain Weight? 98 Should I Cut or Reduce Carbs? 110 Should I Count Calories? 100 What Is a Detox Diet and Detox Tea? 112 Can I Rely on the Scale? 101 Is It OK to Skip Meals? 114 UUSS__008888--008899__cchhaapptteerr__33..iinndddd 8888 2244//0088//22002211 1166::2233 UUSS__008888--008899__cchhaapptteerr__33..iinndddd 8899 2255//0088//22002211 1199::1144
90 SHOULD I GO ON A DIET? 91 IS IT GOOD TO BE SLIM? ‘‘Slim’’ is just one of many cultural constructs about how our bodies should look. But achieving these ideals and being a healthy weight aren’t necessarily related. Body size and shape aren’t always the best relative to your height and build. For indicator of health. Someone who eats a less example, being overweight makes you three healthy diet and does little exercise may be times more likely to develop type 2 diabetes genetically predetermined to have less body (seven times for those with obesity), while fat than someone in a larger body leading a people who are overweight in middle age healthier lifestyle. Scientists have identified have a 35 percent increased risk of Alzheimer’s specific DNA variants common to people disease. Obesity is a risk factor for poor with obesity, and in 2019 researchers found sleep, joint and bone problems, chronic that obese people have a higher genetic risk noncommunicable diseases like cancer and score than those at a “normal” weight. coronary heart disease, and mental health A large body of evidence indicates that disorders. In contrast, healthy weight you are less likely to experience poor long- individuals have improved fertility and term health if you don’t carry too much fat greater chances of conceiving (see page 182). Visceral fat around internal organs is a risk marker for many chronic conditions Subcutaneous fat sits just under the skin and is visible “Slim” outside: Excess abdominal fat can develop due to genetics, lifestyle, and diet, even if someone appears to be a healthy weight. UUSS__009900--009911__sslliimm..iinndddd 9900 2244//0088//22002211 1166::2233
9900 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 9911 IS IT GOOD TO BE SLIM? ‘‘Slim’’ is just one of many cultural constructs about how our bodies Measuring BMI UNDERWEIGHT RUNNING BACK AVERAGE HEIGHT 72.8IN should look. But achieving these ideals and being a healthy weight < 18.5 AVERAGE WEIGHT 212LB aren’t necessarily related. BMI expresses weight relative BMI = 28 to height; however, very fit NORMAL Being nimble and very people like professional football lean can still equate to players can have BMIs that 18.5–24.9 an overweight BMI. Body size and shape aren’t always the best relative to your height and build. For appear unhealthy due to a high muscle-to-fat ratio. To calculate indicator of health. Someone who eats a less example, being overweight makes you three OVERWEIGHT BMI, divide weight in pounds healthy diet and does little exercise may be times more likely to develop type 2 diabetes by height in square inches and WIDE RECEIVER 25–29.9 genetically predetermined to have less body (seven times for those with obesity), while multiply by 703: 212lb / (72.8in AVERAGE HEIGHT 74.8IN fat than someone in a larger body leading a people who are overweight in middle age x 72.8in) x 703 = 28. AVERAGE WEIGHT 243LB healthier lifestyle. Scientists have identified have a 35 percent increased risk of Alzheimer’s OBESE BMI = 30.5 specific DNA variants common to people disease. Obesity is a risk factor for poor 30–34.9 Taller and fast, with more with obesity, and in 2019 researchers found sleep, joint and bone problems, chronic muscle strength, they can be classed as obese. that obese people have a higher genetic risk noncommunicable diseases like cancer and OBESE PLUS score than those at a “normal” weight. coronary heart disease, and mental health CO-MORBIDITIES A large body of evidence indicates that disorders. In contrast, healthy weight >35 OFFENSIVE LINEMAN you are less likely to experience poor long- individuals have improved fertility and AVERAGE HEIGHT 72.8 term health if you don’t carry too much fat greater chances of conceiving (see page 182). AVERAGE WEIGHT 265LB BMI = 35.2 Solid, often shorter to bear weight in scrums, their score may be "dangerously" high. Visceral fat around "HEALTHY" WEIGHT conditions. While both are used to screen for internal organs is a There are different measures of “healthy” and potential risk, they aren’t diagnostic tools. risk marker for many chronic conditions “overweight.” BMI (body mass index) is widely used Instead of using weight or measures like BMI by health professionals and has a strong correlation to define well-being, an alternative “health at with various diseases and chronic conditions. every size” approach focuses on sustainable health- However, it doesn’t reflect the amount of fat or promoting behavior regardless of body size. muscle in a body, bone weight, or cultural factors, Proponents argue a weight-neutral approach is age, and gender (women tend to carry more fat). healthier because it avoids the potentially harmful Subcutaneous fat Waist circumference is another commonly used effects of repeated dieting, such as a higher risk of sits just under the skin and is visible measure because excess abdominal fat—more than early death and psychological distress. Ultimately, 40 inches for men and 35 inches for women— nutrition and health are socioeconomic issues that increases the risk of developing obesity-related require understanding and empathy. “Slim” outside: Excess abdominal fat can develop due to genetics, lifestyle, and diet, even if someone appears to be a healthy weight. UUSS__009900--009911__sslliimm..iinndddd 9900 2244//0088//22002211 1166::2233 UUSS__009900--009911__sslliimm..iinndddd 9911 2255//0088//22002211 1199::1144
92 SHOULD I GO ON A DIET? 93 IS THERE A LIMIT TO HOW MUCH WEIGHT I CAN LOSE? Numerous studies have observed that when some people try to lose weight, their body appears to hold onto fat and may not let them either reach a target weight or sustain it long term. As yet, scientists don’t know precisely why this happens. For many people, trying to lose weight works relatively fast increase in Western populations’ body initially, then they hit a plateau and stop, or they weight and adiposity (excess fat) since the 1980s, or reach their target weight only to find it rises again why obesity levels vary with socioeconomic status. and they end up gaining pounds. Analysis of 29 long-term weight-loss studies found that more than SETTLING POINT half of lost weight was regained within two years, This newer theory says body weight gradually and over 80 percent within five. settles at a level reflecting our genetics but also any significant changes in our diet, activity, SET POINT environment, lifestyle, and stress levels. There is Set point theory says this happens because we have research supporting both theories; taken together, a narrow, genetically determined range of weight they suggest that we have a predetermined weight the body is programmed to protect. When the brain range, but other factors can influence and change it. detects fat levels have fallen below an established level, it adjusts certain hormones in order to burn CAN I RESET? energy more slowly and increase calorie intake. Based on current evidence, weight loss can be (These include leptin, a hormone in fat cells that maintained, but a gradual process is more effective. helps suppress appetite, and ghrelin, which Losing a pound or two a week allows your body to stimulates it.) As weight is regained, the weight adapt and is sustainable in the long run. This means range is set higher to protect fat stores. Scientists eating fewer calories than you use but in a way that’s think these compensating mechanisms may keep realistic, for example, by adding more vegetables to working for up to a year. Some research also suggests your diet and walking more rather than spending repeated dieting means we develop resistance to hours in the gym every day. Studies show increased key hormones, making it even harder to lose weight. physical activity positively affects body composition, The set point concept is supported by significant but weight loss is individual and your body may observational study. But it doesn’t explain the react differently from someone else’s. UUSS__009922--009933__sseett__ppooiinntt..iinndddd 9922 2244//0088//22002211 1166::2233
9922 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 9933 IS THERE A LIMIT TO HOW REACH MUCH WEIGHT I CAN LOSE? SET POINT Weight loss Metabolism slows stops You restrict Numerous studies have observed that when some people try to lose weight, their body to conserve calories You feel energy further appears to hold onto fat and may not let them either reach a target weight or sustain hungrier/less satisfied Further it long term. As yet, scientists don’t know precisely why this happens. decrease in leptin/ increase in ghrelin For many people, trying to lose weight works relatively fast increase in Western populations’ body initially, then they hit a plateau and stop, or they weight and adiposity (excess fat) since the 1980s, or Leptin Appetite levels fall, reach their target weight only to find it rises again why obesity levels vary with socioeconomic status. ghrelin How the set point works increases and they end up gaining pounds. Analysis of 29 rises long-term weight-loss studies found that more than SETTLING POINT Losing weight rapidly can half of lost weight was regained within two years, This newer theory says body weight gradually prompt changes designed to and over 80 percent within five. settles at a level reflecting our genetics but also slow down energy usage and Brain any significant changes in our diet, activity, (hypothalamus) increase hunger, and increase SET POINT environment, lifestyle, and stress levels. There is detects weight your set weight range. loss Metabolism Set point theory says this happens because we have research supporting both theories; taken together, slows further, burns fewer a narrow, genetically determined range of weight they suggest that we have a predetermined weight calories the body is programmed to protect. When the brain range, but other factors can influence and change it. detects fat levels have fallen below an established Give into level, it adjusts certain hormones in order to burn CAN I RESET? Fat burns cravings, first eat more energy more slowly and increase calorie intake. Based on current evidence, weight loss can be (These include leptin, a hormone in fat cells that maintained, but a gradual process is more effective. helps suppress appetite, and ghrelin, which Losing a pound or two a week allows your body to Eat fewer START DIET stimulates it.) As weight is regained, the weight adapt and is sustainable in the long run. This means calories range is set higher to protect fat stores. Scientists eating fewer calories than you use but in a way that’s think these compensating mechanisms may keep realistic, for example, by adding more vegetables to KEY Brain keeps working for up to a year. Some research also suggests your diet and walking more rather than spending metabolic rate slow to protect repeated dieting means we develop resistance to hours in the gym every day. Studies show increased Fat fat stores key hormones, making it even harder to lose weight. physical activity positively affects body composition, Weight The set point concept is supported by significant but weight loss is individual and your body may Leptin increases, raising set point range observational study. But it doesn’t explain the react differently from someone else’s. Ghrelin Metabolism HIGHER SET Appetite POINT Energy intake UUSS__009922--009933__sseett__ppooiinntt..iinndddd 9922 2244//0088//22002211 1166::2233 UUSS__009922--009933__sseett__ppooiinntt..iinndddd 9933 2244//0088//22002211 1166::2233
94 SHOULD I GO ON A DIET? 95 IF DIETS DON’T WORK, WHAT DOES? "Dieting" is often unsustainable, with one analysis of 14 popular diet programs finding that weight loss was mostly reversed after 12 months. By understanding how the concept affects our thinking, we can identify more effective approaches. As babies, we are in tune with our body’s hunger detrimental psychological effects, as veering “off signals and eat only what we need, but as we age, plan” can prompt feelings of failure and guilt and we’re surrounded by messaging and social pressures subsequent overeating. around food and lose this innate ability. The complex psychology surrounding our relationship SLOW AND STEADY with food undoubtedly plays a role in the failure of Studies show that slow, steady weight loss over diets and can be the biggest barrier to weight loss. a longer time period is the most effective way to Research shows that restrained eaters experience lose body fat. According to one study, sustained more intense food cravings, heightened emotions adherence to a diet—rather than following a surrounding food, and greater preoccupation with certain type of diet—is the key to successful weight it. Likewise, categorizing foods as “good” or “bad” management. Instead of cutting out entire meals creates a restrictive mindset that increases food and risking fatigue and cravings, aim to control cravings and, in turn, the risk of overeating these your portion sizes at each meal and choose healthier foods when they are available. Labeling foods as snacks between meals. Eating more variety is also treats implies they can be eaten only once earned, thought to enhance weight loss; try different types which increases desire. Goal setting can also have of meals and vegetables each day. START DIET FEEL FEEL RESTRICTED/ WEIGHT WEIGHT FRUSTRATED RISES REDUCES CRAVINGS WEIGHT LOSS SLOWS/STOPS The psychological diet cycle Dieting can set up a mindset of FEEL GUILTY, GIVE IN, succeeding or failing, which may EAT MORE OVEREAT then lead to potentially harmful yo-yo dieting. UUSS__009944--009955__ddiieettss__ddoonntt__wwoorrkk..iinndddd 9944 2244//0088//22002211 1166::2233
9944 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 9955 IF DIETS DON’T WORK, ENJOY WHAT DOES? HIGHER-CALORIE FOODS LIKE CHEESE BY EATING A SMALL AMOUNT AS "Dieting" is often unsustainable, with one analysis of 14 popular diet programs finding PART OF A BALANCED that weight loss was mostly reversed after 12 months. By understanding how the MEAL concept affects our thinking, we can identify more effective approaches. As babies, we are in tune with our body’s hunger detrimental psychological effects, as veering “off WHAT WORKS signals and eat only what we need, but as we age, plan” can prompt feelings of failure and guilt and Eat a healthy, balanced diet we’re surrounded by messaging and social pressures subsequent overeating. (see pages 40–41). around food and lose this innate ability. The Practice portion control. complex psychology surrounding our relationship SLOW AND STEADY Eat less-healthy foods that with food undoubtedly plays a role in the failure of Studies show that slow, steady weight loss over you enjoy only occasionally. diets and can be the biggest barrier to weight loss. a longer time period is the most effective way to Listen to your body—eat when you’re Research shows that restrained eaters experience lose body fat. According to one study, sustained hungry and stop when you’re full. more intense food cravings, heightened emotions adherence to a diet—rather than following a Exercise regularly, ideally 30 surrounding food, and greater preoccupation with certain type of diet—is the key to successful weight minutes daily. it. Likewise, categorizing foods as “good” or “bad” management. Instead of cutting out entire meals Try some strategies to manage creates a restrictive mindset that increases food and risking fatigue and cravings, aim to control stress; it increases the hormone cortisol, cravings and, in turn, the risk of overeating these your portion sizes at each meal and choose healthier which lowers blood sugar and increases ALONGSIDE foods when they are available. Labeling foods as snacks between meals. Eating more variety is also food cravings. FAT, A THUMB-SIZE treats implies they can be eaten only once earned, thought to enhance weight loss; try different types Get enough sleep; this will help PIECE OF HARD CHEESE which increases desire. Goal setting can also have of meals and vegetables each day. control cortisol. PROVIDES AROUND 180mg OF CALCIUM AND 8g OF PROTEIN START DIET FEEL FEEL RESTRICTED/ WEIGHT WEIGHT FRUSTRATED RISES REDUCES CRAVINGS GOAT CHEESE, BRIE, AND CAMEMBERT SHOULD WEIGHT LOSS BE EATEN LESS OFTEN SLOWS/STOPS THAN LOWER FAT AND The psychological diet cycle SALT OPTIONS LIKE Dieting can set up a mindset of FEEL GUILTY, GIVE IN, succeeding or failing, which may RICOTTA AND COTTAGE “Bad” food: Cheese is often ruled out EAT MORE OVEREAT then lead to potentially harmful CHEESE when trying to lose weight, but its fat yo-yo dieting. content can help you feel satisfied and it contains protein, vitamins, and minerals. UUSS__009944--009955__ddiieettss__ddoonntt__wwoorrkk..iinndddd 9944 2244//0088//22002211 1166::2233 UUSS__009944--009955__ddiieettss__ddoonntt__wwoorrkk..iinndddd 9955 2244//0088//22002211 1166::2233
96 SHOULD I GO ON A DIET? 97 HOW DO I FIND A WEIGHT- LOSS PROGRAM THAT WORKS FOR ME? Rather than following a formal diet plan, fixating on calories, or avoiding particular foods, reaching and maintaining a healthy weight means finding a way of eating that works for your needs—maybe with some professional help. Although there are many weight-loss plans to at how participants respond to different foods, choose from, we don’t all respond to one-size-fits-all the makeup of their gut bacteria, and whether they diets in the same way. A 2020 study into the three should eat to suit their DNA. Results may lead to main types of programs (low carbohydrate, high more research supporting personalized nutrition protein, and low fat) concluded that none is more and eating for better health outcomes. effective and results varied with each individual. Meanwhile, the ideal nutritional plan should Other research has demonstrated that the quality provide flexibility and room for socializing, address of the food we eat is an important factor in emotional and physical needs, and help you feel achieving and maintaining a healthy weight. good. It will consider: “A calorie is a calorie” doesn’t tell the whole What “healthy” means to you—maybe eating story; we need to consider each food’s overall more greens or just less sugar nutritional facts and our habits over time. How active you are and how often you exercise Your food preferences, knowledge, and concerns PERSONALIZING NUTRITION Your physical and mental health, including sleep Studies on individualized nutrition are rare, but quality, hormone function, and anxiety levels scientists conducting a large-scale and long-term Your social, cultural, and family influences observational study called Project 10K are looking Realistic goals and support Keep a food diary This is a good way to learn more about your personal eating patterns. WHICH FOOD HOW FILLING WHAT TIME It will help you understand how different people, Be specific Full and satisfied? Timing triggers times, events, and List each intake in detail, for Score/10 your enjoyment Was it a planned meal, your mood, affect example, "Cup of coffee: 1 tsp and how hungry you felt a regular snack, or an sugar, 2tsp fat-free milk." your intake. before and after eating. impulse? UUSS__009966--009977__wweeiigghhtt__lloossee__ccoossttss..iinndddd 9966 2244//0088//22002211 1166::2233
9966 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 9977 HOW DO I FIND A WEIGHT- LOSS PROGRAM THAT WORKS FOR ME? Rather than following a formal diet plan, fixating on calories, or avoiding particular 10 foods, reaching and maintaining a healthy weight means finding a way of eating that works for your needs—maybe with some professional help. 8 6 Although there are many weight-loss plans to at how participants respond to different foods, choose from, we don’t all respond to one-size-fits-all the makeup of their gut bacteria, and whether they 4 diets in the same way. A 2020 study into the three should eat to suit their DNA. Results may lead to main types of programs (low carbohydrate, high more research supporting personalized nutrition 2 protein, and low fat) concluded that none is more and eating for better health outcomes. 0 1 2 3 4 5 6 effective and results varied with each individual. Meanwhile, the ideal nutritional plan should Other research has demonstrated that the quality provide flexibility and room for socializing, address of the food we eat is an important factor in emotional and physical needs, and help you feel achieving and maintaining a healthy weight. good. It will consider: “A calorie is a calorie” doesn’t tell the whole What “healthy” means to you—maybe eating story; we need to consider each food’s overall more greens or just less sugar KEY nutritional facts and our habits over time. How active you are and how often you exercise Glucose level in blood Your food preferences, knowledge, and concerns PERSONALIZING NUTRITION Your physical and mental health, including sleep Average Studies on individualized nutrition are rare, but quality, hormone function, and anxiety levels Meal eaten scientists conducting a large-scale and long-term Your social, cultural, and family influences observational study called Project 10K are looking Realistic goals and support Keep a food diary This is a good way to learn more about your personal eating patterns. WHICH FOOD HOW FILLING WHAT TIME It will help you understand how different people, Be specific Full and satisfied? Timing triggers times, events, and List each intake in detail, for Score/10 your enjoyment Was it a planned meal, your mood, affect example, "Cup of coffee: 1 tsp and how hungry you felt a regular snack, or an sugar, 2tsp fat-free milk." your intake. before and after eating. impulse? UUSS__009966--009977__wweeiigghhtt__lloossee__ccoossttss..iinndddd 9966 2244//0088//22002211 1166::2233 EKATNI DOOF EKATNI DOOF PROFESSIONAL ADVICE Registered dietitian nutritionists and qualified nutritionists can provide guidance on losing weight sustainably, as well as helping with other health and fitness issues. You’ll be questioned in depth about HOURS AFTER EATING your health and history with food and what your goals are, and you may be asked to keep a food or lifestyle journal. Write down any questions you have beforehand, as it can be overwhelming discussing your life and eating habits at first. You should leave Varying responses to food: A study of more than 1,000 your session with notes, advice on next steps, and, people found eating the same in most cases, a scheduled follow-up appointment. meal caused a much bigger blood sugar spike for some, Professional advice can be expensive; alternative showing the ideal nutritional plan should take individual options include self- and group-help programs and biology into account to be online planning tools. sustainable long term. WHAT YOU WHERE WHO WAS THERE WERE DOING HOW YOU FEEL Location eating Being polite? Food or fuel? Food and mood We often eat Over-/undereating can be Are you refueling at the Note how you were feeling habitually in linked to certain people, gym or busy at work? before consuming and if it certain places. events, or social customs. affected your mood. )l/lomm( LEVEL ESOCULG EFFECT OF FOOD ON DIFFERENT PEOPLE’S BLOOD GLUCOSE UUSS__009966--009977__wweeiigghhtt__lloossee__ccoossttss..iinndddd 9977 2244//0088//22002211 1166::2233
98 SHOULD I GO ON A DIET? 99 7 PEANUT BUTTER AMOUNT 2tbsp CALORIES 184 6 TOTAL 797 HONEY AMOUNT 1 1/2tbsp CALORIES 98 5 MIXED SEEDS TOTAL 613 AMOUNT 1/4 cup CALORIES 180 4 TOTAL 515 5% FAT GREEK YOGURT AMOUNT 2tsp CALORIES 93 3 STRAWBERRIES TOTAL 335 AMOUNT 1/2 cup CALORIES 26 2 WHOLE MILK TOTAL 242 AMOUNT 3oz CALORIES 66 1 OATS TOTAL 216 AMOUNT 1/4 cup CALORIES 150 TOTAL 150 Build a bowl By adding relatively small amounts of nutrient-dense foods to a meal, in this case breakfast, you can boost your calorie intake without having to eat too much. UUSS__009988--009999__ggaaiinn__wweeiigghhtt..iinndddd 9988 2244//0088//22002211 1166::2233
9988 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 9999 7 DO I NEED TO GAIN WEIGHT? PEANUT BUTTER AMOUNT 2tbsp CALORIES 184 Labels like "too thin" and "skinny" can be just as toxic as those put onto heavier 6 people, especially if you are naturally in a smaller body. However, adding fat or muscle to your body is not as simple as increasing portion sizes. TOTAL 797 HONEY AMOUNT 1 1/2tbsp CALORIES 98 Gaining weight is a complex issue. BMI (body mass Weighing too little can lead to fatigue and health 5 index) is a standard measure for whether someone problems like a weakened immune system, fragile MIXED SEEDS TOTAL 613 is overweight, healthy weight, or—if the score is less bones, and missed periods. This may be because AMOUNT 1/4 cup than 18.5—underweight. However, a 2019 study you are not eating enough to get enough key CALORIES 180 of 2,000 subjects with BMIs under 18 found that nutrients, such as calcium. Some people may not 4 75 percent of them were genetically predisposed even be aware they are under their optimal body to be underweight and were examples of “healthy weight; warning signs that you may be include: TOTAL 515 5% FAT GREEK thinness.” So you may have no need to gain weight, Suppressed appetite YOGURT even if your BMI indicates you should. (Although Irregular bathroom habits, especially going less AMOUNT 2tsp someone with a low BMI who appears healthy may Thinning hair, hair loss, or dry skin CALORIES 93 3 still need to put on weight for optimal health.) Regularly feeling unwell STRAWBERRIES TOTAL 335 THE ROLE OF GENETICS GAIN WEIGHT SAFELY AMOUNT 1/2 cup Some people are naturally underweight or smaller Simply eating more won’t work; the types of food CALORIES 26 because their bodies are not set up to store adipose you eat and your activity levels also need to adjust. 2 tissue, where lipids (fats) accumulate. Genes also Talk to your doctor or a registered dietitian about WHOLE MILK play a role in our ability to grow muscles, which gaining weight safely and sustainably. Broadly TOTAL 242 AMOUNT 3oz then affects how rapidly we burn energy. It’s often speaking, aim for 1–2lb of weight gain a week and CALORIES 66 said that people in smaller bodies have great do some low-intensity exercise. Eating more to gain metabolisms; in fact, the larger and more muscular weight can be challenging; to reduce portions, add 1 you are, the harder your metabolism works and the some nutrient-dense foods that are high in calories, OATS TOTAL 216 more calories you burn. Smaller people’s capacity fat, or sugar, into a balanced diet, such as: AMOUNT 1/4 cup for gaining weight partially depends upon their Starchy (ideally whole grain) carbs like potatoes, CALORIES 150 muscle mass. If this is high, they can find it more bread, pasta, and rice difficult to put on weight. Full-fat milk (until your weight begins to rise) Unsaturated oils and spreads TOTAL 150 HEALTH IMPACT Nuts, seeds, and avocados for healthy fats Genetics aside, you may be under a healthy weight Beans, eggs, pulses, meat, and fish for protein for your body for other reasons, such as stress, Yogurts, homemade milkshakes, and puddings illness, or an eating disorder (see pages 32–33). for protein and calories. Build a bowl By adding relatively small amounts of nutrient-dense foods to a meal, in this case breakfast, you can boost your calorie intake without having to eat too much. UUSS__009988--009999__ggaaiinn__wweeiigghhtt..iinndddd 9988 2244//0088//22002211 1166::2233 UUSS__009988--009999__ggaaiinn__wweeiigghhtt..iinndddd 9999 2244//0088//22002211 1166::2233
100 SHOULD I GO ON A DIET? 101 SHOULD I COUNT CALORIES? Counting helps build awareness of our daily energy consumption. But food is more than calories, and reducing it to a number risks oversimplifying its nourishment. Monitoring calories can help you reach the energy deficit needed to lose weight. However, not all calories are nutritionally equal. Also, the body metabolizes foods differently; for instance, it may absorb more calories from a corn tortilla than from an equivalent amount of sweet corn (some of which may be visible in your toilet the next day!). In addition, calories may be more available to one person than to another. UNHEALTHY COUNTING As well as being time consuming, calorie counting can lead to restrictive behaviors or unhealthy habits. It may be tempting to eat highly processed foods because calories are clearly displayed on packaging and easier to count, or to exclude nutrient-dense foods like oily fish and nuts purely on the basis of their calorie content. TRACKING TOOLS Many people count using a fitness tracker; while these encourage regular activity, one study found Unequal calories that popular brands can overestimate the number One hard candy contains similar of calories burned while walking by more than 50 calories to six percent. Relying strictly on these tools, rather than strawberries but is mostly sugar. using them to broadly gauge your energy intake and expenditure, could result in overconsumption. Some apps are designed for continuous engagement, which may encourage compulsive logging. If you choose to count, do it within a healthy, balanced diet and listen to your body’s hunger cues. UUSS__110000--110011__ccaalloorriieess..iinndddd 110000 2244//0088//22002211 1166::2233
110000 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 110011 SHOULD I COUNT CAN I RELY ON THE SCALE? CALORIES? If you’re trying to lose weight, stepping onto the scale feels like a moment of truth. But the number doesn’t show what’s actually Counting helps build awareness of our daily energy consumption. happening in your body. But food is more than calories, and reducing it to a number risks oversimplifying its nourishment. Weighing yourself is a simple way to monitor your In addition, overreliance on the scale could behavior and make small adjustments to improve contribute to an unhealthy body image and Monitoring calories can help you reach the energy your energy balance (see pages 84–85). A research relationship with food, and for some people it deficit needed to lose weight. However, not all review indicated that daily weighing is associated can become a crutch. calories are nutritionally equal. Also, the body with more weight loss than less frequent weighing. metabolizes foods differently; for instance, it may However, there are a number of reasons it’s best WHAT DOESN’T THE SCALE TELL ME? absorb more calories from a corn tortilla than from not to rely on the scale for measuring weight loss. Be mindful that a weight reading doesn’t reflect an equivalent amount of sweet corn (some of which how much body fat you are carrying, your overall may be visible in your toilet the next day!). In WEIGHT VARIABLES body composition, or how healthy you are. Even if addition, calories may be more available to one Your weight can fluctuate after just one meal, and the number on the scale isn’t falling, you could still person than to another. 2–6lb over a day. We’re often heavier in the evening be losing body fat, gaining muscle, sleeping better, after eating and drinking; salt, alcohol, medication, and improving your gut health (see pages 48–51). UNHEALTHY COUNTING and menstruation can also cause water retention. So-called smart scales claim to measure body fat As well as being time consuming, calorie counting One study found that weight is highest after the by sending a tiny electrical current through the body. can lead to restrictive behaviors or unhealthy habits. weekend and the ideal weigh-in time is Wednesday Unfortunately, it’s not that simple; for example, if It may be tempting to eat highly processed foods morning before consuming anything. If you want to you are dehydrated, these can overestimate the because calories are clearly displayed on packaging establish a rough base weight, use the same scale, at amount of body fat. Most research conducted has and easier to count, or to exclude nutrient-dense the same time, without clothes. concluded that they aren’t accurate. foods like oily fish and nuts purely on the basis of their calorie content. TRACKING TOOLS Many people count using a fitness tracker; while these encourage regular activity, one study found Unequal calories that popular brands can overestimate the number One hard candy contains similar of calories burned while walking by more than 50 calories to six percent. Relying strictly on these tools, rather than strawberries but is mostly sugar. using them to broadly gauge your energy intake and 1LB expenditure, could result in overconsumption. Some apps are designed for continuous engagement, which FAT Fat vs muscle 1LB Muscle is more dense and may encourage compulsive logging. If you choose to takes up less space than count, do it within a healthy, balanced diet and listen MUSCLE fat; with more muscle, you weigh more but look leaner. to your body’s hunger cues. UUSS__110000--110011__ccaalloorriieess..iinndddd 110000 2244//0088//22002211 1166::2233 UUSS__110000--110011__ccaalloorriieess..iinndddd 110011 2244//0088//22002211 1166::2233
102 SHOULD I GO ON A DIET? 103 ARE MEALTIMES IMPORTANT? Whether it’s making time for breakfast in the morning or sitting down with family to eat dinner in the evening, research suggests that when and how we eat affects both our mental and physical health. OPTION 1 OPTION 2 OPTION 3 REGULAR MEAL AT BREAKFAST SMALL MEAL AT BREAKFAST REGULAR MEAL IN MORNING REGULAR MEAL AT LUNCH REGULAR MEAL AT LUNCH REGULAR MEAL IN AFTERNOON SMALL MEAL AT DINNER SMALL MEAL AT DINNER A number of studies have shown that it is healthier Appetite and energy expenditure follow the to eat meals at the same time each day. Evidence circadian rhythm, the natural “body clock” that suggests that a regular meal pattern that includes regulates when we wake, sleep, and many things in eating breakfast, having 2–3 meals a day, and between. At the same time, the timing of your meals consuming a greater proportion of your daily energy may have an effect on your circadian rhythm. intake earlier on in the day may have physiological Personalizing your mealtimes to align with your benefits such as reduced inflammation and improved sleep cycle can improve health and limit weight gain. stress resistance. Regular mealtimes provide a sense of rhythm and familiarity, which can have real MAKING THE MOST OF MEALTIMES psychological benefits. Aim to eat sitting at a table and upright in a chair. This position allows your stomach to empty, helping digestion. It also encourages us to eat more Breakfast mindfully (see pages 206–207) and notice feelings of satiety and fullness (see page 104).Even better is A RECENT STUDY LOOKING AT IRREGULAR sitting down to eat with friends or family for some BREAKFAST EATING FOUND THAT THOSE AGED BETWEEN 13–18 YEARS WERE THE social interaction. Having a conversation during a MOST LIKELY TO SKIP BREAKFAST, meal slows down the speed at which you eat, with 22 percent of participants in this age group meaning you are more likely to notice feelings of consuming breakfast on 2 days or fewer out of 4. fullness before overeating. Conversely, eating dinner Previous studies have suggested that differences in family structure, ethnicity, socioeconomic in front of the television not only eliminates the status, time constraints, and lack of food opportunity for social interaction but also distracts enjoyment can all influence this. us from what we are eating, with research suggesting this can cause us to eat more than we need. UUSS__110022--110033__mmeeaallttiimmeess..iinndddd 110022 2244//0088//22002211 1166::2233
110022 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 110033 ARE MEALTIMES IS SNACKING IMPORTANT? BAD FOR YOU? Whether it’s making time for breakfast in the morning or sitting down Snacking is an often-vilified habit, but it can be practiced with family to eat dinner in the evening, research suggests that when and in a way that helps, rather than hinders, good nutrition. how we eat affects both our mental and physical health. It is extremely common to snack between meals, with reports finding that 94 percent of people in A modern trend the US and 66 percent in the UK do so at least ONE REPORT FOUND THAT MILLENNIALS once a day. Snacking is not in itself bad for you; it (THOSE AGED 21–38) ARE MORE LIKELY TO OPTION 1 OPTION 2 OPTION 3 can help maintain energy levels, especially during SNACK THAN OLDER GENERATIONS. REGULAR MEAL AT BREAKFAST SMALL MEAL AT BREAKFAST REGULAR MEAL IN MORNING busy days, and prevent you from feeling so hungry Almost a quarter of this age group were found to REGULAR MEAL AT LUNCH REGULAR MEAL AT LUNCH REGULAR MEAL IN AFTERNOON that you overeat. snack at least four times a day, with a significant proportion of those questioned snacking to deal SMALL MEAL AT DINNER SMALL MEAL AT DINNER However, snacking can become a habit and even with boredom or stress. start to replace meals, which is never a good thing, since your body cannot get all the nutrients you A number of studies have shown that it is healthier Appetite and energy expenditure follow the need just from small snacks. Many people snack to eat meals at the same time each day. Evidence circadian rhythm, the natural “body clock” that due to boredom, a form of hedonic hunger HEALTHY SNACKS: suggests that a regular meal pattern that includes regulates when we wake, sleep, and many things in (see page 104). Eating simply for pleasure is not Carrot sticks and hummus eating breakfast, having 2–3 meals a day, and between. At the same time, the timing of your meals necessarily a bad thing; after all, food is more than Chopped apple with 1 tsp of peanut butter consuming a greater proportion of your daily energy may have an effect on your circadian rhythm. just fuel. But if you choose foods such as cookies, Small handful of nuts intake earlier on in the day may have physiological Personalizing your mealtimes to align with your sweets, and cake, which are high in sugar and fat, A banana and yogurt benefits such as reduced inflammation and improved sleep cycle can improve health and limit weight gain. or snack frequently throughout the day, you may Homemade energy balls stress resistance. Regular mealtimes provide a sense be consuming more than your body needs. Rice cakes with avocado of rhythm and familiarity, which can have real MAKING THE MOST OF MEALTIMES On top of this, the snack-food industry psychological benefits. Aim to eat sitting at a table and upright in a often uses disingenuous marketing. chair. This position allows your stomach to empty, While individually wrapped snack SATISFYING helping digestion. It also encourages us to eat more bars may be convenient, especially PEANUT BUTTER IS Breakfast mindfully (see pages 206–207) and notice feelings when you are on the go, many of NUTRIENT DENSE WITH of satiety and fullness (see page 104).Even better is these contain high quantities of HEALTHY FATS; APPLES A RECENT STUDY LOOKING AT IRREGULAR OFFER SLOW-RELEASE sitting down to eat with friends or family for some sugar, despite being marketed as a BREAKFAST EATING FOUND THAT THOSE CARBS AND AGED BETWEEN 13–18 YEARS WERE THE social interaction. Having a conversation during a “healthy” option (see pages 62–63). FIBER MOST LIKELY TO SKIP BREAKFAST, meal slows down the speed at which you eat, There is no need to avoid certain with 22 percent of participants in this age group meaning you are more likely to notice feelings of foods altogether when it comes to snacks, consuming breakfast on 2 days or fewer out of 4. fullness before overeating. Conversely, eating dinner but the best policy is “everything in moderation.” Previous studies have suggested that differences in family structure, ethnicity, socioeconomic in front of the television not only eliminates the Snacking mindfully (see pages 206–207) and status, time constraints, and lack of food opportunity for social interaction but also distracts choosing healthy, balanced options is a good idea, enjoyment can all influence this. us from what we are eating, with research suggesting as this will prevent blood sugar spikes (see pages this can cause us to eat more than we need. 202–203) and keep you fuller for longer. UUSS__110022--110033__mmeeaallttiimmeess..iinndddd 110022 2244//0088//22002211 1166::2233 UUSS__110022--110033__mmeeaallttiimmeess..iinndddd 110033 2244//0088//22002211 1166::2233
104 SHOULD I GO ON A DIET? 105 WHY DO I FEEL HUNGRY ALL THE TIME? It may be because of the simple reason that you need to eat more food! Or perhaps you are misinterpreting your body’s signals. Hunger is a much-misunderstood phenomenon. We’ve all experienced hunger—the sensation your hungry can do more harm than good, but it’s good body produces when you are in need of fuel and to recognize and acknowledge whether your body is nourishment. Satiety refers to feeling full, while actually in need of fuel or if your hunger is actually satiation refers to feeling satisfied. for the pleasure you associate with food. Hormones control the balance between hunger If you feel tired and your stomach is growling, it’s and satiety, and what you eat affects the balance of likely your body lacks energy and you need to refuel those hormones. Food makes changes to levels of gut to satiety. If this happens often, eat a little more at hormones, which in turn impacts the metabolites in mealtimes, consider increasing your carbohydrate the blood and the signals they transmit in the body. intake, or take a snack with you when out and about. If it’s hedonic hunger that you keep feeling, it’s TYPES OF HUNGER likely you’re in need of satiation. Research suggest Homeostatic hunger refers to the physical feeling some types of food can be more satiating than others. of wanting to eat, caused by the need for energy A diet rich in fiber or high in protein suppresses (see opposite). Hedonic hunger describes the desire ghrelin (the hunger hormone) effectively. to eat for pleasure. When we smell or eat something tasty, the brain releases pleasure hormones like AVOID "I’VE BLOWN IT!" MENTALITY dopamine. We then associate that food with the A destructive pattern emerges with excessive hunger feeling of pleasure we experienced, making us when you give up fighting it and continue to eat want to eat it again, or eat more of it than we need. beyond the point of fullness, out of a tangled blend Psychological and emotional factors affect the of conflicting feelings like desire, guilt, frustration, balance between hunger hormones in ways we and fulfillment. If you are struggling to detect your don’t yet fully understand. Tiredness can also internal cues, try mindful eating and intuitive eating have an impact (see pages 142–143). The human (see pages 208–211). Minimize distractions while body has a complex system of hormones that eating. For example, turn off the TV. interact in myriad ways. For example, cortisol, A satiating square of chocolate after dinner is fine, a stress hormone, suppresses appetite, but in but if you find yourself overeating or eating out of cases of chronic stress, it can enhance appetite. boredom, step back and take a moment to listen to your body and consider whether you are actually MANAGING PERSISTENT HUNGER hungry. Often the craving will pass. Distract It seems as we age, we lose connection with our yourself from the feeling with a different activity. innate hunger signals. For instance, people easily Chewing food for longer may help you eat less confuse thirst for hunger. Fighting against feeling during the meal and feel more satiated afterward. UUSS__110044--110055__hhuunnggrryy..iinndddd 110044 2244//0088//22002211 1166::2233
110044 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 110055 Hunger hormones KEY WHY DO I FEEL HUNGRY Increases hunger ALL THE TIME? Appetite is governed by the balance of several hormones, Decreases hunger notably ghrelin and leptin. It may be because of the simple reason that you need to eat Receptors for these in the hypothalamus in the brain more food! Or perhaps you are misinterpreting your body’s signals. respond to their presence Hunger is a much-misunderstood phenomenon. by setting off bodily processes that cause sensations of hunger We’ve all experienced hunger—the sensation your hungry can do more harm than good, but it’s good or satiety. body produces when you are in need of fuel and to recognize and acknowledge whether your body is nourishment. Satiety refers to feeling full, while actually in need of fuel or if your hunger is actually satiation refers to feeling satisfied. for the pleasure you associate with food. Hormones control the balance between hunger If you feel tired and your stomach is growling, it’s and satiety, and what you eat affects the balance of likely your body lacks energy and you need to refuel those hormones. Food makes changes to levels of gut to satiety. If this happens often, eat a little more at hormones, which in turn impacts the metabolites in mealtimes, consider increasing your carbohydrate the blood and the signals they transmit in the body. intake, or take a snack with you when out and about. If it’s hedonic hunger that you keep feeling, it’s TYPES OF HUNGER likely you’re in need of satiation. Research suggest Homeostatic hunger refers to the physical feeling some types of food can be more satiating than others. of wanting to eat, caused by the need for energy A diet rich in fiber or high in protein suppresses (see opposite). Hedonic hunger describes the desire ghrelin (the hunger hormone) effectively. to eat for pleasure. When we smell or eat something GHRELIN LEPTIN tasty, the brain releases pleasure hormones like AVOID "I’VE BLOWN IT!" MENTALITY PRODUCED IN PRODUCED IN THE STOMACH ADIPOSE TISSUE dopamine. We then associate that food with the A destructive pattern emerges with excessive hunger WHEN EMPTY, THE OVER TIME (NOT MEAL feeling of pleasure we experienced, making us when you give up fighting it and continue to eat STOMACH SECRETES TO MEAL), LEPTIN ACTS want to eat it again, or eat more of it than we need. beyond the point of fullness, out of a tangled blend GHRELIN INTO THE TO ALTER FOOD INTAKE Psychological and emotional factors affect the of conflicting feelings like desire, guilt, frustration, BLOODSTREAM. THIS AND CONTROL ENERGY LEADS TO HUNGER EXPENDITURE VIA balance between hunger hormones in ways we and fulfillment. If you are struggling to detect your PANGS SIGNALS TO THE don’t yet fully understand. Tiredness can also internal cues, try mindful eating and intuitive eating HYPOTHALAMUS have an impact (see pages 142–143). The human (see pages 208–211). Minimize distractions while body has a complex system of hormones that eating. For example, turn off the TV. interact in myriad ways. For example, cortisol, A satiating square of chocolate after dinner is fine, a stress hormone, suppresses appetite, but in but if you find yourself overeating or eating out of cases of chronic stress, it can enhance appetite. boredom, step back and take a moment to listen to your body and consider whether you are actually MANAGING PERSISTENT HUNGER hungry. Often the craving will pass. Distract It seems as we age, we lose connection with our yourself from the feeling with a different activity. innate hunger signals. For instance, people easily Chewing food for longer may help you eat less confuse thirst for hunger. Fighting against feeling during the meal and feel more satiated afterward. UUSS__110044--110055__hhuunnggrryy..iinndddd 110044 2244//0088//22002211 1166::2233 UUSS__110044--110055__hhuunnggrryy..iinndddd 110055 2244//0088//22002211 1166::2233
106 SHOULD I GO ON A DIET? 107 Gut-brain axis Brain and gut are in constant communication, and scientists believe microbiome composition can affect our mood and experience of hunger. Mucous membrane Diverse gut Nutrient-absorbing bacteria villi on gut lining Having a range of different species of bacteria in your gut has been linked to lower risk of obesity. The bacteria shown here (see right) are some that are beneficial to a healthy body and brain when present in balanced amounts in the intestine. UUSS__110066--110077__gguutt..iinndddd 110066 2244//0088//22002211 1166::2233
110066 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 110077 Gut-brain axis CAN MY GUT BACTERIA Brain and gut are in constant communication, HELP ME LOSE WEIGHT? and scientists believe microbiome composition can affect our mood and experience of hunger. Research suggests that the gut microbiome—the trillions of bacteria that reside in our gut—may be an influential factor in our ability to lose weight. Fascination with the gut microbiome has grown in and had increased inflammatory markers, insulin recent years, as research has revealed the importance resistance, and lipid levels, all of which increase of its diversity on physical and mental health. the risk of diabetes and cardiovascular disease. Gut bacteria are involved in many bodily systems, Such effects occur because the types and diversity including immunity, hunger, and digestion (see of the bacteria in our microbiome can impact how pages 48–53 and 140–141). Like our genetic makeup, we process food, as well as other biochemical no two people have the same composition of gut reactions, thereby influencing weight loss. These Mucous membrane bacteria, and studies have uncovered a link between differences are likely due to a combination of microbiome variation and body composition. genetics and environment. For instance, the most Diverse gut Nutrient-absorbing bacteria villi on gut lining heritable bacterial species, Christensenellaceae, is THE IMPACT OF DIVERSITY found more predominantly in lean people and has One study placed 26 participants on a low-calorie been shown to protect against weight gain in rats. diet rich in fruit and vegetables and found that We can increase our gut microbiota diversity by some people lost more weight than others. Analysis consuming a varied, predominantly plant-based showed this was likely due to a difference in their diet that is rich in fiber and by trying probiotic gut bacteria, which could influence how efficiently fermented foods, as well as limiting our use of food was broken down and thus how much weight unnecessary antibiotics and medications. However, was lost. In another study, researchers analyzed the more research is needed to confirm just how much gut bacteria of 169 obese and 123 nonobese adults of an impact our gut bacteria can have on weight and found that 23 percent of those who had lower loss. As yet, there is no definition of what a healthy diversity of bacteria were more likely to be obese, gut microbiome should look like, beyond diversity. Having a range of different species of bacteria in your gut has been linked to lower risk of obesity. The bacteria shown here (see right) are some that are beneficial to a healthy body and brain when present ENTEROCOCCUS AKKERMANSIA CHRISTENSENELLA BIFIDO- LACTOBACILLUS ESCHERICHIA COLI in balanced amounts in the intestine. FAECALIS MUCINIPHILA MINUTA BACTERIUM (HARMLESS STRAINS) UUSS__110066--110077__gguutt..iinndddd 110066 2244//0088//22002211 1166::2233 UUSS__110066--110077__gguutt..iinndddd 110077 2244//0088//22002211 1166::2233
108 SHOULD I GO ON A DIET? 109 CAN LIQUID FOODS HELP ME LOSE WEIGHT? Meal replacement shakes and soups are portion- and calorie-controlled drinks that are consumed in place of meals or snacks in weight-loss programs. These products are often hailed as rapid weight-loss methods, but is there any truth in these claims? For some, liquid meals help kick-start weight Meal-replacement products can be convenient loss, but research suggests they are effective only for busy dieters and eliminate decision-making in the short term. They are not recommended for at mealtimes. Additionally, they are often fortified long-term use. In fact, the potentially tricky return with micronutrients. Some (expensive) brands even to solid foods increases the chance of weight gain. claim to be more nutritious than the average meal! THE PROS Research into liquid meals shows some positive results. Participants who restricted their oral intake to about 800 calories per day on a liquid diet achieved significant weight loss at 12 months. They had foods reintroduced after three to five months and still managed to maintain weight loss. So while liquid diets are unsustainable in the long term, they can help instigate behavior change when people realize that they can lose weight and experience positive benefits to their physical and mental well-being. Smoothies can be great on-the-go options. Having the occasional smoothie for breakfast or lunch can be convenient as well as nutritious. UUSS__110088--110099__sshhaakkeess__ssoouuppss..iinndddd 110088 2244//0088//22002211 1166::2233
110088 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 110099 CAN LIQUID FOODS HELP THE CONS respond to internal hunger cues and skew Chewing enhances nutrient absorption. One study feelings of satiety (see pages 104–105). This ME LOSE WEIGHT? found that chewing almonds between 25 and 40 can make the transition back to solid food even times suppressed hunger and increased the body’s harder. Lastly, but importantly, liquid meals take the ability to absorb nutrients from the almonds. Those enjoyment and social aspect out of eating. Enjoying Meal replacement shakes and soups are portion- and calorie-controlled drinks that who don’t chew properly can experience digestive meals with family and friends makes a significant are consumed in place of meals or snacks in weight-loss programs. These products problems and end up snacking more later. The contribution to your well-being. are often hailed as rapid weight-loss methods, but is there any truth in these claims? same is true for those relying on liquid meals. After relying on liquid meals for a period of time HOMEMADE SOUPS AND SMOOTHIES and achieving some weight loss, problems can arise There’s nothing wrong with liquid foods alongside For some, liquid meals help kick-start weight Meal-replacement products can be convenient when individuals return to eating regular meals. It a balanced diet of solid foods. The odd smoothie loss, but research suggests they are effective only for busy dieters and eliminate decision-making is very easy to slip into old eating habits and regain isn’t such a bad idea. It’s an easy way to get a lot in the short term. They are not recommended for at mealtimes. Additionally, they are often fortified the weight. This can be amplified by hormonal of nutrients into your diet, especially if you don’t long-term use. In fact, the potentially tricky return with micronutrients. Some (expensive) brands even changes in the body following dietary restriction enjoy eating fruits and vegetables (see Nutritious to solid foods increases the chance of weight gain. claim to be more nutritious than the average meal! and a lowered metabolic rate (see pages 92–93). additions, below) or if you’ve lost your appetite Liquid meals can instill a poor relationship with and feel more comfortable drinking than eating. THE PROS food (see pages 210–211). In addition, following a Make soups with lots of vegetables to add Research into liquid meals shows some positive set diet plan can reduce an individual’s ability to plenty of fiber into your diet. results. Participants who restricted their oral intake to about 800 calories per day on a liquid diet achieved significant weight loss at 12 months. They had foods reintroduced after three to five months and still managed to maintain weight loss. So while liquid diets are unsustainable in the long term, they can help instigate behavior change when people realize that they can lose weight and experience positive benefits to their physical and mental well-being. Nutritious additions WHEN MAKING SMOOTHIES AT HOME, YOU CAN SQUEEZE IN EVEN MORE NUTRITION AND REDUCE YOUR SUGAR INTAKE. Many smoothies you buy will be fruit heavy. Manufacturers know that the sugars in the fruit make their products more palatable (see page 64). By making your own smoothies, you can increase the vegetable content. Try adding healthy fats, such Smoothies can be great as flaxseeds and avocado. You could also add some on-the-go options. Having protein powder to help make your nutritious the occasional smoothie beverage more of a complete meal. for breakfast or lunch can be convenient as well as nutritious. UUSS__110088--110099__sshhaakkeess__ssoouuppss..iinndddd 110088 2244//0088//22002211 1166::2233 UUSS__110088--110099__sshhaakkeess__ssoouuppss..iinndddd 110099 2244//0088//22002211 1166::2233
110 SHOULD I GO ON A DIET? 111 SHOULD I CUT OR REDUCE CARBS? Low carb diets, like Keto, Atkins, and Paleo, have gained much popularity in recent years, in large part due to the media coverage describing ‘‘low-carb’’ or ‘‘no-carb’’ as the big secret to weight loss. This simply isn’t true. Everyone’s carbohydrate needs vary depending on FLUID LOSS factors like age, sex, build, and activity levels. If Energy from carbs (in the form of glucose) can be you feel you consume more carbs than is right for converted to glycogen molecules and stored along your body, reducing to bring balance to your diet with the water needed to reconvert it to glucose for is sensible. That’s not the same as going low-carb. future use. On a low-carb diet, the body makes fewer In the media, and in some scientific literature, glycogen stores, so the corresponding water is absent. carbs are often dismissed as fattening. It’s true that This sudden weight loss is often confused for fat loss. society’s carbohydrate requirements have reduced due to less active lifestyles, yet carbs remain an LOW-CARB DIETS important part of a balanced diet. Remember that Low-carb diets advocate the elimination or marked Japan has one of the lowest obesity rates and one reduction of carbs such as grains (pasta, rice, and of the highest rates of carbohydrate consumption! breads), starchy fruits and vegetables, and legumes. The glycogen weight-loss illusion Each gram of glycogen (stored glucose) is stored with 3g water. The elimination of this water when glycogen stores are depleted on a low-carb diet is often confused for loss of fat. DIGEST CARBS STORE GLYCOGEN RELEASE GLUCOSE Carbs are broken down during Excess glucose is converted to The water is used to reconvert digestion, releasing glucose glycogen and stored in the liver and glycogen to glucose when energy is into the bloodstream. muscles alongside water, at a ratio needed. The glucose is released into of 1 part glycogen to 3 parts water. the bloodstream along with the water. UUSS__111100--111111__ccuutt__ccaarrbbss..iinndddd 111100 2244//0088//22002211 1166::2233
111100 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 111111 SHOULD I CUT and maintaining blood sugar levels. The keto Type 2 diabetes diet, which excludes many fruits and vegetables, CARB OR REDUCE CARBS? may cause constipation, and keto dieters can miss REFERENCE THE ONLY CASE IN WHICH A LOW-CARB out on the long-term health benefits of DIET HAS PROVEN HEALTH BENEFITS IS INTAKE Low carb diets, like Keto, Atkins, and Paleo, have gained much popularity THAT OF TYPE 2 DIABETES. nourishing their gut bacteria (see pages 48–53). A low-carb diet can be a safe and effective Restricting carbs can cause fatigue, low mood, 225– in recent years, in large part due to the media coverage describing ‘‘low-carb’’ short-term weight-management solution for and cravings. It’s not recommended for those or ‘‘no-carb’’ as the big secret to weight loss. This simply isn’t true. people with type 2 diabetes who would like 325g to lose weight. It can help improve glycemic recovering from an eating disorder or disordered control and reduce cardiovascular risk. It’s eating (see pages 210–211), or for children. per day important to have the guidance of a health professional to support any necessary changes Everyone’s carbohydrate needs vary depending on FLUID LOSS to relevant diabetes medications and monitor MACRONUTRIENT IMBALANCE factors like age, sex, build, and activity levels. If Energy from carbs (in the form of glucose) can be blood glucose and the risk of hypoglycemia Restricting any food group can result in LOW-CARB you feel you consume more carbs than is right for converted to glycogen molecules and stored along (see pages 170–173). nutritional deficiency. When we remove or restrict DIET your body, reducing to bring balance to your diet with the water needed to reconvert it to glucose for carbs, we may turn to other macronutrients to fill is sensible. That’s not the same as going low-carb. future use. On a low-carb diet, the body makes fewer the gap. Dietary proteins and fats may increase <130g In the media, and in some scientific literature, glycogen stores, so the corresponding water is absent. Studies show there is no significant difference to unhealthy levels, which may reduce feelings carbs are often dismissed as fattening. It’s true that This sudden weight loss is often confused for fat loss. in weight loss between low-carb and low-fat diets. of hunger to help with weight loss but cause per day society’s carbohydrate requirements have reduced While studies reveal a reduction in weight on other issues such as increased cholesterol. due to less active lifestyles, yet carbs remain an LOW-CARB DIETS the ketogenic diet, it is not sustained long term. Research found young healthy adults following important part of a balanced diet. Remember that Low-carb diets advocate the elimination or marked These studies have a high dropout rate, showing a low-carb, high-fat diet had a 44 percent KETO Japan has one of the lowest obesity rates and one reduction of carbs such as grains (pasta, rice, and just how difficult it is to stick to a low-carb diet. increase in LDL (“bad”) cholesterol (see page DIET of the highest rates of carbohydrate consumption! breads), starchy fruits and vegetables, and legumes. Carbs offer many health benefits (see page 12). 17). Keto diets encourage foods high in fat, but If you suddenly restrict them, you may if you don’t differentiate between saturated and <50g experience headaches and digestive symptoms unsaturated fats, your LDL cholesterol levels such as constipation. The fiber in carbs has could easily rise, increasing your risk for heart The glycogen weight-loss illusion per day several benefits, including improving digestion disease and stroke. Each gram of glycogen (stored glucose) is stored with 3g water. The elimination of this water when glycogen stores are depleted on a low-carb diet is often confused for loss of fat. DIGEST CARBS STORE GLYCOGEN RELEASE GLUCOSE STORES IN LIVER STORES IN MUSCLE WATER WEIGHT Carbs are broken down during Excess glucose is converted to The water is used to reconvert Glycogen stored in the liver is used Once the liver’s glycogen supply is When you cut carbs, your body digestion, releasing glucose glycogen and stored in the liver and glycogen to glucose when energy is first. The water used to convert it to depleted, the body relies on the depletes glycogen stores, releasing into the bloodstream. muscles alongside water, at a ratio needed. The glucose is released into glucose travels via the bloodstream glycogen stores in the muscles. stored water. It’s the loss of this, of 1 part glycogen to 3 parts water. the bloodstream along with the water. to the kidneys to be expelled as urine. (not fat) that shows on the scales. UUSS__111100--111111__ccuutt__ccaarrbbss..iinndddd 111100 2244//0088//22002211 1166::2233 UUSS__111100--111111__ccuutt__ccaarrbbss..iinndddd 111111 2244//0088//22002211 1166::2233
112 113 WHAT IS A DETOX DIET AND DETOX TEA? "Detox" is a buzzword in the diet industry. Many products and diet plans promise to rid the body of toxins, help you lose weight, and even reduce cellulite. But detoxes are unnecessary, because your body has a highly effective detoxification system of its own. The truth is, we don’t need to “cleanse” or detox they impose. These restrictions may also leave you ourselves at all, and following this sort of diet will feeling deprived and probably hungry, which may not have this desired effect. lead to subsequent overeating. The idea of feeling clean after a detox diet is what The bottom line is that detox diets are not good spurs dieters to embark repeatedly on expensive and for weight loss and are a serious no-go in terms of immensely restrictive detox diets. We come into nutrition. No health professional would recommend contact with environmental toxins every single day a detox diet because you do not need to detox. in the air we breathe and the food we eat, for instance. The body is a wonderful organism, which, through DETOX TEAS the action of the liver, effectively removes toxins and The “teatox” has gained much attention on social waste products that make it into the body system. media recently, with many people making sensational It is possible that following one of these detox claims of weight loss. It is entirely possible that you plans may lead to some weight loss, but most of may lose weight in the short term on any new diet that will be water loss (see pages 110–111) and will regime, but there is no scientific evidence to support be regained when you return to eating normally. the notion that drinking tea will help you lose weight. Detox teas or “skinny” teas have hit the headlines RISKS WITH NO REWARD with claims of being able to help you lose weight Over time, detox diets also give you a greater risk of fast without dieting by sending your metabolism nutrient deficiencies due to the dietary restrictions into overdrive. You can see the allure! Detoxification by the liver Hepatic portal vein During digestion, nutrient-laden blood from the small intestine is passed to the liver for processing. Cells in the liver called hepatocytes BLOOD INTO LIVER TOXINS sift out nutrients and redirect them to wherever they are needed in the body. At the same time, hepatocytes The liver receives Toxins are break down toxins and send the blood via the removed from waste on to be excreted. hepatic portal vein. blood in the liver. UUSS__111122--111133__ddeettooxx..iinndddd 111122 2244//0088//22002211 1166::2233
111122 111133 WHAT IS A DETOX DIET AND DETOX TEA? In reality, teatoxes can be dangerous and are not "Detox" is a buzzword in the diet industry. Many products and diet plans promise recommended. Some products are simply laxatives to rid the body of toxins, help you lose weight, and even reduce cellulite. But detoxes are in tea form. The key ingredient is senna, which is unnecessary, because your body has a highly effective detoxification system of its own. used to treat constipation. It irritates the stomach lining to stimulate bowel movements and acts as a diuretic. Using senna can cause dehydration, cramps, The truth is, we don’t need to “cleanse” or detox they impose. These restrictions may also leave you and diarrhea and result in inadequate nutrient ourselves at all, and following this sort of diet will feeling deprived and probably hungry, which may absorption, causing depletion of key minerals not have this desired effect. lead to subsequent overeating. such as calcium, sodium, and potassium. The idea of feeling clean after a detox diet is what The bottom line is that detox diets are not good Regular or prolonged use of senna can spurs dieters to embark repeatedly on expensive and for weight loss and are a serious no-go in terms of damage the gut lining and disrupt electrolyte immensely restrictive detox diets. We come into nutrition. No health professional would recommend balance, potentially causing heart damage. contact with environmental toxins every single day a detox diet because you do not need to detox. The loss of bulk caused by using a laxative in the air we breathe and the food we eat, for instance. to “clear out” can make you feel and look The body is a wonderful organism, which, through DETOX TEAS slimmer in the short term, due to your bowels the action of the liver, effectively removes toxins and The “teatox” has gained much attention on social being abnormally empty. But it has no impact waste products that make it into the body system. media recently, with many people making sensational on fat loss, because calories from food are It is possible that following one of these detox claims of weight loss. It is entirely possible that you absorbed in your small intestine (see page 28), plans may lead to some weight loss, but most of may lose weight in the short term on any new diet much further up the digestive tract than that will be water loss (see pages 110–111) and will regime, but there is no scientific evidence to support the colon, which is where the effect of the be regained when you return to eating normally. the notion that drinking tea will help you lose weight. laxative is felt. So with skinny teas, you Detox teas or “skinny” teas have hit the headlines end up spending a fortune on something RISKS WITH NO REWARD with claims of being able to help you lose weight that, basically, confines you to the toilet! Over time, detox diets also give you a greater risk of fast without dieting by sending your metabolism nutrient deficiencies due to the dietary restrictions into overdrive. You can see the allure! EXCRETED IN FECES Via the bowel Waste is returned Detoxification by the liver via the gallbladder to the small intestine and eventually leaves Hepatic the body in feces. portal vein During digestion, nutrient-laden blood from the small intestine is passed to the liver for processing. Cells in the liver called hepatocytes BLOOD INTO LIVER TOXINS sift out nutrients and redirect them EXCRETED IN URINE to wherever they are needed in the body. At the same time, hepatocytes Via the bladder Some toxins are The liver receives Toxins are sent in the blood to the kidneys, break down toxins and send the blood via the removed from then on to the bladder to be waste on to be excreted. hepatic portal vein. blood in the liver. excreted as urine. UUSS__111122--111133__ddeettooxx..iinndddd 111122 2244//0088//22002211 1166::2233 UUSS__111122--111133__ddeettooxx..iinndddd 111133 2244//0088//22002211 1166::2233
114 SHOULD I GO ON A DIET? 115 IS IT OK TO SKIP MEALS? It sounds simple: skip a meal or two here and there, and you will be able to lose weight without having to worry too much about what you’re actually eating. But is it that easy—and could it even be potentially harmful? Intermittent fasting (IF) is a weight-loss Most of the research surrounding IF is based on method that has grown in popularity in recent simple calorie restriction. Most likely, these diets years. There are three main types: are popular because they offer a straightforward Whole-day fasting: this includes the well- way to reduce energy intake that often leads to known 5:2 diet, with a limited 400–500 calorie weight loss. For example, many people following intake on two days of the week only. time-restricted eating “fast” while asleep, then Alternate-day fasting: on fasted days, eat fewer calories than they normally would it’s recommended you eat just one meal. because they also have a shorter time period Time-restricted eating: an example is the within which to eat. However, IF won’t work 16:8 plan, where you eat the same amount of for everyone, such as those with a tendency food as usual, but during an eight-hour window to comfort or binge eat, rather than stop each day. Often, followers opt to skip breakfast. when full (see pages 212–213). Time-restricted eating This method typically takes advantage of our natural fasting period of sleep by setting an evening cutoff for eating. Here is an example of a daily schedule GLYCOGEN based on 16 hours of fasting. DINNER WINDOW EVENING FASTING WINDOW CLOSES DRINKS 0–12 HOURS 6 p.m.: 8 p.m.: Any time: Energy from glycogen Window opens Cut-off point Stay hydrated During this period, the body Eat at any Try not to eat too Drink water, herbal tea, or burns glycogen stored in time within the quickly, even if you tea/coffee without milk; the liver. two-hour window. are running late. avoid alcohol, fruit juice, or sugar-sweetened drinks. UUSS__111144--111155__FFaassttiinngg..iinndddd 111144 2244//0088//22002211 1166::2233
111144 SSHHOOUULLDD II GGOO OONN AA DDIIEETT?? 111155 IS IT OK TO SKIP MEALS? SKIPPING BREAKFAST of the day may help weight outcomes among Breakfast appears to be especially important those with obesity, but it’s unclear why this is It sounds simple: skip a meal or two here and there, and you will be able to lose when maintaining a healthy weight. Research the case, aside from simple calorie restriction. weight without having to worry too much about what you’re actually eating. suggests that controlling food intake Be careful of skipping any meals, especially if But is it that easy—and could it even be potentially harmful? across the day is more your body clock means you’re naturally a person manageable on a full stomach who wakes up hungry. in the morning—people who Intermittent fasting (IF) is a weight-loss Most of the research surrounding IF is based on eat breakfast appear to be FASTING RISKS method that has grown in popularity in recent simple calorie restriction. Most likely, these diets more likely to maintain More worryingly, the restriction inherent in IF years. There are three main types: are popular because they offer a straightforward weight loss in the long term. could potentially lead to overeating, bingeing, Whole-day fasting: this includes the well- way to reduce energy intake that often leads to If you are considering or even eating disorders. Skipping meals is also known 5:2 diet, with a limited 400–500 calorie weight loss. For example, many people following adopting any type of IF, a difficult practice to sustain. There is actually intake on two days of the week only. time-restricted eating “fast” while asleep, then breakfast may not be your some evidence that IF can lower risk factors for Alternate-day fasting: on fasted days, eat fewer calories than they normally would best choice of meal to diabetes and heart disease, but research in this it’s recommended you eat just one meal. because they also have a shorter time period miss. There is some area is still in its infancy. Sustained, healthy Time-restricted eating: an example is the within which to eat. However, IF won’t work evidence that skipping weight loss involves many factors, including 16:8 plan, where you eat the same amount of for everyone, such as those with a tendency the first meal a diet that’s balanced, varied, and enjoyable, food as usual, but during an eight-hour window to comfort or binge eat, rather than stop good quality sleep, physical activity, and stress each day. Often, followers opt to skip breakfast. when full (see pages 212–213). management. There is no need to adopt weight loss diets that disrupt your normal routine. Time-restricted eating This method typically takes advantage of our natural fasting period of sleep by setting an evening cutoff for eating. Here is an example of a daily schedule GLYCOGEN FAT CELLS based on 16 hours of fasting. DINNER WINDOW EVENING FASTING FASTING BREAKFAST WINDOW CLOSES DRINKS 0–12 HOURS 12–16 HOURS LUNCH 6 p.m.: 8 p.m.: Any time: Energy from glycogen Metabolic switch 12 midday: 4 p.m.: Window opens Cut-off point Stay hydrated During this period, the body With glycogen depleted, Break your fast Healthy meal/snack Eat at any Try not to eat too Drink water, herbal tea, or burns glycogen stored in the body starts Nutrient-dense whole grains Although calories aren’t time within the quickly, even if you tea/coffee without milk; the liver. to burn energy from and fruit/veggie kick-start restricted, eat only until two-hour window. are running late. avoid alcohol, fruit juice, or stored fat cells. the system and limit calories. you feel satisfied. sugar-sweetened drinks. UUSS__111144--111155__FFaassttiinngg..iinndddd 111144 2244//0088//22002211 1166::2233 UUSS__111144--111155__FFaassttiinngg..iinndddd 111155 2244//0088//22002211 1166::2233
UUSS__111166--111177__cchhaapptteerr__44..iinndddd 111166 2244//0088//22002211 1166::2233
SHOULD I CHOOSE PLANT-BASED NUTRITION? What Does Plant-based Actually Mean? 118 Will I Struggle to Eat Enough Protein What Are the Health Benefits of on a Vegan Diet? 128 Plant-based Nutrition? 120 What Other Nutrients Should Plant-based Is Plant-based Always Healthier? 122 Eaters Consider? 130 How Can I Eat in a Sustainable Way? 124 Why Is It Important to Increase Is Plant-based Better for the Environment? 126 Plant-based Diversity? 132 UUSS__111166--111177__cchhaapptteerr__44..iinndddd 111166 2244//0088//22002211 1166::2233 UUSS__111166--111177__cchhaapptteerr__44..iinndddd 111177 2255//0088//22002211 1199::1144
118 SHOULD I CHOOSE PLANT-BASED NUTRITION? 119 WHAT DOES PLANT-BASED ACTUALLY MEAN? "Plant-based" has become a trendy term, but it isn’t always clear what exactly people mean by it. Does it mean eating more plants, or just plants? And how does it differ from being vegan or vegetarian? Plant-based eating is becoming increasingly derived vitamin D in cereals or supplements popular, with a total of 57 percent of US households (vitamin D2 and lichen-derived vitamin D3 are purchasing plant-based food in 2020, up from suitable for vegans). 53 percent in 2019. As the trend grows, so do Veganism is often hailed as a healthy way to live, misunderstandings of its original meaning. but it’s possible to be an unhealthy vegan. You could As you might expect, a plant-based diet focuses keep to a vegan diet yet eat fast foods and processed on foods primarily from plants. This includes not ready-made meals, use saturated-fat-rich coconut oil only fruits and vegetables but also nuts, seeds, oils, in your cooking, and enjoy high-sugar foods such as whole grains, and legumes. It doesn’t mean that you vegan chocolate brownies. Veganism itself won’t are vegetarian or vegan and never eat meat or dairy. necessarily give you the nutrients you need for Instead, you are proportionately choosing more of good health and energy (see pages 130–131). your foods from plant sources. A well-planned plant-based diet can support VEGETARIANISM healthy living at every age and life stage. Including Vegetarianism has been around for centuries and is a wide variety of healthy whole foods will ensure becoming increasingly popular. Vegetarians don’t eat your diet is balanced and sustainable. However, like fish, meat, or chicken, including stock and fat from all diets, a poorly planned plant-based diet may leave animals, insects and gelatin, or animal rennet. A diet you at risk of certain nutrient deficiencies, which can include vegetables and fruits, grains and may affect both the mind and body. legumes, nuts and seeds, eggs, dairy products, and honey. There are also a number of different VEGANISM subcategories of vegetarianism: Veganism is more than just a dietary choice; it Lacto-ovo vegetarians eat dairy foods and eggs encompasses a way of life in which the aim is to but not meat, poultry, or seafood. exclude cruelty to animals in all forms and for Ovo-vegetarians include eggs but avoid all other sustainability and environmental reasons. This animal foods, including dairy. means vegans avoid using animal or animal-derived Lacto-vegetarians eat dairy foods but exclude products in food or clothing or for any other purpose. eggs, meat, poultry, and seafood. It can be very easy to use a product or consume a food that isn’t vegan if you don’t know what to look Ultimately, plant-based eating should be about for on the label. Animal-derived ingredients crop up eating meals with more vegetables and legumes and in all sorts of places, such as food additives in bread, swapping animal-derived produce for plant protein gelatin products, some fruit juices, and lanolin- alternatives such as beans, legumes, and tofu. UUSS__111188--111199__ppllaanntt__bbaasseedd..iinndddd 111188 2244//0088//22002211 1166::2233
111188 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 111199 WHAT DOES PLANT-BASED ACTUALLY MEAN? "Plant-based" has become a trendy term, but it isn’t always clear what exactly people mean by it. Does it mean eating more plants, or just plants? And how does it differ from being vegan or vegetarian? L ACTO-VEGETARIAN N A Plant-based eating is becoming increasingly derived vitamin D in cereals or supplements RI O A V popular, with a total of 57 percent of US households (vitamin D2 and lichen-derived vitamin D3 are GET O - V E E purchasing plant-based food in 2020, up from suitable for vegans). O V G E 53 percent in 2019. As the trend grows, so do Veganism is often hailed as a healthy way to live, -OV T A R I misunderstandings of its original meaning. but it’s possible to be an unhealthy vegan. You could O A CT N As you might expect, a plant-based diet focuses keep to a vegan diet yet eat fast foods and processed A L on foods primarily from plants. This includes not ready-made meals, use saturated-fat-rich coconut oil only fruits and vegetables but also nuts, seeds, oils, in your cooking, and enjoy high-sugar foods such as whole grains, and legumes. It doesn’t mean that you vegan chocolate brownies. Veganism itself won’t are vegetarian or vegan and never eat meat or dairy. necessarily give you the nutrients you need for Instead, you are proportionately choosing more of good health and energy (see pages 130–131). your foods from plant sources. A well-planned plant-based diet can support VEGETARIANISM N A I healthy living at every age and life stage. Including Vegetarianism has been around for centuries and is R V A a wide variety of healthy whole foods will ensure becoming increasingly popular. Vegetarians don’t eat E G AT A C your diet is balanced and sustainable. However, like fish, meat, or chicken, including stock and fat from N S E all diets, a poorly planned plant-based diet may leave animals, insects and gelatin, or animal rennet. A diet P you at risk of certain nutrient deficiencies, which can include vegetables and fruits, grains and may affect both the mind and body. legumes, nuts and seeds, eggs, dairy products, and honey. There are also a number of different VEGANISM subcategories of vegetarianism: Veganism is more than just a dietary choice; it Lacto-ovo vegetarians eat dairy foods and eggs encompasses a way of life in which the aim is to but not meat, poultry, or seafood. N e sux sc tlu aid ne a bcr ilu ite ylt ay n t do ea nn vim iroal ns m in e na tl al lf o rer am ss o na sn . d T f ho ir s an iO mv ao l - fv oe og de s,t a inr ci la un ds i nin gc dlu ad ire y .eggs but avoid all other M EAT EATER F L E X IT A RIA means vegans avoid using animal or animal-derived Lacto-vegetarians eat dairy foods but exclude products in food or clothing or for any other purpose. eggs, meat, poultry, and seafood. It can be very easy to use a product or consume a food that isn’t vegan if you don’t know what to look Ultimately, plant-based eating should be about Plant-based combinations Instead of comprising a rigid set of rules, a KEY Eggs for on the label. Animal-derived ingredients crop up eating meals with more vegetables and legumes and plant-based diet can mean a variety of dietary in all sorts of places, such as food additives in bread, swapping animal-derived produce for plant protein combinations, including or excluding animal- Plants Fish derived products as desired but predominantly gelatin products, some fruit juices, and lanolin- alternatives such as beans, legumes, and tofu. featuring foods from plant sources. Dairy foods Meat/Poultry UUSS__111188--111199__ppllaanntt__bbaasseedd..iinndddd 111188 2244//0088//22002211 1166::2233 UUSS__111188--111199__ppllaanntt__bbaasseedd..iinndddd 111199 2244//0088//22002211 1166::2233
120 SHOULD I CHOOSE PLANT-BASED NUTRITION? 121 WHAT ARE THE HEALTH BENEFITS OF PLANT-BASED NUTRITION? The idea that vegetables are good for us is one of the most obvious principles of nutrition. In fact, there are many reasons why increasing the proportion of plants in your diet can improve your health. There are numerous benefits to following a plant- needs on a daily basis, like calcium, iron, omega-3 based diet, including reduced environmental impact fatty acids, and iodine, as well as vitamin B12. It is and lower costs. There can also be significant health easy to get enough of these when following a benefits, provided you avoid nutritional deficiencies. omnivorous diet, but some of these nutrients are less “bio-available” in plant-based foods, meaning it is YOUR NUTRITIONAL NEEDS more difficult for the body to use those nutrients The dietary guidelines for Americans recommends a diet low in saturated fat and high in whole grains, fresh fruits, and vegetables. If you follow a balanced vegetarian diet based on whole grains, legumes, vegetables, fruits, nuts, and seeds, you should easily exceed the guidelines for eating five portions of fruits or vegetables a day, and your meals will be naturally high in fiber and low in saturated fat. Things are more complicated when it comes to the essential vitamins and minerals that your body A balanced vegan diet The key to a healthy vegan diet is 21⁄ 2 making sure all your nutritional needs 2 cups cups are met once animal products have been VEGETABLES FRUIT eliminated. This may mean adjusting your intake of certain foods to achieve a Per day. Per day. varied and balanced mix of protein, Eat as many vegetables One portion is about one fiber, vitamins, minerals, and healthy as you can, of as many piece of fruit, such different colors as you as an apple or banana, fats that will leave you feeling energized can; they are a key or about 3oz (85g). A rich and strong. Aim to match the daily and source of essential source of dietary fiber, weekly amounts recommended here. nutrients and fiber. vitamins, minerals, and antioxidants. UUSS__112200--112211__ppllaanntt__bbaasseedd__ddiieett..iinndddd 112200 2244//0088//22002211 1166::2233
112200 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 112211 WHAT ARE THE HEALTH when you eat the foods. It is important to make sure 28g of fiber each day, which supports the gut you are getting the right amount of protein and microbiome. Prebiotic foods (see pages 52–53), including BENEFITS OF PLANT-BASED micronutrients from your diet (see pages 128–131). garlic, leeks, bananas, and oats, are particularly effective in this regard. Studies show that eating a high-fiber NUTRITION? THE POWER OF PLANTS diet can lead to better control of blood-sugar levels and A lot of research supports the argument that a plant- cholesterol levels, too. And fiber from whole grains has The idea that vegetables are good for us is one of the most obvious principles of based diet increases longevity and reduces the risk of also been linked to a reduced risk of developing several certain health conditions. Well-balanced plant-based diseases, including type 2 diabetes. nutrition. In fact, there are many reasons why increasing the proportion of plants diets that are low in saturated fats can contribute There are healthy and happy people who eat animal in your diet can improve your health. toward managing a healthy weight, reducing your products and healthy and happy people who eat only risk of type 2 diabetes, cardiovascular disease, and plants. By reducing meat, fish, and dairy to minimal even some cancers. There is also a lot of evidence amounts and increasing plant-based alternatives, you There are numerous benefits to following a plant- needs on a daily basis, like calcium, iron, omega-3 that points toward reduced blood pressure when can get the best of both worlds and lessen your based diet, including reduced environmental impact fatty acids, and iodine, as well as vitamin B12. It is omitting animal products from our diets. environmental impact. While this approach does not and lower costs. There can also be significant health easy to get enough of these when following a Fiber is an important factor in why plant-based address the ethical concerns associated with animal benefits, provided you avoid nutritional deficiencies. omnivorous diet, but some of these nutrients are less eating is beneficial to health. Eating a variety of products, it may be a huge step for someone who “bio-available” in plant-based foods, meaning it is plant-based foods may help you get the recommended consumes meat, fish, and dairy daily in their diet. YOUR NUTRITIONAL NEEDS more difficult for the body to use those nutrients The dietary guidelines for Americans recommends a diet low in saturated fat and high in whole grains, fresh fruits, and vegetables. If you follow a balanced vegetarian diet based on whole grains, legumes, vegetables, fruits, nuts, and seeds, you should easily exceed the guidelines for eating five portions of fruits or vegetables a day, and your meals will be naturally high in fiber and low in saturated fat. Things are more complicated when it comes to the essential vitamins and minerals that your body AA bbaallaanncceedd vveeggaann ddiieett The key to a healthy vegan diet is 21⁄ 2 31⁄ oz 11⁄ 6 1 making sure all your nutritional needs 2 cups cups 2 2 cups cups are met once animal products have been VEGETABLES FRUIT WHOLE GRAINS LEAFY GREENS LEGUMES FAT eliminated. This may mean adjusting your intake of certain foods to achieve a Per day. Per day. Per day. Per week. Per week. Per day. varied and balanced mix of protein, Eat as many vegetables One portion is about one One portion is about 1oz One portion is about 3oz One portion is about 125g About 1oz (27g) of oil or nuts, fiber, vitamins, minerals, and healthy as you can, of as many piece of fruit, such (30g) of grain or a slice of (85g) of leafy green of beans, peas, or lentils, or half an avocado. High-fat different colors as you as an apple or banana, whole wheat bread. Choose vegetables such as broccoli, which are high in protein whole foods, as well as dairy fats that will leave you feeling energized can; they are a key or about 3oz (85g). A rich brown rice, whole wheat cabbage, kale, and spinach, and low in fat, and contain substitutes like soy and and strong. Aim to match the daily and source of essential source of dietary fiber, pasta, quinoa, buckwheat, which are packed with no cholesterol. almond, are a healthy weekly amounts recommended here. nutrients and fiber. vitamins, minerals, barley, farro, and sprouted vitamins, minerals, and source of fat but should be and antioxidants. grains for protein and fiber. antioxidants. kept to a single portion. UUSS__112200--112211__ppllaanntt__bbaasseedd__ddiieett..iinndddd 112200 2244//0088//22002211 1166::2233 UUSS__112200--112211__ppllaanntt__bbaasseedd__ddiieett..iinndddd 112211 2244//0088//22002211 1166::2233
122 SHOULD I CHOOSE PLANT-BASED NUTRITION? 123 IS PLANT-BASED ALWAYS HEALTHIER? The benefits of a primarily plant-based diet, while good, are dependent on whether it is followed for optimal health. Without careful choices, plant-based has the potential to be full of unhealthy items and there is a risk of nutritional deficiencies. Debates over our diets are often highly emotive. The foods providing 28g of fiber each day supports gut conversation around plant-based eating often focuses health, while fruits and vegetables are great sources on personal preference and environmental and ethical for a range of vitamins and minerals. concerns, while nutritional science gets left behind. However, some plant-based diets risk not getting the right nutrition through protein, vitamin, and FIND THE RIGHT BALANCE mineral intake. These risks can be overcome by The nutritional benefits of eating more plants are choosing the right vegetarian foods and, when grounded in solid evidence. A diverse range of plant necessary, supplements (see pages 128–131). VEGAN TREAT VEGAN DONUTS TEND TO BE HEALTHIER THAN NONVEGAN BUT STILL CONTAIN SALT, SUGAR, AND SATURATED FATS VEGAN SUGAR DONUTS CAN BE MADE Vegan donuts The eggs, WITH SUGAR THAT ISN’T sugar, and dairy can be PROCESSED USING replaced with vegan BONE CHAR, BUT THIS IS alternatives for a plant- STILL ADDED FREE based version but not SUGAR necessarily a healthy one. UUSS__112222--112233__ppllaannttbbaasseedd__hheeaalltthhyy..iinndddd 112222 2244//0088//22002211 1166::2233
112222 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 112233 IS PLANT-BASED ALWAYS HEALTHIER? The benefits of a primarily plant-based diet, while good, are dependent on whether it is followed for optimal health. Without careful choices, plant-based has the potential to be full of unhealthy items and there is a risk of nutritional deficiencies. Debates over our diets are often highly emotive. The foods providing 28g of fiber each day supports gut For example, soy, quinoa, and nuts are good sources hemorrhagic stroke (bleeding into the brain), conversation around plant-based eating often focuses health, while fruits and vegetables are great sources of protein, and tofu, lentils, and spinach are good and was not observed among pescatarians. on personal preference and environmental and ethical for a range of vitamins and minerals. sources of iron. But some nutrients are trickier to Studies like this may sound groundbreaking, concerns, while nutritional science gets left behind. However, some plant-based diets risk not getting get. Iodine, for instance, is mostly found in dairy but it’s important to check their limitations. the right nutrition through protein, vitamin, and products and fish. The iodine content of plant foods The overall risk of stroke in vegetarians was small, FIND THE RIGHT BALANCE mineral intake. These risks can be overcome by depends on the iodine content of the soil, which is equal to 3 extra cases per 1,000 people over 10 years. The nutritional benefits of eating more plants are choosing the right vegetarian foods and, when variable. Foods grown closer to the ocean tend to What’s more, this particular study was observational, grounded in solid evidence. A diverse range of plant necessary, supplements (see pages 128–131). be higher in iodine. Where soils are iodine deficient, meaning the researchers did not account for other iodized salt and seaweed can provide iodine. relevant variables besides diet. The way vegetables are cooked is also relevant: There are many other factors to consider, but steamed vegetables are far more nutritious than the more robust research suggests that plant-based deep-fried, for instance. And a lot of plant-based diets, if conducted well, can be a healthy way of VEGAN TREAT alternatives to animal products are not nutritionally eating and that meat eaters don’t have to give up VEGAN DONUTS TEND balanced. For example, pulled pork is often replaced meat to be healthier—they can simply reduce it. TO BE HEALTHIER THAN by jackfruit, but the latter contains no protein. NONVEGAN BUT STILL CONTAIN SALT, SUGAR, There are also many vegan processed foods available AND SATURATED that are unhealthy—vegetarian sausage rolls, for The “Anti-nutrient” FATS instance, can be high in salt and saturated fat. PHYTIC ACID IS PRESENT IN PLANT SEEDS SUCH AS NUTS, WHOLE GRAINS, AND BEANS. QUESTIONS TO BE ANSWERED In the body, the acid binds to minerals to form Contrary to what most research suggests, there is phytate, which is known as an "anti-nutrient" since some research indicating that plant-based diets may it impairs the absorption of zinc, iron, and calcium and can cause deficiencies in those on a plant- contribute to cardiovascular disease. A two-decade based diet. As an antioxidant that can help prevent UK-based study of 50,000 people analyzed the risk cardiovascular disease and kidney stones, however, of stroke and other health problems based on diet. it also has health benefits, so avoiding these foods The researchers found that, compared with meat is not advised. Soaking, cooking, fermenting, and VEGAN SUGAR sprouting beans and grains will reduce their eaters, rates of heart disease (such as angina or heart DONUTS CAN BE MADE phytate content, while the minerals affected can attack) were 13 percent lower in pescatarians and 22 Vegan donuts The eggs, WITH SUGAR THAT ISN’T also be sourced elsewhere: zinc in tofu; iron in sugar, and dairy can be PROCESSED USING percent lower in vegetarians. But the results also dates and strawberries; calcium in leafy greens. replaced with vegan BONE CHAR, BUT THIS IS showed that rates of stroke were 20 percent higher alternatives for a plant- STILL ADDED FREE based version but not among vegetarians. This was mostly due to SUGAR necessarily a healthy one. UUSS__112222--112233__ppllaannttbbaasseedd__hheeaalltthhyy..iinndddd 112222 2244//0088//22002211 1166::2233 UUSS__112222--112233__ppllaannttbbaasseedd__hheeaalltthhyy..iinndddd 112233 2244//0088//22002211 1166::2233
124 SHOULD I CHOOSE PLANT-BASED NUTRITION? 125 HOW CAN I EAT IN A SUSTAINABLE WAY? Many people have this question on their minds at the moment, but it can be difficult to understand the issues involved. Be reassured that there are practical steps you can take with your diet to minimize your environmental impact. It is more important than ever before that we take Half of the habitable land on the planet is used care of planet Earth. How we use natural resources for agriculture, with 77 percent of it dedicated to for food production must become more sustainable rearing livestock. Yet meat and dairy produce provide for the sake of future generations and for overall only 17 percent of global energy intake, and only harmony of life on the planet. 33 percent of global protein intake. This resource- The agriculture industry is the biggest driver of intensive approach to food production isn’t paying biodiversity vons (GHGe). Fossil fuels are used all off, either in terms of nutrition or sustainability. along supply chains and beyond, from farming and Meat and dairy production are responsible for production to distribution and delivery to waste two-thirds of the GHGe of the US’s food industry. disposal. We all have the power to make dietary Globally, countless ecosystems are destroyed to make changes, large or small, to contribute to solutions. space for grazing livestock or for raising feed crops for them. At this point, even the most dedicated EAT LESS MEAT AND DAIRY meat eaters are choosing to lean more on plant-based If you eat meat, you don’t need to give it up sources of protein as a lifeline for the planet. completely. Consider reducing your consumption Going plant-based doesn’t mean eating no meat of red meat to one portion per week. This change whatsoever. Try following the planetary diet (see alone could make a difference. page 127) as closely as possible. There are many Producing protein sources This chart compares the use of resources and the GHGe involved in the production of a 3.5oz (100g) portion of protein from various food sources. It shows that producing plant sources such as peas and tofu has considerably less impact than sheep meat or cheese. WATER USE per 3.5oz (100g) LAND USE per sq yard GHG per 3.5oz (100g) NUTS BEANS PEANUTS TOFU EGGS POULTRY FISH PORK CHEESE DAIRY SHELLFISH SHEEP BEEF UUSS__112244--112255__ssuussttaaiinnaabbiilliittyy..iinndddd 112244 2244//0088//22002211 1166::2233
112244 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 112255 HOW CAN I EAT IN A ways of using plant-based proteins (see pages 128– accordingly to help you avoid buying excess 129). Educate yourself on the key nutrients you need perishable produce. SUSTAINABLE WAY? to include on a plant-based diet (see pages 130–131). If you have produce that’s about to spoil, think about freezing it. Blanch vegetables and fruit before THINK LOGISTICS freezing them. Or cook up meals to freeze in Many people have this question on their minds at the moment, but it can be Shopping locally and seasonally where possible portions for your later convenience. difficult to understand the issues involved. Be reassured that there are practical will reduce the length of the supply chains Try growing your own, then pick just what you steps you can take with your diet to minimize your environmental impact. involved in the food you eat, thereby reducing need. Herbs on a windowsill, a sack of potatoes on emissions. Do what you can to reduce food waste. the back doorstep, or a patch of garden dedicated to Plan meals and snacks for the week ahead and shop vegetables is good for body, mind, soul, and planet. It is more important than ever before that we take Half of the habitable land on the planet is used care of planet Earth. How we use natural resources for agriculture, with 77 percent of it dedicated to for food production must become more sustainable rearing livestock. Yet meat and dairy produce provide for the sake of future generations and for overall only 17 percent of global energy intake, and only harmony of life on the planet. 33 percent of global protein intake. This resource- ACCOUNTS The agriculture industry is the biggest driver of intensive approach to food production isn’t paying FOR biodiversity vons (GHGe). Fossil fuels are used all off, either in terms of nutrition or sustainability. 80% along supply chains and beyond, from farming and Meat and dairy production are responsible for OF ALL HUMAN 10% WATER USE production to distribution and delivery to waste two-thirds of the GHGe of the US’s food industry. disposal. We all have the power to make dietary Globally, countless ecosystems are destroyed to make OF TOTAL changes, large or small, to contribute to solutions. space for grazing livestock or for raising feed crops 90% for them. At this point, even the most dedicated GREENHOUSE EAT LESS MEAT AND DAIRY meat eaters are choosing to lean more on plant-based GAS EMISSIONS FOOD OF FISHERIES IN THE US NOW FULLY If you eat meat, you don’t need to give it up sources of protein as a lifeline for the planet. PRODUCTION EXPLOITED AND/OR completely. Consider reducing your consumption Going plant-based doesn’t mean eating no meat INDUSTRY OVERFISHED of red meat to one portion per week. This change whatsoever. Try following the planetary diet (see alone could make a difference. page 127) as closely as possible. There are many IS A LEADING CAUSE OF 27% ECOSYSTEM LOSS Producing protein sources This chart compares the use BIODIVERSITY LOSS BY THE of resources and the GHGe involved in the production of a SOIL POLLUTION INDUSTRY 3.5oz (100g) portion of protein from various food sources. WATER It shows that producing plant sources such as peas and tofu POLLUTION has considerably less impact than sheep meat or cheese. 1.4 71% WATER USE Unsustainable factors Food waste is a major per 3.5oz contributing factor making the food industry BILLION TONS IN OUR (100g) unsustainable. The consumer can have a big impact OF ALL FOOD HOMES here. The majority of food waste takes place in the PRODUCED IS SPOILED LAND USE home, with 30–40 percent of the purchased food OR WASTED GLOBALLY ending up in the trash. We must learn to treat foods as EVERY YEAR per sq yard precious resources and do our best to prevent wastage. GHG per 3.5oz KEY (100g) Food industry accountable NUTS BEANS PEANUTS TOFU EGGS POULTRY FISH PORK CHEESE DAIRY SHELLFISH SHEEP BEEF Food industry and consumers accountable UUSS__112244--112255__ssuussttaaiinnaabbiilliittyy..iinndddd 112244 2244//0088//22002211 1166::2233 UUSS__112244--112255__ssuussttaaiinnaabbiilliittyy..iinndddd 112255 2244//0088//22002211 1166::2233
126 SHOULD I CHOOSE PLANT-BASED NUTRITION? 127 IS PLANT-BASED BETTER FOR THE ENVIRONMENT? Yes, definitely—although not all plant proteins are equal. To make the greatest impact with your plant-based dietary choices, it’s important to know which products have the smallest carbon footprint. When you factor in the impact of the meat and dairy It is a plant-based food made from fungus and industry (see page 124), a plant-based diet is inarguably produced through fermentation. (You might the way forward. But most plant products have the recognize it by the brand name Quorn.) It has a edge over others. For instance, soy, a legume, fixes significantly smaller carbon footprint than animal nitrogen into the soil as it grows, reducing the need proteins and uses 90 per ent less land, making it for nitrogen fertilizers. These produce nitrous oxide, a highly sustainable protein source. a potent greenhouse gas that leaks into waterways, causing damage to marine life and ecosystems. Soy FACTORING WATER USAGE also has a good nutritional profile, so leaning toward Water is becoming more scarce, especially in soy products alongside beans and lentils (for balance) countries that produce most of our foods. The is a sound environmental and nutritional choice. food industry accounts for 70 percent of water use. Mycoprotein, an excellent protein source, is often This is an important point to consider when using mistakenly considered an unhealthy processed food. nuts as a protein source. The country of origin and Rice production both uses and pollutes water excessively WATER POLLUTION per g PO3 eq 4 WATER USE Almond crops are per sq yard unsustainably thirsty LAND USE per sq yard Producing milk This chart compares the environmental impact of producing 7oz GASES (200ml) dairy milk compared with different per kg (2lb) types of plant-based milk. While any plant-based milk is a more environmentally friendly choice than dairy milk, soy milk DAIRY RICE SOY OAT ALMOND has the least overall impact. UUSS__112266--112277__ppllaanntt__bbaasseedd..iinndddd 112266 2244//0088//22002211 1166::2233
112266 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 112277 IS PLANT-BASED BETTER Planetary diet proportions FOR THE ENVIRONMENT? Deprivation is not part of the planetary diet. Proportions of food groups are designed with the health of the planet and human nutrition in mind. Yes, definitely—although not all plant proteins are equal. To make the No food groups are excluded, but the emphasis is greatest impact with your plant-based dietary choices, it’s important on plant-based eating and moderation. to know which products have the smallest carbon footprint. Fruits and green vegetables When you factor in the impact of the meat and dairy It is a plant-based food made from fungus and industry (see page 124), a plant-based diet is inarguably produced through fermentation. (You might the way forward. But most plant products have the recognize it by the brand name Quorn.) It has a edge over others. For instance, soy, a legume, fixes significantly smaller carbon footprint than animal nitrogen into the soil as it grows, reducing the need proteins and uses 90 per ent less land, making it for nitrogen fertilizers. These produce nitrous oxide, a highly sustainable protein source. Unsaturated fats Saturated fats a potent greenhouse gas that leaks into waterways, causing damage to marine life and ecosystems. Soy FACTORING WATER USAGE also has a good nutritional profile, so leaning toward Water is becoming more scarce, especially in soy products alongside beans and lentils (for balance) countries that produce most of our foods. The is a sound environmental and nutritional choice. food industry accounts for 70 percent of water use. Mycoprotein, an excellent protein source, is often This is an important point to consider when using Whole grains Starchy vegetables Added sugars Animal-sourced protein Plant- Dairy sourced protein mistakenly considered an unhealthy processed food. nuts as a protein source. The country of origin and Rice production both uses and pollutes water excessively WATER level of water stress caused by production are of the human body. The focus is on whole produce, POLLUTION relevant. For instance, the production of Californian with half of the diet coming from vegetables and per g PO3 eq 4 almonds is exceptionally water inefficient, using fruits. The other half consists primarily of whole around 1.3 gallons of water per almond (Californian grains and plant proteins (beans and lentils) but WATER USE Almond crops are per sq yard unsustainably thirsty farmers have committed to reduce water usage). also includes fats, modest amounts of meat and dairy, and some added sugars and starchy vegetables. THE PLANETARY DIET Different from a vegan or vegetarian diet, the LAND USE per sq yard The planetary diet is a useful practical guideline planetary diet simply requires you to reduce the for helping you eat sustainably and can be adapted number of portions of meat you consume and get Producing milk This chart compares the environmental impact of producing 7oz to meet a broad range of dietary needs and cultural the bulk of your protein from plant sources. For red GASES (200ml) dairy milk compared with different per kg (2lb) types of plant-based milk. While any preferences. This approach to balancing the diet meat fans, you’re looking at a burger a week or a plant-based milk is a more environmentally takes into account both the environmental impact large steak a month. Or you could have a couple of friendly choice than dairy milk, soy milk DAIRY RICE SOY OAT ALMOND has the least overall impact. of food production and the nutritional requirements portions of chicken and the same of fish per week. UUSS__112266--112277__ppllaanntt__bbaasseedd..iinndddd 112266 2244//0088//22002211 1166::2233 UUSS__112266--112277__ppllaanntt__bbaasseedd..iinndddd 112277 2244//0088//22002211 1166::2233
128 SHOULD I CHOOSE PLANT-BASED NUTRITION? 129 WILL I STRUGGLE TO EAT ENOUGH PROTEIN ON A VEGAN DIET? It’s a misconception that anyone adopting a vegan lifestyle will struggle to get enough protein. Plant proteins may be arranged differently from those in meat and fish, but a sufficiently varied vegan diet will supply more than enough to meet daily needs. Protein is present in every cell and is fundamental low in lysine, which is found in many pulses. So for tissue repair, muscle growth, and more. The idea vegans should consume as many foods as possible. that a vegan diet is protein deficient derives from Previous research suggested it is necessary to the fact that not all protein is “complete.” Protein’s combine incomplete proteins at each meal; now building blocks are 20 amino acids, and the body has we know eating a mixture over the day is effective, to obtain nine of these from diet; complete protein because of how the liver stores amino acids. Also, contains similar amounts of all nine. While meat, the body can efficiently use up to only 20g–40g eggs, fish, and dairy provide complete protein, most of protein at a time. Healthy adults should eat at plant-based foods are incomplete. For example, least 0.8g per 2lb (1kg) of body weight daily—more, white and brown rice are high in methionine and depending on activity levels (see pages 80–81). PROTEIN SOURCES 60g Soy-based foods provide complete protein. 50 Tofu (8g per 3.5oz) is the best known, while tempeh (18g) is a fermented meat substitute 40 with a stronger flavor; soy-based milk and 30 yogurt alternatives are also fermented. Other 20 meat substitutes are mycoprotein (see opposite) and high-protein seitan (75g), which is made 10 from wheat gluten so may not suit those with 0 an intolerance. 10 Quinoa (4.4g per 3.5oz) is another complete protein source. It is actually a seed from the 20 30 40 50 60g Protein intake: Outside of Europe and North America, it is normal for plants to provide the major source of protein. AIDNI AISA ACIRFA ACIREMA .S ANIHC EPORUE ACIREMA .N KEY Plant-based protein Animal-based protein DAILY AVERAGE PROTEIN CONSUMPTION BY REGION Recommended intake UUSS__112288--112299__PPrrootteeiinn__vveeggaann..iinndddd 112288 2244//0088//22002211 1166::2233
128 SHOULD I CHOOSE PLANT-BASED NUTRITION? 129 WILL I STRUGGLE TO EAT ENOUGH PROTEIN ON A VEGAN DIET? It’s a misconception that anyone adopting a vegan lifestyle will struggle to get enough protein. Plant proteins may be arranged differently from those in meat and fish, but a sufficiently varied vegan diet will supply more than enough to meet daily needs. Protein is present in every cell and is fundamental low in lysine, which is found in many pulses. So for tissue repair, muscle growth, and more. The idea vegans should consume as many foods as possible. that a vegan diet is protein deficient derives from Previous research suggested it is necessary to the fact that not all protein is “complete.” Protein’s combine incomplete proteins at each meal; now building blocks are 20 amino acids, and the body has we know eating a mixture over the day is effective, to obtain nine of these from diet; complete protein because of how the liver stores amino acids. Also, contains similar amounts of all nine. While meat, the body can efficiently use up to only 20g–40g eggs, fish, and dairy provide complete protein, most of protein at a time. Healthy adults should eat at plant-based foods are incomplete. For example, least 0.8g per 2lb (1kg) of body weight daily—more, white and brown rice are high in methionine and depending on activity levels (see pages 80–81). PROTEIN SOURCES 60g Soy-based foods provide complete protein. 50 Tofu (8g per 3.5oz) is the best known, while tempeh (18g) is a fermented meat substitute 40 with a stronger flavor; soy-based milk and 30 yogurt alternatives are also fermented. Other 20 meat substitutes are mycoprotein (see opposite) and high-protein seitan (75g), which is made 10 from wheat gluten so may not suit those with 0 an intolerance. 10 Quinoa (4.4g per 3.5oz) is another complete protein source. It is actually a seed from the 20 30 40 50 60g Protein intake: Outside of Europe and North America, it is normal for plants to provide the major source of protein. AIDNI AISA ACIRFA ACIREMA .S ANIHC EPORUE ACIREMA .N 128 SHOULD I CHOOSE PLANT-BASED NUTRITION? 129 EDAMAME THESE GREEN SOYBEANS ARE HARVESTED AT AROUND 80 PERCENT MATURITY Soybeans: US dietary guidelines group soy and A 3oz SERVING OF edamame beans with FROZEN SHELLED vegetables. EDAMAME BEANS PROVIDES AROUND 13g OF PROTEIN Chenopodium quinoa plant, although it’s usually grouped with whole grains. What is mycoprotein? Otherwise, nuts and seeds are ideal for snacking, in salad or cereal, or in nut butters. They can MYCOPROTEIN IS A MEAT SUBSTITUTE contain up to 20g of protein per 3.5oz; a portion BETTER KNOWN AS QUORN. Its main ingredient, Fusarium venenatum, is a is around 1oz. Beans and lentils typically contain microfungus that occurs naturally in soil—fungi about 20g of protein per 3.5oz; one portion is aren’t classified as plants because they lack around 4oz cooked. Fortified foods like vegan chlorophyll and have a different cellular structure. It energy balls, breakfast cereals, and protein bars is fed with carbohydrate in large fermenters and the KEY liquid is separated by centrifugal force to create often have soy added. Check labels—“vegan” mycoprotein dough. A complete protein source (11g Plant-based protein doesn’t mean a food is low in salt, sugar, and fat. per 3.5oz), it also contains various micronutrients. Animal-based protein Some vegetables also provide protein, for DAILY AVERAGE PROTEIN CONSUMPTION BY REGION Recommended intake example, 2.8g per 3.5oz of broccoli. UUSS__112288--112299__PPrrootteeiinn__vveeggaann..iinndddd 112288 2244//0088//22002211 1166::2233 UUSS__112288--112299__PPrrootteeiinn__vveeggaann..iinndddd 112299 2255//0088//22002211 1199::1155
130 SHOULD I CHOOSE PLANT-BASED NUTRITION? 131 WHAT OTHER NUTRIENTS SHOULD PLANT-BASED EATERS CONSIDER? Anyone who eats only certain animal-derived foods, or excludes them completely, should still be able to get most of the nutrients they need from a plant-based diet. In some cases, though, it may be necessary to supplement. Most nutrients are abundant in a plant-based plant sources, but research indicates plants are less diet. Protein is often a concern but shouldn’t be effective sources of eicosapentaenoic (EPA) and a problem, whereas a vitamin D supplement is docosahexaenoic (DHA) acids. EPA and DHA necessary for everyone in winter (see pages 138– can be taken as microalgae-based supplements. 139). These are some key micronutrients to consider. Vitamin B12 keeps nerve and blood cells healthy. As well as ensuring strong bones and teeth, Most plant-based sources can’t be processed by the calcium regulates muscle contractions and helps body, although you can sprinkle nutritional yeast on blood clot normally. Dairy is a key source; only meals or take a daily supplement. Ask your primary some plants contain calcium, at fairly low levels. care physician to periodically check your B12 levels. Even mild iodine deficiency can harm a baby’s Iron helps maintain the immune system and form developing brain. It also affects levels of thyroid hemoglobin, which transports oxygen around the hormones, which support metabolic function. body. Deficiency can lead to anemia (below-average Plant sources are scarce, so those at higher risk, in intake is prevalent among young children and women particular pregnant or breastfeeding women, should of reproductive age). Vitamin C aids iron absorption. consider supplementing. Talk to your doctor first. The body doesn’t make zinc, which helps make Heart and brain-friendly omega-3 fatty acids cells and enzymes and processes fat, protein, and can be obtained only from diet (see pages 16–17). carbohydrates; 30g of hemp or pumpkin seeds ALA (alpha-linolenic-acid) is available in various provides a third of daily adult needs. CALCIUM VITAMIN D IODINE OMEGA-3 RDA RDA RDA RDA (AGE 19–64): 700mg (VARIES): 8.5mcg–10mcg (ADULTS): 140mcg N/A PLANT SOURCES PLANT SOURCES PLANT SOURCES PLANT SOURCES DRIED FRUIT | NUTS | TOFU OILY FISH | FORTIFIED FOODS FORTIFIED WALNUTS | HEMP/CHIA LEAFY GREENS | KIDNEY UV-EXPOSED MUSHROOOMS PLANT MILK SEEDS | FLAXSEED BEANS | TAHINI SOY | CANOLA OIL UUSS__113300--113311__ssppeecciiaall__ccoonnssiiddeerraattiioonnss..iinndddd 113300 2244//0088//22002211 1166::2233
113300 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 113311 WHAT OTHER NUTRIENTS A SINGLE BELL PEPPER CAN SHOULD PLANT-BASED PROVIDE MORE THAN THREE TIMES YOUR EATERS CONSIDER? DAILY VITAMIN C REQUIREMENT Anyone who eats only certain animal-derived foods, or excludes them completely, Eat a rainbow: Vitamin C in should still be able to get most of the nutrients they need from a plant-based diet. orange, red, and yellow peppers, In some cases, though, it may be necessary to supplement. and leafy green vegetables and tomatoes, helps absorb more iron within a vegan diet. Most nutrients are abundant in a plant-based plant sources, but research indicates plants are less Most adults don’t consume enough selenium diet. Protein is often a concern but shouldn’t be effective sources of eicosapentaenoic (EPA) and (see below). It helps with reproductive health, a problem, whereas a vitamin D supplement is docosahexaenoic (DHA) acids. EPA and DHA immune system maintenance, and tissue repair; 2–3 necessary for everyone in winter (see pages 138– can be taken as microalgae-based supplements. Brazil nuts supply the complete daily requirement. 139). These are some key micronutrients to consider. Vitamin B12 keeps nerve and blood cells healthy. As well as ensuring strong bones and teeth, Most plant-based sources can’t be processed by the OXALATES AND PHYTATES calcium regulates muscle contractions and helps body, although you can sprinkle nutritional yeast on While plant foods are healthy, some can have blood clot normally. Dairy is a key source; only meals or take a daily supplement. Ask your primary less beneficial elements—another reason to some plants contain calcium, at fairly low levels. care physician to periodically check your B12 levels. eat a wide variety. For example, spinach Even mild iodine deficiency can harm a baby’s Iron helps maintain the immune system and form and Swiss chard contain oxalates, an developing brain. It also affects levels of thyroid hemoglobin, which transports oxygen around the acid that reduces calcium absorption. hormones, which support metabolic function. body. Deficiency can lead to anemia (below-average Various beans, whole grain cereals, ORANGE, RED, Plant sources are scarce, so those at higher risk, in intake is prevalent among young children and women nuts, and seeds (including almonds, AND YELLOW PEPPERS particular pregnant or breastfeeding women, should of reproductive age). Vitamin C aids iron absorption. sesame seeds, and lentils) contain ARE RIPENED VERSIONS consider supplementing. Talk to your doctor first. The body doesn’t make zinc, which helps make compounds called phytates, which OF GREEN AND ALL HAVE VARYING LEVELS OF Heart and brain-friendly omega-3 fatty acids cells and enzymes and processes fat, protein, and inhibit zinc and iron absorption. MICRONUTRIENTS can be obtained only from diet (see pages 16–17). carbohydrates; 30g of hemp or pumpkin seeds Soaking before eating, or adding ALA (alpha-linolenic-acid) is available in various provides a third of daily adult needs. berries to your cereal, can help. CALCIUM VITAMIN D IODINE OMEGA-3 VITAMIN B12 IRON ZINC SELENIUM RDA RDA RDA RDA RDA RDA: MEN 18+ 8.7mg RDA (19–64): RDA (19–64): (AGE 19–64): 700mg (VARIES): 8.5mcg–10mcg (ADULTS): 140mcg N/A (AGE 19–64): 1.5mcg WOMEN 18+ 14.8mg/50+ 8.7mg MEN 9.5g MEN 75mcg WOMEN 7mg WOMEN 60mcg PLANT SOURCES PLANT SOURCES PLANT SOURCES PLANT SOURCES PLANT SOURCES PLANT SOURCES DRIED FRUIT | NUTS | TOFU OILY FISH | FORTIFIED FOODS FORTIFIED WALNUTS | HEMP/CHIA NUTRITIONAL YEAST | WHOLE WHEAT | BEANS PLANT SOURCES PLANT SOURCES LEAFY GREENS | KIDNEY UV-EXPOSED MUSHROOOMS PLANT MILK SEEDS | FLAXSEED FORTIFIED CEREALS | YEAST LENTILS | PRUNES | DATES NUTS | LENTILS BRAZIL NUTS | WHOLE WHEAT BEANS | TAHINI SOY | CANOLA OIL SPREAD STRAWBERRIES TOFU | QUINOA BREAD | BROWN RICE | LENTILS UUSS__113300--113311__ssppeecciiaall__ccoonnssiiddeerraattiioonnss..iinndddd 113300 2244//0088//22002211 1166::2233 UUSS__113300--113311__ssppeecciiaall__ccoonnssiiddeerraattiioonnss..iinndddd 113311 2244//0088//22002211 1166::2233
132 SHOULD I CHOOSE PLANT-BASED NUTRITION? 133 WHY IS IT IMPORTANT TO INCREASE PLANT-BASED DIVERSITY? There is a lot of buzz around the subject of gut health these days, in particular the positive impact a diverse plant-based diet can have on the gut. This isn’t just a health fad—there’s good research to support it. It is widely accepted that we have a healthier gut EXPERIMENTATION AND FAMILIARITY if we have a diverse range of different gut bacteria, Consuming a diverse plant-based diet helps you which carry out many important functions that increase your microbiota diversity. Aim to eat more benefit our health. In the gut, we have trillions of fruits and vegetables than you normally do—most microorganisms that are collectively referred to people don’t get enough. Experiment with beans as the gut microbiota (see pages 48–49). What we and legumes you don’t usually use. Remember that eat affects the composition of these bacteria. plant-based includes whole grains, nuts, and seeds. Research shows that greater gut microbe diversity When it comes to your macronutrient is associated with better health. One study of over requirements (see page 10), become familiar 10,000 stool samples found that those who eat more with the types of plant-based protein, fat, and than 30 types of plant-based foods per week had more carbohydrate sources available. Plant-based carbs diverse gut microbes than those eating less than 10. can be an excellent source of fiber (see pages 18 A large study of more than 20,000 people found and 44), which is important for feeding and a strong association between a diet high in plant maintaining your microbiota. foods and bowel movement frequency. Chronic Plant-based proteins can be combined to provide constipation can have a serious negative effect on your full protein requirements (see pages 14–15). your health, so staying “regular” is another benefit Aim to include a variety of fruits and vegetables in of being on a largely plant-based diet. your diet. Don’t opt for the same things every day. EAT SPREADS FERMENTED SEASONALLY AND DIPS FOODS SEASONAL PRODUCE CAN USE THESE REGULARLY FERMENTED FOODS PROVIDE A HIGHER YIELD AS ADDITIONAL SOURCES OF CONTAINING LIVE MICROBES OF NUTRIENTS. TRY BOOKING FIBER, PROTEIN, AND NUTRIENTS. CONTRIBUTE TO THE GUT A REGULAR DELIVERY WITH TRY NUT BUTTERS, HUMMUS, MICROBIOTA AND CONTAIN A LOCAL FARM TO HELP SALSAS, AVOCADO DIP, AND PLENTY OF USEFUL YOU TRY OUT NEW BABA GANOUSH NUTRIENTS VEGETABLES UUSS__113322--113333__ppllaanntt__bbaasseedd__ddiivveerrssiiiittyy..iinndddd 113322 2244//0088//22002211 1166::2233
113322 SSHHOOUULLDD II CCHHOOOOSSEE PPLLAANNTT--BBAASSEEDD NNUUTTRRIITTIIOONN?? 113333 WHY IS IT IMPORTANT TO Think switch MASHED SWEET POTATO INCREASE PLANT-BASED The only way to increase your plant-based repertoire is to MASHED MASHED DIVERSITY? explore alternatives. This PARSNIPS SWEDE doesn’t have to be an onerous task. Simply MASHED There is a lot of buzz around the subject of gut health these days, in particular choose one new POTATOES KIWI SLICES STEAMED (WITH SKIN) the positive impact a diverse plant-based diet can have on the gut. This isn’t alternative to try, ASPARAGUS then incorporate just a health fad—there’s good research to support it. that regularly into FRUITS AND your diet before trying another VEGETABLES APPLE PASSION SAUTÉED BOILED switch. SPINACH PEAS A BROAD VARIETY BANANA FRUIT SEEDS It is widely accepted that we have a healthier gut EXPERIMENTATION AND FAMILIARITY WILL SUPPLY THE ORANGE AND PULP if we have a diverse range of different gut bacteria, Consuming a diverse plant-based diet helps you BODY WITH DIFFERENT MICRONUTRIENTS which carry out many important functions that increase your microbiota diversity. Aim to eat more benefit our health. In the gut, we have trillions of fruits and vegetables than you normally do—most STEAMED MELON microorganisms that are collectively referred to people don’t get enough. Experiment with beans KALE SLICES as the gut microbiota (see pages 48–49). What we and legumes you don’t usually use. Remember that eat affects the composition of these bacteria. plant-based includes whole grains, nuts, and seeds. Research shows that greater gut microbe diversity When it comes to your macronutrient is associated with better health. One study of over requirements (see page 10), become familiar HUMMUS GNOCCHI 10,000 stool samples found that those who eat more with the types of plant-based protein, fat, and than 30 types of plant-based foods per week had more carbohydrate sources available. Plant-based carbs diverse gut microbes than those eating less than 10. can be an excellent source of fiber (see pages 18 BRAN SOY SPELT COUS SPREAD YOGURT PASTA COUS A large study of more than 20,000 people found and 44), which is important for feeding and a strong association between a diet high in plant maintaining your microbiota. CHEESE PASTA foods and bowel movement frequency. Chronic Plant-based proteins can be combined to provide MYCO- RYE PROTEIN constipation can have a serious negative effect on your full protein requirements (see pages 14–15). BREAD (QUORN) your health, so staying “regular” is another benefit Aim to include a variety of fruits and vegetables in of being on a largely plant-based diet. your diet. Don’t opt for the same things every day. PROTEIN CARBS TO SWITCH IT UP, TRY UNFAMILIAR RED MEAT/ ADD SOME FERMENTS GRAINS, LEGUMES, AND WHITE SOURDOUGH TOFU POULTRY LIKE TEMPEH AND TRY RICE. TRY BREADS BREAD BREAD DIFFERENT TYPES MADE WITH VARYING EAT SPREADS FERMENTED OF BEANS WHOLE GRAINS SEASONALLY AND DIPS FOODS SPROUTED LENTILS SEASONAL PRODUCE CAN USE THESE REGULARLY FERMENTED FOODS LOAF PROVIDE A HIGHER YIELD AS ADDITIONAL SOURCES OF CONTAINING LIVE MICROBES WHITE CHICKPEAS OF NUTRIENTS. TRY BOOKING FIBER, PROTEIN, AND NUTRIENTS. CONTRIBUTE TO THE GUT RICE A REGULAR DELIVERY WITH TRY NUT BUTTERS, HUMMUS, MICROBIOTA AND CONTAIN BLACK EDAMAME BULGAR A LOCAL FARM TO HELP SALSAS, AVOCADO DIP, AND PLENTY OF USEFUL FREEKAH BEANS BEANS WHEAT YOU TRY OUT NEW BABA GANOUSH NUTRIENTS VEGETABLES BROWN QUINOA RICE UUSS__113322--113333__ppllaanntt__bbaasseedd__ddiivveerrssiiiittyy..iinndddd 113322 2244//0088//22002211 1166::2233 UUSS__113322--113333__ppllaanntt__bbaasseedd__ddiivveerrssiiiittyy..iinndddd 113333 2244//0088//22002211 1166::2233
UUSS__113344--113355__cchhaapptteerr__55..iinndddd 113344 2244//0088//22002211 1166::2233
CAN WE EAT FOR BETTER HEALTH OUTCOMES? Can I Eat to Boost Immunity? 136 Should I Be Worried About Should I Try to Get More Vitamin D Food Allergies and Intolerances? 158 from My Diet? 138 How Can I Find Out if I Have an Does Gut Health Play a Role in Allergy or Intolerance? 160 Immune Response? 140 How Do I Cut Gluten or Lactose from My Diet? 162 Is There a Relationship Between Sleep Could I Have Irritable Bowel Syndrome (IBS)? 164 Quality and Nutrition? 142 What Is IBD? 165 Can Diet Help Reduce Menstrual Symptoms? 144 What Are FODMAPs and Can Avoiding Should I Eat Differently for Menopause? 146 Them Cure IBS? 166 Can I Eat to Support the Aging Process? 148 What Is an Anti-inflammatory Diet? 168 Can I Eat to Guard Against Dementia? 150 What Is Diabetes and What Are the Why Do I Feel Bloated? 152 Risk Factors? 170 Is It Normal to Fart? 154 Can Diet Help Prevent or Why Am I Constipated? 155 Manage Type 2 Diabetes? 172 Is Diarrhea Caused by Diet? 156 Do Some Foods Cause or Prevent Cancer? 174 UUSS__113344--113355__cchhaapptteerr__55..iinndddd 113344 2244//0088//22002211 1166::2233 UUSS__113344--113355__cchhaapptteerr__55..iinndddd 113355 2255//0088//22002211 1199::1155
136 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 137 CAN I EAT TO BOOST IMMUNITY? The immune system is incredibly complex and acts like a surveillance system to recognize and respond to pathogens such as bacteria, viruses, and toxins. Our food choices could help, or hinder, its ability to defend the body. “Boosting” immunity is a marketing concept to system isn’t working well, or you eat a restricted promote supplements; in fact, our immune system diet, talk to your primary care physician (PCP). is programmed to work in a highly specific way. Diet can support it to function at its best when KEY MICRONUTRIENTS under attack and to maintain a protective barrier. Aim to regularly eat foods containing these vitamins Our immune system is always “on,” but when and minerals. (RDAs are based on US guidelines.) activated, it needs additional energy to fuel the Vitamin A supports production of immune cells; creation of millions of new immune cells to fight the a deficiency can increase susceptibility to infections. threat. Poor diet can contribute to weakened immune (RDA age 14–51+: men 900mcg RAE, women response, as can gut health and antibiotic use (see 700mcg RAE) pages 140–141). Healthy adults should be able to Vitamin B6 is involved in producing immune obtain enough of the micronutrients that support cells and processing of antibodies. B9 (folate) and immune function from a balanced and varied diet. B12 are important for red blood cell function; B12 Plant-based eaters may need to supplement certain is also involved in immune cell synthesis. Vegans nutrients (see pages 130–131); otherwise, it shouldn’t may need to supplement B12. (RDA: B6, men age be necessary unless medically advised and could 14–50: 1.3mg, age 51+: 1.7mg; women age 14–18: even cause health problems. If you feel your immune 1.2mg, age 19–50: 1.3mg, age 51+: 1.5mg; Folate, WHITE PATHOGEN The immune response BLOOD CELLS DETECTED Two key immune responses work together. The natural response Different types of white blood One/more of these cell quickly tries to stop the pathogen cells patrol or wait to be types detect a pathogen via from spreading, while the slower alerted. Many types play a its antigen (surface protein). adaptive response requires exposure role in both stages of the They multiply and signal to the pathogen first then learns to immune response. other immune cells. identify it rapidly. UUSS__113366--113377__iimmmmuunniittyy..iinndddd 113366 2244//0088//22002211 1166::2244
113366 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 113377 CAN I EAT TO BOOST IMMUNITY? VIT A VIT B6 FOLATE CARROTS WHOLE GRAINS The immune system is incredibly complex and acts like a surveillance system to VIT B12 VIT C VIT D recognize and respond to pathogens such as bacteria, viruses, and toxins. Our food EGGS VIT E IRON SELENIUM choices could help, or hinder, its ability to defend the body. CITRUS FRUIT ZINC SHELLFISH POULTRY “Boosting” immunity is a marketing concept to system isn’t working well, or you eat a restricted Immunity menu: Micronutrients promote supplements; in fact, our immune system diet, talk to your primary care physician (PCP). essential to a properly functioning PEPPERS immune system are found in high is programmed to work in a highly specific way. amounts in these foods. BROCCOLI Diet can support it to function at its best when KEY MICRONUTRIENTS under attack and to maintain a protective barrier. Aim to regularly eat foods containing these vitamins Our immune system is always “on,” but when and minerals. (RDAs are based on US guidelines.) age 14–19+: 400mcg DFE; B12, age 14–19+: 2.4mcg) tofu, beans, and nuts. Vegans and women who lose activated, it needs additional energy to fuel the Vitamin A supports production of immune cells; Vitamin C protects cells and maintains skin, iron during periods may need to supplement and creation of millions of new immune cells to fight the a deficiency can increase susceptibility to infections. bones, and blood vessels. There’s conflicting should consult their PCP. (RDA: men age 19–51+: threat. Poor diet can contribute to weakened immune (RDA age 14–51+: men 900mcg RAE, women evidence on whether it reduces the risk of catching 8mg; women age 19–50: 18mg, age 51+: 8mg) response, as can gut health and antibiotic use (see 700mcg RAE) viruses. (RDA age 19+: men 40mg, women 75mg) Selenium supports the production of immune pages 140–141). Healthy adults should be able to Vitamin B6 is involved in producing immune Vitamin D deficiency has been linked with a cells; within a plant-based diet, 2–3 Brazil nuts daily obtain enough of the micronutrients that support cells and processing of antibodies. B9 (folate) and reduced immune response (see pages 138–139). can provide a sufficient amount. (RDA age 19–64: immune function from a balanced and varied diet. B12 are important for red blood cell function; B12 Vitamin E deficiency is linked to an increased men 75mcg, women 60mcg) Plant-based eaters may need to supplement certain is also involved in immune cell synthesis. Vegans susceptibility to infection. (RDA age14+: 15mg) Zinc helps produce new immune cells. Deficiency nutrients (see pages 130–131); otherwise, it shouldn’t may need to supplement B12. (RDA: B6, men age Iron is important for immune cells; a low level increases susceptibility to respiratory infection. be necessary unless medically advised and could 14–50: 1.3mg, age 51+: 1.7mg; women age 14–18: also increases the risk of anemia. Meat-derived (RDA men age 14+: 11mg; women age 14–18 9mg, even cause health problems. If you feel your immune 1.2mg, age 19–50: 1.3mg, age 51+: 1.5mg; Folate, iron is easier to absorb than plant-based iron from age 19+ 8mg) KEY White blood cells include: T lymphocyte cells B lymphocyte cells Natural killer lymphocyte cells WHITE PATHOGEN NATURAL ADAPTIVE MEMORY The immune response BLOOD CELLS DETECTED RESPONSE RESPONSE CELLS Phagocytes Others Two key immune responses work together. The natural response Different types of white blood One/more of these cell Some white blood cells try to If it doesn’t work, certain Some antibodies and Pathogen quickly tries to stop the pathogen cells patrol or wait to be types detect a pathogen via absorb or destroy pathogens; white blood cells produce an "memory" cells (T and B cells) from spreading, while the slower alerted. Many types play a its antigen (surface protein). others try to destroy antibody specific to the remain in the body so it can Antibody adaptive response requires exposure role in both stages of the They multiply and signal infected/abnormal cells. pathogen that locks onto it; quickly recognize and target to the pathogen first then learns to immune response. other immune cells. others destroy infected cells. that pathogen next time. Infected cells identify it rapidly. UUSS__113366--113377__iimmmmuunniittyy..iinndddd 113366 2244//0088//22002211 1166::2244 UUSS__113366--113377__iimmmmuunniittyy..iinndddd 113377 2244//0088//22002211 1166::2244
138 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 139 SHOULD I TRY TO GET MORE VITAMIN D FROM MY DIET? Vitamin D is essential for healthy teeth, bones, and muscle, but did you know it’s technically a hormone? Unlike every other vitamin, we make vitamin D ourselves simply by the action of sunlight directly on skin, although we may need supplements. Anyone not living at a high latitude should be HOW MUCH DO I NEED? able to get enough UVB light exposure to meet all Vitamin D’s interaction with other nutrients enables their vitamin D needs in spring and summer, from us to build and maintain healthy bones and muscles, about 15 minutes outside daily with legs or forearms for example, by helping the body to absorb and exposed. (Cloud cover and broad spectrum sun conserve calcium. According to US health guidelines, protection may reduce exposure.) From October most people 14-70 years old need 15mcg/600IU and to March, the sun is too weak in the northern people 71+ need 20mcg/800IU; since it can be hemisphere for us to make enough vitamin D, difficult to obtain this much from diet, a 10mcg so we need other sources to maintain our levels. supplement may be necessary between October Only a few foods contain the two main forms and March. People who get little or no sun exposure, of vitamin D: D2 (ergocalciferol) is plant based those with darker skin, and children under five and found in fortified milks, cereals, and should consider supplementing vitamin D, based on mushrooms. Oily fish and fish oils are good sources blood lab levels. Because breast milk has low levels of D3 (cholecalciferol); one tablespoon of cod liver of vitamin D, breastfed babies and those consuming oil contains around 30mcg (micrograms), while less than 17oz of formula (which is fortified) should other animal sources contain lower levels. be given an 8.5mcg supplement daily. Vitamin D is found in many body tissues and may have multiple health impacts; a deficiency has been linked to inflammation and an increased risk of weight Vitamin D3 gain and diabetes. More positively, recent studies have identified potential benefits for female and male Vitamin D from sunshine is inert and the body converts it into a biologically active fertility and found that women with sufficient levels form in a two-stage process. of vitamin D are less likely to miscarry. UVB DAYLIGHT SKIN PRE-VITAMIN D VITAMIN D Sun exposure Enzymes in the skin Forming D3 Activating UVB rays from UV light reacts with Enzymes convert into Another enzyme exposure to sunlight 7-dehydrocholesterol pre-vitamin D, which turns D3 into calcitriol, penetrate the skin. enzyme in skin cells. restructures into D3. its active form. UUSS__113388--113399__VViittaammiinn__DD..iinndddd 113388 2244//0088//22002211 1166::2244
113388 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 113399 SHOULD I TRY TO GET MORE Food sources of vitamin D VITAMIN D FROM MY DIET? WILD FOODS WITH NATURALLY MUSHROOMS SIGNIFICANT AMOUNTS OF Vitamin D is essential for healthy teeth, bones, and muscle, but did you know it’s MAY CONTAIN UP TO VITAMIN D ARE SCARCE, AND technically a hormone? Unlike every other vitamin, we make vitamin D ourselves 30mcg PER 3.5oz, FAR PRIMARILY ANIMAL-BASED. simply by the action of sunlight directly on skin, although we may need supplements. MORE THAN THOSE GROWN IN THE DARK—BUT BUY FROM A REPUTABLE SELLER OILY FISH Anyone not living at a high latitude should be HOW MUCH DO I NEED? able to get enough UVB light exposure to meet all Vitamin D’s interaction with other nutrients enables TROUT AND SALMON ARE RICH SOURCES AT AROUND 17mcg PER their vitamin D needs in spring and summer, from us to build and maintain healthy bones and muscles, 3.5oz; HERRING, MACKEREL AND about 15 minutes outside daily with legs or forearms for example, by helping the body to absorb and SARDINES CONTAIN LESS BUT exposed. (Cloud cover and broad spectrum sun conserve calcium. According to US health guidelines, ARE STILL GOOD OPTIONS. protection may reduce exposure.) From October most people 14-70 years old need 15mcg/600IU and to March, the sun is too weak in the northern people 71+ need 20mcg/800IU; since it can be hemisphere for us to make enough vitamin D, difficult to obtain this much from diet, a 10mcg so we need other sources to maintain our levels. supplement may be necessary between October EGG YOLKS Only a few foods contain the two main forms and March. People who get little or no sun exposure, EGGS FROM HENS ALLOWED of vitamin D: D2 (ergocalciferol) is plant based those with darker skin, and children under five TO ROAM OUTSIDE IN and found in fortified milks, cereals, and should consider supplementing vitamin D, based on SUNLIGHT HAVE LEVELS OF mushrooms. Oily fish and fish oils are good sources blood lab levels. Because breast milk has low levels VITAMIN D 3–4 TIMES HIGHER THAN FROM INDOOR HENS. of D3 (cholecalciferol); one tablespoon of cod liver of vitamin D, breastfed babies and those consuming oil contains around 30mcg (micrograms), while less than 17oz of formula (which is fortified) should other animal sources contain lower levels. be given an 8.5mcg supplement daily. Vitamin D is found in many body tissues and may have multiple health impacts; a deficiency has been MEAT AND OFFAL linked to inflammation and an increased risk of weight Vitamin D3 PORK, LAMB, BEEF, LIVER, AND gain and diabetes. More positively, recent studies have KIDNEYS CONTAIN SMALL identified potential benefits for female and male Vitamin D from sunshine is inert and the AMOUNTS OF VITAMIN D AT body converts it into a biologically active fertility and found that women with sufficient levels AROUND 0.5mcg–1mcg IN A form in a two-stage process. of vitamin D are less likely to miscarry. TYPICAL PORTION. PLANT EATERS Wild mushrooms are naturally FORTIFIED FOODS SOME CULTIVATED exposed to UV light and therefore a UVB DAYLIGHT SKIN PRE-VITAMIN D VITAMIN D MUSHROOMS HAVE good source of vitamin D; boost the CERTAIN FOODSTUFFS ARE BEEN ENHANCED content in supermarket mushrooms FORTIFIED WITH VITAMIN D IN by leaving them in sunlight. WITH BIOAVAILABLE SOME COUNTRIES. THESE VITAMIN D INCLUDE COW’S MILK AND SOY Sun exposure Enzymes in the skin Forming D3 Activating MILK (AND PRODUCTS MADE UVB rays from UV light reacts with Enzymes convert into Another enzyme exposure to sunlight 7-dehydrocholesterol pre-vitamin D, which turns D3 into calcitriol, FROM THEM), BREAKFAST penetrate the skin. enzyme in skin cells. restructures into D3. its active form. CEREALS, AND ORANGE JUICE. UUSS__113388--113399__VViittaammiinn__DD..iinndddd 113388 2244//0088//22002211 1166::2244 UUSS__113388--113399__VViittaammiinn__DD..iinndddd 113399 2244//0088//22002211 1166::2244
140 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 141 DOES GUT HEALTH PLAY A ROLE IN IMMUNE RESPONSE? While a lot remains unknown with regards to the relationship between the immune system and the gut, scientists have found some highly important links. Research indicates that as much as 70 percent of balance between Th1 and Th2. A failure to achieve our immune system resides in the gastrointestinal that balance, with a bias towards Th2 cells, is linked tract, including 80 percent of the body’s IgA to the development of allergies. Hence some scientists antibody-producing plasma cells. This may seem theorize that poor development of infant gut bacteria surprising until you consider that the digestive tract could be at the root of an increasing incidence in the is the site within our bodies that receives the most West of allergies and conditions that can be triggered day-to-day contact with external elements, mainly by allergic reaction, such as asthma and eczema. food and helpful gut bacteria but also pathogens The link between allergies, autoimmune diseases, and toxic substances. Not only are immune cells and the immune system is complex. In allergies, the in the gut able to mount a defense against such immune system recognizes harmless non-invaders harmful “foreign bodies,” uniquely, they are also as harmful. In autoimmune diseases, such as celiac able to distinguish such invaders from the similarly disease and rheumatoid arthritis, the body attacks “foreign” but needful array of nutrients in food its own tissues. There is growing evidence that and the microbiome of beneficial gut bacteria. composition of gut microbiota has an impact on the risk of developing these immune-related disorders. CRITICAL ROLE OF GUT BACTERIA All this need not mean your gut-immune Scientists are also discovering how seeding of interaction is fixed by age three. In a recent study, the gut microbiome helps ensure healthy eating a high fiber diet that includes plenty of development of the immune system in babies fermented foods (see pages 48–53), was shown and infants. Evidence suggests gut bacteria play to increase both the function and diversity of a key role in stimulating an expansion of intestinal microbiota, leading to personalized immune immune cells; in the proper functioning of antibody responses and decreased inflammation. secretion; and in achieving balance between the two groups of T helper cells—white blood cells Establishing your gut bacteria that activate most other immune cells. Our microbiota is believed to stabilize by the age IMMUNE-RELATED DISORDERS of three, remaining similar from that point into T helper cells divide into two groups: Th1 and Th2. adulthood. This suggests that early life experiences We are born with more Th2, and bacteria colonizing may impact the composition of this bacteria, the gut of newborns is needed to achieve a healthy affecting us throughout our lives. UUSS__114400--114411__iimmmmuunnee__ssyysstteemm..iinndddd 114400 2244//0088//22002211 1166::2244
114400 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 114411 DOES GUT HEALTH ‘‘ALT’’ immune systems PLAY A ROLE IN The immune centers of the NALT gut are part of the lymphatic adenoids Nasal-associated lymphoid tissues system and a wider network tonsils protect against IMMUNE RESPONSE? of lymphoid tissues that airborne pathogens take in the nasal area, skin, upper respiratory in the nose and and lungs—all known mucous membranes throat areas While a lot remains unknown with regards to the relationship between the immune by acronyms ending system and the gut, scientists have found some highly important links. in "ALT" for "-associated lymphoid tissue." BALT Bronchus-associated lower respiratory lymphoid tissues Research indicates that as much as 70 percent of balance between Th1 and Th2. A failure to achieve mucous membrane protect the air our immune system resides in the gastrointestinal that balance, with a bias towards Th2 cells, is linked passages of the lungs SALT tract, including 80 percent of the body’s IgA to the development of allergies. Hence some scientists Skin-associated lymphoid tissues bronchial patches antibody-producing plasma cells. This may seem theorize that poor development of infant gut bacteria sit just below the surprising until you consider that the digestive tract could be at the root of an increasing incidence in the skin and differ in not being part is the site within our bodies that receives the most West of allergies and conditions that can be triggered of a mucous membrane day-to-day contact with external elements, mainly by allergic reaction, such as asthma and eczema. GALT food and helpful gut bacteria but also pathogens The link between allergies, autoimmune diseases, Gut immune cells are and toxic substances. Not only are immune cells and the immune system is complex. In allergies, the found in the mucous Peyer’s patches membrane throughout in the gut able to mount a defense against such immune system recognizes harmless non-invaders the intestines and in harmful “foreign bodies,” uniquely, they are also as harmful. In autoimmune diseases, such as celiac more collected areas, such as Peyer’s able to distinguish such invaders from the similarly disease and rheumatoid arthritis, the body attacks appendix patches and in the “foreign” but needful array of nutrients in food its own tissues. There is growing evidence that appendix and the microbiome of beneficial gut bacteria. composition of gut microbiota has an impact on the urogenital mucous risk of developing these immune-related disorders. membranes CRITICAL ROLE OF GUT BACTERIA All this need not mean your gut-immune Scientists are also discovering how seeding of interaction is fixed by age three. In a recent study, the gut microbiome helps ensure healthy eating a high fiber diet that includes plenty of development of the immune system in babies fermented foods (see pages 48–53), was shown and infants. Evidence suggests gut bacteria play to increase both the function and diversity of a key role in stimulating an expansion of intestinal microbiota, leading to personalized immune immune cells; in the proper functioning of antibody responses and decreased inflammation. secretion; and in achieving balance between the HOME MEDICAL two groups of T helper cells—white blood cells BIRTH FEEDING ENVIRONMENT INTERVENTION EEssttaabblliisshhiinngg yyoouurr gguutt bbaacctteerriiaa that activate most other immune cells. Our microbiota is believed to stabilize by the age IMMUNE-RELATED DISORDERS First contact Mother’s milk Domestic interactions Treating microbes of three, remaining similar from that point into T helper cells divide into two groups: Th1 and Th2. Microbes encountered in the Breastfeeding babies pick Further microbes are picked Premature babies given adulthood. This suggests that early life experiences mother’s birth canal begin up microbes from mother’s up from domestic dirt—no antibiotics and supported in We are born with more Th2, and bacteria colonizing may impact the composition of this bacteria, the process of colonizing skin, and breast milk has need to be overly paranoid sterile incubators have a the gut of newborns is needed to achieve a healthy affecting us throughout our lives. the gut, an experience bacteria from mother’s gut about cleaning!—pets, and different mix of microbiota babies born by Cesarean as well as sugars designed human visitors. to full-term babies. section miss out on. to feed the microbiome. UUSS__114400--114411__iimmmmuunnee__ssyysstteemm..iinndddd 114400 2244//0088//22002211 1166::2244 UUSS__114400--114411__iimmmmuunnee__ssyysstteemm..iinndddd 114411 2244//0088//22002211 1166::2244
142 IS THERE A RELATIONSHIP BETWEEN SLEEP QUALITY AND NUTRITION? It has not been fully determined whether sleep quality affects the diet and vice versa, but there does seem to be a link. What is crystal clear, however, is how important good-quality sleep is to our health and sense of well-being. Sleep deprivation can cause health problems. During standpoint, it makes sense to simply go to sleep when sleep, the body and mind relax and recharge, giving you are tired, rather than snacking to keep awake. us energy for the next day. Much maintenance and repair work takes place in the body when we sleep. POOR SLEEP HYGIENE AND WEIGHT LOSS In one study of dieters sleeping less than six hours SLEEP HYGIENE per night, in the case of 70 percent of participants, Diet and sleep hygiene (sleep quality) seem to all weight lost was from lean muscle mass, not fat. be linked. Having enough good-quality sleep, Fat is an energy-rich substance. When you don’t combined with eating regularly during the day, sleep enough, cortisol levels rise, pushing the body determines the healthy production of hormones. more easily into flight-or-fight mode. The body Following a healthy, balanced diet (see pages becomes reluctant to give up fat, preferring to reserve 40–41) is one of the best ways of achieving good this rich energy source for impending fight or flight. sleep hygiene. Not surprisingly, a lot of evidence So it turns to metabolizing muscle (which is protein). links the Mediterranean diet (see pages 36–39) with In other words, if you’re trying to lose weight but better sleep hygiene and lower levels of insomnia. getting insufficient sleep, you will lose what you want to keep—muscle—and you will hold on to what POOR SLEEP HYGIENE AND WEIGHT GAIN you want to lose—fat. You may be dieting diligently, The relationship between insufficient sleep managing your cravings, then unraveling all your and obesity has been considerably researched. hard work simply because of lack of sleep. Insufficient sleep disrupts the hormones that govern appetite (see opposite), making us hungrier. SLEEP-FRIENDLY DIETARY HABITS Research shows that people who sleep for less Avoid foods that hinder sleep. Spicy foods, caffeine time often consume more calories and tend to opt (see page 72), and alcohol can all prevent sleep. for foods with a high fat content day to day than Try a small protein/carbohydrate snack before those who get sufficient sleep. bed. Tryptophan, an amino acid that promotes sleep, In lab experiments, sleep-deprived people tend to is found in small quantities in all protein foods. keep meal portions the same but snack late into the The best sources are eggs, soybeans, poultry, meat, night (we’ve all been there!), eating on average closer fish, and cheese. It is a precursor to the sleep- to 130 percent of their daily calorie intake, with inducing chemicals serotonin and melatonin. around 500 of these calories consumed late at night. For tryptophan to have a sedative effect, it needs At the risk of stating the obvious, from a nutritional to be consumed with a carbohydrate food. UUSS__114422--114433__sslleeeepp__nnuuttrriittiioonn..iinndddd 114422 2244//0088//22002211 1166::2244
114422 IS THERE A RELATIONSHIP KEY 7–9 hr Sleep deprived BETWEEN SLEEP QUALITY Less leptin released SLEEP PER NIGHT IS More ghrelin released AND NUTRITION? RECOMMENDED Sufficient sleep It has not been fully determined whether sleep quality affects the diet and vice versa, Less ghrelin released 7% OF More leptin released but there does seem to be a link. What is crystal clear, however, is how important AMERICANS GET good-quality sleep is to our health and sense of well-being. 9+ hr SLEEP PER NIGHT Sleep deprivation can cause health problems. During standpoint, it makes sense to simply go to sleep when sleep, the body and mind relax and recharge, giving you are tired, rather than snacking to keep awake. us energy for the next day. Much maintenance and 60% OF AMERICANS GET repair work takes place in the body when we sleep. POOR SLEEP HYGIENE AND WEIGHT LOSS In one study of dieters sleeping less than six hours 7–8 hr SLEEP HYGIENE per night, in the case of 70 percent of participants, PER NIGHT Diet and sleep hygiene (sleep quality) seem to all weight lost was from lean muscle mass, not fat. be linked. Having enough good-quality sleep, Fat is an energy-rich substance. When you don’t combined with eating regularly during the day, sleep enough, cortisol levels rise, pushing the body 33% OF AMERICANS determines the healthy production of hormones. more easily into flight-or-fight mode. The body GET ONLY Following a healthy, balanced diet (see pages becomes reluctant to give up fat, preferring to reserve 6 hr 40–41) is one of the best ways of achieving good this rich energy source for impending fight or flight. sleep hygiene. Not surprisingly, a lot of evidence So it turns to metabolizing muscle (which is protein). OR FEWER PER NIGHT links the Mediterranean diet (see pages 36–39) with In other words, if you’re trying to lose weight but better sleep hygiene and lower levels of insomnia. getting insufficient sleep, you will lose what you want to keep—muscle—and you will hold on to what POOR SLEEP HYGIENE AND WEIGHT GAIN you want to lose—fat. You may be dieting diligently, The relationship between insufficient sleep managing your cravings, then unraveling all your and obesity has been considerably researched. hard work simply because of lack of sleep. Insufficient sleep disrupts the hormones that govern appetite (see opposite), making us hungrier. SLEEP-FRIENDLY DIETARY HABITS Research shows that people who sleep for less Avoid foods that hinder sleep. Spicy foods, caffeine time often consume more calories and tend to opt (see page 72), and alcohol can all prevent sleep. Sleep deprivation for foods with a high fat content day to day than Try a small protein/carbohydrate snack before causes disruption of many hormones, including those those who get sufficient sleep. bed. Tryptophan, an amino acid that promotes sleep, that govern sleep (see page In lab experiments, sleep-deprived people tend to is found in small quantities in all protein foods. 105). More ghrelin (the hunger hormone) than keep meal portions the same but snack late into the The best sources are eggs, soybeans, poultry, meat, normal is released when night (we’ve all been there!), eating on average closer fish, and cheese. It is a precursor to the sleep- sleep deprived, while less leptin (the satiety hormone) to 130 percent of their daily calorie intake, with inducing chemicals serotonin and melatonin. is present, leading to around 500 of these calories consumed late at night. For tryptophan to have a sedative effect, it needs snacking and overeating. This is reversed when At the risk of stating the obvious, from a nutritional to be consumed with a carbohydrate food. you’ve had enough sleep. UUSS__114422--114433__sslleeeepp__nnuuttrriittiioonn..iinndddd 114422 2244//0088//22002211 1166::2244 UUSS__114422--114433__sslleeeepp__nnuuttrriittiioonn..iinndddd 114433 2244//0088//22002211 1166::2244
144 CAN DIET HELP REDUCE MENSTRUAL SYMPTOMS? Half of the world’s population experiences menstruation during their lifetimes, yet there has not been much good-quality research exploring how diet influences menstruation and period pain. What we know is that certain nutrients (see Magnesium and omega-3 may help relieve the opposite) seem to reduce symptoms, so it makes cramping sensation by reducing contractions. sense to try to include them in your diet daily. Recent studies show that supplementation with omega-3 fatty acids, known for their anti- PMS inflammatory properties, may reduce pain intensity. Premenstrual syndrome (PMS) is a combination of Similar results are shown with fish oils. physical and psychological complaints experienced Supplementation with vitamins D and E and during the days leading up to menstruation and ginger may also reduce the severity of cramps. once bleeding begins. Symptoms include low mood Ginger contains gingerol and gingerdione, which and mood swings, headache, bloating, lower may have anti-inflammatory and analgesic effects. backache, breast tenderness, acne, and fatigue. There is some limited evidence indicating that An estimated 30–40 percent of women experience a low-fat vegetarian diet and calcium supplements PMS, with 77 percent experiencing psychological reduce the duration and intensity of period pains. symptoms and 71 percent suffering from tiredness and, in some cases, debilitating exhaustion. One in DIGESTIVE COMPLAINTS three women report being unable to perform as usual. Many people report digestive discomfort and a Low calcium and vitamin D levels may exacerbate change in bowel habits during their period, with the symptoms. Research shows that supplementing and/ most disruption occurring on day one. In the days or consuming foods rich in these nutrients may help leading up to this day, levels of the hormones relieve symptoms. Take a 10mcg supplement of estrogen and progesterone rise in preparation for a vitamin D. Iron may help increase energy levels. possible pregnancy. These hormones are known to As the body loses iron in menstrual blood, it makes slow gastrointestinal motility. sense to ensure dietary sources of iron are kept high Focus on eating as well as possible, including lots during menstruation to avoid risk of a deficiency. of fiber in the diet and drinking plenty of water to keep bowel movements healthy. It also makes sense CRAMPS to reduce your consumption of foods that can Dysmenorrhea (cramps) is the most common period increase inflammation of the gastrointestinal tract symptom, affecting 85 percent of women. When and cause bloating, such as alcohol, caffeine, fatty menstruation begins, there is a rise in inflammatory foods, and fizzy drinks. Some (uncontrolled) research compounds called prostaglandins. These cause the suggests that habitual caffeine intake is associated uterine muscle to contract and release blood. with menstrual abnormalities. UUSS__114444--114455__ppeerriiooddss..iinndddd 114444 2244//0088//22002211 1166::2244
114444 CAN DIET HELP REDUCE WALNUTS MENSTRUAL SYMPTOMS? MANY NUTS CONTAIN MAGNESIUM. WALNUTS ARE A Half of the world’s population experiences menstruation during their lifetimes, GOOD SOURCE yet there has not been much good-quality research exploring how diet influences menstruation and period pain. What we know is that certain nutrients (see Magnesium and omega-3 may help relieve the opposite) seem to reduce symptoms, so it makes cramping sensation by reducing contractions. sense to try to include them in your diet daily. Recent studies show that supplementation with omega-3 fatty acids, known for their anti- PMS inflammatory properties, may reduce pain intensity. Premenstrual syndrome (PMS) is a combination of Similar results are shown with fish oils. physical and psychological complaints experienced Supplementation with vitamins D and E and during the days leading up to menstruation and ginger may also reduce the severity of cramps. once bleeding begins. Symptoms include low mood Ginger contains gingerol and gingerdione, which and mood swings, headache, bloating, lower may have anti-inflammatory and analgesic effects. OMEGA-3 backache, breast tenderness, acne, and fatigue. There is some limited evidence indicating that WALNUTS ARE A An estimated 30–40 percent of women experience a low-fat vegetarian diet and calcium supplements RICH SOURCE OF PMS, with 77 percent experiencing psychological reduce the duration and intensity of period pains. Magnesium reduces menstrual cramping by relaxing uterine muscle OMEGA-3 FATTY and reducing prostaglandins (inflammatory compounds that cause symptoms and 71 percent suffering from tiredness ACIDS period pain). People who experience period pains may produce and, in some cases, debilitating exhaustion. One in DIGESTIVE COMPLAINTS 8–13 times more prostaglandins than nonsufferers. Eat foods containing magnesium, such as walnuts, and try taking a bath three women report being unable to perform as usual. Many people report digestive discomfort and a with Epsom salts dissolved in it for more bioavailable magnesium. Low calcium and vitamin D levels may exacerbate change in bowel habits during their period, with the symptoms. Research shows that supplementing and/ most disruption occurring on day one. In the days SOURCES OF or consuming foods rich in these nutrients may help leading up to this day, levels of the hormones relieve symptoms. Take a 10mcg supplement of estrogen and progesterone rise in preparation for a MAGNESIUM OMEGA-3 CALCIUM VITAMIN D IRON vitamin D. Iron may help increase energy levels. possible pregnancy. These hormones are known to PUMPKIN SEEDS SALMON MILK SALMON LIVER As the body loses iron in menstrual blood, it makes slow gastrointestinal motility. WALNUTS TROUT CHEESE MACKEREL RED MEAT sense to ensure dietary sources of iron are kept high Focus on eating as well as possible, including lots BRAZIL NUTS MACKEREL YOGURT HERRING LEGUMES during menstruation to avoid risk of a deficiency. of fiber in the diet and drinking plenty of water to ALMONDS SARDINES FORTIFIED MILKS SARDINES NUTS keep bowel movements healthy. It also makes sense CASHEW NUTS FLAXSEEDS FORTIFIED BREAD RED MEAT EGGS CRAMPS to reduce your consumption of foods that can PEANUTS CHIA SEEDS SARDINES WITH BONES EGG YOLKS DRIED FRUITS Dysmenorrhea (cramps) is the most common period increase inflammation of the gastrointestinal tract SUNFLOWER SEEDS CANOLA OIL CANNED SALMON FORTIFIED FOODS POULTRY symptom, affecting 85 percent of women. When and cause bloating, such as alcohol, caffeine, fatty SOYBEANS AVOCADOS WITH BONES MUSHROOMS FISH menstruation begins, there is a rise in inflammatory foods, and fizzy drinks. Some (uncontrolled) research WHEAT-BASED FOODS SOYBEANS KALE EXPOSED TO WHOLE GRAINS compounds called prostaglandins. These cause the suggests that habitual caffeine intake is associated COOKED SPINACH LEAFY VEGETABLES ORANGE SUNLIGHT DARK LEAFY uterine muscle to contract and release blood. with menstrual abnormalities. WALNUTS BROCCOLI GREENS UUSS__114444--114455__ppeerriiooddss..iinndddd 114444 2244//0088//22002211 1166::2244 UUSS__114444--114455__ppeerriiooddss..iinndddd 114455 2255//0088//22002211 1199::1155
146 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 147 SHOULD I EAT DIFFERENTLY FOR MENOPAUSE? The symptoms of menopause still aren’t discussed openly enough, even though half of the population will experience them to varying degrees—all the more reason to talk about whether certain foods could help ease this major life transition. For most women, menopause is a natural aging balanced diet. Scientists don’t yet understand process during which they lose their period due to the precise mechanism, but isoflavones appear declining estrogen levels. Plant compounds called to produce a weak estrogen-like effect without phytoestrogens may help ease hot flashes and night actually affecting estrogen levels. sweats, the most common symptoms. So far, research Soy-based cheese and meat alternatives often has focused on one type—isoflavones—found in have added salt and more fat than tofu, tempeh, soybeans and soy-based foods and drinks. Only and soybeans, so eat these less often. Caffeine 10%–20% of Asian women experience hot flashes and alcohol can also exacerbate hot flashes. compared with a large majority of perimenopausal or menopausal women in the US, where soy intake BREAST CANCER RISK is lower. Some breast cancers are estrogen-dependent, Findings from research into soy’s impact on and studies have identified a link between soy symptoms have been mixed. However, a 2021 review and increased risk. The 2021 review concluded of more than 400 studies concluded that eating that isoflavones are chemically different to estrogen, around 50mg (milligrams) of isoflavones daily is and soy isoflavones are safe to eat as part of a associated with both reduced frequency and severity healthy diet without increased risk of breast cancer of hot flashes. This amount can be obtained from occurring or recurring. This is based on eating up two servings of soy foods or drinks within a to 100mg of isoflavones daily—around 9oz of tofu Does soy always help? Soy isoflavones have a similar, though not identical, chemical structure to human estrogen, which means HUMAN they can act in different ESTROGEN SOY ISOFLAVONE ways to ostrogen. Some people may experience Human estrogen is able to Soy isoflavones prefer to bind to one bind to a similar degree to both type of estrogen receptor and may mild estrogen-like effects, types of estrogen receptors affect the body differently to human like less severe hot flashes, found on organs and tissues estrogen, such as having and others no effects. around the body. an antioxidant effect. UUSS__114466--114477__mmeennooppaauussee..iinndddd 114466 2244//0088//22002211 1166::2244
114466 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 114477 SHOULD I EAT DIFFERENTLY FOR MENOPAUSE? STRONG BONES FERMENTED SOY IN The symptoms of menopause still aren’t discussed openly enough, even though half of TEMPEH IS A GOOD SOURCE OF VITAMIN K2, the population will experience them to varying degrees—all the more reason to talk WHICH HELPS PREVENT about whether certain foods could help ease this major life transition. OSTEOPOROSIS For most women, menopause is a natural aging balanced diet. Scientists don’t yet understand or up to 29oz of soy milk. (It may still be process during which they lose their period due to the precise mechanism, but isoflavones appear best to take medical advice before consuming declining estrogen levels. Plant compounds called to produce a weak estrogen-like effect without high-dose phytoestrogen supplements.) phytoestrogens may help ease hot flashes and night actually affecting estrogen levels. sweats, the most common symptoms. So far, research Soy-based cheese and meat alternatives often HEART AND BONE HEALTH has focused on one type—isoflavones—found in have added salt and more fat than tofu, tempeh, Postmenopausal women are at a higher risk of soybeans and soy-based foods and drinks. Only and soybeans, so eat these less often. Caffeine cardiovascular disease and should eat less salt and 10%–20% of Asian women experience hot flashes and alcohol can also exacerbate hot flashes. more fiber and replace saturated fats with compared with a large majority of perimenopausal unsaturated. Calcium is also important because or menopausal women in the US, where soy intake BREAST CANCER RISK menopause can accelerate the age-related decline is lower. Some breast cancers are estrogen-dependent, in bone mineral density. You should get a sufficient Findings from research into soy’s impact on and studies have identified a link between soy amount in a healthy diet from sources, including symptoms have been mixed. However, a 2021 review and increased risk. The 2021 review concluded leafy greens, calcium-fortified foods, dairy of more than 400 studies concluded that eating that isoflavones are chemically different to estrogen, products, and fish eaten with bones. It’s also around 50mg (milligrams) of isoflavones daily is and soy isoflavones are safe to eat as part of a crucial to get enough vitamin D alongside associated with both reduced frequency and severity healthy diet without increased risk of breast cancer calcium, and supplements may be required, of hot flashes. This amount can be obtained from occurring or recurring. This is based on eating up especially if suffering from osteopenia and two servings of soy foods or drinks within a to 100mg of isoflavones daily—around 9oz of tofu osteoporosis (see pages 138–139). Does soy always help? DAILY SOY BOOST TWO SERVINGS OF SOY-BASED FOODS MAY Soy isoflavones have HELP ALLEVIATE HOT FLASHES AND IS BROADLY EQUIVALENT TO: a similar, though not identical, chemical SOY FOOD 3.5oz REGULAR CUBED TOFU structure to human AND DRINKS LOW IN estrogen, which means HUMAN SATURATED FAT, LIKE HALF BLOCK (3.5oz) TEMPEH they can act in different ways to ostrogen. Some ESTROGEN SOY ISOFLAVONE TOFU AND TEMPEH, CAN 3.5oz EDAMAME BEANS HELP MAINTAIN NORMAL people may experience binH du tm o a an s e imst ir lao rg e dn eg i rs e a eb tl oe bto o t h So tyy p i es o of f l ea sv to rn oges e np rr ee cfe er p t to o rb ain nd d t mo ao yn e Tofu and tempeh: Tofu is the CHOLESTEROL 2 x 8.5oz SOY MILK mild estrogen-like effects, types of estrogen receptors affect the body differently to human pressed curds from soy milk; LEVELS like less severe hot flashes, found on organs and tissues estrogen, such as having tempeh is made from cooked 1 x 8.5oz SOY MILK + and others no effects. around the body. an antioxidant effect. soybeans and is fermented. 7oz PLAIN SOY YOGURT ALTERNATIVE UUSS__114466--114477__mmeennooppaauussee..iinndddd 114466 2244//0088//22002211 1166::2244 UUSS__114466--114477__mmeennooppaauussee..iinndddd 114477 2255//0088//22002211 1199::1155
148 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 149 CAN I EAT TO SUPPORT THE AGING PROCESS? As well as affecting appearance, flexibility, and cognitive health, getting older may increase our chances of developing a noncommunicable disease. Can diet help us live longer and improve well-being in later years? By 2050, there are expected to be two billion people from a balanced diet, including a wide range of over 60, more than double the number in 2015. plants, rather than from supplements; these often Aging increases the risk of developing chronic contain highly concentrated doses the body may not conditions like type 2 diabetes, cancer, or heart require, and too many could have damaging effects. disease, but scientists don’t yet fully understand the complex mechanisms behind it. One theory says MEDITERRANEAN EATING antioxidants obtained from diet (like vitamins C Evidence generally points to a correlation between and E, selenium, and zinc) may help protect us from healthy lifestyle and lower risk of chronic disease. potentially harmful free radical molecules and the One study based on long-term data found that those damage to cells that speeds up as we age. Research who routinely ate a Mediterranean-style diet with indicates that it’s safer to obtain these nutrients plenty of vegetables, whole grains, and fish were 30 How excess sugar ages skin A healthy epidermis feels supple and Collagen and elastin keep skin appears smooth firm but can be damaged by glycation. This process is accelerated by eating a lot of high GI (glycemic index) food, especially sugary foods, that rapidly convert to GLYCATION sugar in the blood. Excess sugar molecules Key in the blood "glycate" or attach to collagen and elastin fibers, Sugar linking amino acids in them. Collagen Elastin Collagen and elastin are proteins in the dermis (inner layer) UUSS__114488--114499__aaggiinngg..iinndddd 114488 2244//0088//22002211 1166::2244
114488 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 114499 CAN I EAT TO SUPPORT percent more likely to age successfully (defined Brazil nuts daily for skin-supporting selenium; THE AGING PROCESS? as reaching 70 with no major mobility problems or researchers have found that two 2oz servings of chronic disease, no loss of cognitive skills, and good almonds daily for six months led to significantly psychological health). Researchers tracking data on less severe wrinkles. As well as affecting appearance, flexibility, and cognitive health, getting older may nearly half a million middle-aged adults found an increase our chances of developing a noncommunicable disease. Can diet help overall healthy lifestyle, including diet, could JOINT AND BRAIN SUPPORT us live longer and improve well-being in later years? increase male life expectancy by 6 years and As we age, we lose muscle mass and bone density, female life expectancy by 7.5 years. increasing the risk of osteoporosis and frailty. Between ages 19–70, healthy adults should aim to By 2050, there are expected to be two billion people from a balanced diet, including a wide range of SKIN AGING consume 1,000mg of calcium for men and 1,200mg over 60, more than double the number in 2015. plants, rather than from supplements; these often After age 20, collagen production gradually for women a day to protect bone health and enough Aging increases the risk of developing chronic contain highly concentrated doses the body may not declines, and the number and size of skin cells vitamin D to absorb it (see pages 138–139). Older conditions like type 2 diabetes, cancer, or heart require, and too many could have damaging effects. reduces, making skin thinner. Fatty fish like people (and breastfeeding women) should get at disease, but scientists don’t yet fully understand salmon and mackerel may help support skin cells. least 1,200mg. To help muscles repair and grow, aim the complex mechanisms behind it. One theory says MEDITERRANEAN EATING Antioxidants in fruits and vegetables—especially to eat a portion (about palm size) of protein at each antioxidants obtained from diet (like vitamins C Evidence generally points to a correlation between vitamins A (beta carotene), E and C, lycopene, meal, such as chicken breast, beans and lentils, and and E, selenium, and zinc) may help protect us from healthy lifestyle and lower risk of chronic disease. and lutein—may neutralize harm done by free tofu. Doing some weight-bearing exercise is potentially harmful free radical molecules and the One study based on long-term data found that those radicals. Broaden your intake and include kale, important, too. Several studies have also found an damage to cells that speeds up as we age. Research who routinely ate a Mediterranean-style diet with carrots, spinach, red peppers, tomatoes, and association between lower overall mental decline indicates that it’s safer to obtain these nutrients plenty of vegetables, whole grains, and fish were 30 turnip greens. Adults may also want to eat 2–3 and eating unsaturated fats (see pages 150–151). How excess sugar ages skin A healthy epidermis feels supple and The epidermis becomes thinner; wrinkles deepen Collagen and elastin keep skin appears smooth firm but can be damaged by glycation. This process is accelerated by eating a lot of high GI (glycemic index) food, especially sugary foods, that rapidly convert to GLYCATION AGE FORMATION FREE RADICALS sugar in the blood. Excess sugar molecules Advanced glycogen end products Cross-linked collagen/elastin Key in the blood "glycate" or attach (AGEs) are formed and further becomes stiffer and harder to collagen and elastin fibers, link collagen and elastin, to repair; AGEs also form new Sugar linking amino acids in them. reducing elasticity. free radicals, causing more damage. Collagen Elastin Collagen and elastin are proteins Elastin weakens Collagen is in the dermis (inner layer) and degrades less elastic UUSS__114488--114499__aaggiinngg..iinndddd 114488 2244//0088//22002211 1166::2244 UUSS__114488--114499__aaggiinngg..iinndddd 114499 2244//0088//22002211 1166::2244
150 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 151 CAN I EAT TO GUARD AGAINST DEMENTIA? "Dementia" describes different cognitive impairments that are often minor to start with, then become severe enough to affect daily life. Adopting healthy eating habits before reaching later years may be one way to reduce your risk. Dementia symptoms include difficulties with Notably, though, it emphasizes leafy greens problem-solving or communication, loss of (although eating other vegetables is important) memory, and mood swings. There are many kinds and berries, because their antioxidant properties can of dementia, and specific symptoms depend on the help reduce oxidative stress, an imbalance between underlying cause. In Alzheimer’s disease, the leading free radicals and protective antioxidants in the body cause, structural changes in the brain cause cells to that can lead to cell damage. die, while restricted blood supply can cause vascular Otherwise, some studies indicate eating more dementia. When it develops in older age, in some omega-3 fatty acids reduces the chance of cases, dementia is thought to be lifestyle related. developing dementia; good sources include oily fish (see pages 42–43). However, others found no effects, DEMENTIA AND DIET so longer-term research is needed. A high level of Evidence indicates a healthy weight and lifestyle compounds called advanced glycation end products in midlife lessen the chances of developing (AGEs) is associated with the inflammation and dementia. A body of research also shows a positive oxidative stress that precede many chronic diseases, association between cognition and diet; while this including Alzheimer’s; these may also contribute to doesn’t establish causation, it points toward eating the tangling of Tau proteins inside brain cells. AGEs a varied, Mediterranean-style diet low in salt, sugar, form inside the body when blood glucose combines and saturated fat and rich in vegetables and whole with proteins or fats; high levels may also form in grains. Refined carbohydrates, including sugars foods exposed to high-temperature cooking. added to many processed foods, are absorbed quickly and prompt a bigger insulin response than complex carbohydrates. People with type 2 diabetes AGE LEVELS AND COOKING appear to be at higher risk of dementia, but research into the reasons why is still at an early stage. It’s BEEF HIGH BEEF LOW possible that excess insulin enables a protein called FRIED | 9,522 STEWED | 2,443 beta-amyloid to accumulate faster in the brain. CHICKEN HIGH CHICKEN LOW ROASTED | 5,975 BOILED | 2,232 BRAIN FOOD? SALMON HIGH SALMON LOW Scientists designed the MIND (Mediterranean- GRILLED | 3,012 POACHED | 2,063 DASH Intervention for Neurogenerative Delay) diet to combine elements of two other diets that studies POTATOES HIGH POTATOES LOW FRIED | 694 BOILED | 17 show are both beneficial for reducing the risk of type 2 diabetes and cardiovascular problems. UUSS__115500--115511__ddeemmeennttiiaa..iinndddd 115500 2244//0088//22002211 1166::3388
115500 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 115511 CAN I EAT TO GUARD Alzheimer’s and the brain AGAINST DEMENTIA? Billions of neuron cells transmit information around the brain, then to the body. Alzheimer’s interferes with their communication and repair processes and causes greater loss of neurons than normal. "Dementia" describes different cognitive impairments that are often minor to start Scientists are still investigating exactly why this happens. with, then become severe enough to affect daily life. Adopting healthy eating habits before reaching later years may be one way to reduce your risk. Blood vessels Microglia destroy toxins Dementia symptoms include difficulties with Notably, though, it emphasizes leafy greens HEALTHY BRAIN Cell membrane problem-solving or communication, loss of (although eating other vegetables is important) NETWORKS OF NEURONS memory, and mood swings. There are many kinds and berries, because their antioxidant properties can COMMUNICATE VIA ELECTRICAL AND Astrocytes clear debris CHEMICAL (NEUROTRANSMITTER) of dementia, and specific symptoms depend on the help reduce oxidative stress, an imbalance between SIGNALS ACROSS GAPS CALLED underlying cause. In Alzheimer’s disease, the leading free radicals and protective antioxidants in the body Neuron functions SYNAPSES AND CONTINUALLY cause, structural changes in the brain cause cells to that can lead to cell damage. REPAIR THEMSELVES. die, while restricted blood supply can cause vascular Otherwise, some studies indicate eating more dementia. When it develops in older age, in some omega-3 fatty acids reduces the chance of cases, dementia is thought to be lifestyle related. developing dementia; good sources include oily fish WHEN THINGS GO WRONG (see pages 42–43). However, others found no effects, DEMENTIA AND DIET so longer-term research is needed. A high level of Evidence indicates a healthy weight and lifestyle compounds called advanced glycation end products in midlife lessen the chances of developing (AGEs) is associated with the inflammation and BETA-AMYLOID “SUPPORT” TAU PROTEINS dementia. A body of research also shows a positive oxidative stress that precede many chronic diseases, CELLS IN ALZHEIMER’S BRAINS, NORMALLY, THESE association between cognition and diet; while this including Alzheimer’s; these may also contribute to AN ABNORMAL AMOUNT OF INSTEAD OF REMOVING DEBRIS SUPPORT NEURONS’ doesn’t establish causation, it points toward eating the tangling of Tau proteins inside brain cells. AGEs THIS NORMALLY HARMLESS LIKE BETA-AMYLOID, MICROGLIA INTERNAL STRUCTURE; IN a varied, Mediterranean-style diet low in salt, sugar, form inside the body when blood glucose combines WASTE PRODUCT COLLECTS AND ASTROCYTE CELLS ALZHEIMER’S, THEY FORM BETWEEN NEURONS CAUSE INFLAMMATION TANGLES THAT DISRUPT and saturated fat and rich in vegetables and whole with proteins or fats; high levels may also form in AND FORMS AND FURTHER DAMAGE NEURONS’ SIGNALING grains. Refined carbohydrates, including sugars foods exposed to high-temperature cooking. PLAQUES NEURONS FUNCTION added to many processed foods, are absorbed quickly and prompt a bigger insulin response than complex carbohydrates. People with type 2 diabetes AGE LEVELS AND COOKING appear to be at higher risk of dementia, but research into the reasons why is still at an early stage. It’s BEEF HIGH BEEF LOW possible that excess insulin enables a protein called FRIED | 9,522 STEWED | 2,443 beta-amyloid to accumulate faster in the brain. CHICKEN HIGH CHICKEN LOW ROASTED | 5,975 BOILED | 2,232 BRAIN FOOD? ALZHEIMER’S BRAIN SALMON HIGH SALMON LOW Scientists designed the MIND (Mediterranean- GRILLED | 3,012 POACHED | 2,063 AS MORE NEURONS ARE DAMAGED AND Beta-amyloid plaques DASH Intervention for Neurogenerative Delay) diet DIE, COMMUNICATION BETWEEN to combine elements of two other diets that studies POTATOES HIGH POTATOES LOW NETWORKS BREAKS DOWN AND AREAS Damaged neuron FRIED | 694 BOILED | 17 OF THE BRAIN SHRINK. AT FIRST, THIS show are both beneficial for reducing the risk of AFFECTS MEMORY: LATER, LANGUAGE, type 2 diabetes and cardiovascular problems. REASONING, AND BEHAVIOR. Tau tangles UUSS__115500--115511__ddeemmeennttiiaa..iinndddd 115500 2244//0088//22002211 1166::3388 UUSS__115500--115511__ddeemmeennttiiaa..iinndddd 115511 2244//0088//22002211 1166::3388
152 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 153 Brain Stress can Causes of bloating Before eating, the effect gut motility gastrointestinal tract contains 100–200ml and digestion via of gases, mainly carbon dioxide, oxygen, the gut-brain axis nitrogen, and hydrogen. After we eat, the (see pages 48–49) volume of gas can increase by 65 percent; the several causes are shown here. Mouth We inhale gas while eating Fermentation by Bacteria normally bacteria creates in the colon ferment excess gas food in small intestine Small intestine In those with SIBO, bacteria overgrowing in the small intestine can lead to bloating. Large intestine Fermentation activity of gut bacteria naturally creates gas, while constipation can trap gases Ovaries Female sex hormones have complex effects on the gut and immune system, which can ultimately affect bloating UUSS__115522--115533__bbllooaattiinngg..iinndddd 115522 2255//0088//22002211 1199::1155
115522 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 115533 Brain Stress can Causes of bloating Before eating, the WHY DO I FEEL BLOATED? effect gut motility gastrointestinal tract contains 100–200ml and digestion via of gases, mainly carbon dioxide, oxygen, the gut-brain axis nitrogen, and hydrogen. After we eat, the Bloating is the sensation associated with gas in our gut, when your abdomen feels (see pages 48–49) volume of gas can increase by 65 percent; the several causes are shown here. swollen and is commonly distended. It can make us feel uncomfortably full and is sometimes accompanied by pain, flatulence, and nausea. Mouth We inhale gas while eating Even in a “fasted” state, our gastrointestinal tract Constipation Delay in passing stool and increase contains gases, which increase after eating due to in stool bulk leads to the accumulation and trapping Fermentation by Bacteria normally bacteria creates in the colon ferment swallowing air and gases produced during digestion. of gases, exacerbated by the longer time spent in the excess gas food in small intestine But it is normal for our stomachs to distend when large bowel, which increases bacterial fermentation. we eat a meal—they are not supposed to remain flat! Eating high-fiber foods, such as legumes, may Small intestine bacterial overgrowth (SIBO) lead to bloating due to increased gas production in Most gut bacteria live in our large intestine. SIBO the large intestine; but for many, fiber can reduce occurs when these grow in the small intestine and bloating by improving digestion and speeding up disrupt digestion and absorption, leading to bloating. gut transit. When we swallow, air enters our stomach and can lead to abdominal distention. This is Sensitive gut Evidence has emerged for a condition enhanced when we eat too quickly or drink a lot of called visceral hypersensitivity, which describes liquid while eating, especially carbonated drinks. greater sensitivity or perception to bloating and Other key contributing factors include: other symptoms. Patients with gastrointestinal disorders are particularly prone to sensitive gut. Stress Feeling stressed can affect communication mechanisms of the gut-brain axis (see pages 48–49), Intolerances For those with food intolerances, for example, by changing levels of neurotransmitters, eating such foods can lead to bloating. For example, which can disturb our ability to digest foods well the lactose intolerant do not have the enzyme to Small intestine In those with and lead to constipation or diarrhea by affecting digest lactose in cows’ milk, so this is broken down SIBO, bacteria overgrowing in gut motility. by gut bacteria instead, producing excess gas. the small intestine can lead to bloating. Hormones Women report feeling bloated more than Can you treat it? Large intestine Fermentation men, probably due to differences in sex hormones activity of gut bacteria naturally creates gas, while and fluctuations in their levels. Progesterone rise THERE IS NO ONE-SIZE-FITS-ALL CURE, AND constipation can trap gases before a period can promote bloating and other TREATMENT PLANS TEND TO BE PERSONALIZED. They range from lifestyle changes relating to diet, digestive issues; whereas estrogen stimulates gut exercise, and stress; to administering probiotics; to muscle relaxation by inducing nitric oxide synthase. medications such as laxatives in those with constipation or antibiotics for SIBO sufferers. Gut microbiota Our gut bacteria breaks down Identifying the potential cause is important for addressing underlying conditions, for example, indigestible foods, producing gases as a by-product. dietary issues in patients with IBS or food More bloating is experienced in those with IBS intolerances. While bloating is common and fairly Ovaries Female sex hormones have complex (see page 164) due to visceral hypersensitivity. normal, do see your primary care physician if this is effects on the gut and Studies have shown that the amount of gas persistent to rule out any gastrointestinal disorders. immune system, which can ultimately affect bloating produced is actually similar to healthy individuals. UUSS__115522--115533__bbllooaattiinngg..iinndddd 115522 2255//0088//22002211 1199::1155 UUSS__115522--115533__bbllooaattiinngg..iinndddd 115533 2244//0088//22002211 1166::2244
154 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 155 IS IT NORMAL TO FART? Flatulence might feel embarrassing, but it is perfectly normal to pass wind and an important part of digestion. You will never hear a nutritionist shying away from discussing farts! Most people fart 5 to 15 times a day. Farting is a Foods that can contribute to sulfurous wind include: sign of a healthy gut microbiome (see pages 48–49), Animal proteins—meat, protein powders, eggs but if it becomes foul smelling or increases in Plant foods—broccoli, cabbage, cauliflower, frequency, it may need investigating. garlic, onions Drinks—wine, beer EXCESS GAS We fart because of natural fermentation in the gut. While everyone is unique, many scientists report There can be several reasons for producing abnormal high protein intake as the main cause of smelly wind, quantities of gas: feeding the gut too much; when and advise action to lower levels of protein in the we can’t digest certain foods because of a food diet first (see page 15 for recommended intakes), intolerance; if you suddenly increase the amount before cutting back on any plant foods. By safely and of fiber in the diet, this can shock the body as it slowly increasing fiber in the diet from plant-based isn’t used to working that hard; or if food travels too foods, you may actually help gut microbiota by quickly through our intestines, for example, when enabling them to break down food at a we get diarrhea. Conversely, if food moves too slowly consistent pace instead through our intestines and we become constipated, of waiting around in the fermentation process has a long time to “brew” a stop-start manner. food that would have normally been excreted, producing gas as a by-product. UNPLEASANT ODORS Mostly, the gas we produce when we pass wind has no odor. Foul-smelling farts happen when our gut breaks down sulfur-containing compounds, which contributes to sulfur-containing—and smelly—gases like hydrogen sulfide. If you experience these smells frequently, it may be worth looking at your diet and speaking to a registered dietitian nutritionist for some help. Red cabbage Not only cabbage, but all vegetables in the Brassica family cause smelly farts. UUSS__115544--115555__ffaarrtt..iinndddd 115544 2244//0088//22002211 1166::2244
115544 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 115555 IS IT NORMAL TO FART? WHY AM I CONSTIPATED? Flatulence might feel embarrassing, but it is perfectly normal to pass wind and an important part of digestion. You will never hear a nutritionist shying Constipation can be frustrating and sometimes away from discussing farts! painful, resulting in a lot of straining and hard poop. Most people fart 5 to 15 times a day. Farting is a Foods that can contribute to sulfurous wind include: sign of a healthy gut microbiome (see pages 48–49), Animal proteins—meat, protein powders, eggs Food that can’t be digested by your body goes into your large SIGNS OF but if it becomes foul smelling or increases in Plant foods—broccoli, cabbage, cauliflower, intestine, where it mixes with fluid to form poop. Bowel muscles CONSTIPATION frequency, it may need investigating. garlic, onions then tense and relax to push poop toward your bottom. If you INCLUDE Drinks—wine, beer often find it difficult to poop, you may be constipated: EXCESS GAS We fart because of natural fermentation in the gut. While everyone is unique, many scientists report Slow transit constipation occurs when your stool takes There can be several reasons for producing abnormal high protein intake as the main cause of smelly wind, such a long time to get through the large intestine, much of POOPING LESS quantities of gas: feeding the gut too much; when and advise action to lower levels of protein in the its water content is absorbed and it turns hard and dry. THAN THREE we can’t digest certain foods because of a food diet first (see page 15 for recommended intakes), Evacuation disorder comes down to the final push to CABBAGE TIMES A WEEK intolerance; if you suddenly increase the amount before cutting back on any plant foods. By safely and excrete the stool, when coordination of the muscles involved GLUCOSINOLATES ARE of fiber in the diet, this can shock the body as it slowly increasing fiber in the diet from plant-based is poor. This could have been a result of childhood toilet habits SULFUR-CONTAINING isn’t used to working that hard; or if food travels too foods, you may actually help gut microbiota by or be a structural problem of your biology. PHYTOCHEMICALS IN DIFFICULTY quickly through our intestines, for example, when enabling them to break down food at a CABBAGE THAT CAN Constipation-predominant irritable bowel syndrome AND PAIN we get diarrhea. Conversely, if food moves too slowly consistent pace instead CAUSE ODOROUS is a type of IBS mostly owed to constipation (see page 164). FARTS WHEN through our intestines and we become constipated, of waiting around in the fermentation process has a long time to “brew” a stop-start manner. CONSTIPATION IN CHILDREN POOPING food that would have normally been excreted, Our digestive system undergoes changes from birth to adulthood, producing gas as a by-product. but especially in the first year of life with the introduction of solids, and constipation is a common symptom of weaning. STRAINING UNPLEASANT ODORS Children with abdominal pain often simply have constipation A LOT WHEN Mostly, the gas we produce when we pass wind has but should always be checked out. POOPING no odor. Foul-smelling farts happen when our gut breaks down sulfur-containing compounds, which HOW CAN I RELIEVE CONSTIPATION? contributes to sulfur-containing—and Drink a minimum of 6–8 cups water per day and gradually POOP LIKE smelly—gases like hydrogen sulfide. If increase fiber in your diet from plant foods. Fiber provides bulk you experience these smells frequently, to poop since it isn’t absorbed through the gut lining; fiber also SMALL HARD it may be worth looking at your diet absorbs water, softening poop. Exercise can also help with gut PELLETS and speaking to a registered motility. Laxatives are helpful in some situations, for example, dietitian nutritionist post-childbirth and in those with a lot of pain, but they can also for some help. make constipation worse and you may become dependent on them, A FEELING YOU which can influence gut microbiota and toilet habits. Sitting on the HAVEN’T BEEN Red cabbage Not only toilet with your back straight at 90 degrees to your legs can cause cabbage, but all vegetables the intestine to be pinched. Adopt a more ‘‘crouching’’ position with ABLE TO GET ALL in the Brassica family cause THE POOP OUT smelly farts. your legs at 35 degrees to your torso; a foot rest can be helpful. UUSS__115544--115555__ffaarrtt..iinndddd 115544 2244//0088//22002211 1166::2244 UUSS__115544--115555__ffaarrtt..iinndddd 115555 2244//0088//22002211 1166::2244
156 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 157 IS DIARRHEA CAUSED BY DIET? Diarrhea is a condition where you pass loose, watery stools more frequently than is normal and can be triggered by a whole host of things from stress to travel, anxiety, and the food we eat. Each day, nine liters of water enter our small intestine, Other more general food-related causes include: 90 percent of which is reabsorbed. Diarrhea occurs Spicy food can irritate the stomach lining. when there is too much water in the intestine or not Fatty fried foods contain saturated fats and trans enough of it is reabsorbed. While diarrhea causes fats, which the body can sometimes struggle to break dehydration, simply drinking more water can make down, leading to diarrhea or worsening symptoms. severe diarrhea worse and may require an oral Coffee stimulates your digestive system, as well rehydration solution instead. as making you mentally alert, and many people have a bowel movement soon after a cup of coffee. ELECTROLYTE LOSS Drinking alcohol can lead to loose stools the A bout of diarrhea can leave you with an imbalance next day, especially when drinking beer or wine. of electrolytes—minerals, including sodium, High FODMAP foods containing Fermentable, potassium, and calcium, that help regulate fluids in Oligosaccharides, Disaccharides, Monosaccharides the body and facilitate proper functioning of cells. and Polyols can cause diarrhea. For example, garlic Replenish electrolytes with rehydration powders or and onions contain fructans, which is a carbohydrate by drinking milk or coconut water. Short-term some people with IBS find difficult to digest. (They diarrhea can be managed at home for most people, also contain insoluble fiber, which can make foods but chronic diarrhea can be a cause for concern, so move through the digestive system faster.) visit your primary care physician as soon as possible. Some artificial sweeteners can upset the digestive system, so much so that foods containing CONTAMINATED FOOD AND WATER them may have labels warning of their laxative effect. Diet may cause diarrhea when bacteria, such as Campylobacter or Escherichia coli (E. coli), enter the body through contaminated food; or a parasite, such Good hygiene as that which causes giardiasis (contributing REDUCE YOUR RISK OF DEVELOPING to gastritis), enters in contaminated water. These DIARRHEA FROM CONTAMINANTS BY are common causes of diarrhea when vacationing MAINTAINING HIGH STANDARDS OF HYGIENE: in places with poor hygiene standards: practice Wash your hands thoroughly with soap good hygiene (see right) and avoid potentially and warm water after going to the bathroom and unsafe tap water and undercooked food. before eating or preparing food. Clean the toilet, including the handle and the FOOD TRIGGERS seat, with disinfectant after each bout of diarrhea. If you have a food intolerance, this can lead to loose Avoid sharing towels, sheets, or utensils with other household members. stools; and diet may also trigger IBS symptoms, which can include diarrhea (see pages 158–167). UUSS__115566--115577__ddiiaarrrrhhooeeaa..iinndddd 115566 2244//0088//22002211 1166::2244
115566 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 115577 Food poisoning IS DIARRHEA CAUSED BY DIET? E. coli bacteria are naturally present Diarrhea is a condition where you pass loose, watery stools more frequently in the intestines of humans and than is normal and can be triggered by a whole host of things from stress animals and help with digestion, but certain toxin-producing strains to travel, anxiety, and the food we eat. 9 LITERS ingested through contaminated food and water can cause diarrhea. WATER ENTERS SMALL INTESTINE Each day, nine liters of water enter our small intestine, Other more general food-related causes include: PER DAY 90 percent of which is reabsorbed. Diarrhea occurs Spicy food can irritate the stomach lining. when there is too much water in the intestine or not Fatty fried foods contain saturated fats and trans enough of it is reabsorbed. While diarrhea causes fats, which the body can sometimes struggle to break dehydration, simply drinking more water can make down, leading to diarrhea or worsening symptoms. severe diarrhea worse and may require an oral Coffee stimulates your digestive system, as well rehydration solution instead. as making you mentally alert, and many people have a bowel movement soon after a cup of coffee. Villi on the intestine ELECTROLYTE LOSS Drinking alcohol can lead to loose stools the wall absorb water, as Shiga toxin damages A bout of diarrhea can leave you with an imbalance next day, especially when drinking beer or wine. well as nutrients lining of small intestine of electrolytes—minerals, including sodium, High FODMAP foods containing Fermentable, A strain of E. coli in Blood vessels contaminated food potassium, and calcium, that help regulate fluids in Oligosaccharides, Disaccharides, Monosaccharides conduct produces a toxin the body and facilitate proper functioning of cells. and Polyols can cause diarrhea. For example, garlic water away called shiga Replenish electrolytes with rehydration powders or and onions contain fructans, which is a carbohydrate by drinking milk or coconut water. Short-term some people with IBS find difficult to digest. (They diarrhea can be managed at home for most people, also contain insoluble fiber, which can make foods but chronic diarrhea can be a cause for concern, so move through the digestive system faster.) visit your primary care physician as soon as possible. Some artificial sweeteners can upset the digestive system, so much so that foods containing CONTAMINATED FOOD AND WATER them may have labels warning of their laxative effect. Diet may cause diarrhea when bacteria, such as Campylobacter or Escherichia coli (E. coli), enter the body through contaminated food; or a parasite, such Good hygiene as that which causes giardiasis (contributing REDUCE YOUR RISK OF DEVELOPING to gastritis), enters in contaminated water. These DIARRHEA FROM CONTAMINANTS BY are common causes of diarrhea when vacationing MAINTAINING HIGH STANDARDS OF HYGIENE: NORMAL FUNCTION E. COLI INFECTION in places with poor hygiene standards: practice Wash your hands thoroughly with soap 90% good hygiene (see right) and avoid potentially and warm water after going to the bathroom and unsafe tap water and undercooked food. before eating or preparing food. WATER REABSORBED Damage to the lining interferes with the Clean the toilet, including the handle and the normal reabsorption FOOD TRIGGERS seat, with disinfectant after each bout of diarrhea. 10% of water, leading to diarrhea If you have a food intolerance, this can lead to loose Avoid sharing towels, sheets, or utensils with other household members. WATER EXCRETED stools; and diet may also trigger IBS symptoms, which can include diarrhea (see pages 158–167). UUSS__115566--115577__ddiiaarrrrhhooeeaa..iinndddd 115566 2244//0088//22002211 1166::2244 UUSS__115566--115577__ddiiaarrrrhhooeeaa..iinndddd 115577 2244//0088//22002211 1166::2244
158 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 159 SHOULD I BE WORRIED ABOUT FOOD ALLERGIES AND INTOLERANCES? Abnormal responses to eating food, or ‘‘adverse food reactions,’’ can be the result of either a food intolerance (often called sensitivity) or a food allergy. Despite often being lumped together, these are two very different reactions. All of us experience digestive discomfort from time of how much you eat or how frequently you eat it. to time; if mild and infrequent, there is usually no There are two different types of allergic responses. cause for concern. However, regular discomfort after IgE-mediated allergies are when the body creates eating may indicate an allergy or intolerance. Allergies antibodies specific to the food that has been can be severe and in some cases life threatening, so digested. These allergies can be lifelong, with it’s important to know the signs. About 2 percent of symptoms ranging from hives and swelling to adults and 4–8 percent of kids in the US have a food anaphylactic shock. Non-IgE-mediated food allergies allergy; intolerances seem more prevalent. involve other components of the immune system. These reactions come on more slowly and are harder FOOD ALLERGIES to diagnose, with symptoms including bloating, A food allergy is an adverse immune response. It vomiting, and diarrhea. usually comes on suddenly and can be triggered by Several factors can cause food allergies, including a small amount of food. The allergic reaction will diet, environment, and genetics. The dual-allergen occur every time you consume the food, regardless exposure hypothesis suggests that early exposure to dendritric cell IgE-mediated allergic reaction When IgE first binds an allergen to a dendritic cell, it starts a process to produce more allergen- specific IgEs. The IgEs then stick to mast cells (an immune cell) in the digestive tract, ready to react DENDRITIC CELL to the same protein when it is consumed again. IGE AND ALLERGEN KEY white blood cell—T cells The body creates IgE binds an allergen white blood cell—B cells immunoglobulin E (IgE) to a dentritic cell, antibodies specific to a sending a message IGE (immunoglobulin E) protein in the allergen food to T cells allergen Inflammatory (histamine) UUSS__115588--115599__aalllleerrggiieess..iinndddd 115588 2244//0088//22002211 1166::2244
115588 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 115599 SHOULD I BE WORRIED food allergens via the skin (for instance, nut oils THE DIET-INDUSTRY TRAP FOODS in moisturizers) could increase the likelihood of It may seem like everyone has an intolerance THAT PEOPLE ABOUT FOOD ALLERGIES allergy, while consumption of those allergens in these days, with seemingly more and more CAN BE infancy could encourage tolerance. Researchers people changing their diets for fear of food AND INTOLERANCES? have linked early and regular consumption of intolerance. But there is no evidence to account INTOLERANT peanuts to prevention of peanut allergy, for the rise; the apparent increase is more likely TO INCLUDE: Abnormal responses to eating food, or ‘‘adverse food reactions,’’ can be the result of particularly in children at higher risk because of due to self-reporting and fad diets than actual a compromised skin barrier, such as those with incidences. One study found that 34 percent of either a food intolerance (often called sensitivity) or a food allergy. Despite often eczema. (Whole nuts and peanuts should not be parents reported food allergies in their children being lumped together, these are two very different reactions. given to children under five due to choking risk.) but only 5 percent were found to have an allergy. LACTOSE The media is rife with misinformation that INTOLERANCES encourages people to waste money on expensive MILK AND OTHER LACTOSE- All of us experience digestive discomfort from time of how much you eat or how frequently you eat it. An intolerance is a nonimmune response that products they don’t need. Unregulated CONTAINING to time; if mild and infrequent, there is usually no There are two different types of allergic responses. directly impacts digestion. Intolerances often alternative health therapists are diagnosing PRODUCTS cause for concern. However, regular discomfort after IgE-mediated allergies are when the body creates relate to the amount of food consumed or the people with intolerances they don’t have, MANY TABLETS eating may indicate an allergy or intolerance. Allergies antibodies specific to the food that has been frequency of consumption. Some are triggered potentially delaying a correct diagnosis from a HAVE LACTOSE ADDED AS can be severe and in some cases life threatening, so digested. These allergies can be lifelong, with by certain chemicals in food, while others qualified health professional (see pages 160–161). A FILLER it’s important to know the signs. About 2 percent of symptoms ranging from hives and swelling to develop if you lack the enzymes needed to break Excluding common staples, such as bread, from adults and 4–8 percent of kids in the US have a food anaphylactic shock. Non-IgE-mediated food allergies down a specific food. While not life threatening, our diets can even lead to malnutrition. What’s allergy; intolerances seem more prevalent. involve other components of the immune system. food intolerances can greatly impact quality of more, a developing concern is that by restricting VASO-ACTIVE These reactions come on more slowly and are harder life. Symptoms tend to come on gradually and foods unnecessarily, people may become more AMINES FOOD ALLERGIES to diagnose, with symptoms including bloating, can include rashes, itching, bloating, and at risk of developing an eating disorder. See a A food allergy is an adverse immune response. It vomiting, and diarrhea. diarrhea (the last two can also be caused by credentialed health professional for a proper RED WINE, STRONG AND BLUE usually comes on suddenly and can be triggered by Several factors can cause food allergies, including other diseases, such as bowel cancer, so get diagnosis or advice, and make changes to your CHEESES, TUNA, a small amount of food. The allergic reaction will diet, environment, and genetics. The dual-allergen them checked out by your doctor). diet according to their guidelines. MACKEREL, PORK PRODUCTS, AND occur every time you consume the food, regardless exposure hypothesis suggests that early exposure to OTHER FOODS NATURAL dendritric cell mast cell IgE-mediated allergic reaction CHEMICALS SUCH AS When IgE first binds an allergen to a dendritic SALICYLATE AND GLUTAMATE cell, it starts a process to produce more allergen- specific IgEs. The IgEs then stick to mast cells (an immune cell) in the digestive tract, ready to react DENDRITIC CELL MAST CELL to the same protein when it is consumed again. SOME FOOD IGE AND ALLERGEN T CELLS B CELLS AND ALLERGEN ADDITIVES KEY ESPECIALLY BENZOATE, white blood cell—T cells The body creates IgE binds an allergen T cells are triggered Interleukins cause B cells The IgE, attached to mast SULFITE white blood cell—B cells immunoglobulin E (IgE) to a dentritic cell, to release signaling to produce more IgE that is cells, binds to allergens PRESERVATIVES, antibodies specific to a sending a message compounds called specific to the allergen when they enter the body, AND MONOSODIUM IGE (immunoglobulin E) protein in the allergen food to T cells interleukins causing the mast cell to GLUTAMATE release inflammatory allergen substances See more on common triggers Inflammatory (histamine) on pages 160–161 UUSS__115588--115599__aalllleerrggiieess..iinndddd 115588 2244//0088//22002211 1166::2244 UUSS__115588--115599__aalllleerrggiieess..iinndddd 115599 2244//0088//22002211 1166::2244
160 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 161 HOW CAN I FIND OUT IF I HAVE AN ALLERGY OR INTOLERANCE? Food allergies and intolerances can make life very difficult, so if you think you have one, it’s important to ask for medical support in identifying, treating, and managing the issue. Figuring out whether you suffer from an allergy For instance, sulfites, used to preserve food, can or intolerance can be a difficult process. There are cause some people with asthma to have an attack, many unanswered questions in the scientific world though it is not necessary for all asthma sufferers about allergies. If you are worried, it’s best to seek to avoid them. Oral allergy syndrome is a less help from a doctor before eliminating a food group common type of allergy, in which the body mistakes completely, to enable you to get tested correctly and the protein in some fruits or vegetables for pollen, avoid any nutritional deficiencies. It’s a good idea causing symptoms such as an itchy mouth and/or to keep a food diary and monitor your symptoms. throat, and mild swelling in the mouth area. Celiac disease (where the small intestine is DIAGNOSING FOOD ALLERGIES hypersensitive to gluten) is technically not a food There are many foods that people can be allergic to, allergy,but an autoimmune disease caused by an some of the most common being milk, wheat, eggs, allergic reaction to gluten, a group of proteins found peanuts, tree nuts, fish, shellfish, and some fruits in wheat, barley, and rye. It is thought to affect at and vegetables. In addition, some food additives least 1% of Americans; however, only about 30 can increase the symptoms of an existing allergy. percent of people with the condition are currently Lactose intolerance Lactose is a type of sugar found in most dairy products. In humans, the enzyme lactase is needed to break down lactose, so when the body doesn’t produce enough lactase, the lactose passes through the gut undigested, leading to symptoms of intolerance, such as cramps and bloating. HIGH DIMINISHED LACTASE LEVELS LACTASE KEY Lactase Bacteria Lactose Short-chain fatty acids Digested in small intestine Passes small intestine Glucose Carbon dioxide Lactase splits lactose into Undigested lactose moves to Galactose Hydrogen glucose and galactose, which the large intestine and causes Water Methane can be absorbed into the blood increased water in the colon UUSS__116600--116611__ffiinnddiinngg__oouutt..iinndddd 116600 2244//0088//22002211 1166::2244
116600 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 116611 HOW CAN I FIND OUT IF I HAVE clinically diagnosed. Symptoms may include toxins that enter food. Few food intolerances cramps, bloating, nausea, and reflux. It is are lifelong, and in most cases, people can eat ALLERGIES AN ALLERGY OR INTOLERANCE? possible to have a wheat allergy and not have small amounts without enduring problems. MILK celiac disease, or to have nonceliac gluten- Two of the most widely discussed food PEANUTS sensitivity (though little is known about this). intolerances are to wheat and lactose. Gluten Food allergies and intolerances can make life very difficult, so if you TREE NUTS Not much is known about why people develop is often mistakenly blamed for digestive think you have one, it’s important to ask for medical support in identifying, FISH allergies. If someone in your family has or had discomfort, but gluten intolerances are not treating, and managing the issue. SHELLFISH an allergy, you will be more likely to develop medically recognized, and a lot of the time the SOME FRUITS one, too, though not necessarily the same one. symptoms may be due to something else, such AND VEGETABLES Those with allergies often have other conditions, as IBS, stress, anxiety, IBD, or celiac disease. SOY Figuring out whether you suffer from an allergy For instance, sulfites, used to preserve food, can such as asthma and hay fever. Lactose is a disaccharide sugar mainly found CELERY or intolerance can be a difficult process. There are cause some people with asthma to have an attack, An IgE-mediated food allergy (see pages in dairy products. An intolerance occurs when MUSTARD many unanswered questions in the scientific world though it is not necessary for all asthma sufferers 158–159) can be diagnosed either by measuring a person lacks the enzyme lactase (see below) SESAME about allergies. If you are worried, it’s best to seek to avoid them. Oral allergy syndrome is a less the antibodies present in the blood or by so cannot break down the disaccharide into PINE NUTS help from a doctor before eliminating a food group common type of allergy, in which the body mistakes inserting allergens into the skin and looking for monosaccharides glucose and galactose to then MEAT completely, to enable you to get tested correctly and the protein in some fruits or vegetables for pollen, a reaction. Alternatively, a supervised elimination enter the blood. This can lead to a buildup of avoid any nutritional deficiencies. It’s a good idea causing symptoms such as an itchy mouth and/or diet can help you figure out what the allergen is. lactose in the digestive system, resulting in to keep a food diary and monitor your symptoms. throat, and mild swelling in the mouth area. wind, cramps, diarrhea, bloating, and nausea. EGGS Celiac disease (where the small intestine is SPOTTING FOOD INTOLERANCES (See pages 162–163 for more on cutting gluten WHEAT DIAGNOSING FOOD ALLERGIES hypersensitive to gluten) is technically not a food The list of foods that can cause digestive and lactose from your diet.) There are many foods that people can be allergic to, allergy,but an autoimmune disease caused by an discomfort is long. Common triggers include There is currently no clinically valid test for INTOLERANCES some of the most common being milk, wheat, eggs, allergic reaction to gluten, a group of proteins found food additives such as MSG, caffeine, alcohol, diagnosing an intolerance. Instead, they are MSG peanuts, tree nuts, fish, shellfish, and some fruits in wheat, barley, and rye. It is thought to affect at artificial sweeteners and preservatives, a group confirmed by cutting out suspected trigger CAFFEINE and vegetables. In addition, some food additives least 1% of Americans; however, only about 30 of carbohydrates known as FODMAPs (see foods and observing symptoms, followed by ALCOHOL can increase the symptoms of an existing allergy. percent of people with the condition are currently pages 166–167), eggs, yeast, fructose, and even gradual reintroduction of the excluded foods. ARTIFICIAL SWEETENERS ARTIFICIAL FOOD PRESERVATIVES FODMAPS Lactose intolerance (see pp.166–167) YEAST Lactose is a type of sugar found in most FRUCTOSE dairy products. In humans, the enzyme LACTOSE lactase is needed to break down lactose, GLUTEN so when the body doesn’t produce enough VASO-ACTIVE lactase, the lactose passes through the AMINES gut undigested, leading to symptoms of (found in red intolerance, such as cramps and bloating. HIGH DIMINISHED wine, strong and LACTASE LEVELS LACTASE FERMENTATION BY-PRODUCTS blue cheeses, tuna, KEY mackerel, pork Lactase Bacteria products, and Allergies and Lactose Short-chain fatty acids intolerances Some other foods) Digested in small intestine Passes small intestine Digestion in colon Cramps and bloating Glucose Carbon dioxide foods can cause allergies SALICYLATES Lactase splits lactose into Undigested lactose moves to The lactose is digested by gut The increase in acids and gas while others may cause Galactose Hydrogen glucose and galactose, which the large intestine and causes bacteria instead, causing caused by fermentation can intolerances. Some Water Methane can be absorbed into the blood increased water in the colon fermentation lead to cramps and bloating foods may cause both. UUSS__116600--116611__ffiinnddiinngg__oouutt..iinndddd 116600 2244//0088//22002211 1166::2244 UUSS__116600--116611__ffiinnddiinngg__oouutt..iinndddd 116611 2244//0088//22002211 1166::2244
162 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 163 HOW DO I CUT GLUTEN OR LACTOSE FROM MY DIET? If you’ve been told by your doctor to eliminate either gluten or lactose from your diet, it may seem like a daunting prospect, but you can do so successfully without compromising on nutrition. GLUTEN The majority of people won’t need to cut gluten or SOURCES lactose from their diets, but those who are advised to often see dramatic benefits to their health and GLUTEN CAN BE energy levels, provided they maintain a healthy, FOUND IN ALL SORTS OF PRODUCTS, IN balanced diet with alternative foods. Consult a FOODS YOU MIGHT doctor or registered dietitian to figure out what NOT EXPECT exactly you need to exclude, and remember that FLOUR while going gluten-free can have positive outcomes for some, for others it may do more harm than good. BREADS AND OTHER BAKED GOODS ELIMINATING WHEAT OR GLUTEN CEREALS Be aware of the difference between wheat and gluten PASTA if you need to avoid one or both. If diagnosed with COOKIES celiac disease (when the immune system reacts abnormally to gluten), you may need to exclude CAKES rye, barley, and oats from your diet as well as gluten. SOY SAUCE Some individuals with a wheat allergy (which occurs BEER when the body produces antibodies to proteins found in wheat) will be able to have a variety of SOUPS grains, and others will not; for most, it is safest to PROCESSED MEATS eliminate gluten entirely. Gluten-free products are READY-MADE SAUCES not always wheat-free and vice versa, so check labels. Gluten is found in many everyday products READY MEALS (see left). It also crops up in convenience foods, so be sure to check labels carefully. Thankfully, it’s possible to buy wheat-free or gluten-free versions of these food items. In addition to flour, baking powder GLIADIN GLUTENIN can also be wheat- or gluten-free. And watch out for xanthan gum, which is often added to baking items GLUTEN What is gluten? The proteins gliadin and to help with the texture and binding of the mixture; glutenin combine with water to create a mesh those who have celiac disease or gluten intolerance structure known as gluten, which has an elastic quality that is especially useful in baking. often react to this, too. UUSS__116622--116633__llaaccttoossee__gglluutteenn..iinndddd 116622 2244//0088//22002211 1166::2244
116622 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 116633 HOW DO I CUT GLUTEN OR Lactose digestion Lactose is LACTOSE FROM MY DIET? a natural sugar found in milk. GLUCOSE It is broken down in the small intestine by the enzyme lactase into two simple sugars, glucose If you’ve been told by your doctor to eliminate either gluten or lactose from LACTOSE LACTASE and galactose, which are then absorbed into the blood. your diet, it may seem like a daunting prospect, but you can do so successfully GALACTOSE without compromising on nutrition. LACTOSE GLUTEN The majority of people won’t need to cut gluten or Gluten-free products are lower in fiber SOURCES SOURCES lactose from their diets, but those who are advised and often higher in fat and sugar than to often see dramatic benefits to their health and their gluten-containing equivalents, and LACTOSE CAN BE GLUTEN CAN BE FOUND IN NUMEROUS energy levels, provided they maintain a healthy, since they are not governed by the same FOUND IN ALL SORTS MILK PRODUCTS, OF PRODUCTS, IN balanced diet with alternative foods. Consult a fortification rules as wheat products, they PROTEINS, AND FOODS YOU MIGHT doctor or registered dietitian to figure out what may also contain fewer vitamins and minerals, INGREDIENTS NOT EXPECT exactly you need to exclude, and remember that including iron, B vitamins, and calcium. FLOUR while going gluten-free can have positive outcomes Gluten-free products are also lower in protein BUTTERMILK for some, for others it may do more harm than good. given that the gluten (protein) is removed. products, but they also have lactase added, BREADS AND OTHER CALCIUM OR SODIUM BAKED GOODS which helps the body digest any lactose taken CASEINATE ELIMINATING WHEAT OR GLUTEN ELIMINATING LACTOSE in. Rice, oat, almond, cashew, quinoa, soy, and CEREALS CASEIN (CURDS), Be aware of the difference between wheat and gluten Like the gluten/wheat distinction, there is a pea milk are all great dairy-free milk CASEINATES PASTA if you need to avoid one or both. If diagnosed with difference between lactose intolerance and alternatives (though rice milk should not be COTTAGE CHEESE COOKIES celiac disease (when the immune system reacts a milk allergy. The former results from a lack given to under-fives due to its arsenic content). abnormally to gluten), you may need to exclude of lactase, the enzyme that breaks down Cow’s milk is often added to manufactured GHEE CAKES rye, barley, and oats from your diet as well as gluten. lactose into more easily absorbed sugars foods, so it isn’t just a simple case of HYDROLYZED SOY SAUCE Some individuals with a wheat allergy (which occurs (see above and pages 160–161). The latter is an eliminating obvious dairy products, such as CASEIN BEER when the body produces antibodies to proteins immune reaction to one of the many proteins milk, butter, cheese, and yogurt. For common HYDROLYZED found in wheat) will be able to have a variety of in animal milk, though it’s most often caused ingredients derived from cow’s milk, see right. WHEY PROTEIN SOUPS grains, and others will not; for most, it is safest to by the alpha S1-casein protein in cow’s milk. LACTOALBUMIN PROCESSED MEATS eliminate gluten entirely. Gluten-free products are Bear in mind that while lactose-free products BEING MINDFUL LACTOGLOBULIN READY-MADE SAUCES not always wheat-free and vice versa, so check labels. don’t contain the sugar lactose, they may still You will need to be vigilant about everything Gluten is found in many everyday products contain milk proteins. you eat, to avoid cross-contamination. In the LACTOSE READY MEALS (see left). It also crops up in convenience foods, When eliminating lactose-containing foods, US, it’s law that allergens must be clearly MARGARINE so be sure to check labels carefully. Thankfully, it’s think about the vitamins and minerals you mentioned on food packaging, usually MILK PROTEIN possible to buy wheat-free or gluten-free versions of would be getting in the original product and emphasized in bold and listed in one place. these food items. In addition to flour, baking powder aim to replace them elsewhere in your diet. “May contain” statements are often used on MILK SOLIDS GLIADIN GLUTENIN can also be wheat- or gluten-free. And watch out for Look for foods that contain calcium, vitamins packaging to state that a food may be MILK SUGAR xanthan gum, which is often added to baking items D and B12, and iodine, in particular. For contaminated with one or more common GLUTEN MODIFIED MILK What is gluten? The proteins gliadin and to help with the texture and binding of the mixture; instance, B12 and iodine are found in fish allergens. Restaurants and cafés must also glutenin combine with water to create a mesh those who have celiac disease or gluten intolerance and eggs. Lactose-free products contain the provide all dietary information on allergens WHEY AND structure known as gluten, which has an elastic WHEY SOLIDS quality that is especially useful in baking. often react to this, too. same vitamins and minerals as standard dairy in writing and communicate them verbally. UUSS__116622--116633__llaaccttoossee__gglluutteenn..iinndddd 116622 2244//0088//22002211 1166::2244 UUSS__116622--116633__llaaccttoossee__gglluutteenn..iinndddd 116633 2244//0088//22002211 1166::2244
164 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 165 COULD I HAVE IRRITABLE BOWEL SYNDROME (IBS)? If you suffer from digestive discomfort and don’t believe it to be caused by a food intolerance, it’s possible you could have IBS—though see a doctor to be sure. Irritable bowel syndrome is the most common type Doctors tend to start by assessing a patient’s clinical of functional gastrointestinal disease, with global history to identify and rule out other diseases, then prevalence ranging from 2–15 percent. IBS is more perform a physical examination, sometimes requiring common in women, and, unlike many illnesses, laboratory tests and colonoscopies. There are four incidence decreases with age. Symptoms vary IBS subtypes: IBS-C (constipation predominant), and can change over time, but commonly include IBS-D (diarrhea predominant), IBS-M (mixed bloating, constipation, and/or diarrhea, and bowel habits), and IBS-U (unclassified). recurrent abdominal pain—to be diagnosed with Not everyone responds to IBS treatments in the IBS, you must have experienced frequent abdominal same way. Often, changes to diet and lifestyle are pain for at least three months, with symptom onset recommended first; then you may try antibiotics, at least six months prior to diagnosis. antispasmodics, antidepressants (as neuromodulators that can impact gut motility, visceral hypersensitivity, DIAGNOSIS AND TREATMENT and gastrointestinal transit speed), laxatives, or The mechanisms behind IBS are still unclear, but antidiarrheal drugs, as well as cognitive behavioral research suggests that they are multifactorial and therapy (CBT) and other psychological patient dependent. IBS is now considered a disorder interventions. However, the treatments are by no of the gut-brain axis (see page 49); in many cases, it means a cure and tend to target only one part of can be caused by stress. Certain molecules in food what is a multi-modal condition. Reducing or may play a large part, too (see page 166). Since IBS eliminating intake of certain foods can be effective mimics other disorders, it is often hard to diagnose. (see pages 166–167). Intestinal spasms A common symptom of IBS, intestinal spasms occur when the muscles of the gut contract spontaneously, causing irregular bowel movements and abdominal pain. NORMAL SEQUENCE SPASM UUSS__116644--116655__IIBBSS..iinndddd 116644 2244//0088//22002211 1166::2244
116644 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 116655 COULD I HAVE IRRITABLE WHAT IS IBD? BOWEL SYNDROME (IBS)? Often confused with IBS, irritable bowel disease (IBD) refers to two conditions causing inflammation of the gastrointestinal tract: Crohn’s disease and ulcerative colitis. If you suffer from digestive discomfort and don’t believe it to be caused by a food intolerance, it’s possible you could have IBS—though see a doctor to be sure. Crohn’s disease can affect any part of the diet can also play a huge role, especially in those gastrointestinal (GI) tract, from the mouth to the with mild IBD. Doctors also recommend that Irritable bowel syndrome is the most common type Doctors tend to start by assessing a patient’s clinical anus, though in most patients, the lower part of the IBD patients are given a number of vaccinations of functional gastrointestinal disease, with global history to identify and rule out other diseases, then small intestine is affected. Inflammation is patchy to prevent infections. In severe cases, a patient may prevalence ranging from 2–15 percent. IBS is more perform a physical examination, sometimes requiring and can extend through multiple layers of the GI undergo surgery to remove damaged portions of common in women, and, unlike many illnesses, laboratory tests and colonoscopies. There are four tract walls. In ulcerative colitis, the inflammation is the GI tract, though this has become less common incidence decreases with age. Symptoms vary IBS subtypes: IBS-C (constipation predominant), confined to the colon (or large intestine) and rectum. thanks to advances in treatment with medications. and can change over time, but commonly include IBS-D (diarrhea predominant), IBS-M (mixed This condition causes ulcers to develop on the inner bloating, constipation, and/or diarrhea, and bowel habits), and IBS-U (unclassified). lining of the bowel, which may bleed and produce recurrent abdominal pain—to be diagnosed with Not everyone responds to IBS treatments in the mucus. Symptoms of both types of IBD include IBS, you must have experienced frequent abdominal same way. Often, changes to diet and lifestyle are persistent diarrhea, abdominal pain, rectal bleeding, Celiac disease pain for at least three months, with symptom onset recommended first; then you may try antibiotics, weight loss, and fatigue. at least six months prior to diagnosis. antispasmodics, antidepressants (as neuromodulators As with IBS, the causes of IBD are not yet fully WHILE NOT A FORM OF IBD ITSELF, CELIAC that can impact gut motility, visceral hypersensitivity, understood. Researchers believe that diet, stress, DISEASE CAN CAUSE SIMILAR SYMPTOMS. DIAGNOSIS AND TREATMENT and gastrointestinal transit speed), laxatives, or immunity, and genetics are all factors, with risk The condition causes your immune system to attack your own tissues when you eat gluten, The mechanisms behind IBS are still unclear, but antidiarrheal drugs, as well as cognitive behavioral levels dependent on age, ethnicity, family history, damaging the small intestine’s lining, which can research suggests that they are multifactorial and therapy (CBT) and other psychological cigarette smoking, and high usage of nonsteroidal decrease absorption of nutrients. If untreated, patient dependent. IBS is now considered a disorder interventions. However, the treatments are by no anti-inflammatory drugs (NSAIDs). complications can include anemia, osteoporosis, infertility, neurological conditions and nerve of the gut-brain axis (see page 49); in many cases, it means a cure and tend to target only one part of IBD is diagnosed first through testing levels damage, and, rarely, small bowel cancer and can be caused by stress. Certain molecules in food what is a multi-modal condition. Reducing or of fecal calprotectin, then with an endoscopy or intestinal lymphoma. (See also page 160.) may play a large part, too (see page 166). Since IBS eliminating intake of certain foods can be effective colonoscopy and biopsies. There are a number of mimics other disorders, it is often hard to diagnose. (see pages 166–167). medications that can be used in treatment, but Inflammation patterns Intestinal spasms While both sorts of IBD A common symptom feature inflammation, of IBS, intestinal the location of it differs spasms occur when between Crohn’s and UC. the muscles of the gut contract spontaneously, causing irregular bowel movements and Inflammation in abdominal pain. large and small intestine Continuous inflammation in the large intestine Stricture, between small and large intestine NORMAL SEQUENCE SPASM CROHN’S DISEASE ULCERATIVE COLITIS UUSS__116644--116655__IIBBSS..iinndddd 116644 2244//0088//22002211 1166::2244 UUSS__116644--116655__IIBBSS..iinndddd 116655 2255//0088//22002211 1199::1155
166 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 167 WHAT ARE FODMAPs AND CAN AVOIDING THEM CURE IBS? If you have made changes to your lifestyle to improve symptoms of Irritable Bowel Syndrome (IBS; see page 164) but had no results, the next approach may well be to work with a registered dietitian nutritionist on what is known as the low-FODMAP diet. FODMAPs—fermentable, oligosaccharides, Disaccharides, such as lactose, contain two simple disaccharides, monosaccharides, and polyols—are sugar molecules, and monosaccharides, such as a group of carbohydrates that the small intestine fructose, contain one sugar molecule. Polyols, also cannot digest. When we eat them, they pass through known as “sugar alcohols,” are sorbitol and mannitol, to the large intestine where they are fermented by which are mainly found in sugar-free sweets, mints, gut bacteria. Many foods fall into the FODMAP and chewing gum, plus certain fruits and vegetables. category, including apples, avocados, pears, mangoes, cauliflower, soybeans, rye, beans, and pulses. HOW FODMAPS CAUSE SYMPTOMS “Fermentable” covers foods that can be The mechanisms behind FODMAP fermented and used as food for gut symptom production are not fully known, bacteria. Oligosaccharides are a type of but there are two main hypotheses: carbohydrate that usually contains 3–10 simple sugar molecules, such as fructans, The “small bowel hypothesis” states found in onions, garlic, wheat, rye, and that FODMAPs are osmotically active barley, and galacto-oligosaccharides molecules (i.e., triggering the diffusion of (GOS), found in beans and lentils. water from an area of high concentration Stomach Small intestine Large intestine FODMAPs in the gut FOODS HIGH IN FODMAPs FODMAPs may trigger excess water to either form in the small intestine or ferment Garlic, onions, broccoli, cauliflower, in the large intestine, mushrooms, apples, pears, watermelon, cherries, and wheat causing gut discomfort. products such as pasta and pastries are all classed as FODMAP foods UUSS__116666--116677__ffooddmmaappss..iinndddd 116666 2266//0088//22002211 1111::5511
116666 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 116677 WHAT ARE FODMAPs AND CAN to an area of low concentration through a 1. Elimination: A period of four to six weeks FODMAP s selectively permeable membrane), which cause when patients must restrict all FODMAPs from ARE A GROUP AVOIDING THEM CURE IBS? an increase in water in the small intestine, their diet. At the end of this period, they should OF INDIGESTIBLE leading to distension, bloating, and discomfort. see symptom improvement. CARBOHYDRATES FERMENTED IN If you have made changes to your lifestyle to improve symptoms of Irritable Bowel THE LARGE The “large bowel hypothesis” suggests 2. Reintroduction: Over the following weeks, INTESTINE Syndrome (IBS; see page 164) but had no results, the next approach may well be to that FODMAPs increase colonic bacterial patients begin to reintroduce FODMAPs into work with a registered dietitian nutritionist on what is known as the low-FODMAP diet. F fermentation as well as gas production. This their diet systematically, adding one food at a results in bloating, flatulence, and discomfort. It time back into the diet in increasing quantities, FERMENTABLE is also thought that psychological factors and an allowing the patient to identify their symptom FODMAPs—fermentable, oligosaccharides, Disaccharides, such as lactose, contain two simple altered brain–gut axis (see p.49) caused by stress, threshold for each of the FODMAPs. O disaccharides, monosaccharides, and polyols—are sugar molecules, and monosaccharides, such as anxiety, and a hectic lifestyle may play a role. OLIGOSACCHARIDES a group of carbohydrates that the small intestine fructose, contain one sugar molecule. Polyols, also 3. Personalization: Over time, patients can cannot digest. When we eat them, they pass through known as “sugar alcohols,” are sorbitol and mannitol, THE LOW-FODMAP DIET personalize their diet, adding in the FODMAPs D to the large intestine where they are fermented by which are mainly found in sugar-free sweets, mints, The low-FODMAP diet (LFD) is a popular that don’t cause symptoms and reducing or gut bacteria. Many foods fall into the FODMAP and chewing gum, plus certain fruits and vegetables. and effective way to ease IBS symptoms and eliminating those that do, to control symptoms DISACCHARIDES category, including apples, avocados, pears, mangoes, improve patient quality of life. In fact, it has while maintaining a nutritionally adequate diet. cauliflower, soybeans, rye, beans, and pulses. HOW FODMAPS CAUSE SYMPTOMS been proven to reduce gut symptoms in patients M “Fermentable” covers foods that can be The mechanisms behind FODMAP with IBS 70–80 percent of the time. While it If you think you have IBS, speak to your GP. MONOSACCHARIDES fermented and used as food for gut symptom production are not fully known, can seem highly restrictive, if followed correctly, While the LFD has excellent results in some bacteria. Oligosaccharides are a type of but there are two main hypotheses: this diet allows for individual personalization. people, only follow it if advised to do so and with A carbohydrate that usually contains 3–10 It is incredibly important to embark upon the professional support. It’s also not recommended AND simple sugar molecules, such as fructans, The “small bowel hypothesis” states LFD only with supervision, to ensure you to stay on the diet for a long period of time, as found in onions, garlic, wheat, rye, and that FODMAPs are osmotically active remain healthy throughout. Patients have two reduced gut microbiome diversity, which can be P barley, and galacto-oligosaccharides molecules (i.e., triggering the diffusion of or three appointments with a dietary specialist, a result of the LFD, can be more detrimental to (GOS), found in beans and lentils. water from an area of high concentration and there are three stages in the process: gut health in the long term. POLYOLS Stomach Small intestine Large intestine FODMAPs in the gut BACTERIAL FOODS HIGH SMALL REACTION IN GAS BUILDS UP MESSAGE SENT TO IN FODMAPs INTESTINE LARGE INTESTINE = EXPANSION NERVOUS SYSTEM FODMAPs may trigger excess water to either form in the small intestine or ferment Garlic, onions, broccoli, cauliflower, If the reaction takes place in the Bacteria in the large The muscle wall expands The large intestine in the large intestine, mushrooms, apples, pears, small intestine, then water is intestine react with the with the build up of gas sends a signal via the watermelon, cherries, and wheat absorbed via osmosis, leading food eaten to cause as a result of the expanded muscle to the causing gut discomfort. products such as pasta and pastries to distension and bloating fermentation fermentation nervous system are all classed as FODMAP foods UUSS__116666--116677__ffooddmmaappss..iinndddd 116666 2266//0088//22002211 1111::5511 UUSS__116666--116677__ffooddmmaappss..iinndddd 116677 2244//0088//22002211 1166::2244
168 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 169 SARDINES PROVIDE PROTEIN, CALCIUM, AND VITAMINS B12 AND D AS WELL AS OMEGA-3 FATTY ACIDS Sources of omega-3 Fish such as sardines, mackerel, herring, salmon, and snapper have a darker flesh that is rich in omega-3 fatty acids. Vegan DHA supplements are also an option. UUSS__116688--116699__aannttii__iinnffllaammmmaattoorryy..iinndddd 116688 2244//0088//22002211 1166::2244
116688 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 116699 WHAT IS AN SARDINES PROVIDE PROTEIN, ANTI-INFLAMMATORY DIET? CALCIUM, AND VITAMINS B12 AND D AS WELL AS OMEGA-3 There isn’t such a thing as an anti-inflammatory diet, but there may be a link FATTY ACIDS between diet and inflammation. This area needs more research, but there’s no harm in trying a few recommendations to see how they work for you. Inflammation is a natural process, the body’s Certainly, a healthy diet is unlikely to aggravate normal response to some types of injury or illness inflammation, but there is no conclusive evidence (cuts or colds, for instance). It is part of your linking specific foods to reducing inflammation, body’s defense system and the process of healing. except in the case of rheumatoid arthritis (RA). In some cases, people have chronic inflammation In one study, one group of people with RA was in the body. Arthritis, asthma, eczema, some heart given a diet rich in fiber, oily fish, and probiotics, and lung diseases, diabetes, and some cancers all while another was provided one of protein, red involve chronic inflammation. The gut is susceptible meat, and saturated fat. Members of the first group to inflammation in some. Chronic inflammatory experienced a health improvement. We need more conditions of the gut include IBS and IBDs such research to explore these findings, but following as celiac disease (see pages 164–165). dietary recommendations for people with RA (increase omega-3s, calcium, and iron) may reduce DIETARY CAUSES OF INFLAMMATION inflammation in other parts of the body. Some Many studies have looked into the potential link research shows a reduction of RA symptoms if between the food we consume and inflammation. following a Mediterranean diet (see pages 36–39). These show that consuming the following foods and beverages in excess may cause inflammation: refined carbohydrates, such as white bread, cakes, and pastries Omega-3 fatty acids French fries and other fried foods PEOPLE WITH RHEUMATOID ARTHRITIS ARE soda and other sugar-sweetened beverages ADVISED TO INCLUDE OMEGA-3 IN THE DIET. red meat (like burgers or steaks) and processed Fish oils have been shown to help dampen general meats (like hot dogs or sausage) inflammation and may help reduce joint pain and stiffness. Try to eat two 5oz (140g) portions of oily margarine, shortening, and lard fish per week. Some eggs and breads are enriched Try limiting these items in your diet if you Sources of omega-3 Fish with omega-3. Omega-3 fats from plant sources such as sardines, mackerel, experience inflammation regularly. (such as flaxseed, evening primrose, and borage oil) herring, salmon, and snapper have a weaker effect on reducing inflammation and have a darker flesh that is are of limited benefit. High-dose fish oil rich in omega-3 fatty acids. REDUCING INFLAMMATION supplements (500–1000mg of EPA and DHA per Vegan DHA supplements Inflammation is a response of the immune system, are also an option. capsule) have been shown to reduce symptoms and gut health has been linked to good immunity of RA. Be patient—it can take up to three months (see pages 140–141). This may be how we are able for symptom relief. Speak to your doctor before taking any new supplements. to have some influence on our immune health. Aim for a good diet that supports your gut microbiota. UUSS__116688--116699__aannttii__iinnffllaammmmaattoorryy..iinndddd 116688 2244//0088//22002211 1166::2244 UUSS__116688--116699__aannttii__iinnffllaammmmaattoorryy..iinndddd 116699 2244//0088//22002211 1166::2244
170 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 171 WHAT IS DIABETES AND WHAT ARE THE RISK FACTORS? Diabetes is a condition in which you have elevated levels of glucose in your blood. Type 1 diabetes, an autoimmune disease, has sudden onset and is not caused by diet. The onset of type 2 is gradual and less severe and is heavily influenced by diet. When glucose is obtained from diet, it is released In people with type 2, the pancreas fails to supply into the bloodstream for the body to use as fuel enough insulin. To gain more energy, the body (see pages 12–13). The hormone insulin, supplied by signals to the pancreas to release more insulin. the pancreas, helps cells access this fuel. People with The pancreas then works very hard to pump out diabetes have a problem with insulin, which means insulin, resulting in such high levels of insulin in the their cells cannot access glucose for fuel. Glucose blood that cells become less sensitive to it (insulin remains in the bloodstream, causing hyperglycemia resistance). The pancreas also becomes damaged. (high blood glucose levels). Sustained hyperglycemia can damage the eyes, kidneys, and heart. In people with type 1 diabetes, the pancreas is unable to make insulin. The absence of insulin TYPE 1 TYPE 2 makes it impossible for the body to access glucose LOW INSULIN LEVELS NO INSULIN PRESENT for energy, forcing it to break down fat instead. GLUCOSE LEVELS CAUSING HYPERGLYCEMIA. MORE This releases fatty acids called ketones into the RISE, CAUSING INSULIN IS RELEASED, HYPERGLYCEMIA OR, body. These can make the blood acidic, resulting DAMAGING THE IN SOME CASES, in a life-threatening complication known as DKA. PANCREAS OVER TIME diabetic ketoacidosis (DKA). Small intestine Insulin enables cells to access glucose Pancreas Insulin "unlocks" the cells, enabling glucose to enter so the cell can use it as a source Insulin of energy. When a person has diabetes, this system can GLUCOSE BECOMES INSULIN IS break down at various stages. AVAILABLE RELEASED KEY Glucose is released In response to a rise from carbohydrates that are in blood glucose levels, the Glucose broken down in the small intestine. pancreas produces and Insulin It enters the bloodstream releases the hormone insulin UUSS__117700--117711__ddiiaabbeetteess..iinndddd 117700 2244//0088//22002211 1166::2244
117700 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 117711 RISK FACTORS WHAT IS DIABETES AND Approximately 8 percent of people with diabetes Hyperglycemia symptoms WHAT ARE THE RISK FACTORS? have type 1 and 90 percent have type 2. There may be a genetic risk involved in developing type 1 IF YOU EXPERIENCE THESE SYMPTOMS, ASK YOUR DOCTOR FOR A BLOOD SUGAR CHECK: diabetes. With type 2, there is a strong genetic risk Diabetes is a condition in which you have elevated levels of glucose in your blood. factor—the child of a parent with type 2 has roughly extreme thirst (polydipsia) Type 1 diabetes, an autoimmune disease, has sudden onset and is not caused by diet. a one-in-three chance of developing it. The more frequent urination (polyuria) The onset of type 2 is gradual and less severe and is heavily influenced by diet. distant the genetic relationship, the slimmer the physical exhaustion chance of developing the condition. weight loss and loss of muscle mass Ethnicity is also a factor: South Asian, Chinese, genital thrush (frequent, minor infections) When glucose is obtained from diet, it is released In people with type 2, the pancreas fails to supply Black African, and African Caribbean people have slow healing of cuts and wounds into the bloodstream for the body to use as fuel enough insulin. To gain more energy, the body a higher chance of developing type 2 than people of blurred vision. (see pages 12–13). The hormone insulin, supplied by signals to the pancreas to release more insulin. other ethnicities. the pancreas, helps cells access this fuel. People with The pancreas then works very hard to pump out Age is important. People over 40 (or younger diabetes have a problem with insulin, which means insulin, resulting in such high levels of insulin in the in ethnic groups more at risk) are at higher risk. their cells cannot access glucose for fuel. Glucose blood that cells become less sensitive to it (insulin Being overweight or obese also increases the risk When it comes to type 2 diabetes, the number remains in the bloodstream, causing hyperglycemia resistance). The pancreas also becomes damaged. of developing type 2 diabetes, especially if you of people being diagnosed is growing at a shocking (high blood glucose levels). Sustained hyperglycemia carry excess body fat around the abdomen. rate. This is a health crisis in the making and, to a can damage the eyes, kidneys, and heart. large degree, it is preventable. It’s important that In people with type 1 diabetes, the pancreas is DIET AND DIABETES people with a risk factor are aware that there unable to make insulin. The absence of insulin TYPE 1 TYPE 2 Type 1 is a lifelong condition is much they can do to avoid developing makes it impossible for the body to access glucose NO INSULIN PRESENT LOW INSULIN LEVELS for which insulin injections and TYPE 2 the condition (see pages 172–173). for energy, forcing it to break down fat instead. GLUCOSE LEVELS CAUSING consistent medical monitoring The same advice can help those INSULIN RESISTANCE HYPERGLYCEMIA. MORE This releases fatty acids called ketones into the RISE, CAUSING are necessary. No dietary CELLS RESPOND LESS TO who have already been diagnosed INSULIN IS RELEASED, HYPERGLYCEMIA OR, body. These can make the blood acidic, resulting DAMAGING THE interventions can prevent or INSULIN. GLUCOSE IS with type 2 diabetes to improve IN SOME CASES, UNABLE TO ENTER THE in a life-threatening complication known as DKA. PANCREAS OVER reverse it, although diet plays a their symptoms and even reverse CELL, CAUSING TIME diabetic ketoacidosis (DKA). role in managing the condition. the condition. HYPERGLYCEMIA Small intestine Insulin enables cells to access glucose Pancreas Insulin "unlocks" the cells, enabling glucose to enter so the cell can use it as a source Insulin of energy. When a person has diabetes, this system can GLUCOSE BECOMES INSULIN IS INSULIN AND GLUCOSE INSULIN GLUCOSE break down at various stages. AVAILABLE RELEASED LEVELS RISE OPENS CELL ENTERS CELLS KEY Glucose is released In response to a rise Insulin and glucose Insulin binds to a cell wall Glucose enters the cell and is used from carbohydrates that are in blood glucose levels, the are now in the bloodstream in and effectively opens the cell, for energy in the cell respiration Glucose broken down in the small intestine. pancreas produces and relatively appropriate quantities, like a key in a lock, to allow process, or turned into glycogen (see Insulin It enters the bloodstream releases the hormone insulin ready for the body’s cells to use glucose to enter the cell page 30) to be stored for later use UUSS__117700--117711__ddiiaabbeetteess..iinndddd 117700 2244//0088//22002211 1166::2244 UUSS__117700--117711__ddiiaabbeetteess..iinndddd 117711 2244//0088//22002211 1166::2244
172 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 173 CAN DIET HELP PREVENT OR MANAGE TYPE 2 DIABETES? Yes! A healthy diet in conjunction with regular exercise and weight management drastically reduces the chances of developing type 2 diabetes and can help manage—and even reverse—the condition if it has already developed. You are at a high risk of developing type 2 diabetes could reestablish the condition, and permanent (prediabetes) if your blood sugar levels are damage to the pancreas may also have been done.) consistently high and you experience symptoms of hyperglycemia (see page 170). While not everyone DIETARY MEASURES with prediabetes goes on to develop diabetes, a high There’s no one-size-fits-all approach to getting your percentage do. diet right, but the previous advice to avoid sugar Certain dietary choices can help prevent type 2 altogether and cut carbs is no longer recommended. diabetes if you are prediabetic, but diet alone will People with prediabetes or type 2 diabetes should not do it. You need a healthy lifestyle overall. People enjoy all the food groups and a balanced healthy with a genetic risk factor (see page 171) with more diet, like the Mediterranean diet (see pages 36–39). sedentary lives are more likely to develop type 2 Include plenty of fiber (see pages 18–19) and opt for diabetes than those who exercise regularly. whole grains to help keep blood glucose levels steady. The same dietary choices can help “reverse” type Even small changes can make a difference, such as 2 diabetes. This term is used to describe a significant consuming whole fruit rather than fruit juice. long-term improvement in insulin sensitivity (see Portion control, to avoid overeating, is important page 170) in people with type 2 diabetes. Those who for preventing and controlling type 2 diabetes. can get their HbA1c below 42 mmol/mol (6 percent) Mindful eating (see pages 206–207) can help make without taking diabetes medication are said to have you stop eating on autopilot and become more aware reversed or resolved their diabetes or brought it of your eating patterns. Take a moment to assess how into remission. (This is not the same as completely you feel before you reach for a food item. Are you eliminating it, as unwise dietary and lifestyle choices doing so out of habit, or boredom, or are you actually TESTING BLOOD SUGAR LEVELS < 42 42– 47 48 > To diagnose diabetes, levels of gylcated hemoglobin (Hb1Ac) are tested over a period mnol/mol mnol/mol mnol/mol of 2–3 months. When the body is unable to < > use glucose properly (see pages 170–171), unused 6% 6-6.4% 6.5% glucose in the bloodstream sticks to red blood cells (hemoglobin)—they become glycated. High Hb1Ac levels indicates prediabetes or diabetes. As blood cells are renewed every 2–3 months, testing HEALTHY LEVELS PREDIABETES DIABETES across this period reveals an average reading. UUSS__117722--117733__ddiiaabbeetteess..iinndddd 117722 2244//0088//22002211 1166::2244
117722 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 117733 CAN DIET HELP PREVENT OR GRAINS MANAGE TYPE 2 DIABETES? FIBER WHOLE GRAIN CEREALS (SEE PAGE 45) SLOWS DOWN Yes! A healthy diet in conjunction with regular exercise and weight management ARE GOOD SOURCES DIGESTIONS, CAUSING OF FIBER drastically reduces the chances of developing type 2 diabetes and can help GLUCOSE TO BE RELEASED SLOWLY manage—and even reverse—the condition if it has already developed. AND STEADILY You are at a high risk of developing type 2 diabetes could reestablish the condition, and permanent (prediabetes) if your blood sugar levels are damage to the pancreas may also have been done.) consistently high and you experience symptoms of hyperglycemia (see page 170). While not everyone DIETARY MEASURES with prediabetes goes on to develop diabetes, a high There’s no one-size-fits-all approach to getting your Dietary fiber Research percentage do. diet right, but the previous advice to avoid sugar shows that increasing fiber Certain dietary choices can help prevent type 2 altogether and cut carbs is no longer recommended. helps improve insulin sensitivity (see page 170) in diabetes if you are prediabetic, but diet alone will People with prediabetes or type 2 diabetes should people with type 2 diabetes. BREAD not do it. You need a healthy lifestyle overall. People enjoy all the food groups and a balanced healthy YOU CAN STILL ENJOY with a genetic risk factor (see page 171) with more diet, like the Mediterranean diet (see pages 36–39). BREAD ON A LOW-CARB DIET. STICK TO WHOLE sedentary lives are more likely to develop type 2 Include plenty of fiber (see pages 18–19) and opt for GRAIN BREAD diabetes than those who exercise regularly. whole grains to help keep blood glucose levels steady. AND SENSIBLE The same dietary choices can help “reverse” type Even small changes can make a difference, such as PORTIONS 2 diabetes. This term is used to describe a significant consuming whole fruit rather than fruit juice. long-term improvement in insulin sensitivity (see Portion control, to avoid overeating, is important page 170) in people with type 2 diabetes. Those who for preventing and controlling type 2 diabetes. can get their HbA1c below 42 mmol/mol (6 percent) Mindful eating (see pages 206–207) can help make without taking diabetes medication are said to have you stop eating on autopilot and become more aware thirsty rather than hungry? Questions like a macronutrient is removed from the diet in 30g reversed or resolved their diabetes or brought it of your eating patterns. Take a moment to assess how these can help you identify unhelpful habits. this way, the knock-on effects can be dangerous into remission. (This is not the same as completely you feel before you reach for a food item. Are you (see pages 110–111). For instance, carbs contain eliminating it, as unwise dietary and lifestyle choices doing so out of habit, or boredom, or are you actually LOW-CARB DIET fiber, which helps control blood sugar levels and FIBER One of the few cases in which a low-carb diet keeps the gut microbiome happy (see pages PER DAY is recommended is in the case of type 2 diabetes. 48–53). Suddenly reducing carbs can result in Research indicates that following a low-carb constipation (see page 155). More research is HELPS KEEP BLOOD TESTING BLOOD SUGAR LEVELS diet is an effective way to reverse the condition. needed to identify the best dietary patterns on GLUCOSE A trial showed that consuming 50–130g carbs a low-carb diet to maintain normal blood sugar STEADY per day helped manage weight and improve levels and keep fiber intake up. EXAMPLES: < 42 42– 47 48 > To diagnose diabetes, levels of gylcated blood sugar levels and cardiovascular risk in If you are on certain meds (including insulin hemoglobin (Hb1Ac) are tested over a period people with type 2 diabetes who were tested or gliclazide), there is a possible risk with a WHOLE GRAINS mnol/mol mnol/mol mnol/mol of 2–3 months. When the body is unable to LEGUMES < > use glucose properly (see pages 170–171), unused for up to 12 months. low-carb diet of hypoglycemia or, in rare cases, 6% 6-6.4% 6.5% glucose in the bloodstream sticks to red blood FRUITS cells (hemoglobin)—they become glycated. High If you have type 2 diabetes and are considering ketoacidosis (see page 170). Speak to your doctor NUTS Hb1Ac levels indicates prediabetes or diabetes. As a low-carb diet, get the support of a dietitian to to help you manage risks and adjust medication SEEDS blood cells are renewed every 2–3 months, testing VEGETABLES HEALTHY LEVELS PREDIABETES DIABETES across this period reveals an average reading. ensure your diet is nutritionally adequate. When as necessary. UUSS__117722--117733__ddiiaabbeetteess..iinndddd 117722 2244//0088//22002211 1166::2244 UUSS__117722--117733__ddiiaabbeetteess..iinndddd 117733 2244//0088//22002211 1166::2244
174 CAN WE EAT FOR BETTER HEALTH OUTCOMES? 175 DO SOME FOODS CAUSE OR PREVENT CANCER? Despite what you might have read on the internet, no one food or dietary pattern directly causes—or cures—cancer, so there’s no need to be overly restrictive with your diet. Food is not medicine, but it is essential to overall health and happiness. Good nutrition during and beyond cancer treatment and lungs and kidneys maintain blood pH behind can help maintain a healthy weight, retain muscle the scenes. There is no link at all between eating mass and strength, and decrease overall side effects. acidic foods and cancer. If you have cancer, please speak to your doctor or dietitian before changing diet; the internet is full Juicing Associated with the alkaline diet myth is of dangerous advice, and it is best to seek support a belief that consuming juices derived from alkaline from credentialed health professionals to be safe. ingredients can purge the body of acid and so cure cancer. Not only is this treatment highly unethical, CANCER FOOD MYTHS it is incredibly dangerous and can be life threatening Here are some of the top food myths around (see also detoxing, pages 112–113). By only juicing, cancer that you should be extremely wary of: you are depriving your body of essential nutrients vital for recovery, repair, and overall health, such as Acidic foods You may have read that eliminating protein, fiber, calcium, and healthy fats. acidic foods and eating only alkaline foods can cure cancer. The “alkaline diet” is based on a notion that Soy Fear of eating soy, especially with hormone- the food we eat has the potential to change how acid positive breast cancers, stems from the idea that or alkaline our blood is, but this is scientifically not phytoestrogen compounds in soybeans have true (see opposite). Our bodies are tightly regulated, estrogen-like properties. However, plant-based estrogens are chemically different from human. In fact, research on consumption of soy foods and cancer (though limited), suggests that eating whole Could supplements help? soybean products like tofu, tempeh, edamame, soy IN MOST CASES, SUPPLEMENTS ARE NOT milk, or similar may actually have a positive impact AS EFFECTIVE AS A BALANCED DIET. on overall mortality and prevention of breast cancer. Vitamin D supplements can help to support the immune system, especially if access to sunshine is limited. You should continue to take supplements Fasting Some animal studies have found fasting needed for dietary requirements, such as vitamin may have the potential to enhance chemotherapy. B12 for vegans, which helps keep blood cells healthy. There have been a few human studies, but not If appetite is low or you’re experiencing diarrhea and/or vomiting, and it’s difficult to eat, nutrient enough to warrant suggesting fasting as an aid to supplementation can be arranged often via liquids. cancer treatment. Fasting can carry many risks, Always check with your oncologist because some especially if you’re diabetic, have a history of an supplements may interfere with cancer treatments. eating disorder, a low BMI, or you have lost more than 10 percent of weight in the preceding year. UUSS__117744--117755__ccaanncceerr..iinndddd 117744 2244//0088//22002211 1166::2244
117744 CCAANN WWEE EEAATT FFOORR BBEETTTTEERR HHEEAALLTTHH OOUUTTCCOOMMEESS?? 117755 DO SOME FOODS CAUSE Busting the ‘‘alkaline diet’’ myth Brain controls the removal OR PREVENT CANCER? The ‘‘alkaline diet’’ is based on a mistaken belief of acidic carbon dioxide by how quickly and deeply that the acidity or alkalinity of blood can greatly you breathe fluctuate and that what we eat can impact on the Despite what you might have read on the internet, no one food or dietary pattern pH of blood. In reality, the body keeps blood pH directly causes—or cures—cancer, so there’s no need to be overly restrictive with within a tight range, and the mechanisms for your diet. Food is not medicine, but it is essential to overall health and happiness. doing so have nothing to do with digestion. Good nutrition during and beyond cancer treatment and lungs and kidneys maintain blood pH behind can help maintain a healthy weight, retain muscle the scenes. There is no link at all between eating red blood cells mass and strength, and decrease overall side effects. acidic foods and cancer. plasma If you have cancer, please speak to your doctor or white blood cells dietitian before changing diet; the internet is full Juicing Associated with the alkaline diet myth is of dangerous advice, and it is best to seek support a belief that consuming juices derived from alkaline Blood is maintained at a slightly alkaline pH from credentialed health professionals to be safe. ingredients can purge the body of acid and so cure 7.36– 7.44 through a system cancer. Not only is this treatment highly unethical, of acid-base homeostasis CANCER FOOD MYTHS it is incredibly dangerous and can be life threatening Here are some of the top food myths around (see also detoxing, pages 112–113). By only juicing, cancer that you should be extremely wary of: you are depriving your body of essential nutrients Lungs expel carbon dioxide, an acidic vital for recovery, repair, and overall health, such as waste product from the metabolism of nutrients and oxygen, carried by Acidic foods You may have read that eliminating protein, fiber, calcium, and healthy fats. blood from cells to the lungs acidic foods and eating only alkaline foods can cure cancer. The “alkaline diet” is based on a notion that Soy Fear of eating soy, especially with hormone- Stomach environment can be pH 2–3.5 due to hydrochloric acid for the food we eat has the potential to change how acid positive breast cancers, stems from the idea that breaking down food, but this does or alkaline our blood is, but this is scientifically not phytoestrogen compounds in soybeans have not impact acid-base homeostasis true (see opposite). Our bodies are tightly regulated, estrogen-like properties. However, plant-based Kidneys excrete acidity, hence urine can estrogens are chemically different from human. be more acidic, but these acidic states do In fact, research on consumption of soy foods and not get into our brain, blood, or muscles cancer (though limited), suggests that eating whole Could supplements help? soybean products like tofu, tempeh, edamame, soy IN MOST CASES, SUPPLEMENTS ARE NOT milk, or similar may actually have a positive impact AS EFFECTIVE AS A BALANCED DIET. normal cells on overall mortality and prevention of breast cancer. Vitamin D supplements can help to support the cancer cells immune system, especially if access to sunshine is limited. You should continue to take supplements Fasting Some animal studies have found fasting needed for dietary requirements, such as vitamin may have the potential to enhance chemotherapy. B12 for vegans, which helps keep blood cells healthy. There have been a few human studies, but not If appetite is low or you’re experiencing diarrhea and/or vomiting, and it’s difficult to eat, nutrient enough to warrant suggesting fasting as an aid to supplementation can be arranged often via liquids. cancer treatment. Fasting can carry many risks, Always check with your oncologist because some especially if you’re diabetic, have a history of an Cancer cells create acid environment. supplements may interfere with cancer treatments. Growth rate increases in an acid eating disorder, a low BMI, or you have lost more environment, but the cancer cells than 10 percent of weight in the preceding year. themselves create the acid. UUSS__117744--117755__ccaanncceerr..iinndddd 117744 2244//0088//22002211 1166::2244 UUSS__117744--117755__ccaanncceerr..iinndddd 117755 2244//0088//22002211 1166::2244
UUSS__117766--117777__cchhaapptteerr__66..iinndddd 117766 2244//0088//22002211 1166::2244
HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? Can Diet Improve Fertility? 178 Will My Child Develop Food Allergies? 188 Why Is Nutrition So Important Should I Change My Nutrition After Birth? 189 During Pregnancy? 180 How Can I Give My Child the Best What Foods Should I Avoid Nutritional Start? 190 During Pregnancy? 182 What Are the Best Strategies for Weaning? 192 Why Do Women Get Strange Food Cravings How Do I Prevent My Child from Becoming in Pregnancy? 184 a Picky Eater? 194 What Should I Drink During Pregnancy? 185 How Can I Help My Toddler Eat Well? 196 Do a Child’s Food Preferences Develop What Should I Consider if Raising a in the Womb? 186 Child on a Vegan or Vegetarian Diet? 198 UUSS__117766--117777__cchhaapptteerr__66..iinndddd 117766 2244//0088//22002211 1166::2244 UUSS__117766--117777__cchhaapptteerr__66..iinndddd 117777 2255//0088//22002211 1199::1155
178 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 179 CAN DIET IMPROVE FERTILITY? Diet can improve fertility in both men and women. What you eat affects the quality of sperm and eggs and regulates hormones, including those that facilitate pregnancy. But other lifestyle factors are also crucial. Getting pregnant isn’t as simple as it sounds. to irregular cycles and missed ovulation. At the Multiple factors can affect fertility, including stress low end, the body may shut down the reproductive or lack of exercise or sleep. Along with a good diet, system to preserve fuel for essential body processes. people wanting to maximize their fertility need a lot In men, obesity affects the molecular and physical of fresh air, exercise, and rest. structure of sperm and is linked to reduced fertility. If you smoke, stop! It is linked to reduced fertility in both women and men. Also, avoid or reduce HOW TO EAT caffeine (stick to 1–2 cups per day at most), and A varied and balanced diet promotes fertility. avoid alcohol altogether. Keep in mind the two Qs—quality and quantity. Exercise is important not just because it promotes Focus on nutritional content and the portion that’s physical vigor and mental health but also because it right for you, rather than the number of calories. helps maintain a healthy body weight, which could I am an advocate of the Mediterranean diet improve fertility. (see pages 36–39)—minus the red wine for women. In women, being very low or high in weight can Research indicates that women with this pattern impact fertility. At the higher end of the spectrum, of eating have a 66 percent lower risk of excess fat increases estrogen levels, which could lead experiencing infertility. Women should supplement their diet with folic acid (see left) and vitamin D during the winter months (or all year round if sun exposure is limited). Folate and folic acid Take 10μg daily from September to March. IT’S IMPORTANT FOR WOMEN TO CONSUME Men can improve sperm quality by including DIETARY FOLATE AND ALSO SUPPLEMENT DURING meat, shellfish, nuts, and whole grains in their diet PRECONCEPTION AND EARLY PREGNANCY. (the Mediterranean diet incorporates these). Bear in Folate (vitamin B9) improves the quality and maturation of eggs, helps the body make healthy mind that a high intake of processed and red meat by blood cells, and enables an embryo’s brain, skull, men has been shown to reduce rates of conception. and spinal cord to develop properly (avoiding Certain nutrients are known to influence fertility neural tube defects such as spina bifida). Ensure in men. The body requires selenium (found in Brazil your diet is rich in folate, which is found in dark green leafy veggies like broccoli and spinach and in nuts, fish, meat, and eggs) to make healthy sperm. many legumes, including chickpeas. Folic acid is Low levels of zinc have been linked to reduced the synthetic version. If planning to conceive, build testosterone levels, so it’s important to keep zinc up stores by taking a 400mcg dose daily three months before you begin trying. Continue levels topped off, preferably through diet. Omega-3 supplementing until week 12 of pregnancy. fatty acids, found in oily fish, help produce prostaglandins, compounds that are important for making sperm. UUSS__117788--117799__ccoonncceeppttiioonn__ssppeerrmm..iinndddd 117788 2244//0088//22002211 1166::2244
117788 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 117799 CAN DIET IMPROVE FERTILITY? Diet can improve fertility in both men and women. What you eat affects the quality of sperm and eggs and regulates hormones, including those that facilitate pregnancy. But other lifestyle factors are also crucial. CRAB CONTAINS ZINC, OMEGA-3 FATTY ACIDS, Getting pregnant isn’t as simple as it sounds. to irregular cycles and missed ovulation. At the VITAMINS B9 AND B12, IRON, SELENIUM, AND Multiple factors can affect fertility, including stress low end, the body may shut down the reproductive PROTEIN or lack of exercise or sleep. Along with a good diet, system to preserve fuel for essential body processes. people wanting to maximize their fertility need a lot In men, obesity affects the molecular and physical of fresh air, exercise, and rest. structure of sperm and is linked to reduced fertility. If you smoke, stop! It is linked to reduced fertility in both women and men. Also, avoid or reduce HOW TO EAT caffeine (stick to 1–2 cups per day at most), and A varied and balanced diet promotes fertility. avoid alcohol altogether. Keep in mind the two Qs—quality and quantity. Exercise is important not just because it promotes Focus on nutritional content and the portion that’s physical vigor and mental health but also because it right for you, rather than the number of calories. helps maintain a healthy body weight, which could I am an advocate of the Mediterranean diet improve fertility. (see pages 36–39)—minus the red wine for women. In women, being very low or high in weight can Research indicates that women with this pattern impact fertility. At the higher end of the spectrum, of eating have a 66 percent lower risk of excess fat increases estrogen levels, which could lead experiencing infertility. Women should supplement their diet with folic acid (see left) and vitamin D during the winter months (or all year round if sun exposure is limited). Folate and folic acid Take 10μg daily from September to March. IT’S IMPORTANT FOR WOMEN TO CONSUME Men can improve sperm quality by including DIETARY FOLATE AND ALSO SUPPLEMENT DURING meat, shellfish, nuts, and whole grains in their diet PRECONCEPTION AND EARLY PREGNANCY. (the Mediterranean diet incorporates these). Bear in Folate (vitamin B9) improves the quality and maturation of eggs, helps the body make healthy mind that a high intake of processed and red meat by blood cells, and enables an embryo’s brain, skull, men has been shown to reduce rates of conception. and spinal cord to develop properly (avoiding Certain nutrients are known to influence fertility neural tube defects such as spina bifida). Ensure in men. The body requires selenium (found in Brazil your diet is rich in folate, which is found in dark green leafy veggies like broccoli and spinach and in nuts, fish, meat, and eggs) to make healthy sperm. many legumes, including chickpeas. Folic acid is Low levels of zinc have been linked to reduced the synthetic version. If planning to conceive, build testosterone levels, so it’s important to keep zinc up stores by taking a 400mcg dose daily three months before you begin trying. Continue levels topped off, preferably through diet. Omega-3 Shellfish like crab contain zinc, which is vital supplementing until week 12 of pregnancy. fatty acids, found in oily fish, help produce for DNA repair and function and improves sperm quality. Low levels are associated with prostaglandins, compounds that are important reduce levels of testosterone in men. for making sperm. UUSS__117788--117799__ccoonncceeppttiioonn__ssppeerrmm..iinndddd 117788 2244//0088//22002211 1166::2244 UUSS__117788--117799__ccoonncceeppttiioonn__ssppeerrmm..iinndddd 117799 2244//0088//22002211 1166::2244
180 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 181 WHY IS NUTRITION SO IMPORTANT DURING PREGNANCY? Leading child health experts worldwide agree that care given from conception to a child’s second birthday—the first 1,000 days—has more influence on a child’s future than any other time in their life, and this includes what the mother’s diet looks like. A mother’s diet and her nutrient stores are the only A growing body of research suggests that diseases source of nutrition for a developing baby, so it is later in life such as diabetes, hypertension, and critical that women get the health care and nutritious stroke have their origins in the womb—and that food they need before and during pregnancy. prenatal nutrition plays an important role in whether a child becomes susceptible to these and other CRUCIAL TRIMESTERS illnesses later in life. During the three trimesters of pregnancy, a woman’s But pregnancy can be tough: aversions to food, diet, weight fluctuations, physical and mental well- no appetite, a large appetite, reflux, sickness—you being, environment, and lifestyle habits can have a name it; most women will have some kind of food huge impact on her child’s future health. These challenge throughout those nine months when the factors influence how a child’s metabolism, immune baby is growing. The good news is that as long as system, and organ function begin to develop, not you avoid certain risky foods and drinks (see to mention the outcome of whether a child is born pages 182–185), enjoy a generally healthy diet and prematurely or at a low birth weight, which may lifestyle, and pay attention to particular nutrients have a lasting impact on a child’s health well into that are key to a baby’s development—especially adulthood. The vitamins, nutrients, and calorie their brain (see below and right)—you should have intake needed vary depending on the trimester. little to worry about. CHOLINE VITAMIN D Building the brain The following key FOOD SOURCES FOOD SOURCES nutrients have been studied and linked EGGS | LEAN MEATS | POULTRY | CRUCIFEROUS Limited but include oily fish, egg yolks, meat and offal, fortified foods, and mushrooms VEGETABLES (BRASSICAS) | NUTS | LEGUMES with development grown in sunlight (see pages 138–139). of the baby’s brain in the womb. Before SHOULD I SUPPLEMENT? SHOULD I SUPPLEMENT? taking supplements, Supplementing with twice the recommended If you get little or no sun exposure, take a daily amount of choline (930mg/day) in the third 10mcg supplement all year. Deficiency in speak to a health trimester of pregnancy may improve speed pregnancy has been linked to an increased professional. of processing in infants; especially risk of a child developing ADHD and a important for plant-based eaters. reduction in IQ and language abilities. UUSS__118800--118811__pprreeggnnaannccyy..iinndddd 118800 2244//0088//22002211 1166::2244
118800 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 118811 WHY IS NUTRITION SO IMPORTANT DURING VITAMIN D FOUR LARGE COOKED PREGNANCY? SHRIMP CONTAIN 1 IU 4 WEEKS VITAMIN D, OR 6% DAILY At the 4th week of Leading child health experts worldwide agree that care given from conception to a VALUE (DV) pregnancy, the brain has an estimated 10,000 cells child’s second birthday—the first 1,000 days—has more influence on a child’s future than any other time in their life, and this includes what the mother’s diet looks like. A mother’s diet and her nutrient stores are the only A growing body of research suggests that diseases source of nutrition for a developing baby, so it is later in life such as diabetes, hypertension, and critical that women get the health care and nutritious stroke have their origins in the womb—and that food they need before and during pregnancy. prenatal nutrition plays an important role in whether 24 WEEKS a child becomes susceptible to these and other By the 24th week, the CRUCIAL TRIMESTERS illnesses later in life. child’s brain contains 10 billion cells During the three trimesters of pregnancy, a woman’s But pregnancy can be tough: aversions to food, diet, weight fluctuations, physical and mental well- no appetite, a large appetite, reflux, sickness—you KEY being, environment, and lifestyle habits can have a name it; most women will have some kind of food huge impact on her child’s future health. These challenge throughout those nine months when the 1st trimester (0–13 weeks) factors influence how a child’s metabolism, immune baby is growing. The good news is that as long as 2nd trimester (14–26 weeks) system, and organ function begin to develop, not you avoid certain risky foods and drinks (see 3rd trimester (27–40 weeks) to mention the outcome of whether a child is born pages 182–185), enjoy a generally healthy diet and OMEGA-3 prematurely or at a low birth weight, which may lifestyle, and pay attention to particular nutrients Brain development A child’s SEAFOOD AND OILY have a lasting impact on a child’s health well into that are key to a baby’s development—especially brain begins to grow very early on FISH ARE HIGH IN THE Brain food Omega-3 fatty acids in pregnancy and develops at an adulthood. The vitamins, nutrients, and calorie their brain (see below and right)—you should have HEALTHY FATTY ACIDS and vitamin D have been identified as astonishing speed. The nutrition that intake needed vary depending on the trimester. little to worry about. EPA AND DHA some of the most important nutrients a baby gets from its mother through in supporting brain development. her diet is the fuel that drives much Seafood is a key food source for both. of this incredible transformation. CHOLINE VITAMIN D IRON OMEGA-3 DHA Building the brain The following key FOOD SOURCES FOOD SOURCES FOOD SOURCES FOOD SOURCES nutrients have been studied and linked EGGS | LEAN MEATS | POULTRY | CRUCIFEROUS Limited but include oily fish, egg yolks, meat SHELLFISH | BROCCOLI | RED MEAT | TOFU FISH AND SEAFOOD | SEAWEED AND ALGAE and offal, fortified foods, and mushrooms VEGETABLES (BRASSICAS) | NUTS | LEGUMES NUTS | BEANS | DRIED FRUIT with development grown in sunlight (see pages 138–139). of the baby’s brain in the womb. Before SHOULD I SUPPLEMENT? SHOULD I SUPPLEMENT? SHOULD I SUPPLEMENT? SHOULD I SUPPLEMENT? taking supplements, Supplementing with twice the recommended If you get little or no sun exposure, take a daily Up to 50 percent of pregnant women are iron A fetus’s needs increase sharply in the third amount of choline (930mg/day) in the third 10mcg supplement all year. Deficiency in deficient, which may cause irreversible neural trimester since the brain consists of fatty acids. speak to a health trimester of pregnancy may improve speed pregnancy has been linked to an increased issues in the fetus; those with gestational Some studies suggest DHA supplements could professional. of processing in infants; especially risk of a child developing ADHD and a diabetes are more at risk. Iron in the third give babies better memory, attention, and verbal important for plant-based eaters. reduction in IQ and language abilities. trimester is especially important. skills, and a lower risk of neurological disorders. UUSS__118800--118811__pprreeggnnaannccyy..iinndddd 118800 2244//0088//22002211 1166::2244 UUSS__118800--118811__pprreeggnnaannccyy..iinndddd 118811 2244//0088//22002211 1166::2244
182 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 183 WHAT FOODS SHOULD I AVOID DURING PREGNANCY? Pregnancy can be a very exciting but also anxious time for expectant moms, with many conflicting messages on the internet or from others on what you should and shouldn’t be eating. So what is safe, and what is not? Most foods that pregnant women are advised to DAIRY AND EGGS avoid carry a risk of food poisoning from harmful Unpasteurized milk and dairy: Most milk and bacteria when undercooked or prepared in a certain dairy products, including cheese, sold in the US way. The current US guidelines are as follows: are pasteurized and safe to consume—but avoid unpasteurized (raw) milk and dairy. If you only have PREPARED FOOD access to unpasteurized milk, boil it before using. Premade deli salads: Don’t buy or eat premade ham salad, chicken salad, or seafood salad, which Raw or undercooked eggs: Cook eggs until may contain Listeria—bacteria that can cause an the yolks and whites are firm. If you are making a infection called listeriosis, which can harm a fetus or casserole or other dish containing eggs, make sure newborn. These items are commonly found in delis. the dish is cooked to a temperature of 160°F. Try to avoid foods that contain raw egg, such as homemade Unwashed packaged salad: It is fine to eat mayonnaise or mousse. Make sure that foods that preprepared, prewashed salad if you keep it in contain raw or lightly cooked eggs are made only the fridge and eat it before the use-by date. with pasteurized eggs. Avoid eating raw sprouts of any kind (including alfalfa, clover, radish, and mung bean). MEAT Raw or undercooked meat: Avoid any Pâté: Avoid all types of pâté, as they may undercooked meat, especially poultry, pork, contain Listeria. sausages, and burgers. Meat should always be SAFE TO EAT Which cheeses can I eat? Some cheeses are safe to eat when pregnant, but not all SOFT, HARD of them, as some contain HARD PASTEURIZED CHEESE GOAT’S CHEESE unpasteurized dairy and are CHEESES CHEESES SPREADS without a white more susceptible to bacterial e.g., Cheddar, Stilton, e.g., mozzarella, (processed) coating on the growth. This breakdown and Parmesan feta, and outside (rind) shows which types are safe ricotta and which are best to avoid. UUSS__118822--118833__aavvooiidd..iinndddd 118822 2255//0088//22002211 1199::1155
118822 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 118833 WHAT FOODS SHOULD I cooked thoroughly and not be pink or bloody, as there is a risk of toxoplasmosis, a parasitic Vitamin A AVOID DURING PREGNANCY? infection that can be harmful to you and your baby. TOO MUCH VITAMIN A CAN BE HARMFUL TO AN UNBORN BABY, It’s safe to eat cold, prepackaged meat such as POTENTIALLY LEADING TO BIRTH DEFECTS AND EVEN MISCARRIAGE. ham. You may prefer to avoid raw cured meats; Pregnancy can be a very exciting but also anxious time for expectant moms, Avoid liver and liver products and multivitamins containing vitamin A or if not, freeze for four days before defrosting fish liver oils. It’s fine to eat foods that naturally contain low levels of with many conflicting messages on the internet or from others on what you or eat them cooked. Avoid game, as it may vitamin A, like carrots, but avoid any that have vitamin A added. Cosmetic should and shouldn’t be eating. So what is safe, and what is not? products, like face cream, that contain vitamin A are safe to use. contain lead shot. FISH Most foods that pregnant women are advised to DAIRY AND EGGS Some fish: Pregnant women should eat avoid carry a risk of food poisoning from harmful Unpasteurized milk and dairy: Most milk and a diverse diet that includes 8-12oz of low- bacteria when undercooked or prepared in a certain dairy products, including cheese, sold in the US mercury fish weekly to help boost omega-3 OILY FISH way. The current US guidelines are as follows: are pasteurized and safe to consume—but avoid consumption. All seafood dishes should be TUNA, SALMON, TROUT, unpasteurized (raw) milk and dairy. If you only have cooked to 145°F. AND MACKEREL HELP PREPARED FOOD access to unpasteurized milk, boil it before using. YOUR BABY DEVELOP Premade deli salads: Don’t buy or eat premade Raw shellfish: Cooked shellfish is safe, BUT SHOULD BE LIMITED TO TWO PORTIONS ham salad, chicken salad, or seafood salad, which Raw or undercooked eggs: Cook eggs until but raw shellfish can cause food poisoning. PER WEEK may contain Listeria—bacteria that can cause an the yolks and whites are firm. If you are making a infection called listeriosis, which can harm a fetus or casserole or other dish containing eggs, make sure newborn. These items are commonly found in delis. the dish is cooked to a temperature of 160°F. Try to avoid foods that contain raw egg, such as homemade Unwashed packaged salad: It is fine to eat mayonnaise or mousse. Make sure that foods that preprepared, prewashed salad if you keep it in contain raw or lightly cooked eggs are made only the fridge and eat it before the use-by date. with pasteurized eggs. Avoid eating raw sprouts of any kind (including RAW EGGS alfalfa, clover, radish, and mung bean). MEAT RAW EGGS ARE BEST Raw or undercooked meat: Avoid any AVOIDED AS THEY COULD CONTAIN SALMONELLA Pâté: Avoid all types of pâté, as they may undercooked meat, especially poultry, pork, BACTERIA, WHICH contain Listeria. sausages, and burgers. Meat should always be CAN CAUSE FOOD POISONING SAFE TO EAT AVOID UNLESS COOKED WWhhiicchh cchheeeesseess ccaann II eeaatt?? Some cheeses are safe to eat when pregnant, but not all SOFT, Avoid these types MOLD HARD SOFT BLUE of them, as some contain HARD PASTEURIZED CHEESE of cheese unless -RIPENED GOAT’S CHEESE cooked until SOFT, CHEESES SOFT unpasteurized dairy and are CHEESES CHEESES SPREADS SOFT CHEESES These contain more susceptible to bacterial e.g., Cheddar, Stilton, e.g., mozzarella, w ci ot ah to inu gt a o nw h thit ee (processed) st ae sa tm hein yg c h anot , UNP CA HS ET EE SU ER SIZED e.g G., oD ra gn oi ns zh o b lalu , e, CG HO EA ET S’S E with a white coating, m mo ar ke in m g o itis et au sr ie e, r growth. This breakdown and Parmesan feta, and outside (rind) cause listeriosis. and Roquefort e.g., Brie, Camembert, for bacteria shows which types are safe ricotta and chèvre to grow and which are best to avoid. UUSS__118822--118833__aavvooiidd..iinndddd 118822 2255//0088//22002211 1199::1155 UUSS__118822--118833__aavvooiidd..iinndddd 118833 2244//0088//22002211 1166::2244
184 WHY DO WOMEN GET STRANGE FOOD CRAVINGS IN PREGNANCY? We’ve all heard stories of the unusual items or food combinations women want to eat while pregnant. In truth, there is very little research on the subject and a lot of old wives’ tales. Some women crave unhealthy items during pregnancy, while the lucky few experience healthier cravings for fruits and vegetables. Others have no SALTY FOODS appetite at all. We don’t know much about why this 33% OF WOMEN happens, but cravings may be caused by hormonal REPORT CRAVING changes, physiological changes, or even the SALTY FOODS WHEN PREGNANT emotional roller coaster of pregnancy. SOUR FOODS CITRUS FLAVORS POTENTIAL CAUSES AND MORE SOUR The nausea and sickness some women experience TASTES ARE CRAVED BY AROUND 10% during pregnancy is incredibly taxing on the body. OF WOMEN Sometimes food cravings can materialize as a coping strategy and a way to manage hormonal fluctuations or difficult moments. There has also been talk of women “needing” certain vitamins SWEET FOODS RESEARCH SUGGESTS THAT MANY WOMEN— 40%—CRAVE SWEET FOODS DURING PREGNANCY UUSS__118844--118855__aavvooiidd__ccrraavviinnggss..iinndddd 118844 2244//0088//22002211 1166::2244
118844 WHY DO WOMEN GET WHAT SHOULD STRANGE FOOD CRAVINGS I DRINK DURING IN PREGNANCY? PREGNANCY? We’ve all heard stories of the unusual items or food combinations and minerals and therefore craving foods Staying hydrated is vital during pregnancy. You’ll that contain them. However, if you have women want to eat while pregnant. In truth, there is very little research need seven to 10 glasses of fluid a day—possibly a craving for less healthy options, it’s on the subject and a lot of old wives’ tales. more if you’re active or the weather is hot—but unlikely that your body is trying to meet a be mindful of what you are drinking. nutritional need. If that were the case, you would be more likely to crave foods such as Some women crave unhealthy items during fish, broccoli, and whole grains, which many Try to get your fluid intake from a variety of sources. pregnancy, while the lucky few experience healthier in the US don’t eat enough of. Remember your limits with each type (see below) cravings for fruits and vegetables. Others have no SALTY FOODS and avoid too many sugary drinks, as flavor preferences appetite at all. We don’t know much about why this BEST PRACTICE can be developed in the womb (see pages 186–187). 33% OF WOMEN happens, but cravings may be caused by hormonal REPORT CRAVING It’s good to be aware of what your body is changes, physiological changes, or even the SALTY FOODS WHEN telling you, but it’s also important not to LIMIT CAFFEINE PREGNANT emotional roller coaster of pregnancy. SOUR FOODS always give in to food cravings, as your diet The National Institutes of Health (NIH) warns that during pregnancy needs to too much caffeine can result in miscarriage or a low birth CITRUS FLAVORS POTENTIAL CAUSES AND MORE SOUR be varied to provide all the weight. The NIH advises pregnant women to drink no The nausea and sickness some women experience TASTES ARE CRAVED BY nutrients your baby needs. more than two mugs of regular coffee, or less than AROUND 10% during pregnancy is incredibly taxing on the body. 200mg of caffeine, per day. Research suggests that the OF WOMEN Sometimes food cravings can materialize as a amount of caffeine in coffee can range from 50mg per coping strategy and a way to manage hormonal cup to more than 300mg, while other foods, such as fluctuations or difficult moments. There has also chocolate, and medications also contain caffeine, so it been talk of women “needing” certain vitamins would be easy to exceed the recommended limit without realizing. Ask about caffeine content before you buy. If unknown, it is best to avoid. Note also that green tea contains up to 100mg caffeine, and some herbal teas may not be fully safe in large amounts. AVOID ALCOHOL SWEET It is safest to avoid alcohol completely during pregnancy; it has been linked to low birth weight, premature birth, FOODS SPICY and miscarriage. It can also affect a baby’s development RESEARCH SUGGESTS FOODS and long-term health. Experts cannot be sure that any THAT MANY WOMEN— 40%—CRAVE SWEET AROUND 17% OF amount of alcohol is safe, but drinking heavily during PREGNANT WOMEN FOODS DURING pregnancy can result in your baby developing fetal CRAVE SPICY PREGNANCY alcohol syndrome (FAS), a serious condition with FOODS symptoms including growth, learning, and behavioral issues. If you are finding it difficult to stop drinking, ask for support from your midwife or OB. UUSS__118844--118855__aavvooiidd__ccrraavviinnggss..iinndddd 118844 2244//0088//22002211 1166::2244 UUSS__118844--118855__aavvooiidd__ccrraavviinnggss..iinndddd 118855 2244//0088//22002211 1166::2244
186 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 187 DO A CHILD’S FOOD PREFERENCES DEVELOP IN THE WOMB? There has been a lot of research into the nutrition a baby receives from its mother inside the womb and through breast milk, and the results are fascinating. Research suggests that babies start to develop preferences for certain flavors in the womb, delivered via PUNGENT GARLIC the amniotic fluid, and then via FLAVOR COMES FROM breastfeeding, with potential THE COMPOUND ALLICIN, implications for lifelong eating habits. WHICH ONLY FORMS WHEN CLOVES ARE It is a tricky area to research ethically CRUSHED. when it comes to a poor diet and the impact on a baby’s future health, but we know that the most researched flavors that are easily available in amniotic fluid and breast milk are garlic, carrot, alcohol, anise, and vanilla. INNATE PREFERENCES Around 16 weeks after conception, fetuses develop pores on their taste buds that allow them to identify basic tastes, and they will swallow more amniotic fluid when it’s sweet and less when it’s bitter. Penchants for salt and umami tastes are also innate. Available data suggests that infants are born “hard-wired” to prefer tastes that signal beneficial nutrients (for example, sweet tastes signal calories) and to reject tastes that signal harmful compounds (for example, bitter tastes signal poison). LEARNED PREFERENCES Most of our food preferences, however, are learned, and a growing body of research shows that this Early exposure Babies can detect particularly strong learning also begins before birth. From as early as flavors such as garlic in 21 weeks old the fetus can detect complex flavors, amniotic fluid and breast milk and can develop preferences accordingly. UUSS__118866--118877__pprreeffeerreenncceess__wwoommbb..iinndddd 118866 2244//0088//22002211 1166::2244
118866 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 118877 DO A CHILD’S FOOD such as garlic and carrot, a few hours after the BEST PRACTICE PREFERENCES DEVELOP mother has eaten those foods. This can result in It makes sense to eat a varied diet when pregnant preferences for these flavors at birth through breast and nursing to enhance the likelihood that your IN THE WOMB? milk and weaning. For example, babies who tasted baby will eat a wider variety of foods, but fear not high concentrations of carrot in utero and in their if this has not been an option for you. There is mother’s milk went on to happily eat more carrots ample opportunity to improve this during the There has been a lot of research into the nutrition a baby receives during weaning. However, this has also been shown weaning stage (see pages 194–195). Exposing your from its mother inside the womb and through breast milk, and to ring true with unhealthy items. The more little baby to a variety of novel foods in infancy will the results are fascinating. ones are exposed in the womb to less healthy foods, reduce the likelihood of them developing a the more desensitized they may become to it when neophobia or aversion to foods later on in life. older, meaning they may eat more cake, chocolate, Research suggests that babies start and chips to get the same reward centers to light up. to develop preferences for certain flavors in the womb, delivered via PUNGENT GARLIC the amniotic fluid, and then via FLAVOR COMES FROM Taste development breastfeeding, with potential THE COMPOUND ALLICIN, in unborn babies implications for lifelong eating habits. WHICH ONLY FORMS It is a tricky area to research ethically WHEN CLOVES ARE Olfactory bulb and CRUSHED. Taste and flavor nerves detect all smells when it comes to a poor diet and the perception are central impact on a baby’s future health, but we to the development of Orthonasal olfaction senses smells rising know that the most researched flavors that are food preferences. Both from the nasal cavity easily available in amniotic fluid and breast milk begin in the womb due to changes in the are garlic, carrot, alcohol, anise, and vanilla. gustatory and olfactory system, beginning in INNATE PREFERENCES the first trimester of Around 16 weeks after conception, fetuses develop pregnancy. Retronasal olfaction senses smells rising pores on their taste buds that allow them to identify from the oral cavity basic tastes, and they will swallow more amniotic Sensor cells in the fluid when it’s sweet and less when it’s bitter. tongue identify tastes Penchants for salt and umami tastes are also innate. Available data suggests that infants are born “hard-wired” to prefer tastes that signal beneficial nutrients (for example, sweet tastes signal calories) and to reject tastes that signal harmful compounds THE FIVE A MYRIAD THE SYNTHESIS (for example, bitter tastes signal poison). BASIC TASTES OF ODORS OF FLAVOR LEARNED PREFERENCES Most of our food preferences, however, are learned, Taste sensations result from Thousands of different Flavor perception then results activation of the gustatory system— odors stimulate the olfactory from the integration of the taste and a growing body of research shows that this Early exposure Babies can taste cells in the mouth, neural bulb and nerves in the nasal and smell sensory systems. detect particularly strong learning also begins before birth. From as early as flavors such as garlic in pathways, and the gustatory cortex cavity to create smell Odors sensed orthonasally and in the brain—and are limited sensations. retronasally combine with tastes 21 weeks old the fetus can detect complex flavors, amniotic fluid and breast to sweet, bitter, sour, salty, detected in the oral cavity to milk and can develop and umami. create flavor sensations. preferences accordingly. UUSS__118866--118877__pprreeffeerreenncceess__wwoommbb..iinndddd 118866 2244//0088//22002211 1166::2244 UUSS__118866--118877__pprreeffeerreenncceess__wwoommbb..iinndddd 118877 2244//0088//22002211 1166::2244
188 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 189 WILL MY CHILD DEVELOP FOOD ALLERGIES? Food allergies affect 32 million Americans, including 5.6 million children under age 18. That’s about 1 in 13 children, or two in every classroom. Importantly, if you have a food allergy, that doesn’t in key nutrients for mother or baby. Some research mean your baby will, but there is a higher chance has suggested that taking omega-3 supplements may of that allergy being present, as the likelihood can reduce the risk, but the results are not conclusive. be down to genetics. If a parent or sibling has a food allergy or eczema or asthma, the risk increases for DURING INFANCY a child to have a food allergy. While there is no one food or diet that can eliminate the risk of an allergy, it is thought that exclusively DURING PREGNANCY breastfeeding for the first six months of the baby’s There is no firm evidence that eliminating certain life can reduce the risk. Some allergies, such as eggs foods from the diet during pregnancy has any and milk, can disappear as a child grows up, but impact on the development of future allergies; it may some, like peanuts, can be lifelong. If you have any even have a detrimental effect, causing deficiencies concerns, speak to your health professional before starting to wean. PEANUT ALLERGIES Should I take probiotics? In the US, peanuts are deemed safe to eat during SOME RESEARCH SUGGESTS THAT TAKING A pregnancy. Government advice used to be to avoid PROBIOTIC SUPPLEMENT IN PREGNANCY CAN REDUCE giving foods containing peanuts to children under THE RISK OF THE BABY DEVELOPING A FOOD ALLERGY. three, but the latest research shows no clear evidence However, more research is needed and there are that this reduces the risk and may in fact increase no current guidelines for pregnant women on which probiotic to choose, so be cautious when taking risk (see below), so peanuts—ground or in a butter any supplements. due to the risk of choking on whole nuts—should not be avoided, unless the child has a known allergy. ATE PEANUTS AVOIDED PEANUTS LEAP peanut study A LEAP study looked at peanut allergies The babies who were assigned The babies who avoided in children, as numbers have risen in the to consume peanuts had a peanuts had a last 10 years. They recruited 640 babies aged 4–11 months with eczema and/or an 3.2% 17.2% egg allergy, as this increases the risk of having a peanut allergy. Results showed chance of developing chance of developing a reduced risk in babies who ate peanuts. a peanut allergy a peanut allergy UUSS__118888--118899__pprreevveenntt__aalllleerrggiieess..iinndddd 118888 2244//0088//22002211 1166::2244
118888 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 118899 WILL MY CHILD DEVELOP SHOULD I CHANGE MY NUTRITION FOOD ALLERGIES? AFTER GIVING BIRTH? Food allergies affect 32 million Americans, including 5.6 million children The period after giving birth is an opportunity for the mother to be looked after under age 18. That’s about 1 in 13 children, or two in every classroom. and to recover. Key to that process is making sure she gets the right nutrition. Importantly, if you have a food allergy, that doesn’t in key nutrients for mother or baby. Some research Keeping up with the demands of feeding while Increased calcium: An extra 550mg per day mean your baby will, but there is a higher chance has suggested that taking omega-3 supplements may sleep-deprived and trying to recover from the birth is required to support milk production and replenish of that allergy being present, as the likelihood can reduce the risk, but the results are not conclusive. is hard. For many women, it can take several months calcium stores. Include calcium-rich foods in your be down to genetics. If a parent or sibling has a food to adjust. To help restore health, replenish nutrient diet, e.g., milk, cheese, yogurts. If your diet is allergy or eczema or asthma, the risk increases for DURING INFANCY stores, and, if breastfeeding, support lactation, new plant-based (see pages 116–133), choose calcium- a child to have a food allergy. While there is no one food or diet that can eliminate mothers should eat nourishing foods that meet their enriched options. the risk of an allergy, it is thought that exclusively body’s changing needs. DURING PREGNANCY breastfeeding for the first six months of the baby’s Increased zinc: An extra 4mg per day is There is no firm evidence that eliminating certain life can reduce the risk. Some allergies, such as eggs POSTPARTUM NUTRITION required if breastfeeding. Zinc supports the immune foods from the diet during pregnancy has any and milk, can disappear as a child grows up, but New moms should eat and drink regularly, following system and is found in beef, fish, beans, tofu, nuts, impact on the development of future allergies; it may some, like peanuts, can be lifelong. If you have any a healthy, balanced diet. This should include lots of and seeds. even have a detrimental effect, causing deficiencies concerns, speak to your health professional before fruits and vegetables, as well as proteins to help the starting to wean. body recover, fiber-rich carbohydrates for energy, Omega-3s: Evidence suggests that a diet rich and iron-rich foods to help create new blood cells— in omega-3s results in omega-3-rich breast milk, PEANUT ALLERGIES especially if the mother has anemia or lost a lot of which supports brain development in babies. Should I take probiotics? In the US, peanuts are deemed safe to eat during blood during the birth. They will need on average Sources include some nuts and seeds and oily SOME RESEARCH SUGGESTS THAT TAKING A pregnancy. Government advice used to be to avoid between 1,800 and 2,200 calories a day, and 500 fish (see pages 184–185). PROBIOTIC SUPPLEMENT IN PREGNANCY CAN REDUCE giving foods containing peanuts to children under calories more than this if breastfeeding. THE RISK OF THE BABY DEVELOPING A FOOD ALLERGY. three, but the latest research shows no clear evidence Increased fluids: You will need to drink more However, more research is needed and there are that this reduces the risk and may in fact increase NUTRIENTS FOR BREASTFEEDING fluids, remembering to do so before a headache no current guidelines for pregnant women on which probiotic to choose, so be cautious when taking risk (see below), so peanuts—ground or in a butter Breastfeeding moms should limit their intake of begins. The amount needed will vary, but the any supplements. due to the risk of choking on whole nuts—should alcohol and caffeine, as these can pass into breast Institute of Medicine recommends 14–16 cups not be avoided, unless the child has a known allergy. milk and affect the baby’s digestion, sleep, and of water per day. feeding. They should also be aware of the following additional nutritional needs during this period: ATE PEANUTS AVOIDED PEANUTS LEAP peanut study A LEAP study looked at peanut allergies The babies who were assigned The babies who avoided in children, as numbers have risen in the to consume peanuts had a peanuts had a last 10 years. They recruited 640 babies aged 4–11 months with eczema and/or an 3.2% 17.2% egg allergy, as this increases the risk of having a peanut allergy. Results showed chance of developing chance of developing a reduced risk in babies who ate peanuts. a peanut allergy a peanut allergy UUSS__118888--118899__pprreevveenntt__aalllleerrggiieess..iinndddd 118888 2244//0088//22002211 1166::2244 UUSS__118888--118899__pprreevveenntt__aalllleerrggiieess..iinndddd 118899 2244//0088//22002211 1166::2244
190 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 191 HOW CAN I GIVE MY CHILD THE BEST NUTRITIONAL START? Once born, babies reach a range of milestones as they continue to grow and develop, but it’s important to provide them with the right nutrition to do so from day one. Exclusive breastfeeding for the first six months of a proteins, as well as growth factors, antibodies, and baby’s life is recommended, as the mother’s milk hormones vital to a baby’s development that cannot contains all the nutrients her infant needs. But if be replicated in infant formula. Breast milk and the this isn’t possible, formula—which mimics the process of feeding is also critical to the seeding of a nutrition of breast milk—should be used instead. baby’s gut microbiome (see pages 140–141). Thanks It’s important to note that however you decide to to the mother-to-child interaction, breastfeeding also feed your baby is your choice. plays an important role in strengthening a baby’s sensory and emotional circuitry, critical for cognitive BREASTFEEDING and socio-emotional development. Research also Breastfeeding is the most nutritious, cost-effective, suggests breastfeeding may lower the mother’s risk and often the most convenient option for feeding a of developing breast cancer, ovarian cancer, baby. Breast milk contains a variety of nutrients and cardiovascular disease, osteoporosis, and obesity. alveoli, meeting production at the nipple swelling alveoli of milk DURING BREAST PREGNANCY AT BIRTH FEEDING How breast milk is made Hormonal changes caused by the different stages Estrogen and Prolactin hormone, Oxytocin hormone is progesterone levels released once the released when the baby of pregnancy and birth increase during pregnancy, baby is born, triggers suckles, causing the muscles kick-start the production stimulating the alveoli the alveoli cells to around the alveoli cells to of breast milk. cells and milk ducts in produce milk squeeze the milk out of the the breast to grow milk ducts in the nipple— known as the let-down reflex UUSS__119900--119911__bbaabbyy__nnuuttrriittiioonn..iinndddd 119900 2244//0088//22002211 1166::2244
119900 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 119911 HOW CAN I GIVE MY CHILD THE BEST NUTRITIONAL START? PROTEIN 23 11 Once born, babies reach a range of milestones as they continue to grow and develop, but it’s important to provide them with the right nutrition to do so from day one. IMMUNOGLOBULINS 19 1.14 (ANTIBODIES) FAT 30 46 Exclusive breastfeeding for the first six months of a proteins, as well as growth factors, antibodies, and LACTOSE 57 71 Colostrum and breast milk compared Thick, baby’s life is recommended, as the mother’s milk hormones vital to a baby’s development that cannot yellow colostrum is high contains all the nutrients her infant needs. But if be replicated in infant formula. Breast milk and the in protein and antibodies. Mature breast milk is high this isn’t possible, formula—which mimics the process of feeding is also critical to the seeding of a COLOSTRUM (G/L) BREAST MILK (G/L) in water, fat, and lactose. nutrition of breast milk—should be used instead. baby’s gut microbiome (see pages 140–141). Thanks It’s important to note that however you decide to to the mother-to-child interaction, breastfeeding also feed your baby is your choice. plays an important role in strengthening a baby’s sensory and emotional circuitry, critical for cognitive BREASTFEEDING and socio-emotional development. Research also WHAT IS COLOSTRUM? milk contains diverse gut bacteria, which transfer Breastfeeding is the most nutritious, cost-effective, suggests breastfeeding may lower the mother’s risk The first milk a mother produces in the days after to the baby via breastfeeding and areolar skin and often the most convenient option for feeding a of developing breast cancer, ovarian cancer, giving birth is literally golden—it’s called colostrum contact, seeding a healthy gut microbiome. baby. Breast milk contains a variety of nutrients and cardiovascular disease, osteoporosis, and obesity. and is particularly rich and concentrated to give the baby the best possible start. FORMULA Colostrum is packed with protein, antibodies, and For a variety of reasons, breastfeeding may not be vitamins A, D, and B12, which are all important in an option for you, and there is absolutely no need supporting the baby’s growth and immune system. to feel guilty about that—it does not mean your alveoli, meeting production at the nipple swelling alveoli of milk After a few days, the mother begins to produce baby has to miss out. Formula milks today have “mature” breast milk, which is less concentrated been designed to give all the key nutrition babies and lower in protein and antibodies than colostrum need. Formula comes fortified with essential but higher in fat and lactose (see above). vitamins, so whereas all breastfed babies should be given a daily supplement of vitamin D (400 IU) THE BENEFITS OF BREAST MILK from birth, babies who are given more than 17oz As well as providing essential nutrition from (500ml) of infant formula a day do not need this. protein, lactose, fats, vitamins, and minerals, breast milk boasts numerous other health benefits. CHANGES AT SIX MONTHS Breastfeeding transfers vital antibodies from At six months, a baby’s stores of nutrients such DURING BREAST mother to child to support their immune system, as iron, start to deplete, so guidelines recommend PREGNANCY AT BIRTH FEEDING How breast milk is made reducing the risk of illness and sudden infant that weaning (the introduction of “solid” foods; see death syndrome (SIDS), as well as that of obesity pages 192–193) should start at around this time. Hormonal changes caused by the different stages Estrogen and Prolactin hormone, Oxytocin hormone is and cardiovascular disease in adulthood. The American Academy of Pediatrics recommends of pregnancy and birth progesterone levels released once the released when the baby Many of the body’s hormones, which are vital that exclusively or mostly breastfed babies be given increase during pregnancy, baby is born, triggers suckles, causing the muscles kick-start the production stimulating the alveoli the alveoli cells to around the alveoli cells to for regulating bodily functions, growth, appetite, an iron supplement starting at 4 months of age. of breast milk. cells and milk ducts in produce milk squeeze the milk out of the and weight, are transmitted from mother to baby Talk to your baby’s healthcare provider about the breast to grow milk ducts in the nipple— known as the let-down reflex through breast milk. Studies have shown that breast supplementation. UUSS__119900--119911__bbaabbyy__nnuuttrriittiioonn..iinndddd 119900 2244//0088//22002211 1166::2244 UUSS__119900--119911__bbaabbyy__nnuuttrriittiioonn..iinndddd 119911 2244//0088//22002211 1166::2244
192 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 193 WHAT ARE THE BEST STRATEGIES FOR WEANING? A baby’s first taste experiences can set them up for a great relationship with food. Traditionally, babies are given fruit purées as first foods, but research suggests that offering a variety of flavors encourages an acceptance of variety. There’s nothing wrong with the fruit-purées-first And remember, babies have off days, too. Your baby approach. Babies have an innate preference for sweet won’t want the same amount of food every day. Heat, foods (see page 194). I suggest making weaning all teething, sickness, and tiredness all affect appetite. about experimentation and exposure to lots of foods to help your child become accustomed to as many PURÉES OR FINGER FOODS? flavors as possible. If you start weaning with savory Parents following a baby-led approach to weaning flavors, you enable your child to learn to accept bypass purées and offer finger foods from the start, them and enjoy savory foods before they have a encouraging their baby to feed themselves (once chance to associate solid food very firmly with sweet they can sit upright and have the coordination). flavors (which they are predisposed to, as the flavor Although introducing both purées and finger of milk is sweet). This vegetable-led approach to foods into the diet together has received mixed weaning is by no means the only way to wean, but feedback, it may actually help babies develop it is backed by scientific research that shows it can have a positive influence on vegetable intake. Research suggests that, between the ages of 6 and 12 months, babies are most likely to accept new foods. This gives you the ideal window to introduce BROCCOLI a variety of foods into your baby’s diet. This CONTAINS FIBER, nutritional input will support their development, CALCIUM, FOLATE, AND VITAMINS A and the familiarity with multiple flavors will help AND C set them up with good dietary habits for life. Regardless of the method you choose, repeated exposure is vital, so be patient. It can take more than 10 attempts with the same food for it to become accepted regularly. Just keep offering. Finger foods encourage self-feeding and help babies improve their coordination and become accustomed to new textures. Broccoli and cauliflower “trees” are easy to pick up and have great textures to stimulate your baby’s mouth. UUSS__119922--119933__wweeaanniinngg..iinndddd 119922 2244//0088//22002211 1166::2244
119922 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 119933 WHAT ARE THE BEST coordination and independence and discover what they like to eat best. Some people wrongly believe Bitter flavors STRATEGIES FOR WEANING? that offering a spoon alongside finger foods may INTRODUCING FOODS WITH BITTER FLAVORS confuse a baby and cause them to choke. EARLY ON MAY BE A GOOD IDEA FOR YOUR Finger foods have to be soft enough for babies CHILD’S LONG-TERM HEALTH. A baby’s first taste experiences can set them up for a great relationship with to squash them with their gums and reduce the risk Research suggests that the phytonutrients found food. Traditionally, babies are given fruit purées as first foods, but research suggests in bitter vegetables may help prevent heart disease of choking. Ensure rough skin and hard pieces are that offering a variety of flavors encourages an acceptance of variety. and cancer. Try introducing more bitter veggies, like removed. So if offering zucchini fingers, remove broccoli or Brussels sprouts, alongside less the skin unless it is super soft. With cucumber, intensely flavored veggies like cauliflower, to offer just the seeds at the center. encourage your baby to develop an acceptance of and a liking for their flavors and textures. There’s nothing wrong with the fruit-purées-first And remember, babies have off days, too. Your baby When making purées, steam and then blend the approach. Babies have an innate preference for sweet won’t want the same amount of food every day. Heat, food. The texture of early purées should be watery, foods (see page 194). I suggest making weaning all teething, sickness, and tiredness all affect appetite. like milk, then work up gradually to thicker, lumpier about experimentation and exposure to lots of foods consistencies. Think of this as a gradual transition to help your child become accustomed to as many PURÉES OR FINGER FOODS? from puréed to mashed to minced to chopped. Eat together when you can. Good role modeling can flavors as possible. If you start weaning with savory Parents following a baby-led approach to weaning encourage your baby to eat more and try new foods. flavors, you enable your child to learn to accept bypass purées and offer finger foods from the start, MEALTIME ROUTINES They will even learn by watching you chew and them and enjoy savory foods before they have a encouraging their baby to feed themselves (once Babies thrive on routine. Within reason, try to feed swallow food. chance to associate solid food very firmly with sweet they can sit upright and have the coordination). your baby at the same times each day. You could play Stay positive and calm. Don’t let your baby’s facial flavors (which they are predisposed to, as the flavor Although introducing both purées and finger music to help build a positive association with meals. expressions affect you. Imagine experiencing a food of milk is sweet). This vegetable-led approach to foods into the diet together has received mixed A pleasant routine can be reassuring, allowing you to for the first time—it’s likely to produce a reaction. weaning is by no means the only way to wean, but feedback, it may actually help babies develop expose your baby to new foods in a comfortable way. This is the wonder of weaning! it is backed by scientific research that shows it can have a positive influence on vegetable intake. Research suggests that, between the ages of 6 and 12 months, babies are most likely to accept new foods. This gives you the ideal window to introduce BROCCOLI a variety of foods into your baby’s diet. This CONTAINS FIBER, nutritional input will support their development, CALCIUM, FOLATE, AND VITAMINS A and the familiarity with multiple flavors will help AND C CAULIFLOWER set them up with good dietary habits for life. CONTAINS FIBER, Regardless of the method you choose, repeated POTASSIUM, AND exposure is vital, so be patient. It can take more VITAMINS B6 AND C than 10 attempts with the same food for it to become accepted regularly. Just keep offering. Finger foods encourage self-feeding and help babies improve their coordination and become accustomed to new textures. Broccoli and cauliflower “trees” are easy to pick up and have great textures to stimulate your baby’s mouth. UUSS__119922--119933__wweeaanniinngg..iinndddd 119922 2244//0088//22002211 1166::2244 UUSS__119922--119933__wweeaanniinngg..iinndddd 119933 2244//0088//22002211 1166::2244
194 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 195 HOW DO I PREVENT MY CHILD FROM BECOMING A PICKY EATER? A young child’s relationship with food matters. Picky eaters get less protein, vegetables, and fruits, and research shows that children who don’t eat well at the age of three are more likely to remain picky into adulthood. The phrase “picky eating” describes many behaviors, food as the rest of the family (minus the salt). including the rejection of one or more foods, the The best way for them to learn to eat and enjoy new limited intake or variety of foods, and/or frequent foods is to copy you. Give your child small portions changes in food preferences. Causes can be difficult and a lot of praise for eating, even if it’s only a little. to pinpoint. Studies suggest some children are If they reject a food, don’t force them to eat it. Just predisposed to food pickiness, just as others are take it away without saying anything and try it again more shy. Many studies show that children respond another time. Changing how you serve a food may to their parents’ relationships with food, which gives make it more appealing. For instance, a child might parents the power to influence their child’s diet. refuse cooked carrots but enjoy raw grated carrot. Your child may be a slow eater, so be patient. THINGS YOU CAN DO Make mealtimes enjoyable and not just about eating. Follow the advice for weaning on pages 192–193 Sit down and chat about other things to give them to establish good eating habits early. As often as a chance to finish eating at their own pace. possible, eat together. Give your child the same Don’t give your child too many snacks— two healthy snacks a day is plenty. And don’t leave meals until your child is too hungry or tired to eat. Natural sweet tooth If you know kids who are good eaters, ask them over for a meal to set an example—but don’t openly THE PREFERENCE BABIES AND YOUNG CHILDREN compare them to your child. If there is an adult your SHOW FOR SWEET FLAVORS MAY BE DUE TO THE child looks up to in your life, have them over to eat PAIN AND STRESS RELIEF THEY PROVIDE. In studies, 3–4-month-old babies instantly felt often. Sometimes a child will eat without fuss calmer and more trusting around a new face when around a grandparent, for example. consuming something sweet. We are hardwired to enjoy sweet foods. They trigger the "feel-good" WHAT NOT TO DO reward center in the brain and offer a plentiful supply of energy. As we age, our feel-good receptors Avoid showing your child a reaction if they exhibit don’t work as efficiently—we grow out of our sweet picky behavior. Try to stay calm, even if it’s very tooth. That being said, it’s important to set up good frustrating. Reserve all your attention for verbally eating habits early. Offer kids sweet foods in moderation, ideally after establishing savory foods praising your child whenever they eat well. in the diet (see page 192). Avoid offering added Also avoid offering your child overwhelmingly sugars (see page 64) to children under two. huge portions. Stick with small portions, adding seconds if required. UUSS__119944--119955__ffuussssyy__eeaatteerrss..iinndddd 119944 2244//0088//22002211 1166::2244
119944 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 119955 HOW DO I PREVENT MY CHILD FROM BECOMING A PICKY EATER? A young child’s relationship with food matters. Picky eaters get less protein, vegetables, and fruits, and research shows that children who don’t eat well at the age of three are more likely to remain picky into adulthood. The phrase “picky eating” describes many behaviors, food as the rest of the family (minus the salt). Don’t leave your child to eat alone. If you including the rejection of one or more foods, the The best way for them to learn to eat and enjoy new can’t eat together, stay with them while they limited intake or variety of foods, and/or frequent foods is to copy you. Give your child small portions eat their own meal. changes in food preferences. Causes can be difficult and a lot of praise for eating, even if it’s only a little. Avoid using food as a reward. Your child may start to pinpoint. Studies suggest some children are If they reject a food, don’t force them to eat it. Just to think of sweets as good and vegetables as bad, predisposed to food pickiness, just as others are take it away without saying anything and try it again which can spiral into an unhealthy relationship with more shy. Many studies show that children respond another time. Changing how you serve a food may food. Instead, reward them with a trip to the park or to their parents’ relationships with food, which gives make it more appealing. For instance, a child might promise to play a game with them. parents the power to influence their child’s diet. refuse cooked carrots but enjoy raw grated carrot. It’s tempting to give kids things you know they will Your child may be a slow eater, so be patient. eat, and ultra-processed foods are highly palatable. THINGS YOU CAN DO Make mealtimes enjoyable and not just about eating. Kids might reject foods consistently or eat them Follow the advice for weaning on pages 192–193 Sit down and chat about other things to give them only sometimes. Be patient with this and play the to establish good eating habits early. As often as a chance to finish eating at their own pace. long game. While a bag of chips can guarantee the possible, eat together. Give your child the same Don’t give your child too many snacks— two same flavor at each bite and a child that will happily healthy snacks a day is plenty. And don’t leave eat what you give them, in the long run, it may create meals until your child is too hungry or tired to eat. a preference for less healthy foods that becomes a Natural sweet tooth If you know kids who are good eaters, ask them tricky habit to break. over for a meal to set an example—but don’t openly VITAMIN A THE PREFERENCE BABIES AND YOUNG CHILDREN compare them to your child. If there is an adult your SHOW FOR SWEET FLAVORS MAY BE DUE TO THE CARROTS ARE A GREAT PAIN AND STRESS RELIEF THEY PROVIDE. child looks up to in your life, have them over to eat SOURCE OF VITAMIN A, In studies, 3–4-month-old babies instantly felt often. Sometimes a child will eat without fuss NEEDED TO SUPPORT calmer and more trusting around a new face when around a grandparent, for example. EYE HEALTH AND THE consuming something sweet. We are hardwired to IMMUNE SYSTEM enjoy sweet foods. They trigger the "feel-good" reward center in the brain and offer a plentiful WHAT NOT TO DO Sweet, mild-flavored carrots are a popular choice for transitioning supply of energy. As we age, our feel-good receptors Avoid showing your child a reaction if they exhibit babies to solid food from six months don’t work as efficiently—we grow out of our sweet picky behavior. Try to stay calm, even if it’s very old, but don’t be afraid to try more tooth. That being said, it’s important to set up good strongly flavored vegetables to help frustrating. Reserve all your attention for verbally eating habits early. Offer kids sweet foods in expand your baby’s palate. moderation, ideally after establishing savory foods praising your child whenever they eat well. in the diet (see page 192). Avoid offering added Also avoid offering your child overwhelmingly sugars (see page 64) to children under two. huge portions. Stick with small portions, adding seconds if required. UUSS__119944--119955__ffuussssyy__eeaatteerrss..iinndddd 119944 2244//0088//22002211 1166::2244 UUSS__119944--119955__ffuussssyy__eeaatteerrss..iinndddd 119955 2255//0088//22002211 1199::1155
196 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 197 HOW CAN I HELP MY TODDLER EAT WELL? Toddlers are notorious for being up and down with their food, and this is completely normal. You need to support and guide your toddler to make the right nutritional choices, which can impact growth, development, and future health outcomes. From ages one to three, a toddler will be learning as relaxed as possible before and during mealtimes, to eat with hands and cutlery, in a variety of settings, and be a role model for eating a variety of different and manage energy intake for the amount of growing foods and having a healthy relationship toward food and exploring they will do. Some children will eat intake. The more you pressurize your child, the less more than others. Teething, sickness, activity levels, likely they are to want to try new foods or eat their and sleep can all impact appetite. meal. Limit bribery, as this will only make things more difficult in the long run. For example, avoid ESTABLISH GOOD HABITS saying, “If you eat your greens, you can have A positive eating environment at home is key to dessert,” or making a child feel guilty for not eating encouraging good eating behaviors. Get your a food because others elsewhere have less. children involved with cooking so they can see Ensure your kitchen and home are full of healthy the process from start to finish; sing songs and foods. Utilize the freezer and try batch-cooking a get creative to make it all the more fun. favorite meal with lots of added vegetables. For Provide your child with exposure to lots of example, make a cauliflower mac ’n’ cheese and different foods. Don’t give up after introducing include the cauliflower stalk and leaves, along with a new food once—it can take between 5 and 15 broccoli, peas, and carrots, and freeze into individual times before they will accept a new food. Try to stay portions for the next few weeks. A quick guide to nutritional needs, Each day toddlers need three meals and some snacks made up of the four main food groups, in the right balance and in portion sizes just right for them. For most toddlers, there is no need to offer low-calorie 5+ 5 3 or low-fat options as children of this age need Portions of Portions of Portions of lots of energy for growing FRUITS AND STARCHY DAIRY AND and physical activity. VEGETABLES CARBOHYDRATES ALTERNATIVES UUSS__119966--119977__ttooddddlleerrss..iinndddd 119966 2244//0088//22002211 1166::2244
119966 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 119977 HOW CAN I HELP MY MILK SALT SUGAR FATS & PROTEIN TODDLER EAT WELL? Toddlers are notorious for being up and down with their food, and this is completely normal. You need to support and guide your toddler to make the right nutritional choices, which can impact growth, development, and future health outcomes. Quantity? Quantity? Quantity? Quantity? At least 12oz (350ml) of Children aged 1 to 3 years Avoid added sugar, except Until the age of 2, children milk a day OR 2 servings should have no more than occasionally from 2+. need a lot of energy from From ages one to three, a toddler will be learning as relaxed as possible before and during mealtimes, of dairy foods such as 2g of salt (800mg sodium) fat. After that, lower-fat cheese per day. options can be offered. to eat with hands and cutlery, in a variety of settings, and be a role model for eating a variety of different Desserts or yogurt. and manage energy intake for the amount of growing foods and having a healthy relationship toward food Include to maximize Yes or no? nutrition, but make it fruit Saturated fats and exploring they will do. Some children will eat intake. The more you pressurize your child, the less How long? There is no reason to add and yogurt or something Limit saturated fat to less more than others. Teething, sickness, activity levels, likely they are to want to try new foods or eat their WHO recommends salt to your child’s diet at less sweet. than 10% of overall daily and sleep can all impact appetite. meal. Limit bribery, as this will only make things breastfeeding until 2 years any age unless you have calories. Steam, grill, or more difficult in the long run. For example, avoid old; offer cow’s milk or an been advised to do so by bake foods instead of Drinks alternative after 12 your health professional. frying them to reduce ESTABLISH GOOD HABITS saying, “If you eat your greens, you can have Water and milk are best as months. saturated fat. A positive eating environment at home is key to dessert,” or making a child feel guilty for not eating they do not contain added sugar. No milk after encouraging good eating behaviors. Get your a food because others elsewhere have less. Vegans: brushing teeth from 2+. Protein children involved with cooking so they can see Ensure your kitchen and home are full of healthy Make sure dairy Replace processed and the process from start to finish; sing songs and foods. Utilize the freezer and try batch-cooking a alternatives are fortified. high-fat meats with beans, Snacks Avoid rice milk, which peas, and lentils to meet get creative to make it all the more fun. favorite meal with lots of added vegetables. For Avoid too much dried fruit contains a level of arsenic protein needs, but be Provide your child with exposure to lots of example, make a cauliflower mac ’n’ cheese and as it can get stuck on too high for toddlers. aware that high-fiber teeth and cause decay. different foods. Don’t give up after introducing include the cauliflower stalk and leaves, along with foods may fill little a new food once—it can take between 5 and 15 broccoli, peas, and carrots, and freeze into individual tummies too quickly. times before they will accept a new food. Try to stay portions for the next few weeks. A quick guide to nutritional needs, Should I give my toddler supplements? Each day toddlers need three meals and some GOVERNMENT ADVICE IS THAT CHILDREN AGED snacks made up of the four SIX MONTHS TO FIVE YEARS OLD SHOULD TAKE main food groups, in the A DAILY VITAMIN D SUPPLEMENT right balance and in portion if consuming less than 17oz (500ml) formula milk. However, sizes just right for them. recently it has been reported that more children are getting For most toddlers, there is enough vitamins in their diets. With the exception of vitamin D, no need to offer low-calorie 5+ 5 3 2+ < 1 if your child has a balanced diet with lots of color and variety, or low-fat options as then they will be getting enough. If you are concerned, ask your children of this age need doctor for a referral to a pediatric dietitian. Portions of Portions of Portions of Portions of A small amount of lots of energy for growing FRUITS AND STARCHY DAIRY AND PROTEIN FAT and physical activity. VEGETABLES CARBOHYDRATES ALTERNATIVES (3 IF VEGETARIAN) UUSS__119966--119977__ttooddddlleerrss..iinndddd 119966 2244//0088//22002211 1166::2244 UUSS__119966--119977__ttooddddlleerrss..iinndddd 119977 2244//0088//22002211 1166::2244
198 HOW DOES NUTRITION AFFECT CHILD DEVELOPMENT? 199 WHAT SHOULD I CONSIDER IF RAISING A CHILD ON A VEGAN OR VEGETARIAN DIET? Times have changed substantially in the last 20 years, and plant-based diets are on the rise, but what does that mean for little eaters? Is it possible to give them all the nutrition they require from a plant-based diet? The short answer is “yes,” but it needs to be VITAMINS AND MINERALS carefully thought through, with potential Consider meeting with your pediatrician and supplementation provided if on a vegan diet. registered dietitian to discuss whether supplementation is necessary: NUTRIENT DENSITY FOR VEGANS Iron It’s recommended to give all exclusively For healthy development, a vegan child needs breastfed babies 1mg per kg (2lb) of body weight enough calories, healthy fats, and protein. Vegan a day from four to six months. diets can be high in fiber, which can make toddlers’ Vitamin D Guidance is to supplement with tummies full without sufficient calories. Guard against 400 IU of vitamin D for babies who receive only this by including energy- and nutrient-dense foods breast milk. Babies fed only formula do not need to such as avocados, vegetable oils, seeds, nut butters supplement since formula is fortified with vitamin D. or ground-up nuts (no whole nuts to the age of Iodine Breast milk and formula provide all five, due to choking risk), tofu, and legumes. requirements. Thereafter, it may be wise to speak to your pediatrician about supplementation. PROTEIN CONSIDERATIONS Vitamin B12 There are no quality forms of B12 Protein needs are easily met if your child eats a in a plant-based diet, which is vital for the nervous wide variety of foods containing protein at each system, metabolism, and formation of red blood meal. From nondairy yogurts to beans, peas, and cells. Breast milk will be sufficient only if the mother lentils, to grains like quinoa and buckwheat, to consumes B12, and formula is fortified already. tofu and other soy products, there are many options Choline Important for babies’ brain growth, for vegan and plant-based eaters. Alongside the the main sources of choline are eggs, soy, and essential amino acids we should all be consuming cruciferous vegetables. (see pages 14–15 and 128–129), there are a few more Omega-3 The fatty acid DHA is critical for that are “conditionally” essential for children, since brain development, but even seaweed, algae, eggs, their bodies cannot make enough: arginine, histidine, and fortified foods are unlikely to provide enough, cysteine, glycine, tyrosine, glutamine, and proline. so consider supplements while pregnant and lactating. Provided you give your child a mixture of different Calcium Studies show that calcium, important protein sources over the course of each day, for growing bones and teeth, can be insufficient in including whole grains and vegetables, they should vegan diets. Aim for a variety of fortified plant-based have no problem obtaining both essential and sources: calcium-set tofu, calcium-fortified soy conditional amino acids. yogurts, baked beans, hummus, and nut butters. UUSS__119988--119999__ppllaannttbbaasseedd__cchhiillddrreenn..iinndddd 119988 2244//0088//22002211 1166::2244
119988 HHOOWW DDOOEESS NNUUTTRRIITTIIOONN AAFFFFEECCTT CCHHIILLDD DDEEVVEELLOOPPMMEENNTT?? 119999 WHAT SHOULD I CONSIDER IF RAISING A CHILD ON A AVOCADO VEGAN OR VEGETARIAN DIET? THESE TREE BERRIES ARE AN ENERGY– AND NUTRIENT-DENSE HUMMUS Times have changed substantially in the last 20 years, and plant-based diets are on SOURCE OF HEALTHY THIS CHICKPEA-BASED FATS AND ESSENTIAL the rise, but what does that mean for little eaters? Is it possible to give them DIP CONTAINS CALCIUM AMINO ACIDS all the nutrition they require from a plant-based diet? AND ARGININE, WHICH NUT CAN BE LACKING IN BUTTERS A VEGAN DIET ARE A GOOD SOURCE OF CONDITIONAL AMINO The short answer is “yes,” but it needs to be VITAMINS AND MINERALS ACIDS, ENERGY, AND carefully thought through, with potential Consider meeting with your pediatrician and HEALTHY FATS supplementation provided if on a vegan diet. registered dietitian to discuss whether supplementation is necessary: NUTRIENT DENSITY FOR VEGANS Iron It’s recommended to give all exclusively For healthy development, a vegan child needs breastfed babies 1mg per kg (2lb) of body weight enough calories, healthy fats, and protein. Vegan a day from four to six months. diets can be high in fiber, which can make toddlers’ Vitamin D Guidance is to supplement with tummies full without sufficient calories. Guard against 400 IU of vitamin D for babies who receive only Fortifying foods this by including energy- and nutrient-dense foods breast milk. Babies fed only formula do not need to Various nutrients may be lacking if feeding such as avocados, vegetable oils, seeds, nut butters supplement since formula is fortified with vitamin D. your baby a vegan or or ground-up nuts (no whole nuts to the age of Iodine Breast milk and formula provide all vegetarian diet. Supplements are the five, due to choking risk), tofu, and legumes. requirements. Thereafter, it may be wise to speak best option in many to your pediatrician about supplementation. cases, but often those needs can be met by PROTEIN CONSIDERATIONS Vitamin B12 There are no quality forms of B12 introducing certain Protein needs are easily met if your child eats a in a plant-based diet, which is vital for the nervous nutrient-rich plant foods to your baby’s wide variety of foods containing protein at each system, metabolism, and formation of red blood diet, such as hummus meal. From nondairy yogurts to beans, peas, and cells. Breast milk will be sufficient only if the mother and nut butters. lentils, to grains like quinoa and buckwheat, to consumes B12, and formula is fortified already. tofu and other soy products, there are many options Choline Important for babies’ brain growth, for vegan and plant-based eaters. Alongside the the main sources of choline are eggs, soy, and essential amino acids we should all be consuming cruciferous vegetables. SOURCES OF CONDITIONAL AMINO ACIDS (see pages 14–15 and 128–129), there are a few more Omega-3 The fatty acid DHA is critical for that are “conditionally” essential for children, since brain development, but even seaweed, algae, eggs, their bodies cannot make enough: arginine, histidine, and fortified foods are unlikely to provide enough, ARGININE HISTIDINE CYSTEINE TYROSINE GLUTAMINE PROLINE cysteine, glycine, tyrosine, glutamine, and proline. so consider supplements while pregnant and lactating. PUMPKIN SEEDS TOFU SUNFLOWER SEEDS MILK SOYBEANS BEANS Provided you give your child a mixture of different Calcium Studies show that calcium, important SOYBEANS PUMPKIN SEEDS LENTILS LENTILS RED CABBAGE NUTS protein sources over the course of each day, for growing bones and teeth, can be insufficient in PEANUTS WHOLE WHEAT OATS PUMPKIN SEEDS NUTS SEEDS including whole grains and vegetables, they should vegan diets. Aim for a variety of fortified plant-based CHICKPEAS PASTA CARROTS WILD RICE BEANS have no problem obtaining both essential and sources: calcium-set tofu, calcium-fortified soy LENTILS NAVY BEANS conditional amino acids. yogurts, baked beans, hummus, and nut butters. UUSS__119988--119999__ppllaannttbbaasseedd__cchhiillddrreenn..iinndddd 119988 2244//0088//22002211 1166::2244 UUSS__119988--119999__ppllaannttbbaasseedd__cchhiillddrreenn..iinndddd 119999 2244//0088//22002211 1166::2244
UUSS__220000--220011__cchhaapptteerr__77..iinndddd 220000 2244//0088//22002211 1166::2244
CAN WE EAT TO SUPPORT MENTAL HEALTH? Does What I Eat Affect My Mood? 202 How Can I Stop Binge Eating? 208 Can Intuitive Eating Help Me? 204 Am I Suffering from Disordered Eating? 210 Can Mindful Eating Help Me? 206 UUSS__220000--220011__cchhaapptteerr__77..iinndddd 220000 2244//0088//22002211 1166::2244 UUSS__220000--220011__cchhaapptteerr__77..iinndddd 220011 2244//0088//22002211 1166::2244
202 CAN WE EAT TO SUPPORT MENTAL HEALTH? 203 DOES WHAT I EAT AFFECT MY MOOD? A healthy diet can do a lot to boost your mood and sense of well-being. In fact, improving what you eat can lead to more positive feelings, clearer thinking, higher energy, and calmer moods. Research suggests inflammation in the brain is Mediterranean diet (see pages 36–41), trials suggest impacted by diet, since short-chain fatty acids this can act as a treatment for depression and in produced in the gut have anti-inflammatory some cases help prevent it, often proving more properties, and diversity of gut bacteria encourages impactful than the traditional routes of treatment. production of these fatty acids (see pages 48–53). To protect your brain, you should also aim to eat As a result of a poor diet high in salt, saturated more polyphenols, found in foods such as spinach fat, and sugars, the hippocampus in the brain and dark berries like blueberries. decreases in volume and your neurons may be damaged. This in turn can increase the risk of DIET AND THE ‘‘HAPPY HORMONE’’ depression, low moods, and poor memory and Serotonin is a neurotransmitter that helps relay learning skills. If you aim to follow the messages from one area of the brain to another and GLUCOSE After a brief energy ‘‘high,’’ SPIKE rapid conversion of glucose by insulin results in a sharp drop in blood-sugar levels, leading to an energy crash and food cravings Blood sugar highs and lows When you eat a lot of refined carbs, your pancreas sees a spike in blood glucose levels and releases insulin as quickly as it can to try to catch up. This can Refined carbs for breakfast result in too much glucose being Foods with little or no fiber removed from your blood, causing (such as refined carbs and a blood-sugar crash that can leave sugar) break down and release you feeling fatigued, irritable, energy quickly, leading to a depressed, anxious, and nervous. short-term spike in blood- GLUCOSE CRASH sugar levels UUSS__220022--220033__mmoooodd..iinndddd 220022 2244//0088//22002211 1166::2244
220022 CCAANN WWEE EEAATT TTOO SSUUPPPPOORRTT MMEENNTTAALL HHEEAALLTTHH?? 220033 DOES WHAT I EAT is believed to influence a variety of psychological ENERGY AND MOOD functions; it’s known as the “happy hormone” Mood and concentration can be heavily impacted by AFFECT MY MOOD? because of its mood-stabilizing qualities. Gut how dietary choices affect blood-sugar levels. When bacteria manufacture about 95 percent of the body’s you eat carbohydrates, your body digests it, converts supply of serotonin and interactions between the it into glucose (sugar), and sends the glucose into A healthy diet can do a lot to boost your mood and sense of well-being. gut-brain axis (see pages 48–49), and gut bacteria your blood. Insulin is then produced by the pancreas In fact, improving what you eat can lead to more positive feelings, help you out with many essential functions, such as to convert that glucose into energy. clearer thinking, higher energy, and calmer moods. digestion of nutrients, and may even be implicated Your blood-sugar levels are determined by the in your mental health. type of carbohydrates you eat. The wrong type Those of us with low serotonin levels are said to (such as refined carbs and sugar) will send you on Research suggests inflammation in the brain is Mediterranean diet (see pages 36–41), trials suggest feel better after eating sugar, which is obviously not a roller-coaster ride that will give you a quick hit of impacted by diet, since short-chain fatty acids this can act as a treatment for depression and in the healthiest way to lift mood and often leads to energy, followed by a crash that will leave you feeling produced in the gut have anti-inflammatory some cases help prevent it, often proving more binge eating. Instead, you might be able to help low in energy, lacking in concentration, and craving properties, and diversity of gut bacteria encourages impactful than the traditional routes of treatment. serotonin production by consuming plenty of more energy-dense food. This is when you may want production of these fatty acids (see pages 48–53). To protect your brain, you should also aim to eat quality carbohydrates and proteins containing the to reach for that cookie to give you more sugar. As a result of a poor diet high in salt, saturated more polyphenols, found in foods such as spinach amino acid tryptophan, such as milk or tuna. The Eating the right type of slow-release carbohydrates, fat, and sugars, the hippocampus in the brain and dark berries like blueberries. research has not yet fully determined whether this such as whole grains, vegetables, and fruits, however, decreases in volume and your neurons may be dietary change does improve mood, but it may be will keep you safely off the roller coaster, leaving damaged. This in turn can increase the risk of DIET AND THE ‘‘HAPPY HORMONE’’ that not eating enough carbohydrates, for example, you feeling happier, more focused, and more depression, low moods, and poor memory and Serotonin is a neurotransmitter that helps relay in a high-protein/high-fat diet, leads to low moods. energetic for longer (see below). learning skills. If you aim to follow the messages from one area of the brain to another and GLUCOSE After a brief energy ‘‘high,’’ GLUCOSE SPIKE rapid conversion of glucose SPIKE To step off the roller coaster, by insulin results in a sharp you need to bring your blood- drop in blood-sugar levels, sugar levels onto a steadier leading to an energy crash curve within a narrower range and food cravings of highs and lows Blood sugar highs and lows When you eat a lot of refined carbs, your pancreas sees a spike in blood glucose levels and releases insulin as quickly as it can to try to catch up. This can Refined carbs for breakfast Midmorning snack Healthy lunch result in too much glucose being Foods with little or no fiber Grabbing a sugary snack A lunch of slow-release carbs removed from your blood, causing (such as refined carbs and of more refined carbs, and fiber—from whole grains, a blood-sugar crash that can leave sugar) break down and release such as cake or cookies, veggies, and lentils—takes you feeling fatigued, irritable, energy quickly, leading to a to boost your energy longer to digest and releases depressed, anxious, and nervous. short-term spike in blood- GLUCOSE levels simply repeats GLUCOSE energy slowly, resulting in CRASH CRASH sugar levels the process steady blood-sugar levels UUSS__220022--220033__mmoooodd..iinndddd 220022 2244//0088//22002211 1166::2244 UUSS__220022--220033__mmoooodd..iinndddd 220033 2244//0088//22002211 1166::2244
204 CAN WE EAT TO SUPPORT MENTAL HEALTH? 205 CAN INTUITIVE EATING HELP ME? Adopting the intuitive-eating approach can help you reconnect with your internal cues, listen to your body, and focus on health instead of weight. Many people think intuitive eating (IE) is an all- choose what you think you should be eating. or-nothing approach (much like a diet), but IE is a However, this can often backfire. For example, nondiet approach that helps individuals heal from you crave some chocolate, but you think, “It’s not chronic dieting. It encourages you to ditch food rules, healthy,” so you opt for a low-calorie snack instead. respect your body, and actually enjoy food again. You feel unsatisfied after eating this snack, so you search for something else … and so the pattern CHOOSING THE CHOCOLATE continues. If you had listened to your body and When you are constantly dieting, food choices often eaten the chocolate, you likely would have satisfied come with little enjoyment and a side of guilt, as you your stomach and not needed to overeat. STIMULANTS DARK AND MILK CHOCOLATE CONTAIN Dark chocolate may be able THEOBROMINE AND to boost your well-being, as it CAFFEINE, KNOWN TO BE releases a cascade of feel-good compounds that have been linked PHYSICAL AND MENTAL to improved mood. However, there STIMULANTS is debate as to whether chocolate contains enough of these compounds to have any effect. MOOD BOOST? N-ACYLETHANOLAMINE IN DARK CHOCOLATE IS ASSOCIATED WITH THE BRAIN’S MOOD AND REWARD CYCLES UUSS__220044--220055__iinnttuuiittiivvee..iinndddd 220044 2266//0088//22002211 0077::4466
220044 CCAANN WWEE EEAATT TTOO SSUUPPPPOORRTT MMEENNTTAALL HHEEAALLTTHH?? 220055 CAN INTUITIVE THE INTUITIVE EATING APPROACH IS MADE UP OF 10 PRINCIPLES: EATING HELP ME? 1. 2. Adopting the intuitive-eating approach can help you reconnect with your REJECT THE HONOR HUNGER internal cues, listen to your body, and focus on health instead of weight. DIET MENTALITY You are ‘‘allowed’’ to be hungry. Hunger means our body’s Address your food rules, which can signals are working, but diets be influenced by external factors Many people think intuitive eating (IE) is an all- choose what you think you should be eating. often require us to ignore them. like diets and the media. If you or-nothing approach (much like a diet), but IE is a However, this can often backfire. For example, Hunger cues include feeling have a history of disordered light-headed, irritable, or nondiet approach that helps individuals heal from you crave some chocolate, but you think, “It’s not eating, they will take low on energy. chronic dieting. It encourages you to ditch food rules, healthy,” so you opt for a low-calorie snack instead. time to undo. respect your body, and actually enjoy food again. You feel unsatisfied after eating this snack, so you 3. 4. 5. search for something else … and so the pattern CHOOSING THE CHOCOLATE continues. If you had listened to your body and MAKE PEACE CHALLENGE RESPECT FULLNESS When you are constantly dieting, food choices often eaten the chocolate, you likely would have satisfied WITH FOOD THE FOOD POLICE There is a difference between come with little enjoyment and a side of guilt, as you your stomach and not needed to overeat. feeling full and feeling satisfied. Give yourself permission to eat. Call out the voice in your head Check in with yourself as you This doesn’t mean always binging that tries to dictate your food are eating. Are you satisfied or on unhealthy foods. By allowing choices based on calorie getting full? Mindful eating all foods, ‘‘forbidden’’ loses its content or ‘‘health.’. Think might be helpful here (see appeal, and you strike a more rationally pages 206–207). balance. around food. STIMULANTS 6. 7. DARK AND MILK CHOCOLATE CONTAIN Dark chocolate may be able DISCOVER THE DISASSOCIATE THEOBROMINE AND to boost your well-being, as it CAFFEINE, KNOWN TO BE releases a cascade of feel-good SATISFACTION FACTOR FEELINGS FROM FOOD compounds that have been linked PHYSICAL AND MENTAL to improved mood. However, there To eat less and feel more fulfilled, When comfort eating becomes STIMULANTS is debate as to whether chocolate choose delicious foods you really your only coping mechanism, it contains enough of these want and will satisfy. Prioritize a can be problematic. Try to find compounds to have any effect. few meals a week to focus on other ways to comfort—taking positive eating, to begin a bath, reading a book, with. going for a walk. 8. 9. 10. MOOD BOOST? ACCEPT BODY EXERCISE—FEEL TAKE NUTRITION N-ACYLETHANOLAMINE DIVERSITY THE DIFFERENCE GENTLY IN DARK CHOCOLATE IS We come in all different shapes Movement is good for physical You must first explore your ASSOCIATED WITH THE and sizes and should not sacrifice and mental health and should feel relationship with food and then BRAIN’S MOOD health to try to change our genes. good. Explore different forms of consider the basics of nutrition. AND REWARD Consider psychological movement that make you feel It is important to eat for CYCLES support if you struggle energized—and ditch the well-being as well as with body image. tracking devices . enjoyment. UUSS__220044--220055__iinnttuuiittiivvee..iinndddd 220044 2266//0088//22002211 0077::4466 UUSS__220044--220055__iinnttuuiittiivvee..iinndddd 220055 2244//0088//22002211 1166::2244
206 CAN WE EAT TO SUPPORT MENTAL HEALTH? 207 CAN MINDFUL EATING HELP ME? Too often, eating is something to get done quickly while we multitask. But if we take the time to develop a mindful approach to eating, our relationship with food can drastically improve. Whenever we eat or drink, we have the opportunity NO PORTION CONTROL NIGHTTIME SNACKING to focus on the moment. Unfortunately, many of us Piling the plate high Avoid skipping meals don’t do this; instead, we eat while talking on the when you’re hungry is easy to and make sure you eat enough phone or working on the computer, barely noticing do. Try weighing ingredients during the day to avoid before you cook and sticking to nighttime hunger pangs that the taste or the amount of food we are consuming. sensible portion sizes. interrupt your sleep. With mindful eating, we direct our full awareness to the sensations, thoughts, and emotions that arise as we eat, without criticism or judgment. We notice the colors, shapes, fragrances, flavors, textures, and even sounds of our food. A big movement toward mindful eating has begun, suggesting that slower and more thoughtful eating can help combat weight problems and poor food choices. AM I EATING MINDFULLY? Do you feel in control of the decisions you make around food every day? When you go to the movies, are you aware of how much popcorn you are eating, or are you so distracted by the movie that you finish off the whole bucket, without really tasting it? Being aware of how you eat is key. Only then can you move forward and rebuild your relationship with food. We have taste buds for a reason, and feeling full is a natural sensation, so allow yourself to enjoy your food. This is the first step to eating mindfully. USING TECHNOLOGY MOODY EATER at mealtimes when angry, bored, MAKING CHANGES Watching television or tired, or stressed Key to mindful eating is recognizing the mindless scrolling on your phone while Most people comfort eat eating habits that we all engage in at some time or eating will distract you from sometimes, but if you do this other. Look at the list opposite, identify which habits your food. Try to put your phone often, try to find other ways you regularly fall into, and develop strategies to in another room and turn off to boost your mood, such as the TV during mealtimes. exercising, taking a bath, change them. Be conscious of what, when, and how or journaling. you eat, and take the time to appreciate your food and enjoy the experience as often as you can. UUSS__220066--220077__mmiinnddffuull..iinndddd 220066 2244//0088//22002211 1166::2244
220066 CCAANN WWEE EEAATT TTOO SSUUPPPPOORRTT MMEENNTTAALL HHEEAALLTTHH?? 220077 CAN MINDFUL EATING HELP ME? PICKING ON FOOD EATING ON THE COUCH EATING ON THE GO EATING WHATEVER throughout the day instead of at the table When you’re dashing is in the office that Grazing all day on snacks will The couch is a place to lounge from place to place, eating day at your desk Too often, eating is something to get done quickly while we multitask. likely lead to short-term energy and relax. Eating at the table becomes an expedient rather Sometimes convenience wins, But if we take the time to develop a mindful approach to eating, spikes followed by blood-sugar will encourage a better posture than a priority. Try to stop, but try to deliberately choose our relationship with food can drastically improve. crashes (see pages 202–203). for eating and greater attention even for five minutes, to food you want to eat, and step Try eating substantial meals to your food. savor your food. away from your desk and any with complex carbohydrates other distractions to enjoy it. for a slower release of energy. Whenever we eat or drink, we have the opportunity NO PORTION CONTROL NIGHTTIME SNACKING to focus on the moment. Unfortunately, many of us Piling the plate high Avoid skipping meals don’t do this; instead, we eat while talking on the when you’re hungry is easy to and make sure you eat enough phone or working on the computer, barely noticing do. Try weighing ingredients during the day to avoid before you cook and sticking to nighttime hunger pangs that the taste or the amount of food we are consuming. sensible portion sizes. interrupt your sleep. With mindful eating, we direct our full awareness to the sensations, thoughts, and emotions that arise as we eat, without criticism or judgment. We notice the colors, shapes, fragrances, flavors, textures, and even sounds of our food. A big movement toward mindful Mindless eating habits eating has begun, suggesting that slower and to guard against more thoughtful eating can help combat weight problems and poor food choices. Ask yourself how often you engage in these mindless eating habits and whether AM I EATING MINDFULLY? you can change your behavior. You might not be able to avoid all of them all the Do you feel in control of the decisions you make time, but being aware of them means you around food every day? When you go to the movies, change the areas you choose to. are you aware of how much popcorn you are eating, or are you so distracted by the movie that you finish off the whole bucket, without really tasting it? Being aware of how you eat is key. Only then can you move forward and rebuild your relationship with food. We have taste buds for a reason, and feeling full is a natural sensation, so allow yourself to enjoy your food. This is the first step to eating mindfully. USING TECHNOLOGY MOODY EATER EATING OVERLY WEEKEND EATER NOT CHEWING FOOD SKIPPING BREAKFAST at mealtimes when angry, bored, PROCESSED FOODS You’re good all week, then Shoveling down food This can seem like a quick fix MAKING CHANGES Watching television or tired, or stressed it goes out the window on without chewing makes it if you’re in a hurry or looking to Key to mindful eating is recognizing the mindless Processed and packaged scrolling on your phone while Most people comfort eat the weekend harder for the body to digest cut down on food intake, but eating habits that we all engage in at some time or foods have all sorts of hidden eating will distract you from sometimes, but if you do this Give yourself permission to eat and will leave you feeling taking time for a healthy ingredients in them, many of other. Look at the list opposite, identify which habits your food. Try to put your phone often, try to find other ways more freely all the time, rather unsatisfied and likely to breakfast will set you up for them unhealthy. Try cooking you regularly fall into, and develop strategies to in another room and turn off to boost your mood, such as than being restrictive during overeat. Try to allow sufficient the day with slow-release from scratch as often as you the TV during mealtimes. exercising, taking a bath, the week and binge eating on time to enjoy your meal or energy and prevent you from change them. Be conscious of what, when, and how can, so you know what’s going or journaling. the weekend. snack, and eat slowly. overeating at lunch. you eat, and take the time to appreciate your food into your food. and enjoy the experience as often as you can. UUSS__220066--220077__mmiinnddffuull..iinndddd 220066 2244//0088//22002211 1166::2244 UUSS__220066--220077__mmiinnddffuull..iinndddd 220077 2255//0088//22002211 1199::1155
208 CAN WE EAT TO SUPPORT MENTAL HEALTH? 209 The binge-restrict cycle Restricting unhealthy foods may seem FOODS like the right thing to do, but it can kick- CONSIDERED ‘‘BAD’’ Removing the restriction element start a pattern of bingeing. OR ASSOCIATED can break the binge-restrict cycle. Research suggests that if we allow WITH SHAME OR ourselves to eat items we enjoy, we GUILT ARE are unlikely to want to binge on them. RESTRICTED SW HE A F ME E S EL F E LO FR - CL RA O EC N SK TI TRN RO IG L C T CA I ON N ND T TIA NK TE U, HI E N CG YCLE SHAME RESTRICTION OBSESSION F O R BB IE DC DR A EE U NS ST ER II TC T I SA E B D O FU OW T O TE DH O EB S E S S GUILT BINGEING A SENSE OF WE BINGE ON GUILT FOLLOWS FOR THE FOOD THAT WE DOING SOMETHING HAVE BECOME “WRONG“ FIXATED ON 1 2 3 SELF-WORTH FOOD DIARY FEELINGS How to break the cycle The binge-restrict cycle can happen to anybody, not just Be prepared to Work on a Understand work on yourself. food–mood diary. your feelings. those with eating disorders. If you ever find yourself stuck Analyze your expectations See pages 96–97. Eat three What is your eating style? in the binge-restrict cycle, and values and learn to balanced meals a day and Are there a lot of rules? grow your self-worth and two to three snacks; enough Feelings of guilt or sadness? think about the following: self-esteem. Seek the help to maintain your body How do you feel if you break of a professional therapist weight—you need to get out one of your rules, and how if necessary. of the binge-restrict cycle do you respond? before trying to lose weight. UUSS__220088--220099__bbiinnggee..iinndddd 220088 2255//0088//22002211 1199::1155
220088 CCAANN WWEE EEAATT TTOO SSUUPPPPOORRTT MMEENNTTAALL HHEEAALLTTHH?? 220099 The binge-restrict cycle HOW CAN I STOP BINGE EATING? Restricting unhealthy foods may seem FOODS like the right thing to do, but it can kick- CONSIDERED ‘‘BAD’’ Removing the restriction element start a pattern of bingeing. OR ASSOCIATED c Ra en se b ar re ca hk s t uh ge g b ei sn tg se t- hre as t t ir fi c wt e c ay lc ll oe w. Binge eating is often explained as eating when we’re not hungry or WITH SHAME OR ourselves to eat items we enjoy, we overeating to provide a temporary distraction from something painful. But the GUILT ARE are unlikely to want to binge on them. RESTRICTED relationship between food and mood goes far beyond that specific moment. SW HE A F ME E S EL F E LO FR - CL RA O EC N SK TI TRN RO IG L C T CA I ON N ND T TIA NK TE U, HI E N CG YCLE SHAME RESTRICTION OBSESSION F O R BB IE DC DR A EE U NS ST ER II TC T I SA E B D O FU OW T O TE DH O EB S E S S E f c t ilo ha lm r nn a t e ao b s bt l sei a io c n wcn o gk hma e ol e- e rf ee e aa ac tt po ici p nn eo oi gg mn p di g p ls ei u ss o wok lf si rt hil de ol on s en . r e.Aa aI i s tw n t ’d lasa a y i rsf m go eo r erf p i os qoc o uo urm t sm aa nme np t, t e iet tn t nh io es ti s sa an t lp o oi n fa t t eg t e rn h d r e ine vo i sf dew f t n ii r v c iw iscu dhte l u t a ce amo ya lr c t e t l. p oeI a af n( ri s ednw e s f ee teu r mlo il e c,v bf twe t a) tr e r. he rAca eaat i s r n b si n m fig n o r e e oge t n de s c t p i , ia s no wu tfn g ho as h kl ie l c et o h t wi to n oc e s at d co nh o m be mc ye ab p t ugih en sui negn i sle ag t a- n, t e . food can feel like they are out of control, which Restriction triggers obsessive thoughts of food and can be incredibly distressing. Binges can leave the so the cycle begins again. If you can spend time individual feeling disconnected from what they’re developing new coping strategies for emotional doing during a binge. They can even forget what issues, it may help you get to the core of what is they have eaten afterward. really going on and beat the binge-restrict cycle. If you believe you’re suffering from any form of GUILT BINGEING AM I BINGE EATING? disordered eating, you should seek professional help Everyone is an emotional eater to some degree. at the earliest opportunity. Your doctor should be Stress, boredom, anxiety, or sleep-deprivation can your first point of contact as they can refer you to a A SENSE OF WE BINGE ON GUILT FOLLOWS FOR THE FOOD THAT WE make us want to eat more of different foods (or not qualified health professional. Alternatively, seek DOING SOMETHING HAVE BECOME eat). The problem occurs when our emotions rule support from a registered therapist. “WRONG“ FIXATED ON 1 2 3 4 5 6 SELF-WORTH FOOD DIARY FEELINGS TRIGGERS NEGATIVE THOUGHTS KINDNESS How to break the cycle The binge-restrict cycle can happen to anybody, not just Be prepared to Work on a Understand Identify your triggers. Challenge negative thoughts. Be kind to yourself. work on yourself. food–mood diary. your feelings. those with eating disorders. See whether you can recognize Consider naming that inner critic Remember that your body needs If you ever find yourself stuck Analyze your expectations See pages 96–97. Eat three What is your eating style? yourself in any of the common binge inside your head. You may be surprised food for fuel. If your body is receiving in the binge-restrict cycle, and values and learn to balanced meals a day and Are there a lot of rules? eating triggers below and think about how to find that you are bullying yourself— the nutrition it needs, you are less think about the following: grow your self-worth and two to three snacks; enough Feelings of guilt or sadness? these states might affect you throughout a sign that some self-soothing likely to suffer from ill health. You self-esteem. Seek the help to maintain your body How do you feel if you break your day: Anger | Anxiety | Worry | is required. deserve to eat and enjoy food. of a professional therapist weight—you need to get out one of your rules, and how Fear Depression | Negativity | if necessary. of the binge-restrict cycle do you respond? Boredom Guilt | Shame before trying to lose weight. UUSS__220088--220099__bbiinnggee..iinndddd 220088 2255//0088//22002211 1199::1155 UUSS__220088--220099__bbiinnggee..iinndddd 220099 2244//0088//22002211 1166::2244
210 CAN WE EAT TO SUPPORT MENTAL HEALTH? 211 AM I SUFFERING FROM DISORDERED EATING? An eating disorder is a complex mental illness and is often deeply misunderstood. Anyone, of any gender, age, ethnicity, shape, and size, can get one—they do not discriminate. A healthy relationship with food enables you to eat a variety of foods in a flexible and spontaneous way. Recognizing the This relationship can look different to us all, but symptoms ultimately it means that food does not interfere EATING DISORDERS MANIFEST with your life and you do not have to live by certain DIFFERENTLY FROM PERSON TO dietary rules, such as only eating carbohydrates if PERSON, WHICH CAN MAKE THEM TRICKY TO SPOT. HERE ARE SOME you have exercised that day. SIGNS TO LOOK OUT FOR: WHAT IS DISORDERED EATING? BEHAVIORAL SIGNS Spending a lot of time worrying People with eating disorders use disordered eating about your weight and body shape as a way of coping with difficult situations or Avoiding socializing when you think feelings. This behavior can include limiting food food will be involved intake; eating very large quantities of food at once; Eating very little food getting rid of eaten food through unhealthy means Making yourself vomit or taking (e.g., vomiting, misusing laxatives, fasting, or laxatives after you eat exercising excessively); or a combination of these. Exercising too much There’s no single cause, and sufferers might not Having very strict habits or routines have all the symptoms of any one eating disorder. around food Perhaps the most commonly recognized is anorexia Changes in your mood, such as being withdrawn, anxious, or depressed nervosa, but you don’t have to be underweight to have anorexia. And you can develop symptoms of PHYSICAL SIGNS one eating disorder that then change over time and Feeling cold, tired, or dizzy transform into another; for example, symptoms of Pains, tingling, or numbness in your anorexia can develop into a diagnosis for bulimia. arms and legs (poor circulation) Stereotypes about those who get eating disorders Racing heartbeat, fainting, or might make them even harder to spot among older feeling faint people, men and boys, and ethnic and cultural Problems with your digestion, such minority groups. Many people are diagnosed as bloating, constipation, or diarrhea with “other specified feeding or eating disorder” Your weight being very high or very low for someone of your age and height (OSFED), which means their symptoms don’t Not getting your period or other delayed exactly match what doctors check for to diagnose signs of puberty binge-eating disorder, anorexia, or bulimia, but that doesn’t mean that it’s not still very serious. UUSS__221100--221111__ddiissoorrddeerrss..iinndddd 221100 2244//0088//22002211 1166::2244
221100 CCAANN WWEE EEAATT TTOO SSUUPPPPOORRTT MMEENNTTAALL HHEEAALLTTHH?? 221111 AM I SUFFERING FROM EATING DISORDERS AFFECT AT LEAST DISORDERED EATING? THE PREVALENCE RATES OF EATING ANOREXIA 9% 5% DISORDERS BREAK DOWN AS FOLLOWS: NERVOSA An eating disorder is a complex mental illness and is often deeply OF THE POPULATION misunderstood. Anyone, of any gender, age, ethnicity, shape, and size, WORLDWIDE can get one—they do not discriminate. BULIMIA AROUND 8% 70 NERVOSA A healthy relationship with food enables you to eat million a variety of foods in a flexible and spontaneous way. Recognizing the PEOPLE This relationship can look different to us all, but symptoms INTERNATIONALLY ARFID LIVE WITH EATING ultimately it means that food does not interfere EATING DISORDERS MANIFEST DISORDERS with your life and you do not have to live by certain DIFFERENTLY FROM PERSON TO 19% AVOIDANT/ RESTRICTIVE dietary rules, such as only eating carbohydrates if PERSON, WHICH CAN MAKE THEM FOOD INTAKE you have exercised that day. TRICKY TO SPOT. HERE ARE SOME BINGE- DISORDER SIGNS TO LOOK OUT FOR: 22% AROUND EATING WHAT IS DISORDERED EATING? BEHAVIORAL SIGNS DISORDER 9% Spending a lot of time worrying People with eating disorders use disordered eating about your weight and body shape OF THE U.S. as a way of coping with difficult situations or POPULATION Avoiding socializing when you think WILL SUFFER feelings. This behavior can include limiting food food will be involved AN EATING intake; eating very large quantities of food at once; Eating very little food DISORDER getting rid of eaten food through unhealthy means OSFED DURING Making yourself vomit or taking THEIR LIFE (e.g., vomiting, misusing laxatives, fasting, or laxatives after you eat 47% OTHER SPECIFIED exercising excessively); or a combination of these. Exercising too much SOME FEEDING OR 28.8 There’s no single cause, and sufferers might not Having very strict habits or routines EATING DISORDER have all the symptoms of any one eating disorder. around food million Perhaps the most commonly recognized is anorexia Changes in your mood, such as being PEOPLE withdrawn, anxious, or depressed nervosa, but you don’t have to be underweight to OF WHOM have anorexia. And you can develop symptoms of PHYSICAL SIGNS 1/3 one eating disorder that then change over time and Feeling cold, tired, or dizzy transform into another; for example, symptoms of WHAT HELP CAN I GET? Sometimes people think their eating disorder isn’t ARE MALE Pains, tingling, or numbness in your anorexia can develop into a diagnosis for bulimia. If you suspect you have an eating disorder, try serious enough and don’t want to waste people’s arms and legs (poor circulation) Stereotypes about those who get eating disorders to see your primary care physician (PCP) right time; or they feel guilty, ashamed, or embarrassed. Racing heartbeat, fainting, or might make them even harder to spot among older feeling faint away—the sooner, the better. Ask for a double PCPs don’t specialize in eating disorders and LESS THAN people, men and boys, and ethnic and cultural Problems with your digestion, such appointment to give you time to talk. You can may harbor misconceptions. Look for local 6% minority groups. Many people are diagnosed as bloating, constipation, or diarrhea bring a support person, though some PCPs may support groups that can help you get connected with “other specified feeding or eating disorder” Your weight being very high or very low ask to speak to you alone for some of the time. to help. If a referral to a specialist isn’t the result for someone of your age and height OF PEOPLE (OSFED), which means their symptoms don’t Give as much information as possible; try noting of your appointment, you have the right to ask to DIAGNOSED exactly match what doctors check for to diagnose Not getting your period or other delayed symptoms and concerns ahead of time. Your PCP see a different PCP. Don’t be daunted and don’t WITH EATING signs of puberty DISORDERS ARE binge-eating disorder, anorexia, or bulimia, but should refer you to a specialist who can assess give up; you deserve treatment, and research MEDICALLY that doesn’t mean that it’s not still very serious. your needs and develop a plan for treatment. suggests recovery is possible at any time. UNDERWEIGHT UUSS__221100--221111__ddiissoorrddeerrss..iinndddd 221100 2244//0088//22002211 1166::2244 UUSS__221100--221111__ddiissoorrddeerrss..iinndddd 221111 2244//0088//22002211 1166::2244
212 INDEX 213 INDEX A amniotic fluid 186 natural sweet tooth 192, 194 abdominal fat 91, 171 anaphylactic shock 158 premature babies 141, 185 açai 74 anemia 10, 23, 130, 137, 189 sudden infant death syndrome acid reflux 26, 161 anorexia nervosa 210, 211 (SIDS) 191 acidic foods 174 anthocyanins 17, 37, 38 weaning 187, 191, 192–193 additives 58, 59, 159 anti-inflammatory agents 74, 75, 144 see also children; fetal development adenosine 72 anti-inflammatory diet 168–169 “balanced plate” concept 40–41 adipose tissue 30, 105 antibiotics 51, 52, 107, 136 bananas 75, 82 adrenaline 72 antibodies 136, 191 basal metabolic rate (BMR) 30 advanced glycation end products antioxidants 10, 21, 37, 38, 45, 53, 72, beets 77 (AGEs) 149, 150 73, 123, 148, 149, 150 juice 80, 81 aging process 148–149 appetite 73, 78, 104, 105 bell peppers 74, 75, 131 agriculture industry 124 ARFID (avoidant/restrictive food berries 38, 74, 150, 202 water use 126 intake disorder) 211 beta-amyloid 150, 151 ALA (alpha-linolenic-acid) 130 arginine 198, 199 beta-glucan 17 alcohol 73, 142, 144, 146, 156, 161, arthritis 48, 140, 169 beta-glucose monomers 18, 19 178, 189 asthma 140, 160, 161, 169, 188 Bifidobacteria 51, 52 calories 73 Atkins Diet 110 bile 26, 74 during pregnancy 185 autoimmune diseases 140, 160 binge eating 115, 203, 208–209, 211 health problems 73 avocados 17, 67, 75 binge-restrict cycle 208–209 recommended weekly limits 73 triggers 209 algae oil 43 bloating 19, 48, 52, 144, 152–153, 158, B “alkaline diet” 174, 175 159, 160, 161, 164, 167 alliums 53 B vitamins 21, 45, 68 causes 19, 152, 153 almonds 109, 127 sources of 21 treatment 153 alpha-glucose monomers 19 see also vitamin B6; vitamin B9; blood clotting 16, 21, 48 Alzheimer’s disease 37, 48, 90, vitamin B12 blood pH 174, 175 150, 151 babies and infants blood pressure amino acids 12, 14, 15, 33, 44, 128, 142 breastfeeding 24, 32, 43, 51, 130, blood pressure control 23, 72 branch-chain amino acids 138, 141, 149, 187, 189, 190–191, high blood pressure 37, 54, 64, 68, (BCAAs) 81 197, 198 70–71, 72, 180 children and 198, 199 fetal development 178, 180–181, blood sugar 13, 30, 37, 86, 150, 170, conditional 198, 199 186–187 171, 203 essential 14, 15 food allergies 188 blood sugar levels 18, 26, 30, 31, 48, peptides 15 formula milks 191 97, 121, 172, 202, 203 polypeptides 15 gut bacteria 51, 140–141 blood-sugar crash 202, UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221122 2244//0088//22002211 1166::2244
221122 IINNDDEEXX 221133 INDEX A amniotic fluid 186 natural sweet tooth 192, 194 203, 207 butter 67 carotenoids 75 abdominal fat 91, 171 anaphylactic shock 158 premature babies 141, 185 blood-sugar rush 202, 203 butternut squash 20 carrots 74, 75, 194, 195 açai 74 anemia 10, 23, 130, 137, 189 sudden infant death syndrome testing 172 butyrate 48 cauliflower 193 acid reflux 26, 161 anorexia nervosa 210, 211 (SIDS) 191 BMI (body mass index) 47, 91, 99 celiac disease 140, 160–161, 162, acidic foods 174 anthocyanins 17, 37, 38 weaning 187, 191, 192–193 measuring 91 165, 169 C additives 58, 59, 159 anti-inflammatory agents 74, 75, 144 see also children; fetal development “body clock” 102 cell respiration 171 adenosine 72 anti-inflammatory diet 168–169 “balanced plate” concept 40–41 body fat 12, 33, 90 caffeine 72, 142, 144, 146, 161, 178, cellulose 18, 19 adipose tissue 30, 105 antibiotics 51, 52, 107, 136 bananas 75, 82 abdominal fat 91, 171 185, 189, 204 cheeses 95, 182–183 adrenaline 72 antibodies 136, 191 basal metabolic rate (BMR) 30 exercise and fat burning 86–87 calcium 22, 38, 74, 113, 120, 130, 131, chemotherapy 174 advanced glycation end products antioxidants 10, 21, 37, 38, 45, 53, 72, beets 77 subcutaneous fat 90 144, 147, 149, 156, 169, 189, 198 chewing food 104, 109, 207 (AGEs) 149, 150 73, 123, 148, 149, 150 juice 80, 81 visceral fat 90 sources of 22, 145 chia seeds 74 aging process 148–149 appetite 73, 78, 104, 105 bell peppers 74, 75, 131 body image calories 31 children agriculture industry 124 ARFID (avoidant/restrictive food berries 38, 74, 150, 202 body diversity 205 burning 84, 86 constipation 155 water use 126 intake disorder) 211 beta-amyloid 150, 151 unhealthy 101 calorie counting 100 healthy eating habits, establishing ALA (alpha-linolenic-acid) 130 arginine 198, 199 beta-glucan 17 bone mineral density 147, 149 recommended daily intake 54, 85 196–197 alcohol 73, 142, 144, 146, 156, 161, arthritis 48, 140, 169 beta-glucose monomers 18, 19 borage oil 169 Campylobacter 156 hydration 24 178, 189 asthma 140, 160, 161, 169, 188 Bifidobacteria 51, 52 bowel cancer 68, 159, 165 cancers 19, 37, 44, 90, 148, 169 malnutrition 32, 33 calories 73 Atkins Diet 110 bile 26, 74 brain 12, 24, 42, 105, 106, 175 food myths 174–175 nutritional needs 196–197, 198 during pregnancy 185 autoimmune diseases 140, 160 binge eating 115, 203, 208–209, 211 dementia and 150, 151 risk reduction 37 obesity 54 health problems 73 avocados 17, 67, 75 binge-restrict cycle 208–209 development 180–181, 198 see also specific cancers picky eaters 194–195 recommended weekly limits 73 triggers 209 gut-brain axis 48–49, 152, 153, 164, canned foods 43, 58, 71 plant-based eating 198–199 algae oil 43 bloating 19, 48, 52, 144, 152–153, 158, 167, 203 canola oil 38, 39, 66 poor diet 56 B “alkaline diet” 174, 175 159, 160, 161, 164, 167 inflammation 202 carbohydrates 10, 11, 12–13, 40, 78 snacking 194, 197 alliums 53 B vitamins 21, 45, 68 causes 19, 152, 153 Brazil nuts 38, 131, 137, 149 complex carbs 12, 13, 207 supplements 197, 198 almonds 109, 127 sources of 21 treatment 153 bread 13, 45, 54, 71, 173 digestion 12–13, 29 sweet tooth 194 alpha-glucose monomers 19 see also vitamin B6; vitamin B9; blood clotting 16, 21, 48 breakfast, skipping 102, 115, 207 exercise and 78, 82, 83 toddlers 196–197, 198 Alzheimer’s disease 37, 48, 90, vitamin B12 blood pH 174, 175 breast cancer 146–147, 190 FODMAPs 156, 161, 166–167 see also babies and infants 150, 151 babies and infants blood pressure breastfeeding 24, 32, 43, 51, 130, 138, health benefits 12 chloride 22 amino acids 12, 14, 15, 33, 44, 128, 142 breastfeeding 24, 32, 43, 51, 130, blood pressure control 23, 72 139, 141, 149, 187, 189, 190–191, low-carb diets 110–111, 173 chocolate 204 branch-chain amino acids 138, 141, 149, 187, 189, 190–191, high blood pressure 37, 54, 64, 68, 197, 198 refined carbs 13, 32, 45, 150, 169, cholesterol 16, 17, 18 (BCAAs) 81 197, 198 70–71, 72, 180 benefits of 190, 191 202, 203 high-density lipoproteins (HDL) children and 198, 199 fetal development 178, 180–181, blood sugar 13, 30, 37, 86, 150, 170, colostrum 191 simple carbs 12, 13 17, 37, 67 conditional 198, 199 186–187 171, 203 Bristol stool chart 27 slow-release carbs 13, 203 low-density lipoproteins (LDL) essential 14, 15 food allergies 188 blood sugar levels 18, 26, 30, 31, 48, broccoli 12, 192 sources of 13, 133 17, 18, 66, 67, 68, 111 peptides 15 formula milks 191 97, 121, 172, 202, 203 bulgur 45 starch 12, 18–19, 39, 40, 44 choline 180, 198 polypeptides 15 gut bacteria 51, 140–141 blood-sugar crash 202, bulimia 210, 211 carbonated drinks 25, 65, 144, 153 Christensenellaceae 107 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221122 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221133 2244//0088//22002211 1166::2244
214 INDEX 215 D chyme 26 chronic 156 circadian rhythm 12, 102 dairy 41, 124, 130 electrolyte loss 156 coconut oil 67 dairy alternatives 41, 163 food triggers 156 cod liver oil 138 unpasteurized 182 diffusion 26, 28–29 coffee 72, 156, 185 dehydration 113, 156 digestion 12–13, 18, 26–29 see also caffeine rehydration powders 156 nutrient absorption 28–29 cognitive health 37, 149 signs of 25 process 26–27 collagen 21, 148, 149 dementia 33, 150–151 dips and spreads 132 colon cancer 18, 48, 54 Alzheimer’s disease 37, 48, 90, disaccharides 12, 161, 166 colostrum 191 150, 151 diuretics 113 comfort eating 12, 205, 206 diet and 150 donuts, vegan 122 constipation 18, 25, 111, 132, 152, symptoms 150 dopamine 104 153, 154, 155, 164, 173 depression 48, 73, 202 Dukan Diet 110 children 155 detox diets 112 dysbiosis 48 constipation-predominant detox teas 112–113 dysmenorrhea 144 IBS 155 detoxification by the liver 112 dysphagia 33 evacuation disorder 155 DHA (docosahexaenoic acid) 42, relief of 155 43, 130 E slow transit constipation 155 diabetes, gestational 181 symptoms 155 diabetes type 1 170, 171 eating disorders 32, 115, 210–211 cooking diabetes type 2 16, 18, 19, 37, 44, 48, anorexia nervosa 210, 211 benefits of home cooking 47 54, 64, 90, 148, 170–173 ARFID (avoidant/restrictive food cost-effectiveness 47 dementia risk and 150 intake disorder) 211 fish 43 diagnosis 172 behavioral signs 210 from scratch 46–47 dietary management of 111, binge eating 115, 203, 208–209, 211 high-temperature cooking 172–173 bulimia 210, 211 43, 150 ethnicity and 171 OSFED (other specified feeding or meat 69 genetic factors 171, 172 eating disorder) 210, 211 vegetables 46–47, 123 low-carb diet and 111, 173 physical signs 210 corn chips 59 remission 172 prevalence of 211 cortisol 95, 104, 142 risk factors for 54, 107, 171, 172 seeking help 211 cramps 23, 113, 144, 145, 160, 161 risk reduction 37 eczema 48, 140, 159, 169, 188 creatine 81 diabetic ketoacidosis (DKA) 170 edamame beans 83, 129, 174 Crohn’s disease 32, 48, 165 diarrhea 51, 52, 113, 154, 156–157, 158, eggplants 12, 20 curcumin 74, 75 159, 161, 164, 165 eggs 139 cysteine 198, 199 causes of 23, 156 raw or undercooked 182, 183 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221144 2244//0088//22002211 1166::2244
221144 IINNDDEEXX 221155 D chyme 26 chronic 156 elastin 148, 149 children and 197 frozen and canned 43, 47 circadian rhythm 12, 102 dairy 41, 124, 130 electrolyte loss 156 electrolyte balance 83, 113, 156 digestion 29 oily 17, 42, 43, 138, 139, 149, 168, coconut oil 67 dairy alternatives 41, 163 food triggers 156 emotional eating see binge eating exercise and 78 169, 183 cod liver oil 138 unpasteurized 182 diffusion 26, 28–29 energy 10, 11, 12, 13, 30, 31, 32, 78 food label information 61 shellfish 179, 181, 183 coffee 72, 156, 185 dehydration 113, 156 digestion 12–13, 18, 26–29 EPA (eicosapentaenoic acid) 42, monounsaturated fats 16, 38, fish oil supplements 43, 138, 169 see also caffeine rehydration powders 156 nutrient absorption 28–29 43, 130 66, 67 fitness trackers 100 cognitive health 37, 149 signs of 25 process 26–27 Epsom salts 145 polyunsaturated fats 16, 42, 66 5:2 diet 114 collagen 21, 148, 149 dementia 33, 150–151 dips and spreads 132 Escheria coli (E. coli) 107, 156, 157 saturated fats 16, 54, 66, 67, flatulence 154 colon cancer 18, 48, 54 Alzheimer’s disease 37, 48, 90, disaccharides 12, 161, 166 esophagus 26 68, 197 flaxseed 169 colostrum 191 150, 151 diuretics 113 estrogen 144, 146, 153, 174, sources 16 flexitarians 119 comfort eating 12, 205, 206 diet and 150 donuts, vegan 122 178, 190 trans fats 16, 17, 54, 66 flight-or-fight mode 72, 142 constipation 18, 25, 111, 132, 152, symptoms 150 dopamine 104 evening primrose oil 169 triglycerides 16, 30 FODMAPs 156, 161, 166–167 153, 154, 155, 164, 173 depression 48, 73, 202 Dukan Diet 110 exercise 78–87, 205 unsaturated fats 16, 41, 149 large bowel hypothesis 167 children 155 detox diets 112 dysbiosis 48 aerobic 84, 86 see also omega-3 and omega-6 fats low-FODMAP diet (LFD) 167 constipation-predominant detox teas 112–113 dysmenorrhea 144 cardio 86 fecal transplant 51 small bowel hypothesis 166–167 IBS 155 detoxification by the liver 112 dysphagia 33 fat burning 86–87 feces 27, 113 folate 136, 178 evacuation disorder 155 DHA (docosahexaenoic acid) 42, health benefits 84, 178 fecal transplant 51 folic acid 178 relief of 155 43, 130 hydration 79, 82 fermented foods 51, 52–53, 128, food allergies and intolerances 32, E slow transit constipation 155 diabetes, gestational 181 interval training 72, 86–87 132, 147, 166 153, 154, 156, 158–163 symptoms 155 diabetes type 1 170, 171 eating disorders 32, 115, 210–211 meals and 82 fertility 23, 90, 178–179 adverse immune response 158 cooking diabetes type 2 16, 18, 19, 37, 44, 48, anorexia nervosa 210, 211 recovery foods 83 diet and 178–179 babies and toddlers 188 benefits of home cooking 47 54, 64, 90, 148, 170–173 ARFID (avoidant/restrictive food resistance training 78, 149 fetal alcohol syndrome (FAS) 185 common triggers 159, 160, 161 cost-effectiveness 47 dementia risk and 150 intake disorder) 211 sports nutrition 78–83 fetal development 178, 180–181 cross-contamination 163 fish 43 diagnosis 172 behavioral signs 210 sports supplements 80–81 brain development 180–181 diagnosis 160–161 from scratch 46–47 dietary management of 111, binge eating 115, 203, 208–209, 211 target heart rate 86–87 food preferences 186–187 dual-allergen exposure hypothesis high-temperature cooking 172–173 bulimia 210, 211 weight loss 84–85, 95 fiber 13, 18–19, 31, 44, 45, 48, 51, 158–159 43, 150 ethnicity and 171 OSFED (other specified feeding or 52, 111, 121, 154, 155, 172, 173 elimination diets 161 meat 69 genetic factors 171, 172 eating disorder) 210, 211 bloating and 153 genetic factor 161 F vegetables 46–47, 123 low-carb diet and 111, 173 physical signs 210 daily intake 19, 51 gut health and 140 corn chips 59 remission 172 prevalence of 211 farro 45 food label information 63 IgE-mediated allergies cortisol 95, 104, 142 risk factors for 54, 107, 171, 172 seeking help 211 farting 154 high-fiber diet 121 158–159, 161 cramps 23, 113, 144, 145, 160, 161 risk reduction 37 eczema 48, 140, 159, 169, 188 fast food 54 insoluble 18, 26, 156 non-IgE-mediated allergies 158 creatine 81 diabetic ketoacidosis (DKA) 170 edamame beans 83, 129, 174 fasting 174 soluble 18 nonimmune response 159 Crohn’s disease 32, 48, 165 diarrhea 51, 52, 113, 154, 156–157, 158, eggplants 12, 20 intermittent fasting (IF) 114–115 sources of 19, 51 oral allergy syndrome 160 curcumin 74, 75 159, 161, 164, 165 eggs 139 fat see body fat; fat, dietary fish 42–43, 183 symptoms 158, 159 cysteine 198, 199 causes of 23, 156 raw or undercooked 182, 183 fat, dietary 10, 16–17, 41, 66–67 cooking 43 food cravings 94, 104, 111, 184–185 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221144 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221155 2244//0088//22002211 1166::2244
216 INDEX 217 see also hunger glucagon 31 (SIBO) 153 food diary 96–97, 208 glucose 12–13, 19, 30, 31, 110, visceral hypersensitivity 153 food labels 60–63 170, 203 weight loss and 106–107 buzzwords 62–63 see also blood sugar nutritional information 60–61 glutamine 198, 199 H food poisoning 19, 157 gluten intolerance 160, 161 food, relationship with eliminating gluten from your hay fever 161 intuitive eating 104, 204–205 diet 162–163 heart disease 16, 17, 18, 19, 37, 43, 44, mindful eating 102, 104, 172, 205, nonceliac gluten sensitivity 54, 66, 68, 70–71, 73, 90, 107, 111, 206–207 161 123, 147, 148 poor see eating disorders sources of gluten 162 risk reduction 37, 44 food waste 125 glycemic index (GI) 13, 148 hedonic eating 56, 103, 104 formula milks 191 glycation 148, 172 hemoglobin 130, 172 fortified foods 58, 129, 139 glycine 198 hepatocytes 112 fossil fuels 124 glycogen 13, 30, 33, 72, 78, 82, 83, 86, herbal teas 185 free radicals 21, 148, 149, 150 87, 110–111, 114, 149, 171 high blood pressure 37, 54, 68, 70–71, frozen food 47, 58, 125 advanced glycation end products 72, 180 fructans 156, 166 (AGEs) 149, 150 high-fructose corn syrup 58, 64 fructose 12, 58, 64, 161, 166 goji berries 74, 75 histidine 198, 199 fruit juices 64, 172 grazing 207 hives 158 fruits and vegetables 13, 17, 39, 40, green tea 185 homeostasis 175 41, 133 greenhouse gas emissions 124, homeostatic hunger 104 see also individual index entries 125, 126 hormones 26, 104, 191 growing your own food 125 see also specific hormones gut health 48–53, 132 hot flashes 146 G babies 51, 140–141 hummus 199 galactose 12 feeding your gut microbiome hunger 104–105, 205 gallbladder 26 50–51 cues 205 garlic 53, 186 gut bacteria 19, 27, 48–53, 106–107, hedonic hunger 56, 103, 104 gas 27, 48, 152, 153, 154, 167 111, 132, 152, 153, 154, 155, 166, homeostatic hunger 104 flatulence 154 191, 202, 203 hunger pangs 26, 105, 206 foul-smelling 154 gut-brain axis 48–49, 152, 153, 164, managing 104 gastritis 156 167, 203 sleep deprivation and 142 ghrelin 26, 92, 104, 105, 143 immune response and 140–141 tiredness and 104 giardiasis 156 prebiotics and probiotics 52–53 hydration 24–25, 40, 189 ginger 74, 144 small intestine bacterial overgrowth daily water requirement 24, 25 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221166 2244//0088//22002211 1166::2244
221166 IINNDDEEXX 221177 see also hunger glucagon 31 (SIBO) 153 during pregnancy 185, 189 intuitive eating (IE) 104, 204–205 ketones 30, 170 food diary 96–97, 208 glucose 12–13, 19, 30, 31, 110, visceral hypersensitivity 153 exercise and 79, 82 inulin 52 kidneys 24, 79, 175 food labels 60–63 170, 203 weight loss and 106–107 urine color 25 iodine 22–23, 76, 120, 130, 163, 198 kimchi 53 buzzwords 62–63 see also blood sugar hygiene 156 deficiency 130 kombucha 53 nutritional information 60–61 glutamine 198, 199 hyperglycemia 170, 171, 172 sources of 34, 123 krill oil 43 H food poisoning 19, 157 gluten intolerance 160, 161 hypernatremia 79 iron 13, 22, 23, 68, 76, 120, 130, 131, L food, relationship with eliminating gluten from your hay fever 161 hypertension see high blood pressure 137, 144, 169, 198 intuitive eating 104, 204–205 diet 162–163 heart disease 16, 17, 18, 19, 37, 43, 44, hypoglycemia 173 deficiency 10, 23, 130 lactase 161, 163 mindful eating 102, 104, 172, 205, nonceliac gluten sensitivity 54, 66, 68, 70–71, 73, 90, 107, 111, hyponatremia 79 sources of 23, 123, 137, 145, 181 lacto-ovo vegetarians 118 206–207 161 123, 147, 148 hypothalamus 105 supplements 181, 198 lacto-vegetarians 118 poor see eating disorders sources of gluten 162 risk reduction 37, 44 irritable bowel syndrome (IBS) 48, Lactobacillus 51, 52, 107 food waste 125 glycemic index (GI) 13, 148 hedonic eating 56, 103, 104 51, 52, 72, 153, 155, 161, lactofermentation 53 I formula milks 191 glycation 148, 172 hemoglobin 130, 172 164–165, 169 lactose 12, 166, 191 fortified foods 58, 129, 139 glycine 198 hepatocytes 112 IgA cells 140 diagnosis 164 sources of 163 fossil fuels 124 glycogen 13, 30, 33, 72, 78, 82, 83, 86, herbal teas 185 immune system 23, 48, 49, 169, 191 low-FODMAP diet (LFD) 167 lactose intolerance 153, 159, 160–161 free radicals 21, 148, 149, 150 87, 110–111, 114, 149, 171 high blood pressure 37, 54, 68, 70–71, “ALT” immune systems 141 prevalence of 164 eliminating lactose from your frozen food 47, 58, 125 advanced glycation end products 72, 180 boosting 136–137 subtypes 155, 164 diet 163 fructans 156, 166 (AGEs) 149, 150 high-fructose corn syrup 58, 64 gut health and 140–141 symptoms 164 large intestine 13, 26–27, 28, 29, 152, fructose 12, 58, 64, 161, 166 goji berries 74, 75 histidine 198, 199 immune response 15, 136–137 treatment 164 155, 165, 166, 167 fruit juices 64, 172 grazing 207 hives 158 inflammation 37, 67, 75, 102, 150, islets of Langerhans (pancreatic laxatives 113, 153, 155 fruits and vegetables 13, 17, 39, 40, green tea 185 homeostasis 175 169, 202 islets) 30 legumes 17, 44, 121 41, 133 greenhouse gas emissions 124, homeostatic hunger 104 anti-inflammatory agents 74, isoflavones 146–147 lentils 14, 44, 123, 129 see also individual index entries 125, 126 hormones 26, 104, 191 75, 144 isoleucine 81 leptin 26, 92, 105, 143 growing your own food 125 see also specific hormones anti-inflammatory diet 168–169 leucine 80, 81 gut health 48–53, 132 hot flashes 146 dietary causes 169 life expectancy 37, 121, 149 G J babies 51, 140–141 hummus 199 inflammatory markers 107 liquid meals 108–109 galactose 12 feeding your gut microbiome hunger 104–105, 205 inflammatory bowel disease (IBD) jackfruit 123 liver 12, 17, 30, 48, 111, 112, 128, 183 gallbladder 26 50–51 cues 205 see Crohn’s disease; ulcerative Japanese diet 37, 38–39 low-carb diets 110–111, 173 garlic 53, 186 gut bacteria 19, 27, 48–53, 106–107, hedonic hunger 56, 103, 104 colitis juicing 174 low-fat diets 66, 111 gas 27, 48, 152, 153, 154, 167 111, 132, 152, 153, 154, 155, 166, homeostatic hunger 104 insomnia 142 junk food 32, 187 low-FODMAP diet (LFD) 167 flatulence 154 191, 202, 203 hunger pangs 26, 105, 206 insulin 15, 30, 31, 150, 170–171, lutein 149 foul-smelling 154 gut-brain axis 48–49, 152, 153, 164, managing 104 202, 203 lycopene 75, 149 K gastritis 156 167, 203 sleep deprivation and 142 insulin resistance 16, 107, 170, 171 lymphatic system 141 ghrelin 26, 92, 104, 105, 143 immune response and 140–141 tiredness and 104 insulin sensitivity 64, 170, 172, 173 kefir 51, 53 lysine 126 giardiasis 156 prebiotics and probiotics 52–53 hydration 24–25, 40, 189 intermittent fasting (IF) 114–115 ketoacidosis 173 ginger 74, 144 small intestine bacterial overgrowth daily water requirement 24, 25 interval training 72, 86–87 ketogenic diet 111 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221166 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221177 2244//0088//22002211 1166::2244
218 INDEX 219 142, 148–149, 150, 169, 172, trace quantities 22 M 178, 202 see also specific minerals macronutrients 10 melatonin 12, 142 molasses 64 optimal intake 11 memory loss 33, 37 monosaccharides 12, 19, 161, 166 see also carbohydrates; fat; protein menopause 146–147 monosodium glutamate (MSG) macular degeneration 75 menstrual symptoms 144–145 59, 161 magnesium 22, 23, 144, 145 mental decline 149 mood sources of 23, 145 see also dementia comfort eating 12, 205, 206 malnutrition 32–33, 159 metabolic syndrome 48 diet and 202–203 children 32, 33 metabolism 30–31, 99, 100 emotional eating see binge eating elderly people 33 basal metabolic rate (BMR) 30 enhancing 12 physical causes 32 fasted (postabsorptive) state 30 low 12, 111, 144, 202, 203 physical impact 33 fed (adsorptive) state 30 swings 12, 144 signs of 32 resting metabolic rate (RMR) MSG 59, 161 social causes 32 84, 85 muscle mass 101 maltose 12, 64 methionine 126 muscle repair 83 manganese 23 micronutrients 10 muscle wastage 33, 33, 142, 149 mannitol 166 deficiencies 10 mushrooms 139 meals optimal intake 11 mycoprotein 126, 128, 129 “balanced plate” concept 40–41 see also minerals; vitamins eating in front of the television milk 74, 76, 126, 159, 197, 203 N 102, 206 dairy-free alternatives 163 meal planning 125 plant-based 126 neurons 151, 202 meal-replacement products unpasteurized 182 neurotransmitters 151, 153, 202 108–109 milk allergy 163 niacin 13 regular meal pattern 102 see also lactose intolerance night sweats 146 skipping 102, 114, 115, 206, 207 MIND (Mediterranean-DASH nitrates 68, 80, 81 social interaction 102, 109 Intervention for Neurogenerative nitrites 68 meat Delay) diet 150 non-exercise activity thermogenesis meat substitutes 129 mindful eating 102, 104, 172, 205, (NEAT) 85 processed meats 68, 69, 71 206–207 non-steroidal anti-inflammatory raw or undercooked 182–183 mindless eating habits 206–207 drugs (NSAIDs) 165 red meat 68–69 minerals 22–23 Nordic diet 36, 37, 38–39 reducing consumption of 124, 127 children and 198 nut butters 199 medications, overuse of 51 sources of 22–23 nutrition: definition of 10 Mediterranean diet 36, 37, 38–39, 43, supplements 23 nuts and seeds 17, 31, 38, 74, 109, 123, UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221188 2244//0088//22002211 1166::2244
221188 IINNDDEEXX 221199 O P 142, 148–149, 150, 169, 172, trace quantities 22 polyphenols 38, 73, 202 M 178, 202 see also specific minerals oats 18, 45 pancreas 26, 170, 203 polysaccharides 12, 13, 18, 19 macronutrients 10 melatonin 12, 142 molasses 64 obesity 32, 36, 37, 44, 48, 54, 58, papaya 74, 75 poor diet 54–57 optimal intake 11 memory loss 33, 37 monosaccharides 12, 19, 161, 166 90, 92, 178, 190, 191 parasites 156, 183 defining 54 see also carbohydrates; fat; protein menopause 146–147 monosodium glutamate (MSG) associated health problems pasta 13, 19 economic factors 56, 57 macular degeneration 75 menstrual symptoms 144–145 59, 161 90, 171 pâté 182 health implications 54 magnesium 22, 23, 144, 145 mental decline 149 mood children 54 peanut allergy 159, 188 healthier meal hacks 55 sources of 23, 145 see also dementia comfort eating 12, 205, 206 genetic factor 90 peptides 15 obesity and 54 malnutrition 32–33, 159 metabolic syndrome 48 diet and 202–203 gut bacteria and 107 periods reasons underlying 56–57 children 32, 33 metabolism 30–31, 99, 100 emotional eating see binge eating sleep deprivation and 142 menstrual symptoms 144–145 portion control 94, 172, 206 elderly people 33 basal metabolic rate (BMR) 30 enhancing 12 older people missed 33, 99 potassium 22, 23, 37, 67, 72, physical causes 32 fasted (postabsorptive) state 30 low 12, 111, 144, 202, 203 aging process 148–149 pains 144 82, 113, 156 physical impact 33 fed (adsorptive) state 30 swings 12, 144 hydration 24–25 premenstrual syndrome sources of 23 signs of 32 resting metabolic rate (RMR) MSG 59, 161 malnutrition 33 (PMS) 144 potatoes 12, 19 social causes 32 84, 85 muscle mass 101 protein requirement 15 pescatarians 119, 123 cooking 46–47 maltose 12, 64 methionine 126 muscle repair 83 see also dementia pesticides 76, 77 prebiotics 44, 51, 52–53, 121 manganese 23 micronutrients 10 muscle wastage 33, 33, 142, 149 oleic acid 16 phosphorus 22, 23, 76 pregnancy 21, 32, 43, 72, 130, mannitol 166 deficiencies 10 mushrooms 139 oligosaccharides 166 sources of 23 178–185 meals optimal intake 11 mycoprotein 126, 128, 129 olive oil 17, 36, 38, 39, 66 phytates 131 fertility and 178–179 “balanced plate” concept 40–41 see also minerals; vitamins omega-3 and omega-6 fats 16, phytic acid 123 food cravings 184–185 eating in front of the television milk 74, 76, 126, 159, 197, 203 42, 45, 66, 68, 76, 120, 130, phytoestrogens 146, 174 gestational diabetes 181 N 102, 206 dairy-free alternatives 163 144, 150, 169, 178, 198 planetary diet 127 hydration 185, 189 meal planning 125 plant-based 126 neurons 151, 202 sources of 145, 168, 181, 189 plant-based eating 118–133 postpartum nutrition 189 meal-replacement products unpasteurized 182 neurotransmitters 151, 153, 202 supplements 181, 188, 198 children 198–199 prenatal nutrition 108–109 milk allergy 163 niacin 13 onions 53 dietary combinations 119 180–183, 187 regular meal pattern 102 see also lactose intolerance night sweats 146 oral allergy syndrome 160 diversity, increasing 132–133 unsafe foods 182–183 skipping 102, 114, 115, 206, 207 MIND (Mediterranean-DASH nitrates 68, 80, 81 organic food 63, 76–77 environmental benefits 126–127 see also babies and infants social interaction 102, 109 Intervention for Neurogenerative nitrites 68 OSFED (other specified feeding features of 118 premenstrual syndrome meat Delay) diet 150 non-exercise activity thermogenesis or eating disorder) 210, 211 health benefits 118, 120–123 (PMS) 144 meat substitutes 129 mindful eating 102, 104, 172, 205, (NEAT) 85 osmosis 79, 167 health problems and 123 principles of healthy eating processed meats 68, 69, 71 206–207 non-steroidal anti-inflammatory osteopenia 147 nutrients 122–123, 128–131 40–41 raw or undercooked 182–183 mindless eating habits 206–207 drugs (NSAIDs) 165 osteoporosis 10, 147, 149, 190 sustainability 124–125 probiotics 51, 52–53, 169, 188 red meat 68–69 minerals 22–23 Nordic diet 36, 37, 38–39 ovo-vegetarians 118 see also veganism; vegetarianism processed foods 32, 46, 54, 56, reducing consumption of 124, 127 children and 198 nut butters 199 oxalates 131 PMS (premenstrual syndrome) 144 58–59, 71, 100, 150, 207 medications, overuse of 51 sources of 22–23 nutrition: definition of 10 oxidative stress 150 polyols 166 Mediterranean diet 36, 37, 38–39, 43, supplements 23 nuts and seeds 17, 31, 38, 74, 109, 123, oxytocin 190 polypeptides 15 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221188 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 221199 2255//0088//22002211 1199::1155
220 INDEX 221 additives 58, 59 red meat 68–69 senna 113 canned foods 43, 58, 71 cancer risk and 68, 69 serotonin 12, 49, 142, 202–203 fortified foods 58 cooking 69 shellfish 179, 181, 183 frozen food 47, 58, 125 nutritional benefits 68 shopping for food 125 highly processed foods 58, 195 red wine 73 short-chain fatty acids (SCFAs) 13, processed meat 68, 69, 71 resistance training 78, 149 19, 27, 51, 52, 202 vegan foods 118, 123 resting metabolic rate (RMR) 84, 85 16:8 eating plan 114 progesterone 144, 153, 190 rheumatoid arthritis (RA) 140, 169 skin aging 21, 148–149 prolactin 190 rice 14, 19, 45, 128 “skinny teas” 112–113 proline 198, 199 rice cakes 13 sleep deprivation 142, 143 propionate 48 rice milk 163, 197 sleep quality 72, 142–143 prostaglandins 144, 145, 178 rickets 10 slimness 90 protein 10, 12, 14–15, 30, 130, 154 small intestine 12, 18, 19, 26, 28, 29, amino acids see amino acids 113, 152, 157, 160, 167 S children and 197, 198 Crohn’s disease 32, 48, 165 complete proteins 14, 15, 128–129 salads, packaged 182 small intestine bacterial overgrowth daily intake 15, 41 saliva 26 (SIBO) 153 digestion 29 salt 54, 58, 68, 70–71 smart scales 101 exercise and 78, 83 children and 197 smoking 178 food label information 63 food label information 61 smoothies 83, 108, 109 incomplete proteins 14, 15, 128 guideline daily limit 70, 71 snacking 103, 203, 207 producing protein sources 124 and heart disease 70–71 children 194, 197 sources of 15, 41, 42, 45, 123, hidden salt 71 healthy snacks 103 129, 133 premium salts 70–71 late night 142 vegan diets and 15, 41, 128–129 reducing salt consumption 71 sleep-friendly 142 protein powder 81, 109 sardines 168 social media influencers 63 pulses 14, 44, 123, 126, 129, 153 satiation 104 sodium 22, 70, 71, 79, 113, 156 satiety 18, 26, 104, 109, 205 sofa, eating on the 207 sauerkraut 53 sorbitol 166 Q–R sausages 68 soups 109 quinoa 45, 123, 128–129 seasonal eating 132 soy 37, 123, 126, 128, 129, 146, Quorn 126, 129 seitan 128 147, 174 ready meals 46, 71, 182 selenium 22, 23, 68, 76, 131, 137, sperm quality 178, 179 rectal bleeding 165 148, 149, 178 spinach 123, 131, 202 red cabbage 154 sources of 23 spirulina 74 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222200 2244//0088//22002211 1166::2244
222200 IINNDDEEXX 222211 U additives 58, 59 red meat 68–69 senna 113 sports drinks 82 see also under specific vitamins canned foods 43, 58, 71 cancer risk and 68, 69 serotonin 12, 49, 142, 202–203 sports nutrition 78–83 and minerals ulcerative colitis 32, 48, 165 fortified foods 58 cooking 69 shellfish 179, 181, 183 see also exercise sustainability 43, 124–125 unconscious biological processes frozen food 47, 58, 125 nutritional benefits 68 shopping for food 125 sports supplements 80–81 sweating 24, 79 10, 12 highly processed foods 58, 195 red wine 73 short-chain fatty acids (SCFAs) 13, squashes 65 night sweats 146 underweight 99 processed meat 68, 69, 71 resistance training 78, 149 19, 27, 51, 52, 202 starch 12, 39, 40, 44 sweet potatoes 19, 37, 38, 39, 75 gaining weight safely 98, 99 vegan foods 118, 123 resting metabolic rate (RMR) 84, 85 16:8 eating plan 114 resistant starches 18–19 sweet corn 58–59, 100 genetic factor 99 progesterone 144, 153, 190 rheumatoid arthritis (RA) 140, 169 skin aging 21, 148–149 stomach 26, 28, 29, 105, 153, 175 sweeteners 65, 156, 161 health problems 99 prolactin 190 rice 14, 19, 45, 128 “skinny teas” 112–113 stress 95, 102, 104, 152, 153, 161, Swiss chard 131 “healthy thinness” 99 proline 198, 199 rice cakes 13 sleep deprivation 142, 143 164, 165, 167, 209 urination 24, 25, 72, 113 propionate 48 rice milk 163, 197 sleep quality 72, 142–143 stroke 17, 37, 54, 66, 70, 111, urine color 25 T prostaglandins 144, 145, 178 rickets 10 slimness 90 123, 180 protein 10, 12, 14–15, 30, 130, 154 small intestine 12, 18, 19, 26, 28, 29, risk reduction 37 T cells 140, 159 V amino acids see amino acids 113, 152, 157, 160, 167 subcutaneous fat 90 Tau proteins 150, 151 S children and 197, 198 Crohn’s disease 32, 48, 165 sucrose 12, 64 tempeh 128, 146, 147, 174 vagus nerve 49 complete proteins 14, 15, 128–129 salads, packaged 182 small intestine bacterial overgrowth sudden infant death syndrome testosterone 178, 179 vaso-active amines 159 daily intake 15, 41 saliva 26 (SIBO) 153 (SIDS) 191 thermic effect of food (TEF) 84 veganism digestion 29 salt 54, 58, 68, 70–71 smart scales 101 sugar 12, 46, 54, 58, 64–65, 109, thiamine 13 balanced diet 120–121 exercise and 78, 83 children and 197 smoking 178 186, 203 thinness, healthy 99 characteristics 118 food label information 63 food label information 61 smoothies 83, 108, 109 children and 197 thirst 79, 104 children 197, 198 incomplete proteins 14, 15, 128 guideline daily limit 70, 71 snacking 103, 203, 207 daily limit on 65 see also hydration ethical stance 118 producing protein sources 124 and heart disease 70–71 children 194, 197 food label information 61, 64 thyroid function 22–23, 130 processed foods 118, 123 sources of 15, 41, 42, 45, 123, hidden salt 71 healthy snacks 103 free sugars 64, 65, 194 time-restricted eating 114–115 protein 15, 41, 128–129 129, 133 premium salts 70–71 late night 142 natural sugars 64 tiredness supplements 41, 136, 137 vegan diets and 15, 41, 128–129 reducing salt consumption 71 sleep-friendly 142 reducing consumption 65 hunger and 104 vegetarianism protein powder 81, 109 sardines 168 social media influencers 63 “sugar free”/”no added PMS (premenstrual balanced diet 120 pulses 14, 44, 123, 126, 129, 153 satiation 104 sodium 22, 70, 71, 79, 113, 156 sugar” 63 syndrome) 144 characteristics 118 satiety 18, 26, 104, 109, 205 sofa, eating on the 207 sweeteners 65, 156, 161 tofu 123, 128, 146, 147, 174 lacto-ovo vegetarians 118 sauerkraut 53 sorbitol 166 vegan sugar 122 toxins, environmental 112 lacto-vegetarians 118 Q–R sausages 68 soups 109 sugar alcohols 166 toxoplasmosis 183 ovo-vegetarians 118 quinoa 45, 123, 128–129 seasonal eating 132 soy 37, 123, 126, 128, 129, 146, sulfites 160 trans fats 16, 17, 54, 66 protein 15, 41 Quorn 126, 129 seitan 128 147, 174 sunflower oil 67 triglycerides 16, 30 villi and microvilli 26, 29, 157 ready meals 46, 71, 182 selenium 22, 23, 68, 76, 131, 137, sperm quality 178, 179 superfoods 74–75 tryptophan 12, 142, 203 visceral fat 90 rectal bleeding 165 148, 149, 178 spinach 123, 131, 202 supplements 23, 43, 148, 174 tuna 183, 203 visceral hypersensitivity 153 red cabbage 154 sources of 23 spirulina 74 children 197, 198 turmeric 74, 75 vitamin A 10, 21, 37, 74, 136, 149, sports supplements 80–81 tyrosine 198, 199 183, 191 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222200 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222211 2244//0088//22002211 1166::2244
222 INDEX 223 X–Z sources of 21 water intoxication 79 vitamin B6 21, 136 weaning 187, 191, 192–193 xanthan gum 162 vitamin B9 136 bitter flavors 193 yogurt 53 vitamin B12 21, 36, 120, 130, 131, 136, finger foods 192–193 zinc 22, 23, 68, 130, 131, 137, 148, 178, 163, 174, 191, 198 purées 192, 193 189 sources of 21, 68 vegetable-led approach 192 sources of 23, 189 vitamin C 10, 21, 23, 37, 59, 74, 130, 131, weight 137, 148, 149 fluctuations 101 sources of 21 gaining 98–99 vitamin D 21, 42, 118, 137, 138–139, 144, weighing yourself 101 147, 149, 178, 181, 191 weight-neutral approach 91 deficiency 10, 137 weight loss 92–97 from sunshine 21, 138 exercise and 84–85, 95 sources of 21, 138, 139, 145, 180 food diary 96–97 supplements 130, 138, 174, 178, 180, gut microbiome and 106–107 191, 197, 198 limits on 92–93 vitamin E 21, 76, 137, 144, 148, 149 liquid meals 108–109 sources of 21 personalizing a weight-loss program vitamin K 10, 21, 48 96–97 sources of 21, 147 plateau 92 vitamins 10, 20–21 professional advice 97 children and 198 psychological diet cycle 94 fat soluble 21 regaining weight 92 sources of 10, 21 set point theory 92, 93 supplements 23 what works 94, 95, 115 water soluble 21, 23 wheat allergy 161, 162 see also specific vitamins see also gluten intolerance whey protein 81 white blood cells 136, 137 W whole grains 13, 17, 19, 45, 54, 121, waist circumference 91 172, 173 walnuts 145 World Health Organization 10, 32, 54, water 64, 66, 68, 70 hydration see hydration wound healing 21, 23 loss 110–111, 112 wrinkles see skin aging retention 101 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222222 2244//0088//22002211 1166::2244
222222 IINNDDEEXX 222233 ACKNOWLEDGMENTS X–Z sources of 21 water intoxication 79 BIBLIOGRAPHY vitamin B6 21, 136 weaning 187, 191, 192–193 xanthan gum 162 To access a comprehensive list of source materials, studies, vitamin B9 136 bitter flavors 193 yogurt 53 and research supporting the information in this book, vitamin B12 21, 36, 120, 130, 131, 136, finger foods 192–193 zinc 22, 23, 68, 130, 131, 137, 148, 178, please visit: www.dk.com/science-of-nutrition-biblio 163, 174, 191, 198 purées 192, 193 189 sources of 21, 68 vegetable-led approach 192 sources of 23, 189 AUTHOR’S ACKNOWLEDGMENTS than all the good we’ve done together is the vitamin C 10, 21, 23, 37, 59, 74, 130, 131, weight Although writing a few words will never repay the thought of all the remarkable things you’ll 137, 148, 149 fluctuations 101 faith so many people have shown in me, I hope it goes achieve from here. sources of 21 gaining 98–99 some way to telling them how forever grateful I am. I also want to thank my husband and son, you vitamin D 21, 42, 118, 137, 138–139, 144, weighing yourself 101 Of course, thank you to everyone at DK for inviting bring with you endless joy and love. You have been 147, 149, 178, 181, 191 weight-neutral approach 91 me on such an inspiring project. It’s an honor to there for me through the toughest of pandemic deficiency 10, 137 weight loss 92–97 work with such a dedicated publisher that shares my times. Juggling motherhood and business in from sunshine 21, 138 exercise and 84–85, 95 passion for evidence-based advice. It has been a real lockdown seemed impossible, but you help me sources of 21, 138, 139, 145, 180 food diary 96–97 pleasure to work with you, Alastair, Katie, and Dawn. believe that I can achieve anything I set my supplements 130, 138, 174, 178, 180, gut microbiome and 106–107 The extraordinary contribution of editors Andrea, mind to and make me an infinitely better person. 191, 197, 198 limits on 92–93 Holly, Salima, and Megan in refining my work Last but not least, I would like to thank all of vitamin E 21, 76, 137, 144, 148, 149 liquid meals 108–109 deserves an immense thanks. It must also be said, the you out there. Whether by following me @Rhitrition, sources of 21 personalizing a weight-loss program truly outstanding quality of this book is its design, buying a book, or seeing me in clinic, you are vitamin K 10, 21, 48 96–97 with special thanks to Alison for being so innovative. showing how much you value science. Amidst a sources of 21, 147 plateau 92 It is my earliest teachers, who fostered such a confusing postpandemic time, this feels more vitamins 10, 20–21 professional advice 97 respect and passion for science, without whom I important now than ever, I know you have all been children and 198 psychological diet cycle 94 would not be who I am or doing what I do. In through hardships. That’s also why I leave writing fat soluble 21 regaining weight 92 particular, I would like to thank Dr. Sue Reeves, this book even more optimistic about our health sources of 10, 21 set point theory 92, 93 Kirsty Cotton, and The University of Roehampton, than I was when I started. Because I know this supplements 23 what works 94, 95, 115 all instrumental in launching my nutrition career. book will not only help so many people; it will water soluble 21, 23 wheat allergy 161, 162 Thanks to all those who took the time out of their inspire more to believe you can make a difference see also specific vitamins see also gluten intolerance busy schedules to read and critically review every to your health and that of the planet, too, knowing whey protein 81 page; my mentor Jennifer Low, my amazing gut nutrition is a science. white blood cells 136, 137 health dietitian Kaitlin Colucci, my sports W whole grains 13, 17, 19, 45, 54, 121, nutritionist Faye Townsend, intuitive eating PUBLISHER’S ACKNOWLEDGMENTS waist circumference 91 172, 173 nutritionist Sophie Bertrand, and medical review Dorling Kindersley would like to thank Megan Lea walnuts 145 World Health Organization 10, 32, 54, by Dr. P.—your stamp of approval means so much for editorial assistance; Mandy Earey for design water 64, 66, 68, 70 more knowing how close a friend you are to me. assistance; Ginger Hultin for consulting; Marie hydration see hydration wound healing 21, 23 To my remarkable Rhitrition team: Bea, Jen, Lorimer for indexing; Pankaj Sharma and Vikram loss 110–111, 112 wrinkles see skin aging Kaitlin, Faye, Sophie, Sarah, Caff, Hala, Katie, Singh for reprographic work; Hayley Dodd for food retention 101 and Victoria. The only thing that makes me prouder styling; and Steve Crozier for image retouching. UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222222 2244//0088//22002211 1166::2244 UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222233 2255//0088//22002211 1199::1155
224 ABOUT THE AUTHOR Rhiannon Lambert is one of the UK’s leading Rhiannon also has her own food supplements nutritionists, a best-selling author and chart-topping company, RhiNourish. A healthy, balanced diet podcast host. should provide all the nutrients your body needs but, sometimes, for all sorts of reasons, it falls short. In 2016 she founded Rhitrition, a renowned RhiNourish’s innovative approach uses Rhiannon’s Harley Street clinic, which specializes in weight evidence-based, scientifically sound formulas to management, sports nutrition, eating disorders, and produce supplements for the vitamins and minerals pre- and post-natal nutrition. Its highly qualified, lacking in many diets. professional team of registered nutritionists, registered dietitians, and chartered psychologists Rhiannon hosts the top-rated “Food for Thought” work with individuals to transform their lives. podcast which gives listeners practical, evidence- based advice on how to achieve a healthier lifestyle. As an evidence-based practitioner, Rhiannon is With more than five million downloads since 2018, committed to the benefits of a scientific approach it is firmly established as one of the UK’s most to nutrition. popular health podcasts. She has worked as a consultant to many well-known Registered with the Association for Nutrition, food brands including Deliveroo, Wagamana, Alpro, Rhiannon obtained a first-class degree in Nutrition Yeo Valley, and Little Freddie, refining their menus, and Health, a Master’s degree in Obesity, Risks, and product ranges, and cooking methodology. Rhiannon Prevention, and diplomas in sports nutrition, and in has also advised on nutrition and wellbeing at Six pre- and post-natal nutrition. She is a Master Senses, Four Seasons Hotels & Resorts, Amazon, Practitioner in Eating Disorders, accredited by Microsoft, Samsung, and Coty. The British Psychological Society, and a Level 3 Personal Trainer. In 2017, Rhiannon published her first book, the best-selling Re-Nourish: A Simple Way To Eat Well, Follow Rhiannon on Instagram, Twitter, Facebook, part handbook, part cookbook, in which she shares and YouTube at @Rhitrition and visit Rhitrition.com her food philosophy to lay the foundations for a happy, healthy relationship with eating. She followed this up with Top Of Your Game: Eating For Mind & Body, co-written with world snooker champion, Ronnie O’Sullivan. UUSS__221122--222244__SSOONN__iinnddeexx..iinndddd 222244 2255//0088//22002211 1199::1155
--- End of content from file: The Science of Nutrition Debunk the Diet Myths and Learn How to Eat Responsibly for Health and Happiness (DK Science of).pdf ---
