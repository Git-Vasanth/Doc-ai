

--- Start of content from file: (Addison-Wesley Data & Analytics Series) Krohn, J._Beyleveld, G._Bassens, A. - Deep Learning Illustrated_ A Visual, Interactive Guide to Artificial Intelligence-Pearson Education (2019).pdf ---
History Deep Learning Illustrated TopAic sVisual, Interactive Guide to Artificial Intelligence, First Editon Jon Krohn Tutorials Beyleveld Grant OffeBrsa &s Dseeanlss Aglaé Addison­Wesley Professional Highlights Settings Support Sign Out
y History Part I. Introducing Deep Learning TopCichs apter 1 Biological and Machine Vision Chapter 2 Human and Machine Language Tutorials Chapter 3 Machine Art Offers & Deals Chapter 4 Game­Playing Machines Highlights Settings Support Sign Out
1 Biological and Machine Vision History Throughout this chapter and much of this book, the visual system of biological Topics organisms is used as an analogy to bring deep learning to, um... life. In addition to conveying a high­level understanding of what deep learning is, this analogy also Tutorials provides insight into how deep learning approaches are so powerful and so broadly­ applicable. Offers & Deals BIOLOGICAL VISION Highlights Five hundred and fifty million years ago, in the prehistoric Cambrian Period, the Settninugms ber of species on the planet began to surge (Figure 1.1). From the fossil record, 1 there is evidence that this explosion was driven by the development of light detectors Support in the trilobite (Figure 1.2). A visual system, even a primitive one, bestows a delightful bounty of fresh capabilities. One can, as examples, spot food, foes, and friendly­looking Sign Out mates at some distance. Other senses, like smell, enable animals to detect these as well, but not with the accuracy and light­speed pace of vision. Once the trilobite could see, the hypothesis goes, this set off an arms race that produced the Cambrian explosion: The trilobite’s prey, as well as its predators, themselves had to evolve to survive. Figure 1­1 The “Cambrian explosion”: the number of species on earth began to increase rapidly 550 million years ago, during the prehistoric Cambrian Period
Figure 1­2 A bespectacled trilobite In the half­billion years since trilobites developed vision, the complexity of the sense has increased considerably. Indeed, in modern mammals, a large proportion of the 2 cerebral cortex—the outer, grey matter of the brain —is involved in visual perception. At Johns Hopkins University in the late 1950s, the physiologists David Hubel and Torsten Wiesel (Figure 1.3) began carrying out their pioneering research on how visual 3 information is processed in the mammalian cerebral cortex, work which contributed 4 to them later being awarded a Nobel Prize. As depicted in Figure 1.4, Hubel and Wiesel conducted their research by showing images to anaesthetized cats while simultaneously recording the activity of individual neurons from the primary visual cortex, the first part of the cerebral cortex to receive visual input from the eyes.
Figure 1­3 The Nobel Prize­winning neurophysiologists Torsten Wiesel and David Hubel Projecting slides onto a screen, Hubel and Wiesel began by presenting simple shapes like the dot shown in Figure 1.4 to the cats. Their initial results were disheartening: Their efforts were met with no response from the neurons of the primary visual cortex. They grappled with the frustration of how these cells, which anatomically appear to be the gateway for visual information to the rest of the cerebral cortex, would not respond to visual stimuli. Distraught, Hubel and Wiesel tried in vain to stimulate the neurons by jumping and waving their arms in front of the cat. Nothing. And, then as with many of the great discoveries, from X­rays to penicillin to the microwave oven, Hubel and Wiesel made a serendipitous observation: As they removed one of their slides from the projector, its straight edge elicited the distinctive crackle of their recording equipment to alert them that a primary visual cortex neuron was firing. Overjoyed, they celebrated up and down the Johns Hopkins laboratory corridors.
Figure 1­4 Hubel and Wiesel used a light projector to present slides to anaesthesized cats while they recorded the activity of neurons in the cats’ primary visual cortex. In their experiments, electrical recording equipment was implanted within the cat’s skull. Instead of illustrating this, we suspected it would be a fair bit more palatable to use a lightbulb to represent neuron activation. Depicted in this figure is a primary visual cortex neuron being serendipitously activated by the straight edge of a slide. The serendipitously crackling neuron was not an anomaly. Through further experimentation, Hubel and Wiesel discovered that the neurons that receive visual input from the eye are in general most responsive to simple, straight edges. Fittingly then, they named these cells simple neurons. As shown in Figure 1.5, Hubel and Wiesel determined that a given simple neuron responds optimally to an edge at a particular, specific orientation. A large group of simple neurons, with each specialized to detect a particular edge orientation, together are able to represent all 360 degrees of orientation. These edge­orientation detecting simple cells then pass along information to a large number of so­called complex neurons. A given complex neuron receives visual information that has already been processed by several simple cells so it is well­positioned to recombine multiple line orientations into a more complex shape like a corner or a curve.
Figure 1­5 A “simple” cell in the primary visual cortex of a cat fires at different rates, depending on the orientation of a line shown to the cat. The orientation of the line is provided in the left­hand column of the figure, while the right­hand column shows the firing (electrical activity) in the cell over time (one second). A vertical line (in the fifth row) causes the most electrical activity for this particular simple cell. Lines slightly off vertical (in the intermediate rows) cause less activity for the cell, while lines approaching horizontal (in the top­most and bottom­most rows) cause little to no activity. Figure 1.6 illustrates how, via many hierarchically­organized layers of neurons feeding information into increasingly higher­order neurons, gradually more complex visual stimuli can be represented by the brain. The eyes are focused on an image of a rat’s head. Photons of light stimulate neurons located in the retina of each eye and this raw visual information is transmitted from the eyes to the primary visual cortex of the brain. The first layer of primary visual cortex neurons to receive this input—what Hubel and Wiesel termed simple cells—are specialized to detect edges (straight lines) at specific orientations. There would be many thousands of such neurons; for simplicity, we’re only showing four. In our caricature, we’re illustrating that neurons one, three, and four are activated by viewing the rat’s head. These three simple neurons relay that information to a subsequent layer, where complex cells assimilate the information
about various edge orientations, enabling them to represent more complex visual stimuli, like the curvature of the rat’s head. As information is passed through several subsequent further layers, the complexity and abstractness of the visual stimuli that can be represented incrementally increases. As depicted by the far­right layer of neurons, following many layers of such hierarchical processing, the brain is ultimately able to represent visual concepts as abstract as a rat, a cat, a bird or a dog. Figure 1­6 A caricature of how consecutive layers of biological neurons represent visual information in the brain of, e.g., a cat or a human. See main text for detail. Today, through countless subsequent recordings from the cortical neurons of brain­ 5 surgery patients as well as non­invasive techniques like magnetic resonance imaging, neuroscientists have pieced together a fairly high­resolution map of regions that are specialized to process particular visual stimuli, e.g., color, motion, faces (see Figure 1.7).
Figure 1­7 Regions of the visual cortex. The V1 region receives input from the eyes and contains the “simple” cells that detect edge orientations. Through the recombination of information via myriad subsequent layers of neurons (including within the V2, V3, and V3a regions), increasingly abstract visual stimuli are represented. In the human brain (shown here), there are regions containing neurons with concentrations of specializations in, as examples, the detection of color (V4), motion (V5), and people’s faces (fusiform face area). MACHINE VISION We haven’t been discussing the biological visual system solely because it’s interesting (though hopefully you did find the previous section thoroughly interesting). We covered the biological visual system primarily because it served as the inspiration for the modern deep learning approaches to machine vision, as will become clear in this section. Figure 1.8 provides a concise historical timeline of vision, in both biological organisms and machines. The top timeline, in blue, highlights the development of vision in trilobites as well as Hubel and Wiesel’s 1959 publication on the hierarchical nature of the primary visual cortex, as covered in the previous section. The machine vision timeline is split into two parallel streams to call attention to two alternative approaches. The middle timeline, in pink, represents the deep learning track that is the focus of our book. The bottom timeline, in purple, meanwhile represents the traditional machine learning path to vision, which —through contrast —will clarify why deep learning is distinctively powerful and revolutionary.
Figure 1­8 Abridged timeline of biological and machine vision, highlighting the key historical moments in the deep learning and traditional machine learning approaches to vision that are covered in this section. The Neocognitron Inspired by Hubel and Wiesel’s discovery of the simple and complex cells that form the primary visual cortex hierarchy shown in Figure 1.6, in the late ‘70s the Japanese electrical engineer Kunihiko Fukushima proposed an analogous architecture for 6 machine vision. Figure 1.9 shows Fukushima’s leading diagrams of this model architecture, which he named the neocognitron. Much of the detail of his diagrams is not important at this stage. There are, however, three particular items worth noting. First, Fukushima references Hubel and Wiesel explicitly; indeed, the paper refers to three of their landmark articles on the organization of the primary visual cortex. Second, Fukushima borrows the “simple” and “complex” cell language of Hubel and 7 Wiesel, calling deeper (i.e., further right) layers hypercomplex. The third point, and 8 the most critical one, is that by arranging artificial neurons in this hierarchical manner, they—like their biological inspiration—generally represent line orientations in the cells of the layers closest to the raw visual image (i.e., those on the far left, receiving input from the image U in Figure 1.9) while successively deeper (i.e., further­right) 0 layers represent successively complex, successively abstract objects. To make clear this potent property of the neocognitron and its deep learning descendants, we’ll go through an interactive example at the end of this chapter that demonstrates it.
Figure 1­9 Diagrams of Kunihiko Fukushima’s “neocognitron” from his 1980 paper. Borrowing Hubel and Wiesel’s “simple” and “complex” cell language, Fukushima’s artificial neural network model architecture emulates the hierarchy of the biological visual system. The image (U ) is represented in the furthest­left layer 0 of neurons (US) as edges (straight lines). As we move deeper (i.e., to the right), 1 each successive layer facilitates increasingly complex and increasingly abstract visual representations. This is analogous to Figure 1.6, the caricature of hierarchical representations found in biological visual systems. LeNet-5 While the neocognitron was capable of, for example, identifying handwritten 9 characters, the accuracy and efficiency of Yann LeCun (Figure 1.10) and Yoshua 10 Bengio’s (Figure 1.11) LeNet­5 model made it a significant development. LeNet­5’s hierarchical architecture (Figure 1.12) built on Fukushima’s lead and the biological 11 inspiration uncovered by Hubel and Wiesel. In addition, LeCun and his colleagues’ 12 benefited from superior data for training their model, faster processing power and, critically, the backpropagation algorithm.
Figure 1­10 Paris­born Yann LeCun is one of the pre­eminent figures in artificial neural network and deep learning research. Professor LeCun is the Founding Director of the New York University Center for Data Science as well as the Director of AI Research at the social network Facebook. Figure 1­11 Yoshua Bengio is another of the leading characters in artificial neural networks and deep learning. Born in France, he is a computer science professor at the University of Montreal and co­directs the renowned Machines and Brains program at the Canadian Institute for Advanced Research.
Figure 1­12 LeNet­5 retains the hierarchical architecture uncovered in the primary visual cortex by Hubel and Wiesel and leveraged by Fukushima in his neocognitron. As in those other systems, the left­most layer represents simple edges, while successive layers represent increasingly complex features. Backpropagation, often abbreviated to backprop, facilitates efficient learning 13 throughout the layers of artificial neurons within a deep learning model. Together with their data and processing power, backprop rendered LeNet­5 sufficiently reliable to become an early commercial application of deep learning: It was used by the United 14 States Postal Service to automate the reading of ZIP codes written on mail envelopes. In Chapter 10, on machine vision, we will experience LeNet­5 first­hand by designing it ourselves and training it to (guess what!) recognize handwritten digits. In LeNet­5, Yann LeCun and his colleagues had an algorithm that could correctly predict what handwritten digits had been drawn without them needing to include any expertise about handwritten digits in their code. As such, LeNet­5 provides an opportunity to introduce a fundamental difference between deep learning and the traditional machine learning ideology. As conveyed by Figure 1.13, the traditional machine learning (ML) approach is characterized by practitioners investing the bulk of their efforts into engineering features. This feature engineering is the application of clever, and often elaborate, algorithms to raw data in order to preprocess them into input variables that can be readily modeled by traditional statistical techniques. These techniques—e.g., regression, random forest, support vector machine—are seldom effective on unprocessed data, and so the engineering of input data has historically been a prime focus of machine learning professionals.
Figure 1­13 Feature engineering—the transformation of raw data into thoughtfully­transformed input variables—often predominates the application of traditional machine learning algorithms. In contrast, the application of deep learning often involves little to no feature engineering, with the majority of time spent instead on the design and tuning of model architectures. In general, a minority of the traditional ML practitioner’s time is spent optimizing ML models or selecting the most effective one from those available. The deep learning approach to modeling data turns these priorities upside­down. The deep learning practitioner typically spends little to none of her time engineering features, instead spending it modeling data with various artificial neural network architectures that process the raw inputs into useful features automatically. This distinction between deep learning and traditional machine learning is a core theme of this book. The next section provides a classic example of feature engineering to concretely explicate the distinction. The Traditional Machine Learning Approach Following LeNet­5, research into artificial neural networks, including deep learning, fell out of favor. The consensus became that the approach’s automated feature generation was not pragmatic—that while it worked well for handwritten character recognition, the 15 feature­free ideology was perceived to have limited breadth of applicability. Traditional machine learning, including its feature engineering, appeared to hold more 16 promise and funding shifted away from deep learning research. To make clear what feature engineering is, Figure 1.14 provides a celebrated example 17 from Paul Viola and Michael Jones in the early noughties. Viola and Jones employed rectangular filters such as the vertical or horizontal black and white bars shown in the figure. Features generated by passing these filters over an image can be fed into machine learning algorithms to reliably detect the presence of a face. Their work is
notable because the algorithm was efficient enough to be the first real­time face 18 detector outside the realm of biology . Devising clever face­detecting filters to process raw pixels into features for input into a machine learning model was accomplished via years of research and collaboration on the characteristics of faces. And, of course, it is limited to detecting faces in general, as opposed to being able to recognize a particular face as, say, Angela Merkel’s or Oprah Winfrey’s. To develop features for detecting Oprah in particular, or for detecting some non­face class of objects like houses, cars, or Yorkshire Terriers, would require the development of expertise in that category, which could again take years of academic­community collaboration to execute both efficiently and accurately. If only we could circumnavigate all that time and effort somehow... Figure 1­14 Engineered features leveraged by Viola and Jones (2001) to detect faces reliably. Their efficient algorithm found its way into FujiFilm cameras, facilitating real­time auto­focus. ImageNet and the ILSVRC As mentioned earlier, one of the advantages LeNet­5 had over the neocognitron was a larger, high­quality set of training data. The next breakthrough in neural networks was also facilitated by a high­quality public dataset—this time much larger: ImageNet, a labelled index of photographs devised by Fei­Fei Li (Figure 1.15), armed machine vision 19,20 researchers with an immense catalog of training data. For reference, the handwritten digit data used to train LeNet­5 contained tens of thousands of images. ImageNet, in contrast, contains tens of millions.
Figure 1­15 The hulking ImageNet data set was the brainchild of Chinese­ American computer science professor Fei­Fei Li and her colleagues at Princeton at the time. In addition to her faculty position at Stanford, Li is the Chief Scientist of A.I./Machine Learning for Google’s cloud platform. The fourteen million images in the ImageNet data set are spread across 22,000 categories. These categories are as diverse as container ships, leopards, starfish and 21 elderberries. Since 2010, Professor Li has run an open challenge called ILSVRC on a subset of the ImageNet data that has become the premier ground for assessing the world’s state­of­the­art machine vision algorithms. The ILSVRC subset consists of 1.4 million images across a thousand categories. In addition to providing a broad range of categories, many of the selected categories are breeds of dogs, thereby evaluating the algorithms’ ability not only to distinguish broadly­varying images, but also to specialize 22 in distinguishing subtly varying ones. AlexNet As graphed in Figure 1.16, in the first two years of the ILSVRC all algorithms entered into the competition hailed from the feature­engineering­driven traditional machine learning ideology. In the third year, all entrants except one were traditional ML algorithms. If that one deep learning model in 2012 had not been developed or its creators not competed in ILSVRC, then the year­over­year image classification accuracy would have been negligible. Instead, Alex Krizhevsky and Ilya Sutskever—working out of the University of Toronto lab led by Geoffrey Hinton (Figure 1.17)—crushed the existing benchmarks with their submission, today referred to as AlexNet (Figure 1.18). This was a watershed moment. In an instant, deep learning architectures emerged from the fringes of machine learning to its fore. Academics and commercial practitioners scrambled to grasp the fundamentals of artificial neural networks as well as to create software libraries—many of them open­source—to experiment with deep learning models on their own data and use­cases, be they machine vision or otherwise. As Figure 1.16 illustrates, in the years since 2012 all of the top­performing models in the ILSVRC
have been deep learning­driven. Figure 1­16 Performance of the top entrants to the ILSVRC by year. AlexNet was the victor by a head­and­shoulders margin in the 2012 iteration. All of the best algorithms since have been deep learning models. In 2015, machines surpassed human accuracy.
Figure 1­17 The eminent British­Canadian artificial neural network pioneer Geoffrey Hinton, habitually referred to as “the godfather of deep learning” in the popular press. Hinton is an Emeritus Professor at the University of Toronto and an Engineering Fellow at Google, responsible for managing the search giant’s Brain Team, a research arm, in Toronto. Figure 1­18 AlexNet’s hierarchical architecture is reminiscent of LeNet­5 and the neocognitron with the first (left­hand) layer representing simple visual features like edges and deeper layers representing increasingly complex features and abstract concepts. While the hierarchical architecture of AlexNet is reminiscent of LeNet­5, there are three principal factors that enabled it to be the state­of­the­art machine vision algorithm in 2012. First is the training data. Not only did Krizhevsky and his colleagues have access to the massive ImageNet index, they also artificially expanded the data available to them by applying transformations (e.g., horizontal reflection) to the training images. Second is processing power. Not only had computing power per unit of cost increased dramatically from 1998 to 2012, but Krizhevsky, Hinton and Sutskever also 23 programmed two GPUs to train their large data sets with previously unseen efficiency.
Third is architectural advances. AlexNet is deeper (has more layers) than LeNet­5, and 24 25 it takes advantage of both a new type of artificial neuron and a nifty trick that helps generalize deep learning models beyond the data they’re trained on. As with LeNet­5, we will build AlexNet ourselves in Chapter 10 and use it to classify images. Our ILSVRC case study underlines how deep learning models like AlexNet are so widely useful and disruptive across industries and computational applications: They dramatically reduce the subject­matter expertise required for building highly accurate statistical models. This trend away from expertise­driven feature engineering and toward surprisingly powerful automatic­feature­generating deep learning models has been prevalently borne out across not only vision applications, but, as examples, the playing of complex games (the topic of Chapter 4) and natural language processing 26 (Chapter 2) as well . One no longer needs to be a specialist in the visual attributes of faces to create a face­recognition algorithm. One no longer requires a thorough understanding of a game’s strategies to write a program that can master it. One no longer needs to be an authority on the structure and semantics of each of several languages to develop a language­translation tool. For a rapidly­growing list of use­ cases, one’s ability to apply deep learning techniques outweighs the value of domain­ specific proficiency. While such proficiency may have necessitated a doctoral degree or perhaps years of postdoctoral research within a given domain, a functional level of deep learning capability can be developed with relative ease—as by working through this book! TENSORFLOW PLAYGROUND For a fun, interactive way to crystallize the hierarchical, feature­learning nature of deep learning, make your way to the TensorFlow Playground via the following URL: bit.ly/TFplayground. By using this custom link, your network should automatically look similar to the one shown in Figure 1.19. We’ll be returning to define all of the terms on the screen in Part II; for the present exercise, they can be safely ignored. It suffices at this time to know that this is a deep learning model. The model architecture consists of six layers of artificial neurons: an input layer on the left (below the FEATURES heading), four “hidden” layers (which bear the responsibility of learning), and an output layer (the grid on the far right ranging from 6 to +6 on both axes). The network’s goal is to learn how to distinguish orange dots (negative cases) from blue dots (positive cases) based solely on their location on the grid. As such, in the input layer, we are only feeding in two pieces of information about each dot: its horizontal position (X ) and its 1 vertical position (X ). The dots that will be used as training data are shown by default 2 on the grid. By clicking the Show test data toggle, you can also see the location of dots that will be used to assess the performance of the network as it learns. Critically, these
test data are not available to the network while it’s learning, so they help us ensure that the network generalizes well to new, unseen data. Figure 1­19 A deep neural network ready to learn how to distinguish a spiral of orange dots (negative cases) from blue dots (positive cases) based on their position on the X and X axes of the grid on the right. 1 2 Click the prominent Play arrow in the top­left corner. Enable the network to train until the “Training loss” and “Test loss” in the top­right corner have both approached zero, say less than 0.5. How long this takes will depend on the hardware you’re using but will hopefully not be more than a few minutes. As captured in Figure 1.20, you should now see the network’s artificial neurons representing the input data with increasing complexity and abstraction the deeper (further to the right) they are positioned—as in the neocognitron, LeNet­5, and AlexNet. Every time the network is run, the neuron­level details of how the network solves the spiral classification problem are unique, but the general approach remains the same (you can refresh the page and re­train the network to see this for yourself). The artificial neurons in the left­most “hidden” layer are specialized in distinguishing edges (straight lines), each at a different particular orientation. Neurons from the first hidden layer pass information to neurons in the second hidden layer, each of which recombine the edges into slightly more complex features like curves. The neurons in each successive layer recombine information from the neurons of the previous layer, gradually increasing the complexity and abstraction of the features they can represent. By the final (right­most) layer, the neurons are adept at representing the intricacies of
the spiral shape, enabling the network to accurately predict whether a dot is orange (a negative case) or blue (a positive case) based on its position (X and X coordinates) in 1 2 the grid. Hover over a neuron to project it onto the far­right OUTPUT grid and examine its individual specialization in detail. Figure 1­20 The network after training QUICK, DRAW! To interactively experience a deep learning network carrying out a machine vision task in real­time, navigate to quickdraw.withgoogle.com. Click Let’s Draw! to begin playing the game. You will be prompted to draw an object and a deep learning algorithm will guess what you sketch. By the end of Chapter 10, we will have covered all of the theory and practical code examples needed to devise a machine vision algorithm akin to this one. To boot, the drawings you create will be added to the data set that we’ll leverage in Chapter 12 when we create a deep learning model that can convincingly mimic human­ drawn doodles. Hold onto your seat! We’re embarking on a fantastic ride. SUMMARY Hopefully the parallel between biological vision and machine vision was clear to you. This is a theme that has popped up a few times in deep learning over the years: the ways in which deep learning models represent information are analogous with those same information processing systems in the natural world. Machine vision is a huge
subfield within deep learning, and one which has seen many great advancements in recent years. There is a staggering amount of image­based data available, and previously most of this data was difficult or impossible to access—we did not have a way for computers to understand what was happening in the images. We’ll unpack the details of the various machine vision models in Chapter 10. See you there! 1 . Parker, A. (2004). In The Blink of an Eye: How Vision Sparked the Big Bang of Evolution. New York: Basic Books. 2 . Trilobite Reading SIDEBAR. A couple of tangential facts about the cerebral cortex: First, it is one of the more evolutionary­recent developments of the brain, contributing to the complexity of mammal behavior relative to the behavior of older classes of animals like reptiles and amphibians. Second, while the brain is informally referred to as grey matter because the cerebral cortex is the brain’s external surface and this cortical tissue is grey in color, the bulk of the brain is in fact white matter. By and large, the white matter is responsible for carrying information over longer distances than the grey matter, so its neurons are coated in a white­colored, fatty covering that hurries the pace of signal conduction. A coarse analogy could be to consider neurons in the white matter to act as “highways”. These high­speed motorways have scant on­ramps or exits, but can transport a signal from one part of the brain to another lickety­split. In contrast, the “local roads” of grey matter facilitate myriad opportunities for interconnection between neurons at the expense of speed. A gross generalization, therefore, is to consider the cerebral cortex—the grey matter—as the part of the brain where the most complex computations happen, affording the animals with the largest proportion of it—e.g., mammals, particularly the great apes like Homo sapiens—their complex behaviors. 3 . Hubel, D. H., & Wiesel, T. N. (1959) Receptive fields of single neurones in the cat’s striate cortex. The Journal of Physiology, 148, 574­91. 4 . The 1981 Nobel Prize in Physiology or Medicine, shared with American neurobiologist Roger Sperry. 5 . Especially functional MRI, which provides insight into which regions of cerebral cortex are notably active or inactive when the brain is engaged in a particular activity 6 . Fukushima, K. (1980). Neocognitron: A self­organizing neural network model for a
mechanism of pattern recognition unaffected by shift in position. Biological Cynbernetics, 36, 193­202. 7 . Trilobite Attention SIDEBAR The theoretical concept of a single “grandmother cell” representing one’s mental percept of their grandmother has not been convincingly demonstrated in the biological brain, where she is perhaps represented by the coordinated activation of an ensemble of neurons. As we shall see in the examples in this book, however, in machine­vision systems we do typically configure our model architectures so that the final layer contains individual artificial neurons representing individual concepts (e.g., your grandmother, Barack Obama, the number seven, a school bus) and this works very well. 8 . We will define precisely what these are in Chapter 7, “Artificial Neural Networks”. For the moment, it’s more than sufficient to think of each artificial neuron as a speedy, little algorithm. 9 . Fukushima, K., & Wake, N. (1991). Handwritten alphanumeric character recognition by the neocognitron. IEEE Transactions on Neural Networks, 2, 355­65. 10. LeCun, Y., et al. (1998). Gradient­based learning applied to document recognition. Proceedings of the IEEE, 2, 355­65. 11. LeNet­5 was the first Convolutional Neural Network, a deep learning variant that dominates modern machine vision and that we’ll detail in Chapter 10. 12. Their classic data set, the handwritten MNIST digits, will be detailed later in Part II, “Essential Theory Illustrated”. 13. We will detail the backpropagation algorithm later in Chapter 7. 14. The USPS term for postal code 15. At the time, there were stumbling blocks associated with optimizing deep learning models that have since been resolved, including poor weight initializations (covered in Chapter 9), covariate shift (also in Chapter 9) and the predominance of the relatively inefficient sigmoid activation function (Chapter 6). 16. Public funding for artificial neural network research ebbed globally, with the notable exception of continued support from the Canadian federal government enabling, e.g., the Universities of Montreal, Toronto, and Alberta to become
powerhouses in the field. 17. Viola, P., & Jones, M. (2001). Robust real­time face detection. International Journal of Computer Vision, 57, 137­54. 18. A few years later, the algorithm found its way into digital FujiFilm cameras, facilitating auto­focus on faces for the first time—a now everyday attribute of digital cameras and smartphones alike. 19. www.image­net.org 20. Deng, J., et al. (2009). ImageNet: A large­scale hierarchical image database. Proceedings of the Conference on Computer Vision and Pattern Recognition. 21. ImageNet Large Scale Visual Recognition Challenge 22. On your own time, try to distinguish photos of Yorkshire Terriers from Australian Silky Terriers. It’s tough, but Westminster Dog Show judges, as well as contemporary machine vision models, can do it. Tangentially, these dog­heavy data are why deep learning models trained with ImageNet have a disposition toward “dreaming” about dogs (see, e.g., deepdreamgenerator.com). 23. Graphical Processing Units: These are designed primarily for rendering video games but are well­suited to performing the matrix multiplication that abounds in deep learning across hundreds of parallel computing threads. 24. The Rectified Linear Unit, which will be introduced in Chapter 7 25. Dropout, introduced in Chapter 9 26. An especially entertaining recounting of the disruption to the field of machine translation is provided by Gideon Lewis­Kraus in his article “The Great A.I. Awakening”, published in The New York Times Magazine on December 14th, 2016.
2 Human and Machine Language History In the previous chapter, we introduced the high­level theory of deep learning via Topics analogy to the biological visual system. All the while, we highlighted that one of the technique’s core strengths lies in its ability to learn features automatically from data. In Tutorials this chapter, we’ll build atop our deep learning foundations by examining how it’s incorporated into human language applications, with a particular emphasis on how it Offers & Deals can automatically learn features that represent the meaning of words. Highlights The Austro­British philosopher Ludwig Wittgenstein famously argued, in his posthumous and seminal work Philosophical Investigations, “The meaning of a word is Settings 1 its use in the language.” He further orated that, “One cannot guess how a word functions. One has to look at its use, and learn from that.” Wittgenstein was suggesting Support that words on their own have no real meaning; rather, it is by their use within the larger context of language we’re able to ascertain their meaning. As you’ll see through this Sign Out chapter, natural language processing with deep learning relies heavily on this premise —word2vec quite literally derives its semantic understanding of a word by analyzing it within its contexts across a large corpus. Armed with this notion, let’s begin by breaking down deep learning for natural language processing as a discipline, and then we’ll go on to discuss modern deep learning techniques for representing words and language. By the end of the chapter, you should have a good grasp on what is possible with deep learning and NLP, the groundwork for writing such code in Chapter 11. DEEP LEARNING FOR NATURAL LANGUAGE PROCESSING The two core concepts in this chapter are deep learning and natural language processing. Initially, we’ll cover the relevant aspects of these concepts separately, then we’ll weave them together as the chapter progresses. Deep Learning Networks Learn Representations Automatically As established way back in this book’s introduction, deep learning can be defined as the layering of simple algorithms called artificial neurons into networks several layers
deep. Via the Venn diagram in Figure 2.1, we show how deep learning resides within the machine learning family of representation learning approaches. The representation learning family, which contemporary deep learning dominates, includes any techniques that learn features from data automatically. Indeed, we can use the terms “feature” and “representation” interchangeably. Figure 2.1 Venn diagram that distinguishes the “traditional” family from the “representation learning” family of machine learning techniques. Figure 1.13 summarised the advantage of representation learning relative to traditional machine learning approaches. Traditional ML typically works well because of clever, human­designed code that transforms raw data—whether be it images, audio of speech, or text from documents—into input features for machine learning algorithms (e.g., regression, random forest, support vector machines) that are adept at weighting features but not particularly good at learning features from raw data directly. This manual creation of features is often a highly­specialized task. For working with language data, for example, it might require graduate­level training in linguistics. A primary benefit of deep learning is that it eases this requirement for subject­matter expertise. Instead of manually curating input features from raw data, the data can be fed directly into a deep learning model. Over the course of many examples provided to the deep learning model, the first layer of artificial neurons receiving the input data learn how to represent simple abstractions of these data, while each successive layer learns to represent increasingly complex non­linear abstractions on the layer that precedes it. As we’ll discover in the current chapter, this isn’t solely a matter of convenience; learning features automatically has additional advantages. Features engineered by humans tend to not be comprehensive, tend to be excessively specific, and can involve lengthy, ongoing loops of feature ideation, design and validation that could stretch for years. representation learning models, meanwhile, generate features quickly (typically over hours or days of model training), adapt straightforwardly to
changes in the data (e.g., new words, meanings, or ways of using language), and adapt automatically to shifts in the problem being solved. Natural Language Processing Natural language processing (NLP) is a field of research that sits at the intersection of computer science, linguistics, and “artificial intelligence” (Figure 2.2). NLP involves taking the naturally­spoken or naturally­written language of humans—like this sentence you’re reading right now—and processing it with machines to automatically complete some task or to make a task easier for a human to do. Examples of language use that do not fall under the umbrella of natural language could include code written in a software language or short strings of characters within a spreadsheet. Figure 2.2 NLP sits at the intersection of the fields computer science, linguistics and artificial intelligence. Examples of NLP in industry include: œ classifying documents: using the language within a document (e.g., an email, a Tweet, or a review of a film) to classify it into a particular category (e.g., high urgency, positive sentiment, or predicted direction of the price of a company’s stock) œ machine translation: assisting language­translation firms with machine­generated suggestions from a source language (e.g., English) to a target language (e.g., German or Mandarin); increasingly, fully­automatic—though not always perfect—translations between languages œ search engines: autocompleting users’ searches and predicting what information or website they’re seeking
œ speech recognition: interpret voice commands to provide information or take action, as with virtual assistants like Amazon’s Alexa, Apple’s Siri or Microsoft’s Cortana œ chatbots: modern chatbots fall short of convincingly carrying out a natural conversation for an extended period of time, but are nevertheless helpful for relatively linear conversations on narrow topics like the routine components of a given firm’s customer­service phone calls Some of the easiest NLP applications to build are spell­checkers, synonym­suggesters and keyword­search querying tools. These simple tasks can be fairly straightforwardly solved with deterministic, rules­based code using say, reference dictionaries or thesauruses. deep learning models are unnecessarily sophisticated for these applications and so they won’t be discussed further in this book. Intermediate­complexity NLP tasks include assigning a school­grade reading level to a document, predicting the most likely next words while making a query in a search engine, classifying documents (see above), and extracting information from documents 2 or websites like prices or named entities. These intermediate NLP applications are well­suited to solving with deep learning models. In Chapter 11, for example, we’ll leverage a variety of deep learning architectures to predict the sentiment of film reviews. The most sophisticated NLP implementations are required for machine translation (see above), automated question­answering and chatbots. These are tricky because they need to handle application­critical nuance (as an example, humor is particularly transient), a response to a question can depend on the intermediate responses to previous questions, and meaning can be conveyed over the course of a lengthy passage of text consisting of many sentences. Complex NLP tasks like these are beyond the scope of this book, however the content we cover will serve as superb foundations for their development. A Brief History of Deep Learning for NLP The timeline in Figure 2.3 calls out recent milestones in the application of deep learning to NLP. This timeline begins in 2011, when the University of Toronto computer scientist George Dahl and his colleagues at Microsoft Research revealed the first major 3 breakthrough involving a deep learning algorithm applied to a large data set. This breakthrough happened to involve natural language data. Dahl and his team trained a deep neural network to recognize a substantial vocabulary of words from audio recordings of human speech. A year later, and as detailed already in Chapter 1, the next landmark deep learning feat also came out of Toronto: AlexNet blowing the traditional
machine learning competition out of the water in the ImageNet Large­Scale Visual Recognition Competition (Figure 1.16). For a time, this staggering machine vision performance heralded a focus on applying deep learning to machine vision applications. Figure 2.3 Milestones involving the application of deep learning to natural language processing. See text for details. By 2015, the deep learning progress being made in machine vision began to spill over into NLP competitions such as those that assess the accuracy of machine translations from one language into another. These deep learning models approached the precision of traditional machine learning approaches, however they required less research and development time, while conveniently offering lower computational complexity. Indeed, this reduction in computational complexity provided Microsoft the opportunity to squeeze real­time machine translation software onto mobile phone processors— remarkable progress for a task that previously required an Internet connection and computationally­expensive calculations on a remote server. In 2016 and 2017, deep learning models entered into NLP competitions began to not only be more efficient than traditional machine learning models, they began outperforming them on accuracy as well. The remainder of this chapter will begin to illuminate how. COMPUTATIONAL REPRESENTATIONS OF LANGUAGE In order for deep learning models to process language, we have to supply that language to the model in a way that it can digest. For all computer systems, this means a quantitative representation of language, e.g., a two­dimensional matrix of numerical values. Two popular methods for converting text into numbers are one­hot encoding 4 and word vectors. We’ll discuss both methods in turn in this section. One-Hot Representations of Words The traditional approach to encoding natural language numerically for processing it with a machine is one­hot encoding (Figure 2.4). In this approach, the words of natural language in a sentence (e.g., “the”, “cat”, “sat”, “on”, “the”, and “mat”) are represented by the columns of a matrix. Each row in the matrix, meanwhile, represents a unique
5 word. If there are a hundred unique words across the corpus of documents you’re feeding into your natural language algorithm, then your matrix of one­hot­encoded words will have one hundred rows. If there are a thousand unique words across your corpus, then there will be a thousand rows in your one­hot matrix, and so on. Figure 2.4 One­hot encodings of words, such as this example, predominate the traditional machine learning approach to natural language processing. Cells within one­hot matrices consist of binary values, i.e., they are a zero or a one. Each column contains at most a single one, but is otherwise made up of zeroes, 6 meaning that one­hot matrices are sparse. Values of one indicate the presence of a particular word (row) at a particular position (column) within the corpus. In Figure 2.4, our entire corpus has only six words in it, five of which are unique. Given this, a one­hot representation of the words in our corpus has six columns and five words. The first unique word—“the”—occurs in the first and fifth positions, as indicated by the cells filled with ones in the first row of the matrix. The second unique word in our wee corpus is “cat”, which occurs only in the second position, so it is represented by a value of one in second row of the second column. One­hot word representations like this are fairly straightforward, and they are an acceptable format for feeding into a deep learning model (or, indeed, other machine learning models). As we shall see momentarily, however, the simplicity and sparsity of one­hot representations are limiting when incorporated into a natural language application. Word Vectors Vector representations of words are the information­dense alternative to one­hot encodings of words. While one­hot representations capture information about word
location only, word vectors capture information about word meaning as well as 7 location. This additional information renders word vectors favorable for a variety of reasons that we’ll catalogue over the course of this chapter. The key advantage, however, is that—analogous to the visual features learned automatically by deep­ learning machine­vision models in Chapter 1—word vectors enable deep­learning NLP models to automatically learn linguistic features. When creating word vectors, the overarching concept is that we’d like to assign each word within a corpus to a particular, meaningful location within a multi­dimensional space called the vector space. Initially, each word is assigned to a random location within the vector space. By considering the words that tend to be used around a given word within the natural language of your corpus, however, the locations of the words within the vector space can gradually be shifted into locations that represent the 8 meaning of the words. Figure 2.5 uses a toy­sized example to demonstrate in more detail the mechanics behind how word vectors are constructed. Commencing at the first word in our corpus and moving to the right one word at a time until we reach the final word in our corpus, we consider each word in our corpus to be the target word. At the particular moment captured in Figure 2.5, the target word that happens to be under consideration is word. The next target word would be by, followed by the, then company, and so on. For each target word in turn, we consider it relative to the words around it—its context words. In our toy example, we’re using a context­word window size of three words. This means that while word is the target word, the three words to the left (a, know and shall) combined with the three words to the right (by, company, and the) together constitute 9 a total of six context words. When we move along to the subsequent target word (by), the windows of context words also shift one position to the right, dropping shall and by as context words while adding word and it.
Figure 2.5 A toy example for demonstrating the high­level process behind techniques that convert natural language into word vectors like word2vec and GloVe. See text for details. By a considerable margin, the two most popular techniques for converting natural 10 11 language into word vectors are word2vec and GloVe. With either technique, our objective while considering any given target word is to accurately predict the target 12 word given its context words . Improving at these predictions, target word after target word over a large corpus, we gradually assign words that tend to appear in similar contexts to similar locations in vector space. Figure 2.6 provides a cartoon of vector space. The space can have any number of dimensions so we can call it an n­dimensional vector space. In practice, depending on the richness of the corpus we have to work with and the complexity of our NLP application, we might create a word­vector space with dozens, hundreds or—in extreme cases—thousands of dimensions. As overviewed in the previous paragraph, any given word from our corpus (e.g., king) is assigned a location within the vector space. In, say a 100­dimensional space, the location of the word king is specified by a vector that we can call v that must consist of 100 numbers in order to specify the location of the king word king across all of the available dimensions. Human brains aren’t adept at spatial reasoning in more than three dimensions, so our cartoon in Figure 2.6 has only three dimensions. In this three­dimensional space, any given word from our corpus needs three numeric coordinates to define its location within the vector space: x, y and z. In this cartoon example then, the meaning of the word king is represented by a vector v that consists of three numbers. If v is located at the coordinates x = 1.1, y = 2.4, king king and z = 3.0 in the vector space, we can use the annotation [­1.1, 2.4, 3.0] to describe this location succinctly. This succinct annotation will come in handy later when we perform arithmetic operations on word vectors.
Figure 2.6 Diagram of word meaning as represented by a three­dimensional vector space. See text for details. 13 The closer two words within vector space, the closer their meaning, as determined by the similarity of the context words appearing near them in natural language. Synonyms and common misspellings of a given word—because they share an identical meaning— would be expected to have near­identical context words and therefore near­identical locations in vector space. Words that are used in similar contexts, such as those that denote time for example, tend to occur near each other in vector space. In Figure 2.6, Monday, Tuesday, and Wednesday could be represented by the orange­colored dots located within the orange days­of­the­week cluster in the cube’s top­right corner. Meanwhile, months of the year might occur in their own purple cluster, which is adjacent but distinct to the days of the week—they both relate to the date, but they’re separate sub­clusters within a broader dates cluster. As a second example, we would expect to find programming languages clustering together in some location within the word vector space that is distant from the time­denoting words, say in the top­left corner. Again here, object­oriented programming languages like Java, C++, and Python would be expected to form one sub­cluster, while nearby we would expect to find functional programming languages like Haskell, Clojure and Erlang forming a separate sub­cluster. As we’ll see in Chapter 11 when we build our own word vectors, less concretely­defined terms that nevertheless convey a specific meaning (e.g., the verbs created, developed, built) are also allocated positions within word­vector space
that enable them to be useful in NLP tasks. Word Vector Arithmetic Remarkably, because it turns out to be an efficient way for relevant word information to be stored in a vector space, particular movements across vector space come to represent 14 relative particular meanings between words. This is a bewildering property. Returning to our cube in Figure 2.6, the brown arrows represent the relationship between countries and their capital. That is, if we calculate the direction and distance between the coordinates of the words Paris and France, then trace this direction and distance from London, we should find ourselves in the neighborhood of the coordinate representing the word England. As a second example, we can calculate the direction and distance between the coordinates for man and woman. This movement through vector space represents gender and is symbolized by the green arrows in Figure 2.6. If we trace the green direction and distance from any given male­specific term (e.g., king, uncle), we should find our way to a coordinate near the term’s female counterpart (queen, aunt). A by­product of being able to trace vectors of meaning (e.g., gender, capital­country relationship) from one word in vector space to another is that we can perform word vector arithmetic. The canonical example of this is: If we begin at v , the vector king representing king (continuing with our example from the previous section, this location is described by [­1.1, 2.4, 3.0]), subtract the vector representing man from it (let’s say v = [­1.1, 2.4, 3.0]) and add the vector representing woman man (let’s say v = [­3.2, 2.5, 2.6]), we should find a location near the vector woman representing queen. To make this arithmetic explicit by working through it dimension by dimension, we would estimate the location of v by calculating: queen All three dimensions together then, we expect v to be near [­3.0, 2.0, 1.8]. queen Figure 2.7 provides further, entertaining examples of arithmetic through a word vector space that was trained on a large natural language corpus crawled from the web. As we’ll later observe in practice in Chapter 11, the preservation of these quantitative relationships of meaning between words across vector space is a robust starting point for deep learning models within NLP applications.
Figure 2.7 Examples of word vector arithmetic. word2viz To develop your intuitive appreciation of word vectors, navigate to lamyiowce.github.io/word2viz. The default screen for the word2viz tool for exploring word vectors interactively is shown in Figure 2.8. Leaving the top­right dropdown box set to “Gender analogies”, try adding in pairs of new words under the “Modify words” heading. If you add pairs of corresponding gender­specific words like princess and prince, duchess and duke, and businesswoman and businessman, you should find that they fall in instructive locations. Figure 2.8 The default screen for word2viz, a tool for exploring word vectors interactively. The developer of the word2viz tool, Julia Bazinska, compressed a fifty­dimensional word­vector space down to two dimensions in order to visualize the vectors on an xy­ 15 coordinate system. For the default configuration, Bazinska scaled the x­axis from the words she to he as a reference point for gender, while the y­axis was set to vary from a common base toward a royal peak by orienting it to the words woman and queen. The displayed words, placed into vector space via training on a natural language data set 16 consisting of six billion instances of 400,000 unique words , fall relative to the two axes based on their meaning. The more regal (queen­like) the words, the higher on the plot they should be shown, and the female (she­like) terms fall to the left of their male (he­like) counterparts.
When you’ve indulged yourself sufficiently with word2viz’s “Gender analogies” view, you can experiment with other perspectives of the word vector space. Selecting “Adjectives­Analogies” from the “What do you want to see?” drop­down box, you could for example add the words small and smallest. Subsequently, you could change the x­axis labels to nice and nicer, and then again to small and big. Switching to the “Numbers say­write analogies” view via the drop­down box, you could play around with changing the x­axis to 3 and 7. You may build your own word2viz plot from scratch by moving to the “Empty” view. The (word vector) world is your oyster, but you could perhaps examine the country­ capital relationships mentioned earlier when familiarizing ourselves with Figure 2.6. To do this, set the x­axis to range from west to east and the y­axis to city and country. Word pairs that fall neatly into this plot include london—england, paris —france, berlin —germany and beijing —china. This paragraph is the Trilobite Attention SIDEBAR on Bias. While on the one hand word2viz is an enjoyable way to develop a general understanding of word vectors, on the other hand it can also be a serious tool for gaining insight into specific strengths or weaknesses of a given word­vector space. As an example, use the “What do you want to see?” drop­down box to load the “Verb tenses” view and then add the words lead and led. Doing this, it becomes apparent that the coordinates words were assigned to in this vector space mirror existing gender stereotypes that were present in the natural language data the vector space was trained on. Switching to the “Jobs” view, this gender bias becomes even more stark. It is probably safe to say that any large natural language data set is going to have some biases in it, whether intentional or not. The development 17 of techniques for reducing biases in word vectors is an active area of research. Mindful that these biases may be present in your data, however, the safest bet is to test your downstream NLP application in a range of situations that reflect a diverse user­base, checking that the results are appropriate. END SIDEBAR. Localist versus Distributed Representations With an intuitive understanding of word vectors under our figurative belts, we can contrast them with one­hot representations (Figure 2.4), which have been an established presence in the NLP world for longer. A summary distinction is that we can say word vectors store the meaning of words in a distributed representation across n­ dimensional space. That is, with word vectors, word meaning is distributed gradually —“smeared”—as we move from location to location through vector space. One­hot representations, meanwhile, are localist—they store information on a given word discretely, within a single row of a typically­extremely­sparse matrix.
To more thoroughly characterize the distinction between the localist, one­hot approach and the distributed, vector­based approach to word representation, Table 2.1 compares them across a range of attributes. Firstly, one­hot representations lack nuance; they are simple binary flags. Vector­based representations, on the other hand, are extremely nuanced: Within them, information about words is smeared throughout a continuous, quantitative space. In this high­dimensional space, there are essentially infinite possibilities for capturing the relationships between words. Table 2.1 Table contrasting attributes of localist, one­hot representations of words with distributed, vector­based representations Secondly, the use of one­hot representations in practice often requires labor­intensive, manually­curated taxonomies. These taxonomies include dictionaries and other 18 specialised reference language databases. Such external references are unnecessary for vector­based representations, which are fully­automatic with natural language data alone. Third, one­hot representations don’t handle new words well. A newly introduced word requires a new row in the matrix and then re­analysis relative to the existing rows of the corpus, followed by code changes—perhaps via reference to external information sources. With vector­based representations, new words can be incorporated by training the vector space on natural language that includes examples of the new words in their natural context. A new word gets its own new n­dimensional vector. Initially, there may be few training data points involving the new word so its vector might not be very accurately positioned within n­dimensional space, but the positioning of all existing words remain intact and the model will not fail to function. Over time, as the instances of the new word in natural language increases, the accuracy of its vector­space 19 coordinates will improve. Fourth, and following on from the previous two points, the use of one­hot representations often involves subjective interpretations of the meaning of language. This is because they often require coded rules or reference databases that are designed
by (relatively small groups of) developers. The meaning of language in vector­based 20 representations, meanwhile, is data­driven. Fifth, one­hot representations natively ignore word similarity: Similar words, like couch and sofa are represented no differently than couch and cat. In contrast, vector­based representations innately handle word similarity: As mentioned earlier with respect to Figure 2.6, the more similar two words, the closer they are in vector space. ELEMENTS OF NATURAL HUMAN LANGUAGE Thus far, we have considered only one element of natural human language: the word. Words, however, are made up of constituent language elements. In turn, words themselves are the constituents of more abstract, more complex language elements. We’ll begin with the language elements that make up words and build up from there, following the schematic in Figure 2.9. With each element, we’ll discuss how it is typically encoded from the traditional machine learning perspective as well as from the deep learning perspective. As we move through these elements, notice how the distributed deep learning representations are fluid and flexible vectors while the traditional ML representations are local and rigid (Table 2.2). Figure 2.9 Relationships between the elements of natural human language. The left­most elements are building blocks for further­right elements. As we move to the right, the more abstract the elements become and therefore the more complex they are to model with an NLP application.
Table 2.2 Table of traditional machine learning and deep learning representations, by natural language element. Phonology is concerned with the way that language sounds when it is spoken. Every language has a specific set of phonemes (sounds) that make up its words. The traditional ML approach is to encode segments of auditory input as specific phonemes from the language’s range of available phonemes. With deep learning, we train a model to predict phonemes from features automatically learned from auditory input and then represent those phonemes in a vector space. In this book, we’ll be working with natural language in text format only, but the techniques we cover can be applied directly to speech data if you’re keen to do so in your own time. Morphology is concerned with the forms of words. Like phonemes, every language has a specific set of morphemes, which are the smallest units of language that contain some meaning. For example, the three morphemes out, go, and ing combine to form the word outgoing. The traditional ML approach is to identify morphemes in text from a list of all the morphemes in a given language. With deep learning, we train a model to predict the occurrence of particular morphemes. Hierarchically­deeper layers of artificial neurons can then combine multiple vectors (e.g., the three representing out, go, and ing) into a single vector representing a word. Phonemes (when considering audio) and morphemes (when considering text) combine to form words. Whenever we work with natural language data in this book, we’ll work at the word level. We do this for four reasons. First, it’s straightforward to define what a word is and everyone is familiar with what they are. Second, it’s easy to break up 21 natural language into words via a process called tokenization that we’ll work through in Chapter 11. Third, words are the most­studied level of natural language, particularly with respect to deep learning, so we can readily apply cutting­edge techniques to them. Fourth, and perhaps most critically, for the NLP models we’ll be building, word vectors simply work well: they prove to be functional, efficient and accurate. In the previous section, we already detailed the shortcomings of localist, one­hot representations that predominate traditional ML relative to the word vectors used in deep learning models.
Words are combined to generate syntax. Syntax and morphology (already introduced above) together constitute the entirety of a language’s grammar. Syntax is the arrangement of words into phrases and phrases into sentences in order to convey meaning in a way that is consistent across the users of a given language. In the traditional ML approach, phrases are bucketed into discrete, formal linguistic 22 categories. With deep learning (surprise, surprise!), we employ vectors. Every word and every phrase in a section of text can be represented by a vector in n­dimensional space, with layers of artificial neurons combining words into phrases. Semantics is the most abstract of the elements of natural language in Figure 2.9 and Table 2.2; it is concerned with the meaning of sentences. This meaning is inferred from all the underlying language elements like words and phrases, as well as the overarching context that a piece of text appears in. Inferring meaning is complex because, for example, whether a passage is supposed to be taken literally or as a humorous and sarcastic remark can depend on subtle contextual differences and shifting cultural norms. Traditional ML, because it doesn’t represent the fuzziness of language (e.g., the similarity of related words or phrases), is limited in capturing semantic meaning. With deep learning, vectors come to the rescue once again. Vectors can represent not only every word and every phrase in a passage of text, but every logical expression as well. As with the language elements already covered, layers of artificial neurons can recombine vectors of constituent elements—in this case to calculate semantic vectors via the non­linear combination of phrase vectors. GOOGLE DUPLEX One of the more attention­grabbing examples of deep­learning­based NLP in recent memory is that of Google Duplex, which was unveiled at the their I/O developers conference in May 2018. The search giant’s CEO, Sundar Pichai, held spectators in rapture as he demonstrated Google Assistant making a phone call to a Chinese­food restaurant to book a reservation. The audible gasps from the audience were in response to the natural flow of Duplex’s conversation. It had mastered the cadence of a human conversation, replete with the uh’s and hhhm’s that we sprinkle into conversations while we’re thinking. Furthermore, the phone call was of average audio quality and the human on the line had a strong accent—Duplex never faltered, and managed to make the booking. Bearing in mind that this is a demonstration—and not even a live one—what nevertheless impressed us was the breadth of deep­learning applications that had to come together to facilitate this technology. Consider the flow of information back­and­ forth between the two agents on the call, Duplex and the restaurateur: Duplex needs a
sophisticated speech recognition algorithm that can process audio in realtime and handle an extensive range of accents and call qualities on the other end of the line, and 23 also overcome the background noise. Once the human’s speech has been faithfully transcribed, an NLP model needs to process the sentence and decide what it means. The intention is that the person on the line doesn’t know they’re speaking to a computer and so doesn’t need to modulate their speech accordingly, but in turn, this means that humans respond with complex, multi­ part sentences that can be tricky for a computer to tease apart: “We don’t have anything tomorrow, but we have the next day and Thursday, anytime before 8. Wait no... Thursday at 7 is out. But we do can after 8?” This sentence is poorly structured—you’d never write an email like this—but in natural conversation, these sorts of on­the­fly corrections and replacements happen regularly, and Duplex needs to be able to follow along. With the audio transcribed and meaning of the sentence processed, Duplex’s NLP model conjures up a response. This response must either ask for more information if the human was unclear or if the answers were unsatisfactory, otherwise it should confirm the booking. The NLP model will generate a response in text form, so a test­to­ speech engine is required to synthesize the sound. This paragraph is a Trilobite Reading SIDEBAR Duplex uses a combination of de novo 24 25 waveform synthesis using Tacotron and WaveNet , as well as a more classical 26 “concatenative” text­to­speech engine . This is where the system crosses the uncanny 27 valley —the voice heard by restauranteur is not a human voice at all. WaveNet is able to generate completely synthetic waveforms, one sample at a time, using a deep neural network trained on real waveforms from human speakers. Beneath this, Tacotron maps sequences of words to corresponding sequences of audio features, which capture subtleties of human speech such as pitch, speed, intonation and even pronunciation. These features are then fed into WaveNet which synthesizes the actual waveform that the restauranteur hears. This whole system is able to produce natural sounding voice with the correct cadence, emotion and emphasis. During moments of more­or­less rote moments in the conversation, the simple concatenative TTS engine (comprised of recordings of it’s own “voice”) which is less computationally demanding to execute, is used. The entire model dynamically switches between the various models as needed. END SIDEBAR. To misquote Jerry Maguire: you had all of this at “hello.” The speech recognition
system, NLP models, and TTS engine all work in concert from the instant the call is answered. Things only stand to get more complex for the Duplex from then on. Governing all of this interaction is a deep neural network that is specialized in handling 28 information that occur in a sequence . This governor tracks the conversation and feeds the various inputs and outputs into the appropriate models. It should be clear from this overview that Google Duplex is a highly sophisticated system of deep learning models that work in harmony to produce a seamless interaction on the phone. For now, Duplex is limited to a few very specific domains: scheduling appointments. The system cannot carry out general conversations. So while this represents a significant step forward for artificial intelligence, there is still much work to be done. SUMMARY In this chapter, we learned about applications of deep learning to the processing of natural language. In so doing, we described further the capacity for deep learning models to automatically extract the most pertinent features from data, removing the need for labor­intensive one­hot representations of language. Instead, NLP applications involving deep learning make use of vector­space embeddings, which capture the meaning of words in a nuanced manner that improves both model performance and accuracy. Later, in Chapter 11, we’ll ourselves construct an NLP application by making use of artificial neural networks that handle the input of natural language data all the way through to the output of an inference about those data. In such an “end­to­end” deep learning model, the initial layers create word vectors that flow seamlessly into deeper, specialized layers of artificial neurons, including layers that incorporate “memory”. These model architectures will highlight both the strength and the ease­of­use of deep learning with word vectors. 1 . Wittgenstein, L. (1953). Philosophical Investigations. (Anscombe, G., Trans.). Oxford, UK: Basil Blackwell. 2 . Trilobite Attention SIDEBAR Named entities include places, well­known individuals, company names and products. 3 . Dahl, G., et al. (2011). Large vocabulary continuous speech recognition with context­dependent DBN­HMMs. Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.
4 . If this were a book dedicated to NLP, then we would have been wise to also describe natural language methods based on word frequency, e.g., TF­IDF (term frequency­ inverse document frequency) and PMI (pointwise mutual information). 5 . Trilobite Reading SIDEBAR A corpus is the collection of all of the documents you use as your input data for a given natural language application. In Chapter 11, we’ll make use of a corpus that consists of eighteen classic books. Later in that chapter, we’ll separately make use of a corpus of 25,000 film reviews. An example of a much larger corpus would be all of the English­language articles on Wikipedia. The largest corpuses are crawls of all the publicly­available data on the Internet, e.g., as at commoncrawl.org. 6 . Non­zero values are rare (i.e., they are sparse) within a sparse matrix. In contrast, dense matrices are rich in information: they typically contain few—perhaps even no— zero values. 7 . Strictly speaking, a one­hot representation is technically a “word vector” itself as each column in a one­hot word matrix consists of a vector representing a word at a given location. In the deep learning community, however, use of the term “word vector” is commonly reserved for the dense representations covered in this section—i.e., those derived by word2vec, GloVe, and related techniques. 8 . As mentioned at the beginning of this chapter, this understanding of the meaning of a word from the words around it was proposed by Ludwig Wittgenstein. Later, in 1957, the idea was captured succinctly by the British linguist J.R. Firth with his phrase “You shall know a word by the company it keeps”. 9 . It is mathematically simpler and more efficient to not concern ourselves with the specific ordering of context words, particularly as word order tends to confer negligible extra information to the inference of word vectors. Ergo, we provide the context words in parentheses alphabetically, an effectively random order. 10. Mikolov, T., et al. (2013). Efficient estimation of word representations in vector space. arXiv:1301.3781. 11. Pennington, J., et al. (2014). GloVe: Global vectors for word representations. Proceedings of the Conference on Empirical Methods in natural language processing.
12. Or, alternatively, we could predict context words given a target word. More on that in Chapter 11. 13. Measured by Euclidean distance, which is the plain old straight­line distance between two points. 14. One of your esteemed authors, Jon, prefers terms like “mind­bending” and “trippy” to describe this property of word vectors, but he consulted a thesaurus to narrow in on a more professional­sounding adjective. 15. We’ll detail how to perform vector space dimensionality reduction in Chapter 11. 16. Technically, 400,000 tokens—a distinction that we’ll examine later. 17. E.g.: Bolukbasi, T., et al. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. arXiv:1607.06520. Caliskan, A., et al. (2017). Semantics derived automatically from language corpora contain human­like biases. Science 356: 183­6. Zhang, B., et al. (2018). Mitigating Unwanted Biases with Adversarial Learning. arXiv:1801.07593 18. E.g., WordNet (wordnet.princeton.edu), which describes synonyms as well as hypernyms (“is­a” relationships, so furniture, for example, is a hypernym of chair) 19. An associated problem not addressed here is when an in­production NLP algorithm encounters a word that was not included within its corpus of training data. This out of vocabulary problem impacts both one­hot representations and word vectors. There are approaches—e.g., Facebook’s fastText library—that try to get around the issue by considering sub­word information, but these approaches are beyond the scope of this book. 20. Noting that they may nevertheless include biases found in natural language data. See the “Trilobite­Attention SIDEBAR” on Bias. 21. Essentially, tokenization is the use of characters like commas, periods and whitespace to assume where one word ends and the next begins. 22. These categories have names like “noun­phrase” and “verb­phrase”. 23. This is known as the “cocktail­party problem”—or less jovially, “multi­talker speech separation”. It’s a problem that humans solve innately, isolating single voices from a cacophony quite well without explicit instruction on how to do so. Machines typically struggle with this, though a variety of groups have proposed solutions, e.g., see
Simpson, A., et al. (2015). Deep Karaoke: Extracting Vocals from Musical Mixtures Using a Convolutional Deep Neural Network. arXiv:1504.04658; Yu, D., et al. (2016). Permutation Invariant Training of Deep Models for Speaker­Independent Multi­talker Speech Separation. arXiv:1607.00325 24. ai.googleblog.com/2017/12/tacotron­2­generating­human­like­speech.html 25. deepmind.com/blog/wavenet­generative­model­raw­audio 26. Concatenative TTS engines use vast databases of pre­recorded words and snippets, which can be strung together to form sentences. This approach is common and fairly easy, but yields stilted, unnatural speech and cannot adapt the speed and intonation— you can’t modulate a word to make it sound like a question is being asked, for example. 27. The uncanny valley is a dangerous space wherein humans find human­like simulations weird and creepy because they’re too similar to real humans, but it’s also clear they’re not real humans. Product designers endeavor to avoid the uncanny valley. They’ve learned that users respond well to simulations that are either very robotic or not robotic at all. 28. Called a recurrent neural network. These feature in Chapter 11
3 Machine Art History In this chapter, we’ll introduce some of the concepts that enable deep learning models Toptiocs seemingly create art, an idea that may be paradoxical to some. The University of California, Berkeley philosopher Alva Noë, for one, opined “Art can help us frame a Tutboreiatltser picture of our human nature.” 1 If this is true, how can machines create art? Or put differently, are the creations that emerge from these machines, in fact, art? Another Offers & Deals interpretation—and one we like best—is that these creations are indeed art and that programmers are artists wielding deep­learning models as brushes. Highlights By the end of this chapter, you’ll have learned the essential ideas behind generative Settaindgvsersarial networks (GANs) and you will have seen how they can be used to generate completely novel works. We’ll have hopefully drawn a convincing link between the Support word vector spaces of the previous chapter and the latent spaces associated with GANs. We’ll also cover some intersections between art and deep learning that don’t make use Sign Out of GANs, wherein the deep learning models are more obviously tools that can be applied to some end. Enough with the philosophy, let’s get our overalls covered in paint! But first... a drink. A BOOZY ALL-NIGHTER Sitting below Google’s offices in Montreal sits a bar called Les 3 Brasseurs, a moniker that translates from French to The 3 Brewers. It was at this watering hole in 2014, while a Ph.D. student in Yoshua Bengio’s renowned lab (Figure 1.11), that Ian 2 Goodfellow conceived of an algorithm for fabricating realistic­looking images, a technique that Yann LeCun (Figure 1.10) has hailed as the “most important” recent 3 breakthrough in Deep Learning. Goodfellow’s friends described to him a generative model they were working on, i.e., a computational model that aims to produce something novel, be it a quote in the style of Shakespeare, a musical melody, or a work of abstract art. In their particular case, the fr ie nds w ere att empt ing t,o design. a model th at could gen erate photorealistic images . such as portraits of human faces. For this to work well via the Traditional Machine Learning approach (Figure 1.13), the engineers designing the model would need to not
only catalog and approximate the critical individual features of faces like eyes, noses and mouths, but also accurately estimate how these features should be arranged relative to each other. Thus far, their results had been underwhelming. The generated faces tended to be excessively blurry or they tended to be missing essential elements like the nose or the ears. 4 Perhaps with his creativity heightened by a pint of beer or two, Goodfellow proposed a revolutionary idea: a deep learning model in which two artificial neural networks act against each other competitively as adversaries. As illustrated in Figure 3.1, one of these deep ANNs would be programmed to produce forgeries while the other would be programmed to act as a detective and distinguish the fakes from real images (which would be provided separately). These adversarial deep learning networks would play off one another: As the generator became better at producing fakes, the discriminator would need to become better at identifying them, and so the generator would need to produce even more compelling counterfeits, and so on. This virtuous cycle would eventually lead to convincing novel images in the style of the real training images, be they of faces or otherwise. Best of all, Goodfellow’s approach would circumnavigate the need to program features into the generative model manually. As we’ve already expounded with respect to machine vision (Chapter 1) and natural language processing (Chapter 2), deep learning would sort the model’s features out automatically.
Figure 3.1 High­level schematic of a generative adversarial network (GAN). Real images, as well as forgeries produced by the generator are provided to the discriminator, which is tasked with identifying which are the genuine article. The purple cloud represents latent space (Figure 3.4) “guidance” that is provided to the forger. This guidance can either be random (as is generally the case during network training; see Chapter 12) or selective (during post­training exploration, as in Figure 3.3). Goodfellow’s friends were doubtful his imaginative approach would work. So, when he arrived home and found his girlfriend asleep, he worked late to architect his dual­ANN design. It worked the first time and the astounding deep learning family of generative adversarial networks was born! That same year, Goodfellow and his colleagues revealed GANs to the world at the 5 prestigious Neural Information Processing Systems (NIPS) conference. Some of their results are shown in Figure 3.2. Their GAN produced these novel images by being 6 7 trained on: (a) handwritten digits; (b) photos of human faces; and (c) & (d) photos 8 from across ten classes (e.g., planes, cars, dogs). The results in (c) are markedly less crisp than in (d) because the GAN that produced the latter featured neuron layers 9 specialized for machine vision called convolutional layers while the GAN that 10 produced the former used a more general layer type only.
Figure 3.2 Results presented in Goodfellow and colleagues’ (2014) GAN paper. See text for details. ARITHMETIC ON FAKE HUMAN FACES Following on from Goodfellow’s lead, a research team led by the American machine­ learning engineer Alec Radford determined architectural constraints for GANs that guide considerably more realistic image creation. Some examples of portraits of fake 11 humans that were produced by their deep convolutional GANs are provided in Figure 3.3. In their paper, Radford and his teammates cleverly demonstrated interpolation through, and arithmetic with, the latent space associated with GANs. Let’s start off by explaining what latent space is before moving on to latent­space interpolation and arithmetic.
Figure 3.3 An example of latent­space arithmetic from Radford et al. (2016). The latent­space cartoon in Figure 3.4 may be reminiscent of the word vector­space cartoon in Figure 2.6. As it happens, there are three major similarities between latent spaces and vector spaces. First, while the cartoon is only three­dimensional for simplicity and comprehensibility, latent spaces are n­dimensional spaces, usually in the order of hundreds of dimensions. The latent space of the GAN we’ll later architect ourselves in Chapter 12, for example, will have n = 100 dimensions. Second, the closer two points are in the latent space, the more similar the images that those points represent are. And third, movement through the latent space in any particular direction can correspond to a gradual change in a concept being represented, such as age or gender for the case of photorealistic faces.
Figure 3.4 A cartoon of the latent space associated with generative adversarial networks (GANs). Moving along the purple arrow, the latent space corresponds to images of a similar­looking individual aging. The green arrow represents gender while the orange one represents the inclusion of glasses on the face. By picking two points far away from each other along some n­dimensional axis representing age, interpolating between them, and sampling points from the interpolated line, we could find what appears to be the same (fabricated) man gradually 12 appearing to be older and older. In our latent­space cartoon (Figure 3.4), we represented such an “age” axis in purple. To observe interpolation through an authentic GAN latent space, we recommend scanning through Radford and colleagues’ paper for, as an example, smooth rotations of the “photo angle” of synthetic bedrooms. At the time of writing the book manuscript you’re presently reading, the state of the art in GANs can be viewed at youtu.be/G06dEcZ­QTg. This video, produced by researchers 13 at the graphics­card manufacturer Nvidia , provides a breathtaking interpolation through high­quality portrait “photographs” of ersatz celebrities. Moving a step further with what we’ve learned, we could now perform arithmetic with images sampled from a GAN’s latent space. When sampling a point within the latent space, that point can be represented by the co­ordinates of its location—the resulting
vector is analogous to the word vectors described in Chapter 2. Just as with word vectors, we can perform arithmetic with these vectors and move through the latent space in a semantic way. Figure 3.3 showcases an instance of latent­space arithmetic from Radford and his co­workers. Starting with a point in their GAN’s latent space that represents a man with glasses, subtracting a point that represents a man without glasses, and adding a point representing a woman without glasses, the resulting point exists in the latent space near to images that represent women with glasses. Our cartoon in Figure 3.4 illustrates how the relationships between meaning in latent space are stored (again, akin to the way they are in word­vector space), thereby making arithmetic on points in latent space possible. STYLE TRANSFER: CONVERTING PHOTOS INTO MONET (AND VICE VERSA) One of the more magical applications of GANs is style transfer. Zhu, Park and their coworkers from the University of California, Berkeley’s A.I. Research lab introduced a 14 new flavor of GAN that enables stunning examples of this, as shown in Figure 3.5. Alexei Efros, one of the paper’s co­authors, took photos while on holiday in France and they employed their CycleGAN to transfer these photos into the style of the Impressionist Claude Monet, the 19th­century Dutch artist Van Gogh and the Japanese “Ukiyo­e” genre, amongst others. If you navigate to junyanz.github.io/CycleGAN, you’ll be delighted to discover instances of the inverse (Monet paintings converted into photorealistic images), as well as: œ summer scenes converted into wintry ones, and vice versa œ baskets of apples converted into baskets of oranges, and vice versa œ flat, low­quality photos converted into what appear to be ones captured by high­end single­lens reflex cameras œ a video of a horse running in a field converted into a zebra œ a video of a drive taken during the day converted into a nighttime one
Figure 3.5 Photos converted into the styles of well­known painters by CycleGANs. MAKE YOUR OWN SKETCHES PHOTOREALISTIC Another GAN application out of Alexei Efros’ A.I. Research lab at Berkeley, and one 15 that you can amuse yourself with yourself straightaway, is pix2pix. If you make your way to affinelayer.com/pixsrv (convert to bit.ly), you can interactively translate images from one type to another. Using the “edges2cats” tool, for example, we sketched the three­eyed cat in the left­hand panel of Figure 3.6 to generate the photorealistic(­ ish) mutant kitty in the right­hand panel. As it takes your fancy, you are also welcome to convert your own creative visions of felines, shoes, handbags and building façades into photorealistic analogs within your browser. The authors of the pix2pix paper called their approach a conditional GAN (cGAN for short) because the generative adversarial network produces an output that is conditional on the particular input provided to it.
Figure 3.6 A mutant three­eyed cat (right­hand panel) synthesized via the pix2pix web application. The sketch in the left­hand panel that the GAN output was conditioned on was clearly not doodled by this book’s illustrator, Aglaé, but one of its other authors (who shall remain nameless). CREATING PHOTOREALISTIC IMAGES FROM TEXT To round out this chapter, we’d like you to take a gander at the truly photorealistic 16 high­resolution images in Figure 3.7. These images were generated by StackGAN , an approach that stacks two GANs on top of each other. The first GAN in the architecture is configured to produce a rough, low­resolution image with the general shape and colors of the relevant objects in place. This is then supplied to the second GAN as its input, where the forged “photo” is refined by fixing up imperfections and adding considerable detail. The StackGAN is a cGAN like the pix2pix network in the previous section, however, the image output is conditioned on text input instead of an image.
Figure 3.7 Photorealistic high­resolution images output by StackGAN, which involves two GANs stacked upon each other. See text for further detail. IMAGE PROCESSING USING DEEP LEARNING Since the advent of digital camera technology, image processing (both on­device and post­processing) has become a staple in most (if not all) photographers’ workflows. This ranges from simple on­device processing, such as increasing saturation and sharpness immediately after capture, to complex editing of RAW image files in software applications like Adobe Photoshop and Lightroom. Machine learning has been used extensively in on­device processing, where the camera manufacturer would like the image that the consumer sees to be vibrant and pleasing to the eye with minimal end­user effort. Some examples of this are: œ early face recognition algorithms in point­and­shoot cameras which optimize the exposure and focus for faces or even selectively fire the shutter when they recognize that the subject is smiling (as in Figure 1.14); œ scene­detection algorithms that adjust the exposure settings to capture the whiteness of snow or activate the flash for nighttime photos. In the post­processing arena, a variety of automatic tools exist although generally photographers who are taking the time to post­process images are investing considerable time and domain­specific knowledge into color and exposure correction,
de­noising, sharpening, tone­mapping and touching up (to name just a few of the corrections that may be applied). These corrections have been difficult to execute programmatically because, for example, de­noising might need to be applied selectively to different images and even different parts of the same image. This is exactly the kind of intelligent application that deep learning is poised to excel at. 17 In a 2018 paper from Chen Chen and his collaborators at Intel Labs , deep learning was applied to the enhancement of images that were taken in near total darkness, with astonishing results (Figure 3.8). In a phrase, their deep learning model involves 18 convolutional layers organized into the innovative U­Net architecture (that we’ll break down for you in Chapter 10). The authors generated a custom dataset for training this model: the See­in­the­Dark (SID) dataset consists of 5094 raw images taken in near total darkness using a short­exposure (that is, a short enough exposure time to enable practical hand­held capture without motion blur but which renders images too dark to be useful) with a corresponding long­exposure image (using a tripod for stability) of the same scene. The exposure times on the long­exposure images were 100–300 times that of the short­exposure training images, with actual exposure times in the range of 10–30 seconds. The raw short­exposure images were fed into the convolutional layers, using the long­exposure images as the ground truth. Figure 3.8 A sample image (left) processed using a traditional pipeline (center) and the deep learning pipeline by Chen at al. (2018) The trained model demonstrates a remarkable ability to brighten images from near total darkness with successful noise suppression and correct color transformation (Figure 3.8)—the deep­learning image­processing pipeline far outperforms the results of the traditional pipeline shown in the center panel. There are, however, limitations as yet: œ the model is not fast enough to perform this correction in real time (and certainly not on­device), however runtime optimization will certainly help here; œ a dedicated network must be trained for different camera models and sensors,
whereas a more generalized and camera model­agnostic approach would be favorable; œ while the results far exceed the capabilities of traditional pipelines, there are still some artifacts present in the enhanced photos which could stand to be improved; œ and, finally, the dataset is limited to selected static scenes and needs to be expanded to other subjects (most notably, humans). Limitations aside, this work nevertheless provides a beguiling peek into how deep learning can adaptively correct images in photograph post­processing pipelines with a level of sophistication not before seen from machines. SUMMARY I’m not sure if this chapter will have convinced you that GANs are artists, or even that you can be an artist yourself if you skip ahead to Chapter 12, but hopefully it has introduced the idea that deep learning models—GANs in particular, in this case— encode some fairly sophisticated representations in their latent spaces. That said, a model is only as good as the data, and that certainly applies here. These networks produce some sensational results, but like all deep­learning models, they’re constrained by what they’re trained on. A model that turns your family photos into Monet paintings can’t do very much else. As it turns out, the model has just learned the features typical to Monet paintings and can mathematically transform pixels so that they conform and we’re thusly entertained. This, I think, is a long way from general creativity. This has been a brief introduction to an incomplete list of creative applications for deep learning. In Chapter 12 we’ll be making our own GAN using sketches from Google’s Quick, Draw! dataset (introduced in Chapter 1). Take a gander at Figure 3.9 for a preview.
Figure 3.9 Novel “hand­drawings” of apples produced by the GAN architecture we’ll develop together in Chapter 12. Using this approach, you’ll trivially be able to produce machine­drawn “sketches” from across any of the hundreds of categories involved in the Quick, Draw! game. 1 . Noë, A. (2015, October 5). What Art Unveils. The New York Times. bit.ly/unveils 2 . Giles, M. (2018, February 21). The GANfather: The man who’s given machines the gift of imagination. MIT Technology Review. 3 . LeCun, Y. (2016, July 28). Quora. bit.ly/DLbreakthru. 4 . Jarosz, A., et al. (2012). Uncorking the muse: Alcohol intoxication facilitates creative problem solving. Consciousness and Cognition, 21, 487­93. 5 . Goodfellow, I., et al. (2014). Generative Adversarial Networks. arXiv:1406.2661. 6 . From LeCun’s classic MNIST data set that we’ll use ourselves in Part II. 7 . From the Hinton research group’s Toronto Face Database. 8 . The CIFAR­10 dataset, which is named after the Canadian Institute for Advanced
Research that supported its creation. 9 . We’ll detail these in Chapter 10. 10. Dense layers, which will be introduced in the next chapter and detailed in Chapter 7. 11. Radford, A., et al. (2016). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial networks. arXiv:1511.06434v2. 12. A technical aside: As was the case with vector spaces, this “age” axis (or any other direction within latent space that represents some meaningful attribute) may be orthogonal to all of the n dimensions that constitute the axes of the n­dimensional space. 13. Karras, T., et al. (2018). Progressive Growing of GANs for Improved Quality, Stability, and Variation. Proceedings of the International Conference on Learning Representations. 14. Called “CycleGANs” because they retain image consistency over multiple cycles of network training. Zhu, J.­Y., et al. (2017). Unpaired Image­to­Image Translation using Cycle­Consistent Adversarial Networks. arXiv:1703.10593. 15. Isola, P., et al. (2017). Image­to­Image Translation with Conditional Adversarial Networks. arXiv:1611.07004. 16. Zhang, H., et al. (2017). StackGAN: Text to Photo­realistic Image Synthesis with Stacked Generative Adversarial Networks. arXiv:1612.03242v2. 17. Chen, C. et al. (2018) Learning to See in the Dark. arXiv:1805.01934 18. Ronneberger et al. (2015) U­Net: Convolutional Networks for Biomedical Image Segmentation. arXiv: 1505.04597
4 Game-Playing Machines History Alongside the generative adversarial networks introduced in Chapter 3, deep Topics reinforcement learning has produced some of the most surprising artificial­neural­ network advances, including the lion’s share of the headline­grabbing “artificial Tutorials intelligence” breakthroughs of recent years. In this chapter, we’ll introduce what reinforcement learning is as well as how its fusion with deep learning has enabled Offers & Deals machines to meet or surpass human­level performance on a diverse range of complex challenges, including Atari video games, the board­game Go, and subtle physical­ Highlights manipulation tasks. Settings DEEP LEARNING, AI, AND OTHER BEASTS Support Earlier in this book, we introduced deep learning with respect to vision (Chapter 1), language (Chapter 2) and the generation of novel “art” (Chapter 3). In doing this, we’ve Sign Out loosely alluded to deep learning’s relationship to the concept of artificial intelligence. At this stage, as we begin to cover deep reinforcement learning, it is worthwhile to define these terms more thoroughly as well as the terms’ relationships to one another. As usual, we will be assisted by visual cues—in this case, the Venn diagram in Figure 4.1. Figure 4.1 Venn diagram that conveys the relative positioning of the major concepts covered over the course of this book. Artificial Intelligence
Artificial Intelligence Artificial intelligence is the buzziest, vaguest and broadest of the terms we’ll be covering in this section. Taking a stab at a technical definition regardless, a decent one is that AI involves a machine processing information from its surrounding environment and then factoring that information into decisions toward achieving some desired outcome. Perhaps given this, some consider the goal of AI to be the achievement of “general intelligence”—intelligence as it is generally referred to with respect to broad 1 reasoning and problem­solving capabilities. In practice and particularly in the popular press, “AI” is used to describe any cutting­edge machine capability. Presently, these capabilities include voice recognition, describing what’s happening in a video, question­answering, driving a car, industrial robots that mimic human exemplars in the factory, and dominating humans at “intuition­heavy” board games like Go. Once an AI capability becomes commonplace (e.g., recognizing handwritten digits, which was cutting­edge in the ‘90s; see Chapter 1), the “AI” moniker is typically dropped by the popular press for that capability such that the goal posts on the definition of AI are always moving. Machine Learning Machine learning is a subset of AI alongside other facets of AI like robotics. It is a field of computer science concerned with setting up software in a manner so that the software can recognize patterns in data without the programmer needing to explicitly dictate how the software should carry out all aspects of this recognition. That said, the programmer would typically have some insight into or hypothesis about how the problem might be solved, and would thereby provide a rough model framework and relevant data such that the learning software is well­prepared and well­equipped to solve the problem. As depicted in Figure 1.13 and discussed time and again within the earlier chapters of this book, machine learning traditionally involves cleverly—albeit manually, and therefore laboriously—processing raw inputs to extract features that jive well with data­modeling algorithms. Representation Learning Peeling back another layer of the Figure 4.1 onion, we find representation learning. This term was introduced at the start of Chapter 2 so we won’t explicate on it in too much detail again here. To recap briefly, representation learning is a branch of machine learning in which models are constructed in a way that—provided they are fed enough data—they learn features (or representations) automatically. These learned features may wind up being both more nuanced and more comprehensive than their manually­ curated cousins. The trade­off is that the learned features might not be as well
understood nor as straightforward to explain, although academic and industrial 2 researchers alike are increasingly tackling these hitches. Artificial Neural Networks Artificial neural networks (ANNs) dominate the field of representation learning today. As was touched on in earlier chapters and will be laid bare in Chapter 6, artificial neurons are simple algorithms inspired by biological brain cells, especially in the sense that individual neurons—whether biological or artificial—receive input from many other neurons, perform some computation, and then produce a single output. An artificial neural network, then, is a collection of artificial neurons arranged so that they send and receive information between each other. Data (e.g., images of handwritten digits) are fed into an ANN, which processes these data in some way with the goal of producing some desired result (e.g., an accurate guess as to what digits are represented by the handwriting). Deep Learning Of all the terms in Figure 4.1, deep learning is the easiest to define because it’s so precise. We mentioned a couple of times already in this book that a network composed of at least a few layers of artificial neurons can be called a deep learning network. As exemplified by the classic architectures in Figures 1.9, 1.12 and 1.18; diagramed simply in Figure 4.2; and will be fleshed out fully in Chapter 7, deep learning networks have a total of five or more layers with the following structure: 1. A single input layer that is reserved for the data being fed into the network. 2. Three or more hidden layers that learn representations from the input data. A general­purpose and frequently used type of hidden layer is the dense type, in which all of the neurons in a given layer can receive information from each of the neurons in the previous layer (it is apt, then, that a common synonym for “dense layer” is fully­ connected layer). In addition to this versatile hidden­layer type, there is a cornucopia of specialized types for particular use cases; we’ll touch on the most popular ones as we make our way through this section. 3. A single output layer that is reserved for the values (e.g., predictions) that the network yields.
Figure 4.2 Generalization of deep­learning model architectures. With each successive layer in the network being able to represent increasingly abstract, non­linear recombinations of the previous layers, deep learning models with fewer than a dozen layers of artificial neurons are often sufficient for learning the representations that are of value for a given problem being solved with a given data set. That said, deep learning networks with hundreds or even upwards of a thousand layers have in 3 occasional circumstances been demonstrated to be of utility. As rapidly­improving accuracy benchmarks and countless competition wins since AlexNet’s 2012 victory in the ILSVRC (Figure 1.16) have demonstrated, the deep learning approach to modeling excels at a broad range of machine­learning tasks. Indeed, with deep learning driving so much of the contemporary progress in AI capabilities, the words “deep learning” and “artificial intelligence” are used essentially interchangeably by the popular press. Let’s move inside the deep learning ring of Figure 4.1 to explore classes of tasks that deep learning algorithms are leveraged for: machine vision, natural language processing and reinforcement learning. Machine Vision Via analogy to the biological vision system, Chapter 1 introduced machine vision. There we focused on object recognition tasks such as distinguishing handwritten digits or breeds of dogs. Other prominent examples of applications that involve machine vision algorithms include self­driving cars, face­tagging suggestions, and phone­unlocking via face recognition on smartphones. More broadly, machine vision is relevant to any AI that is going to need to recognize objects by their appearance at a distance or navigate a real­world environment.
Convolutional neural networks (ConvNets or CNNs for short) are a prominent type of deep learning architecture in contemporary machine vision applications. A CNN is any deep learning model architecture that features hidden layers of the convolutional type. We mentioned convolutional layers with respect to Ian Goodfellow’s generative adversarial network results in Figure 3.2; we will detail and deploy them in Chapter 10. Natural Language Processing In Chapter 2, we covered language and natural language processing. Deep learning doesn’t dominate natural language applications as comprehensively as it does machine vision applications, so our Venn diagram in Figure 4.1 shows “NLP” in both the deep learning region as well as the broader machine­learning territory. As depicted by the timeline in Figure 2.3, however, deep learning approaches to NLP are beginning to overtake traditional machine learning approaches in the field with respect to both efficiency and accuracy. Indeed, in particular NLP areas like voice recognition (e.g., Amazon’s Alexa or Google’s Assistant), machine translation (including real­time voice translation over the phone), and aspects of Internet­search engines (like predicting the characters or words that will be typed next by a user), deep learning already predominates. More generally, deep learning for NLP is relevant to any AI that interacts via natural language—be it spoken or typed—including to answer a complex series of questions automatically. A type of hidden layer that is incorporated into many deep learning architectures in the NLP sphere is the long short­term memory (LSTM) cell, a member of the recurrent neural network (RNN) family. RNNs are applicable to any data that occur in a sequence such as financial time series data, inventory levels, traffic and weather. We will expound on RNNs, including LSTMs, in Chapter 11 when we incorporate them into predictive models involving natural language data. These language examples will provide a firm foundation even if you’re primarily seeking to apply deep learning techniques to the other classes of sequential data. THREE CATEGORIES OF MACHINE LEARNING PROBLEMS The one remaining section of the Venn diagram in Figure 4.1 involves reinforcement learning, which is the focus the rest of this chapter. To introduce reinforcement learning, we’ll contrast it with the two other principal categories of machine­learning problems: supervised and unsupervised learning. Supervised Learning
In supervised learning problems, we have both an x variable and a y variable, where: œ x represents the data we’re providing as input into our model, and œ y represents an outcome we’re building a model to predict. This y variable can also be called a label. The goal with supervised learning is to have our model learn some function that uses x to approximate y. Supervised learning typically involves either: 1. Classification, where our y­values consist of labels that assign each instance of x into a particular category. In other words, y is a so­called categorical variable. Examples include identifying handwritten digits (we will code up models that do this in Chapter 10) or predicting whether someone who has reviewed a film loved it or loathed it (as we’ll do in Chapter 11). 2. Regression, where our y is a continuous variable. Examples include predicting the number of sales of a product, or predicting the future price of an asset like a home or a share in an exchange­listed company. Unsupervised Learning Unsupervised learning problems are distinguishable from supervised learning problems by the absence of a label y. Ergo, in unsupervised learning problems, we have some data x that we can put into a model, but we have no outcome y to predict. Rather, our goal with unsupervised learning is to have our model discover some hidden, underlying structure within our data. An oft­used example is that of grouping news articles by their theme. Instead of providing a pre­defined list of categories that the news articles belong to (politics, sports, finance, etc.), we configure the model to group those with similar topics for us automatically. Other examples of unsupervised learning include creating a word­vector space (see Chapter 2) from natural language data (we’ll do this in Chapter 11), or producing novel images with a generative adversarial network (as in Chapter 12). Reinforcement Learning Returning to Figure 4.1, we’re now well­positioned to cover reinforcement learning problems, which are markedly different from the supervised and unsupervised varieties. As illustrated light­heartedly in Figure 4.3, reinforcement learning problems are ones that we can frame as having an agent take actions within some environment. The agent could, for example, be a human or an algorithm playing an Atari video game, or it could be a human or an algorithm driving a car. Perhaps the primary way in which
reinforcement learning problems diverge from supervised or unsupervised ones is that the actions that the agent takes influences the information that the environment provides to the agent—that is, the agent receives direct feedback on the actions it takes. In supervised or unsupervised problems, in contrast, the model never impacts the underlying data, it simply consumes it.
Figure 4.3 The reinforcement learning loop. The top diagram is a generalized version. The bottom diagram is specific to the example elaborated on in the text of an agent playing a video game on an Atari console. To our knowledge, trilobites can’t actually play video games; we’re using the trilobite as a symbolic representation of the reinforcement learning agent, which could either be a human or a machine.
Let’s dive a bit further into the relationship between a reinforcement learning agent and its environment by exploring some examples. In Figure 4.3, the agent is represented by an anthropomorphized trilobite but this agent could be either human or it could be a machine. Where the agent is playing an Atari video game: œ The possible actions that can be taken are the buttons that can be pressed on the 4 video game controller. œ The environment (the Atari console) returns information back to the agent. This information comes in two delicious flavors: state (the pixels on the screen that represent the current condition of the environment) and reward (the point score in the game, which is what the agent is endeavoring to maximize). œ If the agent is playing Pac­Man, then selecting the action of pressing the “up” button results in the environment returning an updated state where the pixels representing the video­game character on the screen have moved upward. Prior to playing any of the game, a typical reinforcement learning algorithm would not even have knowledge of this simple relationship between the “up” button and the Pac­Man character moving upward; everything is learned from the ground up via trial and error. œ If the agent selects an action that causes Pac­Man to cross paths with a pair of delectable cherries, then the environment will return a positive reward: an increase in points. On the other hand, if the agent selects an action that causes Pac­Man to cross paths with a spooky ghost, then the environment will return a negative reward: a decrease in points. In a second example where the agent is driving a car: œ The available actions are much broader and richer than for Pac­Man. The agent can adjust the steering column, the accelerator and the brakes to varying degrees ranging from subtle to dramatic. œ The environment in this case is the real world, consisting of roads, traffic, pedestrians, trees, sky and so on. The state then is the condition of the vehicle’s surroundings, as perceived by a human agent’s eyes and ears, or by an autonomous vehicle’s cameras and lidar. œ The reward, in the case of an algorithm, could be programmed to be positive for, say, every meter of distance travelled toward a destination; it could be slightly negative for minor traffic infractions and severely negative in the event of a collision. DEEP REINFORCEMENT LEARNING
DEEP REINFORCEMENT LEARNING At long last, we reach the deep reinforcement learning section near the center of the Venn diagram in Figure 4.3. A reinforcement learning algorithm earns its “deep” prefix when an artificial neural network is involved in it, e.g., to learn what actions to take when presented with a given state from the environment in order to have a high 5 probability of obtaining a positive reward. As we’ll see in the examples coming up in the next section, the marriage of deep learning and reinforcement learning approaches has proved a prosperous one. This is because: œ Deep neural networks excel at processing the complex sensory input provided by real environments or advanced, simulated environments in order to distill relevant signals out from a cacophony of incoming data. This is analogous to the functionality of the biological neurons of your brain’s visual and auditory cortices, which receive input from the eyes and ears, respectively. œ Reinforcement learning algorithms, meanwhile, shine at selecting an appropriate action from a vast scope of possibilities. Taken together, deep learning and reinforcement learning are a powerful problem­ solving combination. Increasingly complex problems tend to require increasingly large data sets for deep reinforcement learning agents to wade through vast noise as well as vast randomness in order to discover an effective policy for what actions it should take in a given circumstance. Since many reinforcement learning problems take place in a simulated environment, obtaining a sufficient amount of data is usually not a problem: The agent can simply be trained on further rounds of simulations. While the theoretical foundations for deep reinforcement learning have been around 6 for a couple of decades, as with AlexNet for vanilla deep learning (Figure 1.18) deep reinforcement learning has in the past few years benefited from a confluence of tailwinds: 1. Exponentially larger data sets and much richer simulated environments; 2. Parallel computing across many graphics processing units (GPUs) to model efficiently with large data sets as well as the breadth of associated possible states and possible actions; 3. A research ecosystem that bridges academia and industry, producing a quickly­ developing body of new ideas on deep neural networks in general as well as on deep reinforcement learning algorithms in particular, e.g., to identify optimal actions across
a wide variety of noisy states. VIDEO GAMES Many readers of this book recall learning a new video game as a child. Perhaps while at an arcade or staring at the family’s heavy cathode­ray­tube television set, it quickly became apparent that missing the ball in Pong or Breakout was an unproductive move. We processed the visual information on the screen and, yearning for a score in excess of our friends’, devised strategies to manipulate the controller effectively and achieve this aim. In recent years, researchers at a firm called DeepMind have been producing software that likewise learns how to play classic Atari games. DeepMind was a British technology startup founded by Demis Hassabis (Figure 4.4), Shane Legg and Mustafa Suleyman in London in 2010. Their stated mission was to “solve intelligence”, which is to say they were interested in extending the field of AI by developing increasingly general­purpose learning algorithms. One of their early contributions was the introduction of Deep Q­Learning Networks (DQNs; noted within Figure 4.1). Via this approach, a single model architecture was able to learn to play multiple Atari 2600 games well—from scratch, simply through trial and error. Figure 4.4 Demis Hassabis co­founded DeepMind in 2010 after completing his Ph.D. in cognitive neuroscience at University College London. 7 8 In 2013, Volodymyr Mnih and his DeepMind colleagues published on their DQN agent, a deep reinforcement learning approach that we will come to understand intimately when we construct a variant of it ourselves line by line in Chapter 13. Their 9 agent received raw pixel values from its environment, a video­game emulator, as its
state information—akin to the way human players of Atari games view a TV screen. In order to efficiently process this information, Mnih et al.’s DQN included a convolutional neural network (CNN), a common tactic for any deep reinforcement learning model that is fed visual data (this is why we elected to overlap “Deep RL” somewhat with “Machine Vision” in Figure 4.1). The handling of the flood of visual input from Atari games (in this case, a little over two million pixels per second) underscores how well­suited deep learning in general is to filtering out pertinent features from noise. Further, playing Atari games within an emulator is a problem that is well­suited to deep reinforcement learning in particular: While they provide a rich set of possible actions that are engineered to be challenging to master, there is thankfully no finite limit on the amount of training data available since the agent can engage in endless rounds of play. During training, the DeepMind DQN was not provided any hints or strategies—it was provided only with state (screen pixels), reward (its point score, which it is programmed to maximize) and the range of possible actions (game­controller buttons) available in a given Atari game. The model was not altered for specific games, yet it was able to outperform existing machine­learning approaches in six of the seven games Mnih and his co­workers tested it on, even surpassing the performance of expert human players on three. Presumably influenced by this impressive progress, Google acquired DeepMind in 2014 for the equivalent of half a billion U.S. dollars. In a follow­up paper published in the distinguished journal Nature, Mnih and his teammates at now­Google DeepMind assessed their DQN algorithm across 49 Atari 10 games. The results are shown in Figure 4.5: It outperformed other machine­learning approaches on all but three of the games (94% of them), and astonishingly, it scored 11 above human level on the majority of them (59%).
Figure 4.5 The normalized performance scores of Mnih and colleagues’ (2015) DQN relative to a professional game tester: Zero percent represents random play, while 100% represents the pro’s best performance. The horizontal line represents the authors’ defined threshold of “human­level” play: the 75th percentile of professionals’ scores. BOARD GAMES It might sound sensible that board games would serve as a logical prelude to video games given their analog nature and their chronological head­start, however the use of
software emulators provided a simple and easy way to interact with video games digitally. Instead, the availability of these emulation tools provided the means, and so the principal advances in modern deep reinforcement learning initially took place in the realm of video games. Additionally, relative to Atari games, the complexity of some classical board games is much greater. There are myriad strategies and long­plays associated with chess expertise that are not readily apparent in Pac­Man or Space Invaders, for example. In this section, we provide an overview of how deep reinforcement learning strategies mastered the board games Go, chess and shogi despite the data­availability and computational­complexity headwinds. AlphaGo Invented several millennia ago in China, Go (illustrated in Figure 4.6) is a very popular two­player strategy board game in Asia. The game has a simple set of rules based around the idea of capturing one’s opponents’ pieces (called stones) by encircling them 12 with one’s own. This uncomplicated premise belies intricacy in practice, however. The larger board and the larger set of possible moves per turn make the game much more complex than, say, chess, for which we’ve had algorithms that can defeat the best 13 170 human players for two decades. There are a touch over 2 × 10 possible legal board 14 positions in Go, which is far more than the number of atoms in the universe and about 100 a googol (10 ) more complex than chess. Figure 4.6 The Go board game. One player uses the white stones while the other uses the black stones. The objective is to encircle the stones of your opponent, thereby capturing them. An algorithm called Monte Carlo tree search (MCTS) can be employed to play uncomplicated games competently. In its purest form, MCTS involves selecting random 15 moves until the end of gameplay. By repeating this many times, moves that tended to lead to victorious game outcomes can be weighted as favorable options. Because of the extreme complexity and sheer number of possibilities within sophisticated games like
Go, pure MCTS approach is impractical: There are simply too many options to search through and evaluate. Instead of pure MCTS, an alternative approach involve MCTS applied to a much more finite subset of actions that were curated by, for example, an established policy of optimal play. This curated approach has proved sufficient for defeating amateur human Go players but is uncompetitive against professionals. To bridge the gap from amateur­ to professional­level capability, David Silver (Figure 4.7) and his colleagues at Google DeepMind devised a program called AlphaGo that 16 combines MCTS with both supervised learning and deep reinforcement learning. . Figure 4.7 David Silver is a Cambridge­ and Alberta­educated researcher at Google DeepMind. He has been instrumental in combining the deep learning and reinforcement learning paradigms. This paragraph will be in a trilobite­reading sidebar. Silver et al. (2016) used supervised learning on a historical database of expert human Go moves to establish something called a policy network, which provides a shortlist of possible moves for a given situation. Subsequently, this policy network was refined via self­play deep reinforcement learning, wherein both opponents are Go­playing agents of a comparable skill level. Through this self­play, the agent iteratively improves upon itself, and whenever it improves, it is pitted against its now­improved self, producing a positive­ feedback loop of continuous advancement. Finally, the cherry atop the AlphaGo algorithm: a so­called value network that predicts the winner of the self­play games, thereby evaluating positions on the board and learning to identify strong moves. The combination of these policy and value networks reduces the breadth of search space for
the MCTS. END SIDEBAR. AlphaGo was able to win the vast majority of games it played against other computer­ based Go players. Perhaps most strikingly, AlphaGo was also able to defeat Fan Hui, the then­reigning European Go champion, five games to zero. This marked the first time a computer defeated a professional human player in a full play of the game. As 17 exemplified by the Elo ratings in Figure 4.8, AlphaGo performed at or above the level of the best players in the world. Following this success, AlphaGo was famously matched against Lee Sedol in March 2016 in Seoul, South Korea. Sedol has 18 world titles and is considered one of the great players. The five­game match was broadcast and viewed live by 200 million people. AlphaGo won the match 4­1 launching DeepMind, Go, and the artificially­intelligent future into public imagination. Figure 4.8 The Elo score of AlphaGo (blue) relative to Fan Hui (green) and several Go programs (red). The approximate human rank is shown on the right. AlphaGo Zero Having just learned about the incredible feats achieved by AlphaGo, it should come as no surprise that the folks at DeepMind took their work further and created a second­ generation Go player: AlphaGo Zero. If you recall from the previous section, AlphaGo was initially trained in a supervised manner; that is, expert human moves were used to train the network initially, whereafter the network learned by reinforcement learning through self­play. While this is still an impressive achievement, it doesn’t exactly “solve intelligence” as DeepMind’s founders would have liked. A better approximation of
general intelligence would be a network that can learn to play Go in a completely de novo setting —where the network is not supplied with any human input or domain knowledge, but improves by deep reinforcement learning alone. Enter: AlphaGo Zero. As we’ve alluded to before, the game of Go requires sophisticated lookahead capabilities through vast search spaces. That is, there are so many possible moves and such a tiny fraction of them are good moves in the short­ and long­play of the game that performing a search for the optimal move, keeping the likely future state of the game in mind, becomes impossibly complex and computationally impractical. It is for this reason that it was thought at the time that Go would be a final frontier for machine intelligence —indeed, it was thought that the achievements of AlphaGo in 2016 were still a decade or more away. Working off the momentum from the AlphaGo­Sedol match in Seoul, researchers at DeepMind created AlphaGo Zero which learns to play Go far beyond the level of the original AlphaGo—while being revolutionary in several 18 ways. First and foremost, it is trained without any data from human gameplay. That means it learns purely by trial­and­error. Second, it uses only the stones on the board as input features. Contrastingly, AlphaGo received 15 additional features during training, which included information such as how many turns since a move was played or how many opponent stones would be captured. Third, a single (deep) neural network was used to evaluate the board and decide on a next move, rather than separate policy and value networks. Finally, the tree search is simpler and relies on the neural network to evaluate positions and possible moves. AlphaGo Zero played almost five million games of self­play over three days, taking an estimated 0.4s per move to “think”. Within 36 hours, it had begun to outperform the model that beat Lee Sedol in Seoul (retrospectively termed AlphaGo Lee), which—in stark contrast—took several months to train. At the 72­hour mark, the model was pitted against AlphaGo Lee in match conditions, where it handily won every single one of a hundred games. Even more remarkable is that AlphaGo Zero achieved this on a single 19 machine with four tensor processing units (TPUs) whereas AlphaGo Lee was distributed over multiple machines and used 48 TPUs (Similarly, AlphaGo Fan, which 20 beat Fan Hui, was distributed over 176 GPUs). In Figure 4.9, the Elo score for 21 AlphaGo Zero is shown over time compared to the scores for AlphaGo Master and AlphaGo Lee. On the right we can see the absolute Elo scores for a variety of iterations of AlphaGo and some other Go programs. AlphaGo Zero is far­and­away the superior Go player amongst this prestigious group. Another interesting point that emerged from this research was that the nature of the game­play by AlphaGo Zero is qualitatively different to that of human players and AlphaGo Lee. AlphaGo Zero began with random play, but quickly learned professional
joseki —corner sequences that are considered heuristics of good play. Eventually, after further training, the mature model tended to prefer novel joseki that were previously unknown to humankind. AlphaGo Zero did spontaneously learn a whole range of classical Go moves, implying a pragmatic alignment with these techniques. However, the model did this in a novel manner: It did not learn the concept of shicho (ladder sequences), for example, until much later in its training, whereas this is one of the first concepts taught to new human players. The authors additionally trained another iteration of the model in a supervised manner. This supervised model performed better initially, however it began to succumb to the self­learned model within the first 24 hours of training and ultimately achieves a lower Elo score (Figure 4.9). Together, these results suggest that the self­learned model might have a distinct style of play to that of human players; a more dominating style and one that the supervised model fails to develop. Figure 4.9 Comparing Elo scores between AlphaGo Zero and other AlphaGo variations or other Go programs. In the left­hand figure, the comparison is over days of AlphaGo Zero training. AlphaZero Following these successes, the team at DeepMind went on to look into even more general game­playing neural networks. While AlphaGo Zero is adept at playing Go, could a comparable network learn to play multiple games well? To put this to the test, 22 they added two new games to the repertoire: chess and shogi . Most readers are likely familiar with the game of chess, and shogi—referred to by some as Japanese chess—is similar. Both games are two­player strategy games, both take place on a grid­format board, both culminate in a checkmate of the opponent’s king, and both consist of a range of pieces with different moving abilities. However, shogi is significantly more complex than chess given the larger board size (9x9, versus 8x8 in chess) and the fact that opponent pieces can be replaced anywhere on the board after their capture. Historically, artificial intelligence has had a rich interaction with the game of chess.
Over several decades, computer programs that play chess have been developed extensively. Perhaps the most famous to date is Deep Blue, a creation of IBM, that went 23 on to beat the world champion Garry Kasparov in 1997 . It was heavily reliant on 24 brute­force computing power to execute complex searches through possible moves, and combined this with hand­crafted features and domain­specific adaptations. The evaluation function of Deep Blue was fine­tuned by analyzing thousands of master games (it was a supervised­learning system!) and this function was even tweaked 25 between games. It’s clear that Deep Blue, and other chess programs like it, were hard­ coded to play chess and nothing else. While Deep Blue was an achievement two decades ago, however their system was not generalizable—it conceivably would not perform well at any task other than chess. After AlphaGo Zero demonstrated that the game of Go could be learned by a neural network from first principles alone, given nothing but the board and the rules of the game, Silver and his colleagues set out to devise a generalist neural network, a single network architecture that could dominate not only at Go, but other board games as well. Compared to Go, chess and shogi present pronounced obstacles. The rules of the games are position­dependent (in that pieces can move differently based on where they are on 26 the board), asymmetrical (pieces can only move in one direction) , long­range actions are possible (such as the queen moving across the entire game), captured pieces can be replaced in shogi, and the game can result in a draw. AlphaZero feeds the board positions into a neural network and outputs a vector of move 27 probabilities for each possible action, as well as a scalar outcome value for that move. The network learns the parameters for these move probabilities and outcomes entirely from self­play deep reinforcement learning, AlphaGo Zero did. An MCTS is then performed on the reduced space guided by these probabilities, returning a vector of probabilities over the possible moves. Where AlphaGo Zero optimized the probability of winning (Go is a binary win/loss game), Alpha Zero instead optimizes for the expected outcome. During self­play, AlphaGo Zero would retain the best player to date, and evaluate updated versions of itself against that player, continually replacing the player with the next best version; Alpha Zero instead maintains a single network and at any given time is playing against the latest version of itself. Alpha Zero was trained to play each of chess, shogi and Go for a mere 24 hours. There were no game­specific modifications, with the exception of a noise parameter that promotes move exploration —this was scaled to the number of legal moves in each game. After one hundred games, Alpha Zero had not lost a single game against the 2016 Top Chess Engine Championship (TCEC) world champion Stockfish. In shogi, the Computer
Shogi Association (CSA) world champion Elmo managed to beat Alpha Zero only eight times in 100 games. Perhaps its most worthy opponent, AlphaGo Zero was able to defeat Alpha Zero in forty of their hundred games. Figure 4.10 shows the Elo scores for Alpha Zero relative these three adversaries. Not only was Alpha Zero superior; it was efficient. Alpha Zero’s Elo score exceeded its greatest foes’ after just two, four and eight hours for shogi, chess and Go, respectively. This is a sensationally rapid rate of learning, considering that in the case of Elmo and Stockfish, these computer programs represent the culmination of decades of research and fine­tuning in a focused, domain­ specific manner. The generalizable Alpha Zero algorithm is able to play all three games with aplomb: Simply switching out learned weights from otherwise identical neural network architectures imbues each with the same skills that have taken years to develop by other means. These results demonstrate that deep reinforcement learning is a strikingly powerful approach for developing general expert gameplay in an undirected fashion. Figure 4.10 Comparing Elo scores between Alpha Zero and each of its opponents in chess, shogi and Go. Alpha Zero rapidly outperformed all three opponents. MANIPULATION OF OBJECTS As this chapter’s title might have suggested, we’ve centered our coverage of deep reinforcement learning on its game­playing applications. While games offer a hot testbed for exploring the generalization of machine intelligence, in this section we’ll spend a few moments expounding on a practical, real­world applications of deep reinforcement learning as well. We mentioned some such applications earlier in this chapter: autonomous vehicles are an excellent example (and it might be plain to see, this application isn’t that different from game­playing, albeit with higher stakes). As an example, we’ll provide an overview of research by Sergey Levine, Chelsea Finn 28 (Figure 4.11) and lab­mates at the University of California, Berkeley . These researchers trained a robot to perform a number of motor skills that require complex visual understanding and depth perception, such as screwing the cap back onto a bottle, removing a nail with a toy hammer, placing a hanger on a rack or inserting a cube in a shape­fitting game (Figure 4.12).
Figure 4.11 Chelsea Finn is a doctoral candidate at the University of California, Berkeley in its AI Research Lab. Figure 4.12 Sample images from Levine, Finn et al. (2016) exhibiting various object­manipulation actions the robot was trained to perform. Levine, Finn and colleagues’ algorithm maps raw visual input directly to the movement of the motors in the robot’s arm. Their policy network was a seven­layer­deep convolutional neural network (CNN) consisting of less than a hundred thousand artificial neurons—a minuscule amount in deep­learning terms, as we’ll see when we train orders­of­magnitude larger networks later in this book. While it would be tricky to
elaborate further on this approach before we’ve delved much into artificial­neural­ network theory (Part II, which is just around the corner), there are three takeaway points we’d like to highlight on this elegant practical application of deep reinforcement learning. First, it is an “end­to­end” deep learning model in the sense that the model takes in raw images (pixels) as inputs and then outputs directly to the robot’s motors. Second, the model generalizes neatly to a broad range of unique object­manipulation tasks. Third, it is an example of the policy gradient family of deep reinforcement learning approaches, rounding out the terms featured in the Venn diagram in Figure 4.1. Policy gradient methods are distinct from the DQN approach that is the focus of Chapter 13 but we’ll touch on it then too. POPULAR DEEP REINFORCEMENT LEARNING ENVIRONMENTS Over the last few sections, we’ve talked a fair bit about software emulation of environments in which to train reinforcement learning models. This area of development is crucial to the ongoing progression of reinforcement learning—without environments in which our agents can play and explore (and gather data!) there would be no training of models. Here we’ll introduce a the three most popular environments, discussing their high­level attributes. OpenAI Gym 29 30 The OpenAI Gym is developed by the non­profit AI research company OpenAI . The mission of OpenAI is to advance artificial general intelligence (more on that in the next section!) in a safe and equitable manner. To that end, the researchers at OpenAI have produced and open­sourced a number of tools for AI research, including the OpenAI Gym. This toolkit is designed to provide an interface for training reinforcement learning models, be they deep or otherwise. As captured in Figure 4.13, the Gym 31 includes a wide variety of environments, including a number of Atari 2600 games, multiple robotics simulators, a few simple text­based algorithmic games and several 32 robotics simulations using the MuJoCo physics engine . In Chapter 13, we’ll install OpenAI Gym in a single line of code and then employ an environment it provides to train the DQN agent that we build. The gym is written in Python and is compatible with any deep­learning computation library, e.g., TensorFlow (Chapter 14) and PyTorch (Chapter 15).
Figure 4.13 A sampling of OpenAI Gym environments. (a) CartPole, a classic control­theory problem. (b) LunarLander, a continuous­control task run inside a two­dimensional simulation. (c) Skiing, an Atari 2600 game. (d) Humanoid, a three­dimensional MuJuCo physics engine simulation of a bipedal person. (e) FetchPickAndPlace, one of several available simulations of real­world robot arms, in this case involving one called Fetch with the goal of grasping a block and placing it in a target location. (f) HandManipulateBlock, another practical simulation of a robotic arm, the Shadow Dexterous Hand. DeepMind Lab 33 DeepMind Lab is another RL environment, this time from the developers at Google DeepMind (although they point out that DeepMind Lab is not an official Google product). As can be seen in Figure 4.14, the environment is built on top of id software’s 34 Quake III Arena and provides a sci­fi inspired three­dimensional world in which agents can explore. The agent experiences the environment from the first­person
perspective, which is distinct from the Atari emulators available via the OpenAI Gym. Figure 4.14 A DeepMind Lab environment, in which positive­reward points are awarded for capturing scrumptious green apples. There are a variety of levels available which can be roughly divided into four categories: 1. Fruit gathering levels, where the agent simply tries to find and collect rewards (apples and melons) while avoiding penalties (lemons). 2. Navigation levels with a static map, where the agent is tasked with finding a goal and remembering the layout of the map. The agent can either be randomly placed within a map at the start of each episode while the goal remains stationary, which tests initial exploration followed by a reliance on memory to repeatedly find the goal; or the agent can start in the same place while the goal is moved for every episode, testing the agents ability to explore. 3. Navigation levels with random maps, where the agent is required to explore a novel map in each episode and find the goal, and then repeatedly return to the goal as many times as possible within a time limit. 4. Laser­tag levels, where the agent is rewarded for hunting and attacking bots in an array of different scenes. The color and texture of the bots are randomized for each episode to prevent the agent from recognizing simple colors too easily. 35 Installation of DeepMind Lab is not as straightforward as the OpenAI Gym, but it provides a rich, dynamic first­person environements in which to train agents, and the
levels provide complex scenarios involving navigation, memory, strategy, planning and fine­motor skills. These challenging environments push the limits of what is tractable with contemporary deep reinforcement learning. Unity ML-Agents Unity is a highly sophisticated engine for two­ and three­dimensional video games and digital simulations. Given everything we’ve learned about reinforcement learning over the course of this chapter, it should come as no surprise that the makers of a popular game engine are also in the business of providing environments to incorporate 36 reinforcement learning into video games. The Unity ML­Agents plugin enables reinforcement learning models to be trained within Unity­based video games or simulations and, perhaps more fitting with the purpose of Unity itself, allows reinforcement learning models to guide the actions of agents within the game. From a distance, it appears inevitable that the development of sophisticated reinforcement learning models to control other characters within video games is going to be a momentous step forward in the gaming experience. 37 As with DeepMind Lab, installation of Unity ML­Agents is not a one­liner. THREE CATEGORIES OF AI Of all deep learning topics, deep reinforcement learning is perhaps the one most closely tied to the popular perception of artificial intelligence as a system for replicating the cognitive, decision­making capacity of humans. In light of that, to wrap up this chapter, in this section we introduce three categories of AI. Artificial Narrow Intelligence Artificial narrow intelligence (ANI) is machine expertise at a very specific task. Many diverse examples of ANI exist today, and we’ve mentioned plenty already, such as the visual recognition of objects, real­time machine translation between natural languages, automated financial­trading systems, AlphaZero and self­driving cars. Artificial General Intelligence Artificial general intelligence (AGI) would involve a single algorithm that could perform well at all of the tasks described in the previous paragraph: It would be able to recognize your face, translate this book into another language, optimize your investment portfolio, beat you at Go, and take you safely to your holiday destination. Indeed, this algorithm would be approximately indistinguishable from all of the intellectual capabilities that humans have. There are so many hurdles to overcome in
order for AGI to be realized that it is probably impossible to guess when it will be achieved, if it will be achieved at all. That said, AI experts are happy to wave a finger in the air and speculate on timing. In a study conducted by the philosopher Vincent 38 Müller and the influential futurist Nick Bostrom, the median estimate across hundreds of professional AI researchers was that AGI will be attained in the year 2040. Artificial Super Intelligence Artificial Super Intelligence (ASI) is difficult to describe because it’s properly mind­ boggling. ASI would be an algorithm that is markedly more advanced than the 39 intellectual capabilities of a human. If AGI is possible, then ASI may be as well. Of course, there are even more hurdles on the road to ASI than to AGI, presumably most of which we can’t foresee clearly today. Citing the Müller and Bostrom (2014) survey again, however, AI experts’ median estimate for the arrival of ASI is 2060, a rather hypothetical date that falls within the lifespan of many earthlings alive today. In Chapter 16, at which point you’ll be well­versed in deep learning both in theory and in practice, we’ll discuss both how deep learning models could contribute to AGI as well as the present limitations associated with deep learning that would need to be bridged in order to attain AGI or, gasp, ASI. SUMMARY The chapter began with an overview relating deep learning to the broader field of artificial intelligence. We then detailed deep reinforcement learning, an approach that blends deep learning with the feedback­providing reinforcement learning paradigm. As discussed via real­world examples ranging from the boardgame Go to the grasping of physical objects, such deep reinforcement learning enables machines to process vast amounts of data and take sensible actions on complex tasks, associating it with popular conceptions of AI. 1 . Defining “intelligence” is not straightforward and the great debate on it is beyond the scope of this book. A century­old definition of the term that we find amusing and that still today has some proponents among contemporary experts is that “intelligence is whatever IQ­tests measure.” See, e.g.: van der Mass, H., et al. (2014). Intelligence Is What the Intelligence Test Measures. Seriously. Journal of Intelligence, 2, 12­15. 2 . E.g.: Kindermans, P.­J., et al. (2018). Learning how to explain neural networks: PatternNet and PatternAttribution. International Conference on Learning Representations.
3 . E.g.: He, K., et al. (2016). Identity Mappings in Deep Residual Networks. arXiv:1603.05027. 4 . We’re not aware of video game­playing algorithms that literally press the buttons on the game console’s controllers. They would typically interact with a video game directly via a software­based emulation. We’ll go through the most popular open­source packages for doing this at the end of the chapter. 5 . Earlier in this chapter (see Figure 4.2), we indicated that the “deep learning” moniker applies to an artificial neural network that has at least three hidden layers. While in general this is the case, when used by the reinforcement learning community, the term “deep reinforcement learning” may be used even if the artificial neural network involved in the model is shallow, i.e., composed of as few as one or two hidden layers. 6 . Tesauro, G. (1995). Temporal Difference Learning and TD­Gammon. Communications of the Association for Computing Machinery, 38, 58­68. 7 . Mnih obtained his doctorate at the University of Toronto under the supervision of Geoff Hinton (Figure 1.17). 8 . Mnih, V. et al. (2013). Playing Atari with Deep Reinforcement Learning. arXiv: 1312.5602. 9 . Bellemare, M. et al. (2012). The Arcade Learning Environment: An Evaluation Platform for General Agents. arXiv: 1207.4708. 10. Mnih, V., et al. (2015). Human­level control through deep reinforcement learning. Nature, 518, 529­33. 11. You can be entertained by watching the Google DeepMind DQN learn to master Space Invaders and Pong here: youtu.be/iqXKQf2BOSE 12. Indeed, Go in Chinese translates literally to “encirclement board game”. 13. IBM’s Deep Blue defeated Garry Kasparov, arguably the world’s greatest­ever chess player, in 1997. More on that storied match coming up shortly in this section. 80
80 14. There are an estimated 10 atoms in the observable universe. 15. Hence “Monte Carlo”: the casino­dense district of Monaco evokes imagery of random outcomes. 16. Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529, 484­9. 17. Elo ratings enable the skill level of human and artificial game­players alike to be compared. Derived from calculations of head­to­head wins and losses, an individual with a higher Elo score is more likely to win a game against an opponent with a lower score. The larger the score gap between the two players, the greater the probability that the player with the higher score will win. 18. Silver, D., et al. (2016). Mastering the game of Go without human knowledge. Nature 550, 354­359. 19. Google built custom processor units for training neural networks, known as tensor processing units (TPUs). They took the existing architecture of a GPU and specifically optimized it for performing tensor calculations (these will become familiar in Chapter 14) during neural network training. At the time of writing, TPUs were available to the public via the Google Cloud Platform. 20. The Elo rating system was created by Arpad Elo as a means to compare the relative skill levels in zero­sum games such as chess. A 200­point margin corresponds to a 75% likelihood of winning a game. 21. AlphaGo Master is a hybrid between AlphaGo Lee and AlphaGo Zero, however it uses the extra input features enjoyed by AlphaGo Lee and initializes training in a supervised manner. AlphaGo Master famously played online anonymously in January 2017 under the pseudonyms Master and Magister. It won all 60 of the games it played against some of the world’s strongest Go players. 22. Silver, D., et al. (2017). Mastering Chess and Shogi by Self­Play with a General Reinforcement Learning Algorithm. arXiv:1712.01815. 23. It’s worth pointing out that Deep Blue lost its first match against Kasparov in 1996, and after significant upgrades went on to narrowly beat Kasparov in 1997. This was not the total domination of man by machine that AI proponents might have hoped for. 24. Deep Blue was the planet’s 259th most powerful supercomputer at the time of the
match against Kasparov. 25. This was a point of contention between IBM and Kasparov after his loss in 1997. IBM refused to release the logs initially and dismantled Deep Blue. Their computer system never received an official chess rating because it played so few games against rated chess masters. 26. This makes expanding the training data via synthetic augmentation—an approach used copiously for AlphaGo—more challenging 27. A single value 28. Levine, S., Finn, C., et al. (2016). End­to­End Training of Deep Visuomotor Policies. Journal of Machine Learning Research, 17, 1­40. 29. github.com/openai/gym 30. openai.com 31. OpenAI Gym uses the Arcade Learning Environment to emulate Atari 2600 games. This same framework is used in the Mnih et al. (2013) paper described in the Video Games section. You can find the framework yourself at https://github.com/mgbellemare/Arcade­Learning­Environment 32. MuJoCo is an abbreviation of Multi­Joint dynamics with Contact. It is a physics engine that was developed by Emo Todorov for Roboti LLC. 33. Beattie, C. et al. (2016). DeepMind Lab. arXiv:1612.03801 34. id software. Quake3, 1999. https://github.com/id­Software/Quake­III­Arena. 35. First the Github repository (github.com/deepmind/lab) is cloned, and then the software must be built using Bazel (https://docs.bazel.build/versions/master/install.html). The DeepMind Lab repository provides detailed instructions (https://github.com/deepmind/lab/blob/master/docs/users/build.md) 36. github.com/Unity­Technologies/ml­agents 37. It requires the user to first install Unity (for download and installation instructions, see https://store.unity.com/download) and then clone the Github repository. Full instructions are available at the Unity ML­Agents Github repository
(https://github.com/Unity­Technologies/ml­ agents/blob/master/docs/Installation.md) 38. Müller, V. and Bostrom, N. (2014). Future Progress in Artificial Intelligence: A Survey of Expert Opinion. In V. Müller (Ed.), Fundamental Issues of Artificial Intelligence. Berlin: Springer. 39. In 2015, Tim Urban provided a lengthy two­part series of posts that entertainingly covers ASI and the related literature. It is available here (to bitly): waitbutwhy.com/2015/01/artificial­intelligence­revolution­1.html
History Part II. Essential Theory Illustrated TopCichs apter 5 The (Code) Cart Ahead of the (Theory) Horse Chapter 6 Artificial Neurons Detecting Hot Dogs Tutorials Chapter 7 Artificial Neural Networks Offers & Deals Chapter 8 Training Deep Networks Highlights Chapter 9 Improving Deep Networks Settings Support Sign Out
5 The (Code) Cart Ahead of the (Theory) Horse Playlists In Part I, we provided a high­level overview of deep learning by demonstrating its use History across a spectrum of cutting­edge applications. Along the way, we sprinkled in foundational deep learning concepts from its hierarchical, representation­learning Topics nature through to its relationship to the field of artificial intelligence. Repeatedly, as we touched on a concept, we noted that in the second part of the book we would dive into Tutorials the low­level theory and mathematics behind it. While we promise this is true, we are going to take this final opportunity to put the fun, hands­on coding cart ahead of the Offers & Deals proverbial—in this case, theory­laden—horse. Highlights In this chapter we will do a line­by­line walk through of a notebook of code featuring a deep learning model. While you will need to bear with us because we have not yet Settings detailed much of the theory underpinning the code, this serpentine approach will make the apprehension of theory in the subsequent chapters easier: Instead of being an Support abstract idea, each element of theory we introduce in this part of the book will be rooted Signb Oyu at tangible line of applied code. PREREQUISITES Working through the examples in this book will be easiest if you are familiar with the basics of the Unix command line. These are provided by Zed Shaw in Appendix A of his 1 deceptively enjoyable Learn Python the Hard Way. Speaking of Python, since it is comfortably the most popular software language in the data science community (at time of writing, anyway), it’s the language we selected for our example code throughout the book. Python’s prevalence extends both across the composition of standalone scripts through to the deployment of machine­learning models into production systems. If you’re new to Python or you’re feeling a tad rusty, Shaw’s book serves as an appropriate general reference while Daniel Chen’s Pandas for 2 Everyone is appropriate for applying the language to modeling data in particular. INSTALLATION Regardless of whether you’re planning on executing our code notebooks via Unix,
Linux, Mac OS or Windows, we have made step­by­step installation instructions available in the GitHub repository that accompanies this book: github.com/illustrated­series/deep­learning­illustrated. If you’d prefer to view the completed notebooks instead of running them on your own machine, you are more than welcome to do that from the GitHub repo as well. 3 We elected to provide our code within the comfort of interactive Jupyter notebooks. Jupyter is a common option today for writing and sharing scripts, particularly during exploratory phases in which a data scientist is experimenting with preprocessing, visualizing and modeling her data. Our installation instructions suggest running 4 Jupyter from within a Docker container. This containerization ensures that you’ll have all of the software dependencies you need to running our notebooks while simultaneously preventing these dependencies from clashing with software you already have installed on your system. A SHALLOW NETWORK IN KERAS To kick off the code portion of our book, we will: œ Detail a revered data set of handwritten digits, œ Load these data into a Jupyter notebook, œ Use Python to prepare the data for modeling, and œ Write a few lines of code in the high­level API Keras to construct an artificial neural network (in TensorFlow, behind the scenes) that predicts what digit a given handwritten sample represents. The MNIST Handwritten Digits Back in Chapter 1 when we introduced the LeNet­5 machine­vision architecture (Figure 1.12), we mentioned that one of the advantages Yann LeCun (Figure 1.10) and his colleagues had over previous deep­learning practitioners was a superior data set for training their model. This data set of handwritten digits, called MNIST (see the samples in Figure 5.1), came up again in the context of being imitated by Ian Goodfellow’s generative adversarial network (Figure 3.2a). The MNIST data set is ubiquitous across deep­learning tutorials, and for good reason. By modern standards, the data set is small enough that it can be modeled rapidly, even on a laptop computer processor. In addition to their portable size, the MNIST digits are also handy because they occupy a sweet spot with respect to how challenging they are to classify: The handwriting
samples are sufficiently diverse and contain complex enough details that they are not easy for a machine­learning algorithm to identify with high accuracy, and yet by no means do they pose an insurmountable problem. However, as we shall observe ourselves as this part of the book develops, a well­designed deep­learning model can near­faultlessly classify the handwriting as the appropriate digit. Figure 5.1 A sample of a dozen images from the MNIST dataset. Each image contains a single digit handwritten by either a high­school student or a U.S. census worker. The MNIST data set was curated by LeCun, Corinna Cortes (Figure 5.2) and the 5 Microsoft­AI­researcher­turned­musician Chris Burges in the 1990s. It consists of sixty thousand handwritten digits for training an algorithm and ten thousand more for validating the algorithm’s performance on previously unseen data. The data are a subset (nay, a modification) of a larger body of handwriting samples collected from high school students and census workers by the United States’ National Institute of Standards and Technology (NIST ).
Figure 5.2 The Danish computer scientist Corinna Cortes is Head of Research at Google’s New York office. Among her countless contributions to both pure and applied machine learning, Cortes (with Chris Burges and Yann LeCun) curated the ubiquitous MNIST dataset. 6 As exemplified by Figure 5.3, every MNIST digit is a 28­by­28 pixel image. We quickly became bored of drawing individual pixels so only depicted them individually in the top­left corner of the figure, but of course the entirety of the handwritten digit (in this example, the number two) is represented by pixels. Each pixel is 8­bit, meaning that the pixel darkness can vary from zero (white) to 255 (black), with the intervening range of integers representing gradually darker shades of gray.
Figure 5.3 Each handwritten MNIST digit is stored as a 28­by­28­pixel grayscale image. See the Jupyter notebook titled MNIST digit pixel by pixel that accompanies this book for the code we used to create this figure. A Schematic Diagram of the Network 7 In our Shallow Net in Keras Jupyter notebook , we create an artificial neural network to predict what digit a given handwritten MNIST image represents. As shown in the rough schematic diagram in Figure 5.4, this artificial neural network features one hidden layer of artificial neurons for a total of three layers. Recalling Figure 4.2, with so few layers this ANN would not generally be considered a deep learning architecture; hence it is shallow. Figure 5.4 A rough schematic of the shallow artificial­neural­network architecture we’re whipping up in this chapter. We’ll cover the particular sigmoid and softmax flavors of artificial neurons in Chapter 6. The first layer of the network is reserved for inputting our MNIST digits. As they are
28­by­28 pixel images, each one has a total of 784 values. After we load in the images, we’ll flatten them from their native, two­dimensional 28­by­28 shape to a one­ dimensional array of 784 elements. This paragraph is a Trilobite­attention SIDEBAR. You could argue that collapsing the images from two dimensions to one will cause us to lose a lot of the meaningful structure of the handwritten digits. Well, if you argued that, you’d be right! Working with one­dimensional data, however, means we can use less sophisticated deep learning models, which is appropriate at this early stage in our journey. Later, in Chapter 10, we’ll be in a position to appreciate more complex models that can handle multi­ dimensional inputs. END SIDEBAR. The pixel­data inputs will be passed through a single, hidden layer of 64 artificial 8 neurons. The number (64) and type (sigmoid) of these neurons aren’t critical details at present; we’ll begin to explain these model attributes in the next chapter. The key piece of information at this time is that, like we demonstrated in Chapter 1 (see Figures 1.19 and 1.20), the neurons in the hidden layer are responsible for learning representations of the input data so that the network can predict what digit a given image represents. Finally, the information output by the hidden layer will be passed to ten neurons in the output layer. Again, we’ll detail how softmax neurons work in the next chapter but, in essence, we have ten neurons because we have ten types of digit to classify. These ten neurons each output a probability: one for each of the ten possible digits that a given MNIST image could represent. As an example, a fairly­well­trained network which is fed the image in Figure 5.3 might output that there is a 0.92 probability that the image is of a two, a 0.06 probability that it’s a three, a 0.02 probability that it’s an eight, and a zero probability for the other seven digits. Loading the Data At the top of the notebook we import our software dependencies, which is unexciting but necessary: Example 5.1 Software dependencies for shallow net in Keras import keras from keras.datasets import mnist from keras.models import Sequential
from keras.layers import Dense from keras.optimizers import SGD from matplotlib import pyplot as plt We import Keras because that’s the library we’re using to fashion our neural network. We also import the MNIST dataset because these, of course, are the data we’re working with in this example. The lines ending in Sequential, Dense and SGD will make sense later; no need to worry about them at this stage. Finally, the matplotlib line will enable us to plot MNIST digits out to our screen. With these dependencies imported, we can conveniently load the MNIST data in a single line of code: (X_train, y_train), (X_valid, y_valid) = mnist.load_data() Let’s examine these data. As mentioned in Chapter 4, the mathematical notation x is used to represent the data we’re feeding into a model as input while y is used for the labelled output that we’re training the model to predict. With this in mind, X_train 9 stores the MNIST digits we’ll be training our model on. Executing X_train.shape yields the output (60000, 28, 28). This shows us that, as expected, we have 60,000 images in our training data set, each of which is a 28­by­28 matrix of values. Running y_train.shape, we unsurprisingly discover we have 60,000 labels indicating what digit each of the 60,000 training images contains. y_train[0:12] outputs an array of twelve integers representing the first dozen labels, so we can see that the first handwritten digit in the training set (X_train[0]) is the number five, the second is a zero, the third is a four, and so on: array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5], dtype=uint8) These happen to be the same dozen MNIST digits that were shown above in Figure 5.1, a figure we created by running the following chunk of code:
plt.figure(figsize=(5,5)) for k in range(12): plt.subplot(3, 4, k+1) plt.imshow(X_train[k], cmap='Greys') plt.axis('off') plt.tight_layout() plt.show() Akin to the training data, by examining the shape of the validation data (X_valid.shape, y_valid.shape), we note that there are the expected ten thousand 28­by­28 pixel validation images, each with a corresponding label: (10000, 28, 28), (10000,). Investigating the values that make up an individual image like X_valid[0], we observe that the matrix of integers representing the handwriting is primarily zeros (whitespace). Tilting our head, you might even be able to make out that the digit in this example is a seven with the highest integers (e.g., 254, 255) representing the black core of the handwritten figure and the outline of the figure (composed of intermediate integers) fading toward white. To corroborate that this is indeed the number seven, we both printed out the image with plt.imshow(X_valid[0], cmap='Greys') (output shown in Figure 5.5) and printed out its label with y_valid[0] (output was 7).
Figure 5.5 The first MNIST digit in the validation data set (X_valid[0]) is a seven. Reformatting the Data The MNIST data now loaded, we come across the heading Preprocess data in the notebook. We won’t, however, be preprocessing the images by applying functions to, say, extract features that provide hints to our artificial neural network. We will simply be rearranging the shape of the data so that they match up with the shapes of the input and output layers of the network. Thus, we’ll flatten our 28­by­28 pixel images into 784­element­long arrays. We employed the reshape() method to do this: X_train = X_train.reshape(60000, 784).astype('float32') X_valid = X_valid.reshape(10000, 784).astype('float32') Simultaneously, we used astype('float32') to convert the pixel darknesses from 10 integers into single­precision float values. This conversion was preparation for the subsequent step, in which we divided all of the values by 255 so that they range from 11 zero to one: X_train /= 255
X_valid /= 255 Revisiting our example handwritten seven from Figure 5.5 by running X_valid[0], we can verify that it is now represented by a one­dimensional array made up of float values as low as zero and as high as one. That’s all for reformatting our model inputs X. For the labels y, we need to convert them from integers into one­hot encodings; we’ll demonstrate what these are via application: n_classes = 10 y_train = keras.utils.to_categorical(y_train, n_classes) y_valid = keras.utils.to_categorical(y_valid, n_classes) There are ten possible handwritten digits, so we set n_classes equal to 10. In the other two lines of code we use a convenient utility function to_categorical, which is provided within the Keras library, to transform both the training and validation labels from integers into the one­hot format. Execute y_valid to see how the label seven is represented now: array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32) Instead of using an integer to represent seven, we have an array of length ten consisting entirely of zeroes, with the exception of a 1 in the eighth position. In such a one­hot encoding, the label zero would be represented by a lone 1 in the first position, one by a lone 1 in the second position, and so on. We arrange the labels with such one­hot encodings so that they line up with the ten probabilities being output by the final layer of our artificial neural network. They represent the ideal output that we are striving to attain with our network: If the input image is a handwritten seven then a perfectly­ trained network would output a probability of 1.00 that it is a seven and a probability of 0.00 for each of the other nine classes of digits. Designing a Neural Network Architecture From your authors’ perspective, this is the most pleasurable bit of any script featuring
deep learning code: architecting the artificial neural net itself. There are infinite possibilities here and, as we progress through the book, you will begin to develop an intuition that guides the selection of the architectures you experiment with for a given problem. Referring back to Figure 5.4, for the time being, we’re keeping the architecture as elementary as possible in Example 5.2: Example 5.2 Keras code to architect a shallow neural network model = Sequential() model.add(Dense(64, activation='sigmoid', input_shape=(784,))) model.add(Dense(10, activation='softmax')) In the first line of code, we instantiate the simplest type of neural­network model 12 object, the sequential type and—in a dash of extreme creativity—name the model model. In the second line, we use the add() method of our model object to specify the attributes of our network’s hidden layer (64 sigmoid­type artificial neurons in the 13 general­purpose, fully­connected arrangement defined by the Dense() method) as well as the shape of our input layer (one­dimensional array of length 784). In the third and final line we use the add() method again to specify the output layer and its parameters: ten artificial neurons of the softmax variety, corresponding to the ten probabilities (one for each of the ten possible digits) that the network will output when fed a given handwritten image. Training a Deep Learning Model Later, we’ll return to the model.summary() and model.compile() steps of the Shallow Net in Keras notebook, as well as its three lines of arithmetic. For now, skipping ahead to the model­fitting step: Example 5.3 Keras code to train our shallow neural network model.fit(X_train, y_train, batch_size=128, epochs=200, verbose=1,
validation_data=(X_valid, y_valid)) The critical aspects are that: 1. The fit() method of our model object enables us to train our artificial neural network with the training images X_train as inputs and their associated labels y_train as the desired outputs. 2. As the network trains, the fit() method also provides us with the option to evaluate the performance of our network by passing our validation data X_valid and y_valid into the validation_data parameter. 3. With machine learning, and especially with deep learning, it is commonplace to train our model on the same data multiple times. One pass through all of our training data (60,000 images in the current case) is called one epoch of training. By setting the epochs parameter to 200, we cycle through all 60,000 training images two hundred separate times. 4. By setting verbose to 1, model.fit() will provide us with plenty of feedback as we train. At the moment, we’ll focus on the val_acc statistic output following each epoch of training. Validation accuracy is the proportion of the 10,000 handwritten images in X_valid where the network’s highest probability in the output layer corresponds to the correct digit as per the labels in y_valid. 14,15 Following the first epoch of training, we observe val_acc: 0.1010. That is, 10.1% of the images from the held­out validation dataset were correctly classified by our shallow architecture. Given that there are ten classes of handwritten digits, we’d expect a random process to guess ten percent of the digits correctly by chance, so this is not an impressive result. As the network continues to train, however, the results improve. After ten epochs of training, it is correctly classifying 36.5% of the validation images— far better than would be expected by chance! And this is only the beginning: After 200 epochs, the network’s improvements appears to be plateauing as it approaches 86% validation accuracy. Since we constructed such an uninvolved, shallow neural­network architecture, this is not too shabby! SUMMARY
Putting the cart before the horse, in this chapter we coded up a shallow, elementary artificial neural network. With decent accuracy, it is able to classify the MNIST images. Over the coming chapters, as we dive into theory, unearth artificial neural network best­practices, and layer up to authentic deep learning architectures, we should surely be able to do much better, no? Let’s see... 1 . Shaw, Z. (2013). Learn Python the Hard Way, 3rd Ed. New York, NY: Addison­ Wesley. This relevant appendix, Shaw’s Command Line Crash Course, is available online at learnpythonthehardway.org/book/appendixa.html 2 . Chen. D. (2017). Pandas for Everyone: Python Data Analysis. New York, NY: Addison­Wesley. 3 . jupyter.org; we recommend familiarizing yourself with the hot keys to breeze through Jupyter notebooks with pizzazz. 4 . docker.com 5 . yann.lecun.com/exdb/mnist/ 6 . Python uses zero­indexing so the first row and column are denoted with a zero. The 28th row and 28th column of pixels are therefore both denoted with 27. 7 . Within this book’s GitHub repository, navigate into the notebooks directory. 8 . “Hidden” layers are so called because they are not exposed; data impact them only indirectly, via the input layer or the output layer of neurons. 9 . The convention is to use an upper­case letter like X when the variable being represented is a two­dimensional matrix or a data structure with even higher dimensionality. In contrast, a lower­case letter like x is used to represent a single value (a scalar) or a one­dimensional array. 10. The data are initially stored as uint8, which is an unsigned integer from 0 to 255. This is more memory efficient, but of course it doesn’t encapsulate much precision since there are only 256 possible values. Without specifying, Python would default to a
64­bit float which is overkill. Thus, by specifying a 32­bit float we can be sure of what we’re feeding to the network. 11. Machine learning models tend to learn more efficiently when fed standardized inputs. Binary inputs would typically be a 0 or a 1, while distributions are often normalized to have a mean of zero and a standard deviation of one. As we’ve done here, pixel intensities are generally scaled to range from zero to one. 12. So named because each layer in the network passes information to only the next layer in the sequence of layers 13. Once more, these now­esoteric terms will become comprehensible over the coming chapters. 14. Artificial neural networks are stochastic (due to the way they’re initialized as well as the way they learn) so your results will vary slightly from ours. Indeed, if you re­run the whole notebook (e.g., by clicking on the Kernel option in the Jupyter menu bar and selecting Restart & Run All), you should obtain new, slightly different results yourself. 15. By the end of Chapter 8, we’ll have enough theory under our belts to study the output model.fit() in all its glory. For our immediate “cart before the horse” purposes, coverage of the validation accuracy metric alone suffices.
Playlists 6 Artificial Neurons Detecting Hot Dogs History Having received tantalizing exposure to applications of deep learning in the first part of Topics this book and having coded up a functioning neural network in the preceding chapter, the moment has come to delve into the nitty­gritty theory underlying these capabilities. Tutorials We will begin by dissecting artificial neurons, the units that—when wired together— constitute an artificial neural network. Offers & Deals BIOLOGICAL NEUROANATOMY 101 Highlights As presented in the opening paragraphs of this book, ersatz neurons are inspired by Settbiniogslogical ones. Given that, let’s take a gander at Figure 6.1 for a précis of the first lecture in any neuroanatomy course: A given biological neuron receives input into its Support cell body from many (generally thousands) of dendrites, with each dendrite receiving signals of information from another neuron in the nervous system—a biological neural Sign Out network. When the signal conveyed along a dendrite reaches the cell body, it causes a 1 small change in the voltage of the cell body. Some dendrites cause a small positive change in voltage, while the others cause a small negative change. If the cumulative effect of these changes causes the voltage to increase from its resting state of ­70 millivolts to the critical threshold of ­55 millivolts, the neuron will fire something called an action potential away from its cell body, down its axon, which transmits a signal to other neurons in the network.
Figure 6.1 Cartoon of the anatomy of a biological neuron. To summarize, biological neurons typically: 1. receive information from many other neurons, 2. aggregate this information via changes in cell voltage at the cell body, and 3. transmit a signal if the cell voltage crosses a threshold level, which can be received by many other neurons in the network. TRILOBITE SIDEBAR: An observant reader might have noticed the use of matching colors in the text and figures here. This is intentional, and it’s a tool we’ll use more often in the coming chapters as we begin to discuss a few important equations and the variables they contain—so keep your eye out for it! THE PERCEPTRON In the late 1950s, the American neurobiologist Frank Rosenblatt (Figure 6.2) published on his perceptron, an algorithm influenced by his understanding of biological neurons, 2 making it the earliest formulation of an artificial neuron. Analogous to their living inspiration, the perceptron (Figure 6.3) can: 1. receive input from multiple other neurons, 2. aggregate those inputs via a simple arithmetic operation called the weighted sum, and 3. generate an output if this weighted sum crosses a threshold level, which can then be
sent on to many other neurons within a network. Figure 6.2 The American neurobiology and behavior researcher Frank Rosenblatt. He conducted much of his work out of the Cornell Aeronautical Laboratory, including physically constructing his Mark I Perceptron there. This machine, an early relic of artificial intelligence, can today be viewed at the Smithsonian Institution in Washington, D.C. Figure 6.3 Schematic diagram of a perceptron, an early artificial neuron. Note the structural similarity to the biological neuron in Figure 6.1. The Hot Dog / Not Hot Dog Detector Let’s work through a light­hearted example to understand how the perceptron algorithm works. We’re going to look at a perceptron that is specialized in distinguishing whether a given object is a hot dog or, well... not a hot dog.
A critical attribute of perceptrons is that they can only be fed binary information as inputs, and their output is restricted to being binary as well. Thus, our hot dog­ detecting perceptron must be fed its particular three inputs (indicating whether the object involves ketchup, mustard, or a bun, respectively) as either a 0 or a 1. In Figure 6.4: œ The first input (a purple 1) indicates the object being presented to the perceptron involves ketchup. œ The second input (also a purple 1) indicates the object has mustard. œ The third input (a purple 0) indicates the object does not include a bun. Figure 6.4 Example 1 of a hot dog­detecting perceptron: In this instance, it predicts there is indeed a hot dog. To make a prediction as to whether the object is a hot dog or not, the perceptron 3 independently weights each of these three inputs. The weights that we arbitrarily selected in this (entirely contrived) hot dog example indicate that the presence of a bun, with its weight of six, is the most influential predictor of whether the object is a hot dog or not. The intermediate predictor is ketchup with its weight of three, and the least influential predictor is mustard with a weight of two. Let’s determine the weighted sum of the inputs: One input at a time (i.e., element­ wise), we multiply the input by its weight, and then sum the individual results. So, first let’s calculate the weighted inputs: 1. for the ketchup input: 3 × 1 = 3
2. for mustard: 2 × 2 = 2 3. and for bun: 6 × 0 = 0 With those three products, we can compute that the weighted sum of the inputs is five: 3 + 2 + 0 = 5. To generalize from this example, the calculation of the weighted sum of inputs is captured by Equation 6.1: Where: œ w is the weight of a given input i (in our example, w = 3, w = 2 and w = 6) i 1 2 3 œ x is the value of a given input i (in our example, x = 1, x = 1 and x = 0) i 1 2 3 œ w · x represents the product of w and x —i.e., the weighted value of a given input i i i i i œ indicates that we sum all of the individual weighted inputs w · x , where n is i i the total number of inputs (in our example, we had three inputs but artificial neurons can have any number of inputs). The final step of the perceptron algorithm is to evaluate whether the weighted sum of the inputs is greater than the neuron’s threshold. As with the weights above, we have again arbitrarily chosen a threshold value for our perceptron example: four (shown in red in the center of the neuron in Figure 6.4). The perceptron algorithm is shown below : Where: œ If the weighted sum of a perceptron’s inputs is greater than its threshold, then it outputs a 1, indicating that the perceptron predicts the object is a hot dog. œ Otherwise, if the weighted sum is less than or equal to the threshold, the perceptron outputs a 0, indicating that it predicts there is not a hot dog. Knowing this, we can wrap up our example from Figure 6.4: The weighted sum of five is greater than the neuron’s threshold of four, and so our hot dog­detecting perceptron
outputs a 1. Riffing on our first hot dog example, in Figure 6.5 the object evaluated by the perceptron now includes mustard only—there is no ketchup and it is still without a bun. In this case the weighted sum of inputs comes out to 2. Because 2 is less than the perceptron’s threshold, the neuron outputs 0, indicating that it predicts this object is not a hot dog. Figure 6.5 Example 2 of a hot dog­detecting perceptron: In this instance, it predicts there is not a hot dog. In our third and final perceptron example, shown in Figure 6.6, the artificial neuron evaluates an object that involves neither mustard nor ketchup, but is on a bun. The presence of a bun alone corresponds to the calculation of a weighted sum of 6. Since 6 is greater than the perceptron’s threshold, the algorithm predicts the object is a hot dog and outputs a 1.
Figure 6.6 Example 3 of a hot dog­detecting perceptron: In this instance, it again predicts the object presented to it is a hot dog. The Most Important Equation in this Book To achieve the formulation of a simplified and universal perceptron equation, we must introduce a term called the bias, which we annotate as b and which is equivalent to the negative of an artificial neuron’s threshold value (Equation 6.3): Together, a neuron’s bias and its weights constitute all of its parameters—the changeable variables that prescribe what the neuron will output in response to its inputs. With the concept of a neuron’s bias now available to us, we arrive at the most widely­ used perceptron equation (Equation 6.4): Notice that we made the following five updates to our initial perceptron equation (from Equation 6.2): 1. substituted the bias b in place of the neuron’s threshold 2. flipped b onto the same side of the equation as all of the other variables
3. used the array w to represent all of the w weights from w through to w i 1 n 4. likewise, used the array x to represent all of the x values from x through to x i 1 n 5. used the dot product notation w x to abbreviate the representation of the weighted sum of neuron inputs (the longer form of this was already shown in Equation 6.1: ) Right at the heart of the perceptron equation in Equation 6.4 is w · x + b, which we have cut out for emphasis and placed alone in Figure 6.7. If there is one item you note down to remember from this chapter, it should be this three­variable formula, which is an equation that represents artificial neurons in general. We will refer back to this equation many times over the course of this book, especially over the remainder of Part II and in Chapter 14 when we move beyond the Keras API to create our own artificial neurons from scratch in TensorFlow proper. Figure 6.7 The general equation for artificial neurons that we will return to time and again. It is the most important equation in this book. This paragraph is a Trilobite­attention SIDEBAR. To keep the arithmetic as undemanding as possible in our hot dog­detecting perceptron examples, all of the parameter values we made up—the perceptron’s weights as well as its bias—were positive integers. These parameters could, however, be negative values and, in practice, they would rarely be integers—instead, parameters are configured as float values, which are less clunky. This paragraph is also in the same Trilobite­attention SIDEBAR. Finally, while all of the parameters in these examples were fabricated by us, they would usually be learned through the training of artificial neurons on data. In Chapter 8, we’ll cover how this training is accomplished in practice. END SIDEBAR. MODERN NEURONS AND ACTIVATION FUNCTIONS
MODERN NEURONS AND ACTIVATION FUNCTIONS Modern artificial neurons, such as those in the hidden layer of the shallow architecture we built in the previous chapter (look back to Figure 5.4 or to our Shallow Net in Keras notebook), are not perceptrons. While the perceptron provides a relatively uncomplicated introduction to artificial neurons, it is not used in practice today. The most obvious restriction of the perceptron is that operates solely with binary values: it receives only binary inputs and provides only a binary output. In many cases, we’d like to make predictions from inputs that are continuous variables not binary integers, and so this restriction alone would make perceptrons unsuitable. A less obvious (yet even more critical) corollary of the perceptron’s binary­only restriction is that it makes learning rather challenging. Consider Figure 6.8, in which we use a new term, z, as short­hand for the value of the lauded w · x + b equation from Figure 6.7. When z is any value less than or equal to zero, the perceptron outputs its smallest possible output, 0. If z becomes positive to even the tiniest extent, the perceptron outputs its largest possible output, 1. This sudden and extreme transition is not optimal during training: When we train a network, we make slight adjustments to w and b based on whether it appears the adjustment will improve the network’s output. 4 With the perceptron, the majority of slight adjustments to w and b would make no difference whatsoever to its output; z would generally be moving around at negative values much lower than zero or at positive values much higher than zero. That behavior on its own would be unhelpful, but the situation is even worse: Every once and a while, a slight adjustment to w or b will cause z to cross from negative to positive (or vice versa), leading to a whopping, drastic swing in output from zero all the way to one (or vice versa). Essentially, the perceptron has no finesse—it’s either yelling or it’s silent. Figure 6.8 The perceptron’s transition from outputting zero to outputting one happens suddenly, making it challenging to gently tune w and b to match a desired output. The Sigmoid Neuron
The Sigmoid Neuron Figure 6.9 provides an alternative to the erratic behavior of the perceptron: a gentle curve from 0 to 1. This particular curve shape is called the sigmoid function and is defined by , where: œ z is equivalent to w · x + b œ e is the mathematical constant beginning in 2.718... that is perhaps best known for its starring role in the natural exponential function œ σ is the Greek letter sigma, the root word for “sigmoid” Figure 6.9 The sigmoid activation function. z is the input into the The sigmoid function is our first example of an artificial neuron activation function. It may be ringing a bell for you already because it was the neuron type that we selected for the hidden layer of our Shallow Net in Keras from Chapter 5. As we’ll see as this section progresses, the sigmoid function is the canonical activation function; so much so that the Greek letter σ (sigma) is conventionally used to denote any activation function. The output from any given neuron’s activation function is referred to simply as its activation and throughout this book, we will use the variable term a—as shown along the vertical axis in Figure 6.9—to denote it. In our view, there is no need to memorize the sigmoid function (or indeed any of the activation functions). Instead, we believe it’s easier to understand a given function by playing around with its behavior interactively. With that in mind, feel free to join us in the Sigmoid Function Jupyter notebook from the book’s GitHub repository as we work through the following lines of code. Our only dependency in the notebook is the constant e, which we load with from math
import e. Next is the fun bit, where we define the sigmoid function itself: def sigmoid(z): return 1/(1+e**­z) As depicted in Figure 6.9 and demonstrated by executing sigmoid(.00001), near­ zero inputs into the sigmoid function will lead it to return values near 0.5. Increasingly large positive inputs will result in values that approach 1. As an extreme example, an input of 10000 results in an output of 1.0. Moving more gradually with our inputs— this time in the negative direction—we obtain outputs that gently approach zero: As examples, sigmoid(­1) returns 0.2689 while sigmoid(­10) returns 4.5398e­05. 5 Any artificial neuron that features the sigmoid function as its activation function is called a sigmoid neuron and the advantage of these over the perceptron should now be tangible: Small, gradual changes in a given sigmoid neuron’s parameters w or b cause small, gradual changes in z, thereby producing similarly gradual changes in the neuron’s activation, a. Large negative or large positive values of z illustrate an exception: At extreme z values, sigmoid neurons—like perceptrons—will output 0’s (when z is negative) or 1’s (when z is positive). Just like the perceptron, this means that subtle updates to the weights and biases during training will have little to no effect on the output and thus learning will stall. This situation is called neuron saturation and can occur with any activation function. Thankfully, there are tricks to avoid saturation, as we’ll see in Chapter 9. The Tanh Neuron A popular cousin of the sigmoid neuron is the tanh (pronounced “tanch” in the deep­ learning community) neuron. The tanh activation function is pictured in Figure 6.10 and is defined by . The shape of the tanh curve is similar to the sigmoid curve, with the chief distinction being that the sigmoid function exists in the range [0 : 1], while the tanh neuron’s output has the range [ −1 : 1]. This difference is more than cosmetic. With negative z inputs corresponding to negative a activations, z = 0 corresponding to a = 0, and positive z corresponding to positive a activations, the output from tanh neurons tends to be centered near zero. As we’ll cover further in Chapters 7 through 9 , these zero­centered a outputs usually serve as the inputs x to other artificial neurons in a network, and such zero­centered inputs make (the
dreaded!) neuron saturation markedly less likely, thereby enabling the entire network to learn more efficiently. Figure 6.10 The tanh activation function. ReLU: Rectified Linear Units The final neuron we’ll detail in this book is the Rectified Linear Unit, or ReLU neuron, whose behavior we’ve graphed in Figure 6.11. The ReLU activation function, whose shape diverges glaringly from the sigmoid and tanh sorts, was inspired by properties of 6 biological neurons and popularized within artificial neural networks by Vinod Nair 7 and Geoff Hinton (Figure 1.17). The shape of the ReLU function is defined by a = max(0, z). This function is uncomplicated: œ If z is a positive value, the ReLU activation function returns z (unadulterated) as a = z. œ If z = 0 or z is negative, the function returns its floor value of zero, i.e., the activation a = 0. Figure 6.11 The ReLU activation function.
The ReLU function is one of the simplest functions to imagine that is non­linear. That is, like the sigmoid and tanh functions, its output a does not vary uniformly linearly across all values of z. The ReLU is in essence two distinct linear functions combined (one at negative z values returning zero, and the other at positive z values returning z, as is visible in Figure 6.11) to form a straightforward, non­linear function overall. This non­linear nature is a critical property of all activation functions used within deep learning architectures. As demonstrated via a series of captivating interactive applets in Chapter 4 of Michael Nielsen’s Neural Networks and Deep Learning e­book, these non­linearities permit deep learning models to approximate any continuous function. 8 This universal ability to approximate some output y given some input x is one of the hallmarks of deep learning—the characteristic that makes the approach so effective across such a breadth of applications. The relatively simple shape of the ReLU function’s particular brand of non­linearity works to its advantage. As we’ll see in Chapter 8, learning appropriate values for w and b within deep learning networks involves partial derivative calculus, and these calculus operations are much more computationally efficient on the linear portions of the ReLU 9 function relative to on the curves of, say, the sigmoid and tanh functions. As a testament to its utility, the incorporation of ReLU neurons into AlexNet (Figure 1.18) was one of the factors behind it trampling existing machine­vision benchmarks in 2012 and shepherding in the era of deep learning. Today, ReLU units are the most widely­ used neuron within the hidden layers of deep artificial neural networks and they appear in the majority of the Jupyter notebooks associated with this book. CHOOSING A NEURON Within a given hidden layer of an artificial neural network, you are able to choose any activation function you fancy. With the constraint that you should select a non­linear function if you’d like to be able to approximate any continuous function with your deep learning model, you’re nevertheless left with quite a bit of room for choice. To assist your decision­making process, let’s rank the neuron types we already discussed in this chapter, ordering them from those we recommend least through to those we recommend most: 1. The perceptron, with its binary inputs and the aggressive step of its binary output, is not a practical consideration for deep learning models. 2. The sigmoid neuron is an acceptable option but it tends to lead to neural networks that train less rapidly than those composed of, say, tanh or ReLU neurons. Thus, we recommend limiting your use of sigmoid neurons to situations where it would be 10
10 helpful to have a neuron provide output within the range of [0, 1]. 3. The tanh neuron is a solid choice. As we covered above, their zero­centered output helps deep learning networks learn rapidly. 4. Our preferred neuron is the ReLU because of how efficiently learning algorithms can perform computations with them. In our experience they tend to lead to well­calibrated artificial neural networks in the shortest period of training time. In addition to the neurons covered in this chapter, there is a veritable zoo of activation functions available and the list is ever­growing. At time of writing, some of the 11 “advanced” activation functions provided by Keras are the Leaky ReLU, the Parametric ReLU, and the Exponential Linear Unit—all three of which are derivations from the ReLU neuron. We encourage you to check these activations out in the Keras documentation and read about them on your own time. Furthermore, you are welcome to swap out the neurons we use in any of the Jupyter notebooks in this book to compare the results. We’d be pleasantly surprised if you discover they provide efficiency or accuracy gains in your neural networks that are far beyond the performance of ours. SUMMARY In this chapter, we detailed the mathematics behind the neural units that make up artificial neural networks, including deep learning models. We also summarized the pros and cons of the most established neuron types, providing you with guidance on which ones you might select for your own deep learning models. In the upcoming chapter, we’ll cover how artificial neurons are networked together in order to learn features from raw data and approximate complex functions. KEY CONCEPTS This section should probably appear as a sidebar­style box. As the list lengthens in subsequent chapters, it would probably appear tidier if the list were laid out across multiple columns. As we move through the chapters of the book, we will gradually add terms to this list of Key Concepts. If you keep these foundational concepts fresh in your mind, you should have little difficulty understanding subsequent chapters and, by book’s end, possessing a firm grip on deep learning theory and application. The critical concepts thus far are: œ parameter œ weight w
œ bias b œ activation a œ artificial neuron œ sigmoid œ tanh œ ReLU 1 . More precisely, it causes a change in the voltage difference between the cell’s interior and its surroundings. 2 . Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and the Organization in the Brain. Psychological Review, 65, 386­408. 3 . If you are well­accustomed to regression modeling, this should be a familiar paradigm. 4 . Improvement here means providing output more closely in line with the true output y given some input x. We’ll discuss this further soon, in Chapter 8. 5 . The e in 4.5398e­05 should not be confused with the base of the natural logarithm. Here, it refers to an exponent, so the output is the equivalent of 4.5398 × −5 10 . 6 . The action potentials of biological neurons have only a “positive” firing mode; they have no “negative” firing mode. See Hahnloser, R., & Seung, H. (2001). Permitted and Forbidden Sets in Symmetric Threshold­Linear Networks. Proceedings of Neural Information Processing Systems. 7 . Nair, V. & Hinton, G. (2010). Rectified Linear Units Improve Restricted Boltzmann Machines. Proceedings of the International Conference on Machine Learning. 8 . neuralnetworksanddeeplearning.com/chap4.html
9 . In addition, there is mounting research that suggests ReLU activations encourage parameter sparsity—i.e., less elaborate neural­network­level functions that tend to generalize to validation data better. More on model generalization coming up in Chapter 9. 10. In Chapter 11, we will encounter a couple of these situations—most notably, with a sigmoid neuron as the sole neuron in the output layer of a binary­classifier network. 11. See keras.io/layers/advanced­activations
y 7 Artificial Neural Networks History In the preceding chapter, we examined the intricacies of artificial neurons. The theme Topics of the current chapter is the natural extension of that: We’ll cover how individual neural units are linked together to form artificial neural networks, including deep learning Tutorials networks. Offers & Deals THE INPUT LAYER HigIhnlig ohtusr Shallow Net in Keras Jupyter notebook (a schematic of which is available back in Figure 5.4), we crafted an artificial neural network with: Settings 1. an input layer consisting of 784 neurons, one for each of the 784 pixels in an MNIST Supipmoratge Sign2 O.u at hidden layer composed of 64 sigmoid neurons 3. an output layer consisting of 10 softmax neurons, one for each of the ten classes of digits Of these three, the input layer is the most straightforward to detail. We’ll start with it, and then move onto discussion of the hidden and output layers. Neurons in the input layer don’t perform any calculations; they are simply placeholders for input data. This place­holding is essential because, as we’ll see first­hand in Chapter 14 when we begin writing code in low­level TensorFlow, the use of artificial neural networks involves performing computations on matrices that have pre­defined dimensions. At least one of these pre­defined dimensions in the network architecture corresponds directly to the shape of the input data. DENSE LAYERS T he re ar e ma ny kind s of h idden la yers, but a s menti oned in Chapter 4, the most general type is the dense layer, which can also be called a fully­connected layer. Dense layers are found in many deep learning architectures, including the majority of the models
we’ll go over in this book. Their definition is uncomplicated: Each of the neurons in a given dense layer receive information from every one of the neurons in the previous layer of the network. In other words, a dense layer is fully­connected to the layer before it! While they might not be as specialized nor as efficient as the other flavors of hidden layers we’ll get to in Part III, dense layers are broadly useful because they can non­ 1 linearly recombine the information provided by the previous layer of the network. Reviewing the TensorFlow Playground demo from the end of Chapter 1, we’re now better­positioned to appreciate the deep learning model we built. Breaking it down layer by layer, the network in Figures 1.19 and 1.20 has: 1. An input layer with two neurons: one for storing the vertical position of a given dot within the grid on the far right, and the other for storing the dot’s horizontal position. 2. A hidden layer composed of eight ReLU neurons. Visually, we can see that this is a dense layer because each of the eight neurons in it is connected (i.e., is receiving information) from both of the input­layer neurons, for a total of 16 (= 8 × 2) incoming connections. 3. Another hidden layer composed of eight ReLU neurons. We can again discern that this is a dense layer because its eight neurons each receive input from each of the eight neurons in the previous layer, for a total of 64 (= 8 × 8) inbound connections. Note how the neurons in this layer are non­linearly recombining the straight­edge features provided by the neurons in the first hidden layer to produce more elaborate features 2 like curves and circles. 4. A third dense hidden layer, this one consisting of four ReLU neurons for a total of 32 (= 4 × 8) connecting inputs. This layer non­linearly recombines the features from the previous hidden layer to learn more complex features that begin to look directly relevant to the binary (orange versus blue) classification problem shown in the grid on the right. 5. A fourth and final dense hidden layer. With its two ReLU neurons, it receives a total of eight (= 2 × 4) inputs from the previous layer. The neurons in this layer devise such elaborate features via non­linear recombination that they visually approximate the overall boundary dividing blue from orange on the grid. 6. An output layer made up of a single sigmoid neuron. Sigmoid is the typical choice of neuron for a binary classification problem like this one. As shown in Figure 6.9, the sigmoid function outputs activations that range from zero up to one, allowing us to
obtain the network’s estimated probability that a given input x is a positive case (a blue dot in the current example) or inversely, the probability that it is a negative case. Like the hidden layers, the output layer is dense too: Its neuron receives information from both neurons of the final hidden layer for a total of two (= 1 × 2) connections. In summary, every layer within the networks provided by the TensorFlow Playground is a dense layer. We can call such a network a dense network and we’ll be experimenting with these versatile creatures for the remainder of Part II. A HOT DOG-DETECTING DENSE NETWORK Let’s further strengthen our comprehension of dense networks by returning to two old flames of ours from Chapter 6: a frivolous hot dog­detecting binary classifier and the mathematical notation we used to define artificial neurons. As shown in Figure 7.1, our hot dog classifier is no longer a single neuron; in this chapter, it is a dense network of artificial neurons. More specifically, with this network architecture: œ We have reduced the number of input neurons down to two for simplicity: œ The first input neuron, x , represents the volume of ketchup (in, say, milliliters, 1 which abbreviates to mL) on the object being considered by the network. (We are no longer working with perceptrons, so we are no longer restricted to binary inputs only.) œ The second input neuron, x , represents mL of mustard. 2 œ We have two dense hidden layers: œ The first hidden layer has three ReLU neurons. œ The second hidden layer has two ReLU neurons. œ The output neuron is denoted by ŷ in the network. This is a binary classification problem, so—as outlined in the previous section—this neuron should be sigmoid. As in our perceptron examples in Chapter 6, y = 1 corresponds to the presence of a hot dog and y = 0 corresponds to the presence of some other object.
Figure 7.1 A dense network of artificial neurons, highlighting the inputs to the neuron labelled a. 1 Forward Propagation through the First Hidden Layer Having described the architecture of our hot dog­detecting network, let’s turn our 3 attention to its functionality by focusing on the neuron labelled a . This particular 1 neuron, like its siblings a and a , receives input regarding a given object’s ketchup­y­ 2 3 ness and mustard­y­ness from x and x , respectively. Despite receiving the same data 1 2 as a and a , a treats these data uniquely by having its own unique parameters. 2 3 1 Remembering Figure 6.7, “the most important equation in this book” —w x + b—we may grasp this behavior more concretely. Breaking this equation down for the neuron labelled a , we consider that it has two inputs from the previous layer, x and x . This 1 1 2 neuron also has two weights: w (which applies to the importance of the ketchup 1 measurement x ) and w (which applies to the importance of the mustard measurement 1 2 x ). With these five pieces of information we can calculate z, the weighted input to that 2 neuron: In turn, with the z value for the neuron labelled a , we can calculate the activation a it 1 outputs. Since the neuron labelled a is a ReLU neuron, we use the equation introduced 1
in Figure 6.11: To make this computation of the output of neuron a tangible, let’s concoct some 1 numbers and work through the arithmetic together: œ x is 4.0 mL of ketchup for a given object presented to the network 1 œ x is 3.0 mL of mustard for that same object 2 œ w = −0.5 1 œ w = 1.5 2 œ b = −0.9 To calculate z let’s start with Equation 7.1 and then fill in our contrived values: Finally, to compute a—the activation output of the neuron labelled a —we can leverage 1 Equation 7.2: As suggested by the right­facing arrow along the bottom of Figure 7.1, executing the calculations through an artificial neural network from the input layer (the x values) through to the output layer (ŷ) is called forward propagation. Immediately above, we detailed the process for forward propagating through a single neuron in the first hidden layer of our hot dog­detecting network. To forward propagate through the remaining neurons of the first hidden layer—that is, to calculate the a values for the neurons labelled a and a —we would follow the same process as we did for the neuron labelled 2 3 a . The inputs x and x are identical for all three neurons, but despite being fed the 1 1 2 same measurements of ketchup and mustard, each neuron in the first hidden layer will output a different activation a because the parameters w , w and b vary for each of the 1 2
neurons in the layer. Forward Propagation through Subsequent Layers The process of forward propagating through the remaining layers of the network is essentially the same as propagating through the first hidden layer, but for clarity’s sake, let’s work through it together. In Figure 7.2, we’ll assume that we’ve already calculated the activation value a for each of the neurons in the first hidden layer. Returning our focus to the neuron labelled a , the activation it outputs (a = 1.6) becomes one of the 1 1 three inputs into the neuron labelled a (and, as highlighted in the figure, this same 4 activation of a = 1.6 is also fed as one of the three inputs into the neuron labelled a ). 5 Figure 7.2 Our hot dog­detecting network from Figure 7.1, now highlighting the activation output of neuron a, which is provided as an input to both neuron a and 1 4 neuron a . 5 To provide an example of forward propagation through the second hidden layer, let’s compute a for the neuron labelled a . Again, we employ the all­important equation w · 4 x + b. For brevity’s sake, we’ve combined it with the ReLU activation function: This is sufficiently similar to Equations 7.3 and 7.4 that it would be superfluous to walk through the arithmetic again with feigned values. The only twist, as we propagate
through the second hidden layer, is that the layer’s inputs (i.e., x in the equation w x + b) come not from outside the network—instead they are provided by the first hidden layer. Thus, in Equation 7.5: œ x is the value a = 1.6, which we obtained earlier from the neuron labelled a 1 1 œ x is the activation output a (whatever it happens to equal) from the neuron labelled 2 a , and 2 œ x is likewise a unique activation a from the neuron labelled a 3 3 In this manner, the neuron labelled a is able to non­linearly recombine the 4 information provided by the three neurons of the first hidden layer. The neuron labelled a also non­linearly recombines this information, but it would do it in its own 5 distinctive way: The unique parameters w , w , w and b for this neuron would lead it 1 2 3 to output a unique a activation of its own. Having illustrated forward propagation through all of the hidden layers of our hot dog­ detecting network, let’s round the process off by propagating through the output layer. Figure 7.3 highlights that our single output neuron receives its inputs from the neurons labelled a and a . Let’s begin by calculating z for this output neuron. The formula is 4 5 identical to Equation 7.1, which we used to calculate z for the neuron labelled a , except 1 that the (contrived, as usual) values we plug into the variables are different:
Figure 7.3 Our hot dog­detecting network, with the activations providing input to the output neuron ŷ highlighted. The output neuron is sigmoid so to compute its activation a we pass its z value through the sigmoid function from Figure 6.9: We are lazy, so we didn’t work out the final line of this equation manually. Instead, we used the Sigmoid Function Jupyter notebook that we created in Chapter 6. By executing the line sigmoid(­2.0) within it, our machine did the heavy lifting for us and kindly informed us that a comes out to 0.1192 and change. The activation a computed by the sigmoid neuron in the output layer is a very special case because it is the final output of our entire hot dog­detecting neural network. Since it’s so special, we assign it a distinctive designation: ŷ, which is pronounced “why hat”. This value ŷ is the network’s guess as to whether the object presented to it was a hot dog or not a hot dog, and we can express this in probabilistic language. Given the inputs x and x that we fed into the network—that is, 4.0 mL of ketchup and 3.0 mL of 1 2 mustard—the network estimates that there is an 11.92% chance that an object with
4 those particular condiment measurements is a hot dog. If the object presented to the network was indeed a hot dog (y = 1) then this ŷ of 0.1192 was pretty far off the mark. On the other hand, if the object was truly not a hot dog (y = 0) then the ŷ is quite good. We’ll formalize the evaluation of ŷ in Chapter 8, but the general notion is is that the closer ŷ is to the true value y, the better. THE SOFTMAX LAYER OF A FAST FOOD-CLASSIFYING NETWORK As demonstrated thus far in the chapter, the sigmoid neuron suits us well if we’re building a network to distinguish two classes, e.g., a blue dot versus an orange dot, or a hot dog versus something other than hot dog. In many other circumstances, however, we have more than two classes to distinguish between. For example, MNIST consists of the ten numerical digits, so our Shallow Net in Keras from Chapter 6 had to accommodate ten output probabilities—one representing each digit. When concerned with a multi­class problem, the solution is to use a softmax layer as the output layer of our network. Softmax is in fact the activation function that we specified for the output layer in our Shallow Net in Keras Jupyter notebook, but we initially suggested you not worry yourself with that detail too much. Now, a couple of chapters later, the time to unravel softmax has arrived. In Figure 7.4, we’ve provided a new architecture that builds upon our binary hot dog classifier. The schematic is the same—right down to its volume­of­ketchup­and­ mustard inputs—except that instead of having a single output neuron, we now have three. This multi­class output layer is still dense, so each of the three neurons receives information from both of the neurons in the final hidden layer. Continuing on with our proclivity for fast food, let’s say that now: œ y represents hot dogs, 1 œ y is for burgers, and 2 œ y is for pizza. 3
Figure 7.4 Our hot dog­detecting network, now with three softmax neurons in the output layer. Note that with this configuration, there can be no alternatives to hot dog, burger or pizza. The assumption is that all objects presented to the network belong to one of these three classes of fast food, and one of the classes only. Because the sigmoid function applies only to binary problems, the output neurons in Figure 7.4 take advantage of the softmax activation function. Let’s use code from our Softmax Demo Jupyter notebook to elucidate how this activation function operates. The only dependency is the exp function, which calculates the natural exponential of whatever value it’s given. More specifically, if we pass x into it with the command x exp(x), we will get back e . The effect of this exponentiation will become clear as we move through the forthcoming example. We import the exp function into the notebook with from math import exp. To concoct another example, let’s say that we presented a slice of pizza to the network in Figure 7.4. Presumably this pizza slice has negligible amounts of ketchup and mustard on it, and so x and x are near­zero values. Provided these inputs, we use 1 2 forward propagation to pass information through the network toward the output layer. Based on the information that the three neurons receive from the final hidden layer, they individually use our old friend w · x + b to calculate three unique z values:
œ z for the neuron labelled y , which represents hot dogs, comes out to ­1.0 1 œ for the neuron labelled y , which represents burgers, z is 1.0 2 œ and for the pizza neuron y , z comes out to 5.0 3 These values indicate that the network estimates that the object presented to it is most likely to be pizza and least likely to be a hot dog. Expressed as z, however, it isn’t straightforward to intuit how much more likely the network predicts the object to be pizza relative to the other two classes. That’s where the softmax function comes in. After importing our dependency, we create a list named z to store our three z values: z = [­1.0, 1.0, 5.0] Applying the softmax function to this list involves a three­step process. The first step is to calculate the exponential of each of the z values. More explicitly: 5 œ exp(z[0]) comes out to 0.3679 for hot dog œ exp(z[1]) gives us 2.718 for burger œ and exp(z[2]) gives us the much much larger (exponentially so!) 148.4 for pizza The second step of the softmax function is to sum up our exponentials: total = exp(z[0]) + exp(z[1]) + exp(z[2]) With this total variable we can execute the third and final step, which provides proportions for each of our three classes relative to sum of all of the classes: œ exp(z[0])/total outputs 0.002428, indicating that the network estimates there’s a ~0.2% chance that the object presented to it is a hot dog œ exp(z[1])/total gives us 0.01794, indicating an estimated ~1.8% chance that it’s a burger, and œ exp(z[2])/total returns 0.9796 for an estimated ~98.0% chance that the object is pizza
Given the above arithmetic, the etymology of the “softmax” name should now be discernible: The function returns z with the highest value (the max), but it does so soft­ ly. That is, instead of indicating that there’s a 100% chance the object is pizza and a zero percent chance it’s either of the other two fast food classes (that would be... hard), the network hedges its bets, to an extent, and provides a likelihood that the object is each of the three classes. This leaves us to make the decision over how much confidence we 6 would require before we make a decision. Trilobite­reading sidebar: The use of the softmax function with a single neuron is a special case of softmax that is mathematically equivalent to using a sigmoid neuron. REVISITING OUR SHALLOW NETWORK With the knowledge of dense networks that we developed over the course of this chapter, we can return to our Shallow Net in Keras notebook and understand the model summary within it. Example 5.2 shows the three lines of Keras code we used to architect a shallow neural network for classifying MNIST digits. As detailed in Chapter 5, over those three lines of code we instantiated a model object and added layers of artificial neurons to it. By calling the summary() on the model, we see the model­ summarizing table provided in Figure 7.5. The table has three columns: œ Layer (type): the name and type of each of our layers œ Output Shape: the dimensionality of the layer œ Param #: the number of parameters (weights w and biases b) associated with the layer Figure 7.5 A summary of the model object from our “Shallow Net in Keras” Jupyter notebook. The input layer performs no calculations and never has any of its own parameters so no information on it is displayed directly. The first row in the table, therefore, corresponds
to the first hidden layer of the network. The table indicates that this layer: œ is called dense_1; this is a default name as we did not designate one explicitly œ is a Dense layer, as we specified in Example 5.2 œ is composed of 64 neurons, as we further specified in Example 5.2 œ has 50240 parameters associated with it, broken down into: œ 50176 weights, corresponding to each of the 64 neurons in this dense layer receiving input from each of the 784 neurons in the input layer (64*784) œ plus 64 biases, one for each of the neurons in the layer œ giving us a total of 50240 parameters: n = n + n = 50176 + 64 = 50240 parameters w b The second row of the table in Figure 7.5 corresponds to the model’s output layer. The table tells us that this layer: œ is called dense_2 œ is a Dense layer, as we specified it to be œ consists of 10 neurons—yet again, as we specified œ has 650 parameters associated with it: œ 640 weights, corresponding to each of the ten neurons receiving input from each of the 64 neurons in the hidden layer (64*10) œ plus 10 biases, one for each of the output neurons From the parameter counts for each layer, we can calculate for ourselves the Total params line displayed in Figure 7.5: All 50890 of these parameters are “Trainable params” because—during the
subsequent model.fit() call in the Shallow Net in Keras notebook—they are permitted to be tuned during model training. This is the norm, but as we’ll see in Part III, there are situations where it is fruitful to freeze some of the parameters in a model rendering them “Non­trainable params”. SUMMARY In this chapter, we detailed how artificial neurons are networked together to approximate an output y given some inputs x. In the remaining chapters of Part II, we’ll detail how a network learns to improve its approximations of y by using data to tune the parameters of its constituent artificial neurons. Simultaneously, we’ll broaden our understanding of best practices for designing and training artificial neural networks so that we can add hidden layers and form a high­calibre deep learning model. KEY CONCEPTS Here are the essential foundational concepts thus far. New terms from the current chapter are highlighted in purple: œ parameters: œ weight w œ bias b œ activation a œ artificial neurons: œ sigmoid œ tanh œ ReLU œ input layer œ hidden layer œ output layer œ layer types:
œ dense (fully­connected) œ softmax œ forward propagation 1 . This statement assumes the dense layer is made up of neurons with a non­linear activation function like the sigmoid, tanh and ReLU neurons introduced in the previous chapter, which should be a safe assumption. 2 . By optionally returning to playground.tensorflow.org, you can observe these features by hovering over these neurons with your mouse. 3 . We’re using a shorthand notation for conveniently identifying neurons in this chapter. See Appendix 17 (This should presumably be labelled Appendix A) for a more precise and formal neural network notation. 4 . Don’t say we didn’t warn you from the start that this was a silly example! If we’re lucky, its outlandishness will make it memorable. 5 . Recall that Python uses zero indexing so z[0] corresponds to the z of neuron y . 1 6 . Typically, we’d simply choose the class with the highest likelihood. This is easily achieved with the argmax() function in Python, which returns the index position (i.e., the class label) of the largest value.
Playlists 8 Training Deep Networks History In the preceding chapters, we described artificial neurons comprehensively and we Topics walked through the process of forward propagating information through a network of neurons to output a prediction, such as whether a given fast­food item is a hot dog, a Tutorials juicy burger or a greasy slice of pizza. In those culinary examples from Chapters 6 and 7 , we fabricated numbers for the neuron parameters—i.e., weights and biases. In real­ Offers & Deals world applications, however, these parameters are not typically concocted arbitrarily: HigThlhigehyts are learned by training the network on data. SettIinng tshis chapter, we will become acquainted with two methods—called gradient descent and backpropagation—that work in tandem to learn artificial neural network Supppoarrtameters. As usual in this book, our presentation of these methods is not only theoretical: We will provide pragmatic best practices for implementing the techniques. Sign Out The chapter will culminate in the application of these practices to the construction of an intermediate­depth neural network. COST FUNCTIONS In Chapter 7, we discovered that, upon forward propagating some input values all the way through an artificial neural network, the network provides its estimated output, which is denoted ŷ. If a network was perfectly calibrated, it would output ŷ values that are exactly equal to the true label y. In our binary classifier for detecting hot dogs, for example (Figure 7.3), y = 1 indicated that the object presented to the network is a hot dog while y = 0 indicated that it’s something else. In an instance where we have in fact presented a hot dog to the network, therefore, it would ideally output ŷ = 1. In practice, the gold standard of ŷ = y is not always attained and so may be an excessively stringent definition of the “correct” ŷ. Instead, we might be quite pleased to see a ŷ of, say, 0.9997 as that would indicate that the network has an extremely high co n fiden ce th at the o bject is a hot dog. A ŷ of 0.9 mi ght b e considered acceptable, ŷ = 0.6 to be rather disappointing and ŷ = 0.1192 (as computed in Equation 7.7) to be downright awful.
To quantify the spectrum of output­evaluation sentiments from “quite pleased” all the way to “downright awful”, machine learning algorithms often involve cost functions (also known as loss functions). The two such functions that we’ll cover in this book are called quadratic cost and cross­entropy cost. Let’s cover them in turn. Quadratic Cost Quadratic cost is one of the simplest cost functions to calculate. It is alternatively called mean squared error, which handily describes all that there is to its calculation: For any given instance i, we calculate the difference (the error) between the true label y and the network’s estimated ŷ . We then square this difference, for two reasons: i i 1. Squaring ensures that whether y is greater than ŷ or vice versa, the difference between the two is stated as a positive value. 2. Squaring penalizes large differences between y and ŷ much more severely than small differences. 2 Having obtained a squared error for each instance i with (y − ŷ ) , we can at last i i calculate the mean cost C across all n of our instances by: 1. Summing up all of our instances with 2. Dividing by however many instances we have with By taking a peek inside the Quadratic Cost Jupyter notebook from the book’s GitHub repo, you can play around with Equation 8.1 yourself. At the top of the notebook, we define a function to calculate squared error for an instance i: def squared_error(y, yhat): return (y ­ yhat)**2 By plugging a true y of 1 and the ideal yhat of 1 into the function with squared_error(1, 1), we observe that—as desired—this perfect estimate is associated with a cost of 0. Likewise, minor deviations from the ideal such as a yhat of
0.9997, correspond to an extremely small cost: 9.0e­08. 1 As the difference between y and yhat increases, we witness the expected exponential increase in cost: Holding y steady at 1 but lowering yhat from 0.9 to 0.6, and then to 0.1192, the cost climbs increasingly rapidly from 0.01 to 0.16 and then 0.78. As a final bit of amusement in the notebook, we note that had y truly been 0, our yhat of 0.1192 would be associated with a small cost: 0.0142. Saturated Neurons While quadratic cost serves as a straightforward introduction to loss functions, it has a vital flaw. Consider Figure 8.1, in which we summarize the tanh activation function from back in Figure 6.10. The issue presented in the figure, called neuron saturation, is common across all activation functions but we’ll use tanh as our lone exemplar. A neuron is considered saturated when the combination of its inputs and parameters (interacting as per “the most important equation” z = w · x + b) produce extreme values of z—the areas encircled with red in the plot. In these areas, changes in z (via adjustments to the neuron’s underlying parameters w and b) cause only teensy­weensy 2 changes in the neuron’s activation a. Figure 8.1 Plot reproducing the tanh activation function shown in Figure 6.10, drawing attention to the high and low values of z at which a neuron is saturated. Using methods that we’ll cover later in this chapter—namely, gradient descent and backpropagation—a neural network is able to learn to approximate y through the tuning of its neurons’ parameters w and b. In a saturated neuron, where changes to w and b lead to only minuscule changes in a, this learning slows to a crawl: If adjustments to w and b make no discernible impact on a given neuron’s activation a then these adjustments cannot have any discernible impact downstream (via forward propagation) on the network’s ŷ, its approximation of y. Cross-Entropy Cost
3 One of the ways to minimize the impact of saturated neurons on learning speed is to use cross­entropy cost in lieu of quadratic cost. This alternative loss function is configured to enable efficient learning anywhere within the activation function curve of Figure 8.1. Because of this, it is a far more popular choice of cost function and it is the selection that predominates the remainder of this book. You need not preoccupy yourself heavily with the equation for cross­entropy cost, but for the sake of completeness, here it is: The most pertinent aspects of the equation are that: 1. Like quadratic cost, divergence of from ŷ from y corresponds to increased cost. 2. Also, similar to the use of the square in quadratic cost, the use of the natural logarithm ln in cross­entropy cost causes larger differences between ŷ and y to be associated with exponentially larger cost. 3. Cross­entropy cost is structured so that the larger the difference between ŷ and y, the 4 faster the neuron is able to learn. To make it easier to remember that the greater the cost, the more quickly a neural network incorporating cross­entropy cost learns, here’s a flippant analogy that would absolutely never happen to any of your esteemed authors: Let’s say you’re at a cocktail party leading the conversation to a group of cool, stylish people that you’ve met that evening. The strong martini you’re holding has already gone to your head and so you go out on a limb by throwing a vulgar joke into your otherwise charming repartee. Unexpectedly, your audience reacts with immediate, visible disgust. With this response clearly indicating that your quip was well off the mark, you learn pretty darn quickly. It’s exceedingly unlikely you’ll be repeating the joke anytime soon. Anyway, that’s plenty enough on disasters of social etiquette. The final item to note on cross­entropy cost is that, by including ŷ, the formula provided in Equation 8.2 applies to only the output layer. Recall from the previous chapter (specifically the discussion of Figure 7.3) that ŷ is a special case of a: It’s actually just another plain old a value— except that it’s being calculated by neurons in the output layer of a neural network. With this in mind, Equation 8.3 could be expressed with a substituted in for ŷ so that i i the equation generalizes neatly beyond the output layer to neurons in any layer of a network:
To cement all of this theoretical chatter about cross­entropy cost, let’s interactively explore our aptly­named Cross Entropy Cost Jupyter notebook. There is only one dependency in the notebook: the log function from the NumPy package, which enables us to compute the natural logarithm ln shown twice in Equation 8.3. We load this dependency with from numpy import log. Next, we define a function for calculating cross­entropy cost for an instance i: def cross_entropy(y, a): return ­1*(y*log(a) + (1­y)*log(1­a)) Plugging the same values into our cross_entropy() function as we did into the squared_error() function earlier in this chapter, we observe comparable behavior. As shown in Table 8.1, by holding y steady at 1 and gradually decreasing a from the near­ideal estimate of 0.9997 downward, we get exponential increases in cross­ entropy cost. The table further illustrates that—again, consistent with the behavior of its quadratic cousin—cross­entropy cost would be low with an a of 0.1192 if y happened to in fact be 0. These results reiterate for us that the chief distinction between the quadratic and cross­entropy functions is not the particular cost value that they calculate per se, but rather it is the rate at which they learn within a neural net— especially if saturated neurons are involved. Table 8.1 Table of cross­entropy costs associated with selected example inputs OPTIMIZATION: LEARNING TO MINIMIZE COST
Cost functions provide us with a quantification of how incorrect our model’s estimate of the ideal y is. This is most helpful because it arms us with a metric we can track if we’d like to reduce our network’s incorrectness. And, we’d pretty well always like to reduce its incorrectness! As alluded to a couple of times already in this chapter, the primary approach for minimizing cost in deep learning paradigms is to pair a method called gradient descent with another one called backpropagation. Together, these methods are optimizers that enable the network to learn. They accomplish this by adjusting the model’s parameters so that its estimated ŷ gradually converges toward the target of y, and thus the cost decreases. We’ll cover gradient descent now and move on to backpropagation immediately afterward. Gradient Descent Gradient is a handy, efficient tool for adjusting a model’s parameters with the aim of minimizing cost, particularly if we have a lot of data. It is widely used in other families of machine learning techniques as well, not only in deep learning. In Figure 8.2, we’ve used a nimble trilobite in a cartoon to illustrate how gradient descent works. Along the horizontal axis in each frame is some parameter that we’ve denoted as p. In an artificial neural network, this parameter would be either a neuron’s weight w or bias b. In the top frame, the trilobite finds itself on a hill. Its goal is to descend the gradient, thereby finding the location with the minimum cost, C. But, there’s a twist: The trilobite is blind! It cannot see whether deeper valleys lie far away somewhere, it can only use its cane to investigate the slope of the terrain in its immediate vicinity.
Figure 8.2 A trilobite using gradient descent to find the value of a parameter p associated with minimal cost, C. The orange line in Figure 8.2 indicates the blind trilobite’s calculation of the slope at the point that it finds itself. According to that slope line, if the trilobite takes a step to the left (i.e., to a slightly lower value of p), it would be moving to a location with smaller cost. On the hand, if the trilobite takes a step to the right (a slightly higher value of p), it would be moving to a location with higher cost. Given the trilobite’s desire to descend the gradient, it chooses to take a step to the left. By the second frame, the trilobite has taken several steps to the left. Here again, we see it evaluating the slope with the orange line and discovering that, yet again, a step to the left will bring it to a location with lower cost and so it takes another step left. In the third frame, the trilobite has succeeded in making its way to the location—i.e., the value of the parameter p—corresponding to the minimum cost. From this position, if it were to take a step to the left or to the right, cost would go up, so it gleefully remains in place. In practice, a deep learning model would not have just one parameter in it. It is not
uncommon for deep learning networks to have millions of parameters and some industrial applications have billions of them. Even our Shallow Net in Keras—one of the smallest models we’ll build in this book—has 50,890 parameters (Figure 7.5). While it’s impossible for the human mind to imagine a billion­dimensional space, the two­parameter cartoon shown in Figure 8.3 provides a sense of how gradient descent scales up to minimize cost across multiple parameters simultaneously. Across however many trainable parameters there are in a model, gradient descent iteratively evaluates 5 slopes to identify the adjustments to those parameters that correspond to the steepest reduction in cost. With two parameters, as in the trilobite cartoon in Figure 8.3 for example, this procedure can be likened to a blind hike through the mountains where: œ latitude represents one parameter, say p 1 œ longitude represents the other parameter, p 2 œ altitude represents cost—the lower the altitude, the better! Figure 8.3 A trilobite exploring along two model parameters, p and p , in order 1 2 to minimize cost via gradient descent. In a mountain­adventure analogy, p and p 1 2 could be thought of as latitude and longitude, while altitude represents cost. The trilobite randomly finds itself at a location in the mountains. From that point, it identifies the direction of the step it can take that will reduce its altitude the most. It then takes that single step. Repeating this process many times, the trilobite may eventually find itself at the latitude and longitude coordinates that correspond to the lowest­possible altitude (minimum cost), at which point the trilobite’s surreal alpine
adventure is complete. Learning Rate For conceptual simplicity, in Figure 8.4, let’s return to a blind trilobite navigating a single­parameter world. Now, let’s imagine that we have a ray­gun that can shrink or enlarge trilobites. In the middle panel, we’ve used our ray­gun to make our trilobite very small. The trilobite’s steps will then be correspondingly small and so it will take our intrepid little hiker a very long time to find its way to the legendary valley of minimum cost. On the other hand, consider the bottom panel, in which we’ve used our ray­gun to make the trilobite very large. The situation here is even worse! The trilobite’s steps will now be so large that it will step right over the valley of minimum cost and so it never has any hope of finding it. Figure 8.4 The learning rate (η) of gradient descent expressed as the size of a trilobite. The middle panel has a small learning rate and the bottom panel, a large one.
In gradient descent terminology, step size is referred to as learning rate and denoted with the Greek letter η (eta, pronounced “ee­ta”). Learning rate is the first of several model hyperparameters that we will cover in this book. In machine learning, including deep learning, hyperparameters are aspects of the model that we configure before we begin training the model. So, hyperparameters like η are pre­set while, in contrast, parameters—namely, w and b—are learned during training. Getting your hyperparameters right for a given deep learning model often requires some trial and error. For the learning rate η, it’s something like the fairy tale of Goldilocks and the Three Bears: too small and too large are both inadequate, but there’s a sweet spot in the middle. More specifically, as we cartooned in Figure 8.4, if η is too small, then it will take many, many iterations of gradient descent (read: a long time) to reach the minimal cost. On the other hand, picking an η that is too large means we might never reach minimal cost at all: The gradient descent algorithm will act erratically as it zooms right over the parameters associated with minimal cost. Coming up in Chapter 9, we have a clever trick waiting for you that will circumnavigate the need for you to pick a given neural network’s η hyperparameter entirely. In the interim, however, here are our rules of thumb on the topic: œ Begin with a learning rate of about 0.01 or 0.001. œ If your model is able to learn (i.e., if cost decreases consistently epoch over epoch) but training happens very slowly (i.e., each epoch, the cost decreases only a small amount), then increase your learning rate by an order of magnitude (e.g., from 0.01 to 0.1). If the cost begins to jump up and down erratically epoch over epoch, then you’ve gone too far, so reign your learning rate back down. œ At the other extreme, if your model is unable to learn, then it could be because your learning rate is too high. Try decreasing it by orders of magnitude (e.g., from 0.001 to 0.0001) until cost decreases consistently epoch over epoch. Batch Size and Stochastic Gradient Descent When we introduced gradient descent, we suggested that it is efficient for machine learning problems that involve a large data set. In the strictest sense, we outright lied to you. The truth is that if we have a very large data set, ordinary gradient descent would not work at all because it wouldn’t be possible to fit all of the data into the memory (RAM) of our machine. Memory isn’t the only potential snag—compute power could cause us headaches too. A
relatively large data set might squeeze into the memory of our machine, but if we tried to train a neural network containing millions of parameters with all those data, vanilla gradient descent would be highly inefficient because of the computational complexity of the associated high­volume, high­dimensional calculations. Thankfully, there’s a solution to these memory and compute limitations: the stochastic variant of gradient descent. With this variation, we split up our training data into mini­ batches—small subsets of our full training data set—to render gradient descent both manageable and productive. Although we didn’t focus on it at the time, when we trained the model in our Shallow Net in Keras notebook back in Chapter 5 we were already using stochastic gradient descent by setting our optimizer to SGD in the model.compile() step. Further, in the subsequent line of code when we called the model.fit() method, we set batch_size to 128 to specify the size of our mini­batches—the number of training data points that we use for a given iteration of SGD. Like the learning rate η presented earlier in this chapter, batch size is also a hyperparameter. Let’s work through some numbers to make the concepts of batches and stochastic gradient descent more tangible. In the MNIST data set, there are 60,000 training 6 images. With a batch size of 128 images, we then have 468.75 = 469 batches of gradient descent per epoch: Before carrying out any training, we initialize our network with random values for each 7 neuron’s parameters w and b. To begin the first epoch of training: 1. We shuffle and divide the training images into mini­batches of 128 images each. These 128 MNIST images provide 784 pixels each, which all together constitute the inputs x that are passed into our neural network. The shuffling step puts the stochastic into stochastic gradient descent. 2. By forward propagation, information about the 128 images is processed by the network, layer through layer, until the output layer ultimately produces ŷ values.
3. A cost function (e.g., cross­entropy cost) evaluates the network’s ŷ values against the true y values, providing a cost C for this particular mini­batch of 128 images. 4. To minimize cost and thereby improve the network’s estimates of y given x, the gradient decent part of stochastic gradient descent is performed: Every single w and b parameter in the network is adjusted proportional to how much each contributed to the error (i.e., the cost) in this batch (note that the adjustments are scaled by the learning 8 rate hyperparameter η). The above four steps constitute a round of training, as summarized by Figure 8.5. Figure 8.5 An individual round of training with stochastic gradient descent. Although mini­batch size is a hyperparameter that can vary, in this particular case, the mini­batch consists of 128 MNIST digits, as exemplified by our hike­loving trilobite carrying a small bag of data. Figure 8.6 captures how rounds of training are repeated until we run out of training images to sample. The sampling in step one is done without replacement, meaning that at the end of an epoch each image has been seen by the algorithm only once, yet between different epochs the mini­batches are sampled randomly. After a total of 468 rounds, the last batch contains only 96 samples.
Figure 8.6 An outline of the overall process for training a neural network with stochastic gradient descent. The entire dataset is shuffled and split into batches. Each batch is forward propagated through the network, the output is compared to the ground truth and the cost is calculated, backpropagation calculates the gradients, and the parameters are updated. The next batch (indicated by a dotted line) is forward propagated, and so on until all of the batches have moved through the network. Once all the batches have been used, a single epoch is complete and the process starts again with a re­shuffling of the data. This marks the end of the first epoch of training. Assuming we’ve set our model up to train for further epochs, we begin the next epoch by replenishing our pool with all 60,000 training images. As we did through the previous epoch, we then proceed 9 through a further 468 rounds of stochastic gradient descent. Training continues in this way until the total desired number of epochs is reached. The total number of epochs that we set our network to train for is yet another hyperparameter, by the way. This hyperparameter, though, is one of the easiest to get right: œ If the cost on your validation data is going down epoch over epoch, and your final epoch attained the lowest cost yet, then you can try training for additional epochs. œ Once the cost on your validation data begins to creep upward, that’s an indicator that your model has begun to overfit to your training data because you’ve trained for too
many epochs. (We’ll elaborate much more on overfitting in Chapter 9.) œ There are methods you can use to automatically monitor training and validation cost and stop training early if things start to go awry. In this way, you could set the number of epochs to be arbitrarily large and know that training will continue until the validation cost stops improving—and certainly before the model begins overfitting! Escaping the Local Minimum In all of the examples of gradient descent thus far in the chapter, our hiking trilobite has encountered no hurdles on its journey toward minimum cost. There are no guarantees that this would be the case, however. Indeed, such smooth sailing would be unusual. Figure 8.7 shows the mountaineering trilobite exploring the cost of some new model that is designed for solving some new problem. With this new problem, the relationship between the parameter p and cost C is more complex. To have our neural network estimate y as accurately as possible, gradient descent needs to identify the parameter values associated with the lowest­attainable cost. However, as our trilobite makes its way from its random starting point in the top panel, gradient descent leads it to getting trapped in a local minimum. As shown in the middle panel, while in the local minimum a step to the left or a step to the right both lead to an increase in cost and so the blind trilobite stays put, completely oblivious to the existence of a deeper valley—the global minimum—lying yonder.
Figure 8.7 A trilobite applying vanilla gradient descent from a random starting point (top panel) is ensnared by a local minimum of cost (middle panel). By turning to stochastic gradient descent in the bottom panel, the daring trilobite is able to bypass the local minimum and make its way toward the global minimum. All is not lost, friends, for stochastic gradient descent comes to the rescue here again. The sampling of mini­batches can have the effect of smoothing out the cost curve, as exemplified by the dotted curve shown in the bottom panel of Figure 8.7. This smoothing happens because when estimating the gradient from a smaller mini­batch (versus from the entire data set), the estimate is inherently noisier—while the actual gradient in the local minimum truly is zero, estimates of the gradient from small subsets of the data don’t tell the whole picture and might give an inaccurate reading causing our trilobite to take a step thinking there is a gradient when there really isn’t one. This is a good thing! The incorrect gradient may result in a step that is large enough for the trilobite to escape the valley and continue down the mountain. Thus, by estimating the gradient many times on these mini­batches, the noise of all of these gradient estimates is eventually smoothed and we are able to avoid local minima. 10 Conversely, if the gradient were estimated from the entire data set there would be no
noise. The trilobite would receive a noise­free reading of a zero gradient in that local minimum and would never know there was a whole world of cost­gains to be had just 11 over the rise, and so this approach will get stuck in the first local minimum it fell into. So, although each mini­batch on its own lacks complete information about the cost curve, in the long run—over a large number of mini­batches—this tends to work to our advantage. Like the learning rate hyperparameter η, there is also a Goldilocks­style sweet spot for batch size. If the batch size is too large (perhaps even enveloping the entire data set), the estimate of the gradient of the cost function is far more accurate. In this way, the trilobite has a more complete image of the mountain in that moment and is able to take a step (proportional to η) in the direction of the steepest possible descent. However, the model is at risk of becoming trapped in local minima as we described above. Besides that, the model might not fit in memory on your machine and the compute time per iteration of gradient descent could be very long. On the other hand, if the batch size is too small, each gradient estimate will be noisier (since a very small subset of the data is being used to estimate the gradient of the entire data set) and the corresponding path down the mountain will be more circuitous—training will take longer because of these erratic gradient descent steps. Furthermore, you’re not taking advantage of the memory 12 and compute resources on your machine. With that in mind, here are our rules of thumb for finding the batch­size sweet spot: œ Start with a batch size of 32 or 64. œ If the mini­batch is too large to fit into memory on your machine or if epochs of training proceed very slowly, try decreasing your batch size by powers of two (e.g., from 32 to 16). œ If your model trains well (i.e., cost is going down consistently) but each epoch is 13 taking very long and you are aware that you have RAM to spare, try increasing your batch size by powers of two, (e.g., from 64 to 128). BACKPROPAGATION While stochastic gradient descent operates well on its own to adjust parameters and minimize cost in many types of machine learning models, for deep learning models in particular there is an extra hurdle: We need to be able to efficiently adjust parameters through multiple layers of artificial neurons. To do this, stochastic gradient descent is partnered up with a method called backpropagation. Backpropagation—or backprop for short—is an elegant application of the “chain rule” 14
14 from calculus. As shown along the bottom of Figure 8.6 and as suggested by its very name, backpropagation courses through a neural network in the opposite direction of forward propagation. While forward propagation carries information about the input x through successive layers of neurons to approximate y with ŷ, backpropagation carries information about the cost C backwards through the layers in reverse and, with the overarching aim of reducing cost, adjusts neuron parameters throughout the network. While the nitty­gritty of backpropagation has been relegated to an appendix, it’s worth understanding (in broad strokes) what the backpropagation algorithm does: As we’ve seen thus far, any given model is randomly initialized with network parameters (w and b values). Thus, at the very beginning of training when the first x value is fed in, the network essentially outputs a completely random guess at ŷ. Of course, this won’t be a very good guess and the associated cost of the random guess will be high. At this point, we need to update the weights in order to minimize the cost—the very essence of learning in neural networks. Backpropagation calculates the gradient of the cost function with respect to each weight in the network. Recall from our mountaineering analogies earlier that the cost function represents a hiking trail and our trilobite is trying to reach basecamp. At each step along the way, the trilobite finds the gradient (or the slope) of the cost function and moves down that gradient. That movement that the trilobite just made is a weight update: By adjusting the weight in proportion to the cost function’s gradient with respect to that weight, we essentially adjust that weight in a way that reduces the cost! We know that last sentence might be hard to digest at first, so hang with us. If you recall the most important equation from Figure 6.7 in Chapter 6 (w · x + b), and you follow that neural networks are stacked and everything feeds together, it shouldn’t be hard to imagine that any given weight in the network contributes to the final ŷ output, and thus the cost. Using backpropagation, we move layer­by­layer backwards through the network, starting at the cost in the output layer, and we find the gradients of every single parameter. We then use the product of the gradient of that parameter—i.e., the relative amount which that parameter contributed to the total cost—and the learning rate η to update the parameter. This is not the lightest section of this book, by a wide margin. Also, you wouldn’t be the first deep learning practitioner who isn’t able to sketch out the specifics of backpropagation on a whiteboard. So if there’s only one thing you take away from this whole section, let it be this: Backpropagation uses the cost to calculate the relative contribution by every single parameter to the total cost, and then updates each parameter accordingly. In this way, the network slowly begins to reduce cost and, well... learn! NETWORK DEPTH: TUNING HIDDEN-LAYER COUNT
NETWORK DEPTH: TUNING HIDDEN-LAYER COUNT As with learning rate and batch size, the number of hidden layers you add into your neural network is also a hyperparameter. And as with the previous two, there is yet again a Goldilocks sweet spot for your network’s count of layers. Throughout this book, we’ve reiterated that with each additional hidden layer within a deep learning network, the more abstract the representations that the network can represent. That is the primary advantage of adding layers. The disadvantage of adding layers is that backpropagation becomes less effective: As demonstrated by the plot of learning speed across the layers of a four­hidden­layer network in Figure 8.8, backprop is able to have its greatest impact on the parameters of the hidden layer of neurons closest to the output ŷ. The further a layer is away from ŷ (where cost is calculated), the more diluted the effect of that layer’s parameters becomes on the overall cost. This is because with more hidden layers there are simply more parameters, and the relative contribution of each parameter is diminished. Thus, the fourth layer, which is closest to the output ŷ, learns most rapidly because those weights will have larger gradients. In contrast, the second layer, which is three layers away from the cost calculation, learns about an order of magnitude more slowly than the final layer. Here are our rules of thumb for selecting the number of hidden layers in your network: œ The more abstract the ground­truth value y you’d like to estimate with your network is, the more helpful additional hidden layers may be. With that in mind, we recommend starting off with about two to four hidden layers. œ If reducing the number of layers does not increase the cost you can achieve on your validation data set, then do it. Following the problem­solving principle called Occam’s razor, the simplest network architecture that can provide the desired result is the best —it will train more quickly and require fewer compute resources. œ On the other hand, if increasing the number of layers decreases the validation cost then you should layer away!
Figure 8.8 The speed of learning over epochs of training for a deep learning network with four hidden layers. The fourth layer, which is closest to the output ŷ learns about an order of magnitude more quickly than the second hidden layer. AN INTERMEDIATE NET IN KERAS To wrap up this chapter, let’s incorporate the new material we’ve uncovered into a neural network to see if we can outperform our previous Shallow Net in Keras model at classifying handwritten digits. The first few stages of our Intermediate Net in Keras Jupyter notebook are identical to its Shallow Net predecessor. We load the same Keras dependencies, load the MNIST data set in the same way, and preprocess the data the same way. The situation begins to get interesting at the cell where we design our neural network architecture: Example 8.1 Keras code to architect an intermediate­depth neural network model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(Dense(64, activation='relu')) model.add(Dense(10, activation='softmax')) The first line of this code chunk, model = Sequential(), is still the same as before
(refer back to Example 5.2)—this is our instantiation of a neural network model object. It’s in the second line that we begin to diverge. In it, we specify that we’ll substitute the sigmoid activation function in the first hidden layer with our most­highly­ recommended neuron from Chapter 6, the relu. Other than this neuron swap, the first hidden layer remains the same: It still consists of 64 neurons and the dimensionality of the 784­neuron input layer is unchanged. The other significant change in Example 8.1 relative to the shallow architecture of Example 5.2 is that we specify a second hidden layer of artificial neurons. By calling the model.add() method, we near­effortlessly add a second Dense layer of 64 relu neurons, providing us with the notebook’s namesake: an intermediate­depth neural network. With a call to model.summary(), we can see from Figure 8.9 that this additional layer corresponds to an additional 4160 trainable parameters relative to our shallow architecture (refer back to Figure 7.5). We can break these parameters down into: œ 4096 weights, corresponding to each of the 64 neurons in the second hidden layer densely receiving input from each of the 64 neurons in the first hidden layer (64 × 64 = 4096) œ plus 64 biases, one for each of the neurons in the second hidden layer œ giving us a total of 4160 parameters: n = n + n = 4096 + 64 = 4160 parameters w b Figure 8.9 A summary of the model object from our “Intermediate Net in Keras” Jupyter notebook. In addition to changes to the model architecture, we’ve also made changes to the parameters we specify when compiling our model: Example 8.2 Keras code to compile our intermediate­depth neural network
model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1), metrics=['accuracy']) With these lines from Example 8.2, we: œ set our loss function to cross­entropy cost with loss='categorical_crossentropy' (in Shallow Net in Keras, we used quadratic cost by setting loss='mean_squared_error') œ set our cost­minimizing method to stochastic gradient descent with optimizer=SGD 15 œ specified our SGD learning rate hyperparametr η to lr=0.1 œ indicated that, in addition to the Keras default of providing feedback on loss, by setting metrics=['accuracy'], we’d also like to receive feedback on model 16 accuracy Finally, we train our intermediate net by running: Example 8.3 Keras code to train our intermediate­depth neural network model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_valid, y_valid)) Relative to the way we trained our shallow net (see code at Example 5.3), the only change we’ve made is reducing our epochs hyperparameter from 200 down by an order of magnitude to 20. As we’ll see next, our much­more­efficient intermediate architecture required far fewer epochs to train.
Figure 8.10 provides the results of the first three epochs of training the network. Recalling that our shallow architecture plateaued as it approached 86% accuracy on the validation dataset after 200 epochs, our intermediate­depth network is clearly superior: The val_acc field shows that we attained 92.34% accuracy after a single epoch of training. This accuracy climbs to over 95% by the third epoch and appears to plateau around 97.6% by the twentieth. My, how far we’ve come already! Figure 8.10 The performance of our intermediate­depth neural network over its first three epochs of training. Breaking down the verbose model.fit() output shown in Figure 8.10 in a further detail: œ The progress bar shown below fills in over the course of the 468 “rounds of training” (Figure 8.5): 60000/60000 [==============================] œ 1s 15us/step indicates that all 468 rounds in the first epoch required one second to train, at an average rate of 15 microseconds per round. œ loss shows the average cost on our training data for the epoch. For the first epoch this is 0.4744 and, epoch over epoch, this cost is reliably minimized via stochastic gradient descent and backpropagation, eventually diminishing to 0.0332 by the 20th epoch. œ acc is the classification accuracy on training data for the epoch. The model correctly classified 86.37% for the first epoch, increasing to over 99% by the twentieth. Because a model can easily overfit to the training data, one shouldn’t be overly impressed by high accuracy on the training data. œ Thankfully, our cost on the validation data (val_loss) set does generally decrease as well, eventually plateauing as it approaches 0.08 over the final five epochs of training.
œ Corresponding to the decreasing cost of the validation data is a corresponding increase in accuracy (val_acc). As mentioned in the previous paragraph, validation accuracy plateaued at about 97.6%, which is a vast improvement over the 86% of our shallow net. SUMMARY We covered a lot of ground in this chapter. Starting from an appreciation of how a neural network with fixed parameters processes information, we developed an understanding of the cooperating methods—cost functions, stochastic gradient descent, and backpropagation—that enable network parameters to be learned so that we can approximate any y that has a continuous relationship to some input x. Along the way, we introduced several network hyperparameters, including learning rate, mini­batch size, and number of epochs of training—as well as our rules of thumb for configuring each of these. The chapter concluded by applying our new­found knowledge to a develop an intermediate­depth neural network that greatly outperformed our previous, shallow network on the same handwritten­digit­classification task. Up next, we have techniques for improving the stability of artificial neural networks as they deepen, enabling us to architect and train a bonafide deep learning model for the first time. KEY CONCEPTS Here are the essential foundational concepts thus far. New terms from the current chapter are highlighted in purple: œ parameters: œ weight w œ bias b œ activation a œ artificial neurons: œ sigmoid œ tanh œ ReLU œ input layer
œ hidden layer œ output layer œ layer types: œ dense (fully­connected) œ softmax œ cost (loss) functions: œ quadratic (mean squared error) œ cross­entropy œ forward propagation œ backpropagation œ optimizers: œ stochastic gradient descent œ optimizer hyperparameters: œ learning rate η œ batch size −8 1 . 9.0e­08 is equivalent to 9.0 × 10 2 . Recall from Chapter 6 that a = σ(z), where σ is some activation function—in this example, the tanh function. 3 . More methods for attenuating saturated neurons and their negative effects on a network will be covered in Chapter 9. 4 . This footnote is a Trilobite­Reading SIDEBAR. To understand how the cross­ entropy cost function in Equation 8.2 enables a neuron with larger cost to learn more rapidly, we require a touch of partial­derivative calculus. (Since we endeavor to
minimize the use of advanced mathematics in this book, we’ve relegated this calculusfocused explanation to this sidebar.) Central to the two computational methods that enable neural networks to learn—gradient descent and backpropagation—is the comparison of the rate of change of cost C relative to neuron parameters like weight w. Using partial­derivative notation, we can represent these relative rates of change as . The cross­entropy cost function is deliberately structured so that, when we calculate its derivative, is related to (ŷ − y). Thus, the larger the difference between the ideal output y and the neuron’s estimated output ŷ, the greater the rate of change of cost C with respect to weight w. 5 . Using partial­derivative calculus 6 . Since 60,000 is not perfectly divisible by 128, that 469th batch would only contain 0.75 × 128 = 96 images. 7 . We’ll detail parameter initialization with random values in Chapter 9. 8 . This error­proportional adjustment is calculated during backpropagation. We haven’t covered backpropagation explicitly yet, but it’s coming up in the next section so hang on tight! 9 . Because we’re sampling randomly, the order in which we select training images for our 468 mini­batches is completely different for every epoch. 10. This is often not even a possibility due to memory constraints. 11. It’s worth noting that the learning rate η plays a role here. If the size of the local minimum was smaller than the step size, the trilobite would likely breeze right past the local minimum just as we step over cracks in the sidewalk. 12. A batch­size of one is also known as online learning. It’s worth noting that this is not the fastest method in terms of compute ­ as it happens, the matrix multiplications in mini­batches are highly optimized and so training is several orders of magnitude faster when using mini­batches as compared to online learning. 13. On a Unix­based operating system, including Mac OS, RAM usage could be assessed by running the top or htop command within a Terminal window. 14. To elucidate the mathematics underlying backpropagation, a fair bit of partial­
derivative calculus is necessary. While we encourage the development of an in­depth understanding of the beautiful phenomenon of backprop, we also appreciate that calculus might not be the most appetizing topic for everyone. As such, we’ve placed our content on backprop mathematics in Appendix 18. (This should presumably be labelled Appendix B) 15. On your own time, you can play around with increasing this learning rate by several orders of magnitude as well as decreasing it by several orders magnitude, and observing how it impacts training. 16. Trilobite­Attention Sidebar: While loss provides the most important metric for tracking a model’s performance epoch over epoch, its particular values are specific to the characteristics of a given model and are not generally interpretable or comparable between models. Because of this, other than knowing that we would like our loss to be as close to zero as possible, it can be esoteric to interpret how close to zero loss should be for any particular model. Accuracy, on the other hand, is highly interpretable and highly generalizable: We know exactly what it means (e.g., “the shallow neural network correctly classified 86% of the handwritten digits in the validation dataset”) and we can compare this classification accuracy to any other model (“86% is worse than the performance of our deep neural network”).
9 Improving Deep Networks History In Chapter 6, we detailed individual artificial neurons. In Chapter 7, we arranged these Topics neural units together as the nodes of a network, enabling the forward propagation of some input x through the network to produce some output ŷ. Most recently, in Chapter Tutorials 8, we described how to quantify the inaccuracies of a network (compare ŷ to the true y with a cost function) as well as how to minimize these inaccuracies (adjust the network Offers & Deals parameters w and b via optimization with stochastic gradient descent and backpropagation). In this chapter, we’ll cover common barriers to the creation of high­ Highlights performing neural networks and techniques that overcome them. We’ll apply these 1 ideas directly in code while architecting our first deep neural network. Combining Settings this additional network depth with our new­found best­practices, we’ll see if we can outperform the handwritten­digit classification accuracy of our simpler, shallower Support architectures from previous chapters. Sign Out WEIGHT INITIALIZATION Back in Chapter 8, we introduced the concept of neuron saturation (see Figure 8.1), where very low or very high values of z diminish the capacity for a given neuron to learn. At the time, we offered cross­entropy cost as a solution. While cross­entropy does effectively attenuate the effects of neuron saturation, pairing it with thoughtful weight initialization will reduce the likelihood of saturation occurring in the first place. As mentioned in a footnote in Chapter 1, modern weight initialization provided a significant leap forward in deep learning capability: It is one of the landmark theoretical advances between LeNet­5 (Figure 1.12) and AlexNet (Figure 1.18) that dramatically broadened the range of problems artificial neural networks could reliably solve. In this section, we’ll play around with several weight initializations to develop an intuition around how they’re so impactful. While describing neural­network training in Chapter 8, we mentioned that the parameters w and b are initialized with random values such that a network’s starting approximation of y will be far off the mark, thereby leading to a high initial cost C. We haven’t needed to dwell on this much because, in the background, Keras by default constructs TensorFlow models that are initialized with sensible values for w and b. In
Chapters 14 and 15, we’ll get our hands dirty with raw TensorFlow and PyTorch code, at which point we’ll explicitly need to make decisions about parameter initialization ourselves. Even now however, it’s worthwhile discussing this initialization: not only to be aware of another method for avoiding neuron saturation, but also to fill in a gap in our understanding of how network training works. While Keras does a sensible job of choosing default values—and that’s a key benefit of using Keras in the first place—it’s certainly possible, and sometimes even necessary, to change these defaults to suit your problem. To make this section interactive, we encourage you to check out our accompanying Jupyter notebook, First TensorFlow Neurons. This marks our inaugural foray into TensorFlow code, but we’ll save the details for Chapter 14, skimming over them for the moment. As shown in the upcoming chunk of code, our library dependencies are NumPy (for numerical operations), matplotlib (for generating plots) and, as promised, TensorFlow: import numpy as np import tensorflow as tf import matplotlib.pyplot as plt In this notebook, we’re going to simulate 784 pixel values as inputs to a single dense layer of artificial neurons. The inspiration behind our simulation of these 784 inputs comes of course from our beloved MNIST digits (Figure 5.3). For the number of neurons in the dense layer, we picked a number large enough so that, when we make some plots later on, they look pretty: n_input = 784 n_dense = 256 When Keras creates TensorFlow networks for us, it handily generates all of the components of the network, including arrays of data for storing all of the relevant network values. As we’ll detail in Chapter 14, these arrays are called tensors. Without Keras doing the heavy lifting for us, we’ll have to initialize all the relevant tensors ourselves. We begin by creating a tensor for holding our 784 input values:
x = tf.placeholder(tf.float32, [None, n_input]) Now, for the primary point of this section: the initialization of the network parameters w and b. Before we begin passing training data into our network, we’d like to start with reasonably scaled parameters. This is because: 1. Large w and b values tend to correspond to larger z values, and therefore saturated neurons. 2. Large parameter values would imply that the network has a strong opinion about how x is related to y—before any training on data has occurred, any such strong opinions are wholly unmerited. Parameter values of zero, on the other hand, imply the weakest opinion on how x is related to y. To bring back the fairytale yet again, we’re aiming for a Goldilocks­style, middle­of­the­road approach that starts training off from a balanced and learnable beginning. With that in mind, let’s use the TensorFlow zeros() method to initialize the 256 neurons in our dense layer with b = 0: b = tf.Variable(tf.zeros([n_dense])) Following the line of thinking from the previous paragraph to its natural conclusion, we might be tempted to think that we should also initialize our network weights w with zeros as well. In fact, this would be a training disaster: If all weights and biases were identical, many neurons in the network would treat a given input x identically, giving stochastic gradient descent a minimum of heterogeneity for identifying individual parameter adjustments that might reduce the cost C. It would be more productive to initialize weights with a range of different values so that neurons treat a given x in unique ways, thereby providing SGD with a wide variety of starting points for approximating y. By chance, some of the initial neuron outputs may partly contribute to a sensible mapping from x to y. While this contribution will be weak at first, SGD can experiment with it to determine if it might contribute to a reduction in the cost C between the predicted ŷ and the target y. As worked through earlier (e.g., in discussion of Figures 7.5 and 8.9), the vast majority of the parameters in a typical network are weights; relatively few are biases. As such, it’s acceptable (indeed, it’s the most common practice) to initialize biases with zeros
and the weights with a range of values near zero. One straightforward way to generate random values near zero is to use TensorFlow’s random_normal() method to sample 2 values from a normal distribution like so: Example 9.1 Weight initialization with values sampled from a normal distribution W = tf.Variable(tf.random_normal([n_input, n_dense])) To observe the impact of the weight initialization we’ve chosen, we write some code to represent our dense layer of neurons: Example 9.2 Code for calculating the output of a layer of neurons z = tf.add(tf.matmul(x, W), b) a = tf.sigmoid(z) If you decompose the first line, you can see that it is our “most important equation” (Figure 6.7), z = w · x + b: 3 œ tf.matmul(x, W) uses the TensorFlow matrix multiplication operation to calculate the dot product w · x œ tf.add() adds b to that product, returning us z Simply beautiful, isn’t it? In the second line of Example 9.2, we go on to apply whatever activation function we fancy—in this case the sigmoid() function—to z, giving us the neuron activation a. Since these activation functions are such a core part of deep learning, TensorFlow has implemented many of them within the library (recall how we defined the sigmoid() function in the Sigmoid Function Jupyter notebook in Chapter 6), and they’re optimized to help speed up the compute time! We won’t explore the details until Chapter 14, but in the following lines of code we use 4 the NumPy random() method to feed 784 random numbers as inputs into our dense layer of 256 neurons, returning 256 a activations to a variable named layer_output: initializer_op = tf.global_variables_initializer()
with tf.Session() as session: session.run(initializer_op) layer_output = session.run(a, {x: np.random.random([1, n_input])}) With one more line of code, we use a histogram to visualize the a values stored in 5 layer_output: _ = plt.hist(np.transpose(layer_output)) Your result will look slightly different from ours because of the random() method we used to generate our input values, but your outputs should look approximately like those shown in Figure 9.1. Figure 9.1 Histogram of the a activations output by a layer of sigmoid neurons, with weights initialized using a normal distribution. As expected given Figure 6.9, the a activations output from our sigmoid layer of neurons is constrained to a range from zero to one. What is undesirable about these activations, however, is that they are chiefly pressed up against the extremes of the range: Most of them are either immediately adjacent to 0 or immediately adjacent to 1. This indicates that with the normal distribution that we sampled from to initialize the layer’s weights w, we ended up encouraging our artificial neurons to produce large z values. This is unwelcome for two reasons:
1. It means the vast majority of the neurons in the layer are saturated. 2. It implies that the neurons have strong opinions about how x would influence y prior to any training on data. Thankfully, this ickiness can be resolved by initializing our network weights with values sampled from alternative distributions. Xavier Glorot Distributions In deep­learning circles, popular distributions for sampling weight­initialization values 6 were devised by Xavier Glorot and Yoshua Bengio (portrait provided in Figure 1.11). These Glorot distributions, as they are typically called, are tailored such that sampling from them will lead to neurons initially outputting small z values. Let’s examine them in action. By replacing the normal­distribution­sampling code (Example 9.1) of our First TensorFlow Neurons notebook with the following line, we sample from a Glorot distribution instead: Example 9.3 Weights initialization with values sampled from a Glorot distribution W = tf.get_variable('W', [n_input, n_dense], initializer=tf.contrib.layers.xavier_initializer()) 7 By restarting and re­running the notebook, you should now observe a distribution of layer_output similar to the histogram shown in Figure 9.2. Figure 9.2 Histogram of the a activations output by a layer of sigmoid neurons, with weights initialized using a Glorot distribution.
In stark contrast to Figure 9.1, the a activations obtained from our layer of sigmoid neurons is now normally distributed with a mean of ~0.5 and few (if any) values at the extremes of the sigmoid range (i.e., less than 0.1 or greater than 0.9). This is a good starting point for a neural network because: 8 1. Few, if any, of the neurons are saturated. 2. It implies that the neurons generally have weak opinions about how x would influence y, which—prior to any training on data—is sensible. This paragraph is a Trilobite­attention SIDEBAR: As demonstrated in this section, one of the potentially confusing aspects of weight initialization is that, if we would like the a values returned by a layer of artificial neurons to be normally distributed (and we do!), we should not sample our initial weights from a standard normal distribution. END SIDEBAR. There are two Glorot distributions you can select between: Glorot uniform and Glorot normal. By using the TensorFlow xavier_initializer() method in Example 9.3, we were using the default option, which is Glorot uniform. On the other hand, by setting the method’s uniform parameter to false—as in, xavier_initializer(uniform=False)—we are opting to sample values from the Glorot normal distribution. The impact of selecting one of these Glorot distributions over the other when initializing your model weights is generally imperceptible. You’re welcome to re­run the notebook while sampling values from the Glorot normal distribution; your histogram of activations should come out more or less indistinguishable from Figure 9.2. By swapping out the sigmoid activation function in Example 9.2 with tanh (a = tf.tanh(z)) or ReLU (a = tf.nn.relu(z)) in the First TensorFlow Neurons notebook, you can observe the consequences of initializing weights with values sampled from a standard normal distribution (Figure 9.1) relative to a Glorot distribution (Figure 9.2). Regardless of activation function, weight initialization with the standard normal leads to relatively extreme a activations from the layer of dense neurons, as shown in Figure 9.3.
Figure 9.3 The activations output by a dense layer of 256 neurons, while varying activation function (tanh or ReLU) and weight initialization (standard normal or Glorot uniform) We hope the exposure to TensorFlow in this section instilled a sense of eager anticipation for what’s to come in Chapter 14. With respect to parameter initialization in Keras, you can delve into the API’s documentation on a layer­by­layer basis but, just as we’ve suggested here, its default configuration is typically to initialize biases with zero and to initialize weights with a Glorot distribution. UNSTABLE GRADIENTS Another issue associated with artificial neural networks, and one that becomes especially perturbing as we add more hidden layers, is unstable gradients. Unstable gradients can either be vanishing or explosive in nature. We’ll cover both varieties in turn here. Vanishing Gradients Recall that using the cost C between the network’s predicted ŷ and the true y, as diagrammed in Figure 8.6, backpropagation works its way from the output layer toward the input layer, adjusting network parameters with the aim of minimizing cost. As exemplified by the mountaineering trilobite in Figure 8.2, the parameters are each
adjusted in proportion to their gradient with respect to cost: If, for example, the gradient of a parameter (with respect to the cost) was large and positive, this implies that the parameter contributes a large amount to the cost and so decreasing it 9 proportionally would correspond to a decrease in cost. In the hidden layer that is closest to the output layer, the relationship between its parameters and cost is the most direct. The further away a hidden layer is from the output layer, the more muddled the relationship between its parameters and cost becomes. The impact of this is that, as we move from the final hidden layer toward the first hidden layer, the gradient of a given parameter relative to cost tends to flatten—it gradually vanishes. As a result of this, and as plotted in Figure 8.8, the further a layer is from the output layer, the more slowly it tends to learn. Because of the vanishing gradient problem, if we were to naïvely add more and more hidden layers to our neural network, eventually the hidden layers furthest from the output would not be able to learn to any extent, crippling the capacity for the network as a whole to learn to approximate y given x. Exploding Gradients While they occur much less frequently than vanishing gradients, certain network architectures can induce exploding gradients. In this case, the gradient between a given parameter relative to cost becomes increasingly steep as we move from the final hidden layer toward the first hidden layer. As with vanishing gradients, exploding gradients can inhibit an entire neural network’s capacity to learn by saturating the neurons with extreme values (recall that this was a problem from our discussion about weights initialization). Batch Normalization During the course of normal training, the distribution of hidden parameters in a layer may gradually move around—this is known as internal covariate shift. In fact, this is sort of the point—we want the parameters to change in order to learn things about the underlying data. But as the distribution of the weights in a layer changes, so the inputs to the next layer might be shifted away from an ideal distribution. Enter batch 10 normalization (or batch norm for short). Batch norm takes the a activations output from the previous layer and subtracts the batch mean and divides by the batch standard deviation. This acts to re­center the distribution of the a values with a mean of 0 and a standard deviation of 1 (Figure 9.4). Thus, if there are any extreme values in the previous layer, they won’t cause exploding or vanishing gradients in the next layer. Batch norm has a few advantages:
œ It allows layers to learn more independently from each other, since large values in one layer won’t adversely influence the next layer. œ It allows for selection of a higher learning rate because there are no extreme values in the normalized activations, thus enabling faster learning. œ The layer outputs are normalized to the batch mean and standard deviation, which adds a noise element (especially with smaller batch sizes) which, in turn, contributes to regularization. (Regularization will be covered in the next section, but suffice to say here that regularization helps a network generalize, which is a good thing.) Figure 9.4 Batch normalization transforms the distribution of the activations output by a given layer of neurons toward a standard normal distribution. Another point to consider with batch norm is that it adds two extra learnable parameters to the normalized layers: γ (gamma) and β (beta). In the final step of batch norm, the outputs are linearly transformed by multiplying by γ and adding β, where γ is analogous to the standard deviation, and β to the mean. If your math is on point, you’ll notice this is the exact inverse of the operation that normalized the output values in the first place! However, the output values were originally normalized by the batch mean and batch standard deviation, whereas these two parameters are learned by SGD. We initialize the batch norm layer with γ = 1 and β = 0, thus at the start of training this
linear transformation makes no changes—batch norm is allowed to normalize the outputs as intended. As the network learns though, it may determine that de­ normalizing any given layer’s activations is optimal for reducing cost. In this way, if batch norm is not helpful the network will learn to stop using it on a layer­by­layer basis. In fact, the network can decide to what degree it would like to de­normalize the outputs, depending on what works best to minimize the cost. Pretty neat! MODEL GENERALIZATION (AVOIDING OVERFITTING) In Chapter 8, we mentioned that after training a model for a certain number of epochs the cost calculated on the validation dataset—which may have been decreasing nicely over earlier epochs—could begin to increase paradoxically, despite the fact that the cost calculated on the training dataset is still decreasing! This situation—where training cost continues to go down while validation cost goes up—is formally known as overfitting. Overfitting is nicely illustrated in Figure 9.5. Notice we have the same data points scattered along x and y axes in each panel. We can imagine that there is some distribution that describes these points, and here we have a sampling from that distribution. Our goal is to generate a model that explains the relationship between x and y, but perhaps most importantly that also approximates the original distribution— in this way, the model will be able to generalize to new data points drawn from the distribution and not just model the sampling of points we already have. In the first panel (top left), we use a single­parameter model, which is limited to fitting a straight 11 line to the data. This straight line underfits the data: The cost (represented by the vertical gaps between the line and the data points) is high and the model would not generalize well to new data points. Put simply, the line misses most of the points because this kind of model is not complex enough. In the next panel (top right), we use 12 a model with two parameters, which fits a parabola­shaped curve to the data. With this parabolic model, the cost is much lower relative to the linear model and it appears the model would also generalize well to new data—great!
Figure 9.5 Fitting y given x using models with varying numbers of parameters. Top right: A single­parameter model underfits the data. Top left: A two­parameter model fits a parabola that suits the relationship between x and y well. Bottom left: A many­parameter model overfits the data, generalizing poorly to new data points (shown in green in the bottom­right panel). In the third panel (bottom left) of Figure 9.5, we use a model with too many parameters —more parameters than we have data points. With this approach we reduce the cost associated with our training data points to nil: There is no perceptible gap between the curve and the data. In the last panel (bottom right), however, we show new data points
from the original distribution in green, which were unseen by the model during training and so can be used to validate the model. Despite eliminating training cost entirely, the model fits these validation data poorly and so it gets a correspondingly sizeable validation cost. The many­parameter model, dear friends, is overfit: It is a perfect model for the training data, but it doesn’t actually capture the true relationship between x and y—rather, it has learned the exact features of the training data too closely and subsequently it performs badly on unseen data. Consider how in three lines of code in Example 5.2, we created a shallow neural network architecture with over 50,000 parameters (Figure 7.5). Given this, it should not be surprising that deep learning architectures regularly have millions of 13 parameters. This hints at why deep learning models typically require large amounts of data: Working with data sets that may only have thousands of training samples but 14 millions of parameters could be a recipe for severe overfitting. Since we yearn to capitalize on deep, sophisticated network architectures even if we don’t have oodles of data at hand, thankfully we can rely on techniques specifically designed to reduce overfitting. We’ll cover three of the best­known such techniques now. L1 and L2 Regularization In branches of machine learning other than deep learning, the use of L1 regularization or L2 regularization to reduce overfitting is prevalent. These techniques—which are 15 alternately known as ridge regression and LASSO regression, respectively—both penalize models for including parameters by adding the parameters to the model’s cost function. The larger a given parameter’s size, the more that parameter adds to the cost function. Because of this, parameters will not be retained by the model unless they appreciably contribute to the reduction of the difference between the model’s estimated ŷ and the true y. In other words, extraneous parameters are pared away. This paragraph is a Trilobite­reading SIDEBAR: The distinction between L1 and L2 regularization is that L1’s additions to cost correspond to the square of parameter sizes while L2’s additions correspond to the absolute value. The net effect of this is that L1 regularization tends to lead to the inclusion of a larger number of smaller­sized parameters in the model, while L2 regularization tends to lead to the inclusion of a smaller number of larger­sized parameters. END SIDEBAR. Dropout L1 and L2 regularization work fine to reduce overfitting in deep learning models, but deep learning practitioners tend to favor the use of a neural network­specific regularization technique instead. This technique, called dropout, was developed by 16
16 Geoff Hinton (Figure 1.17) and his colleagues at the University of Toronto and was made famous its incorporation in their benchmark­smashing AlexNet architecture (Figure 1.18). Hinton and his coworkers’ intuitive yet powerful concept for preventing overfitting is captured by Figure 9.6. In a nutshell, dropout simply pretends that a randomly­selected proportion of the neurons in each layer don’t exist during each round of training. To 17 illustrate this, three rounds of training are shown in the figure. For each round, we remove a specified proportion of hidden layers by random selection: œ For the first hidden layer in the network, we’ve configured it to drop out one third (33.3%) of the neurons. œ For the second hidden layer, we’ve configured 50% of the neurons to be dropped out.
Figure 9.6 Dropout, a technique for reducing model overfitting, involves the removal of randomly­selected neurons from a network’s hidden layers in each round of training. Three rounds of training with dropout are shown here. Let’s cover each of the three training rounds in turn: 1. In the top panel, the second neuron of the first hidden layer and the first neuron of the second hidden layer are randomly dropped out. 2. In the middle panel, it is the first neuron of the first hidden layer and the second one of the second hidden layer that are selected for dropout. There is no “memory” of which neurons have been dropped out on previous training rounds, and so it is by chance alone that the neurons dropped out in the second round are distinct from those dropped out in the first.
3. In the bottom panel, the third neuron of the first hidden layer is dropped out for the first time. For the second consecutive round of training, the second neuron of the second hidden layer is also randomly selected. Instead of reining in parameter sizes toward zero (as with batch normalization), dropout doesn’t (in theory) constrain how large a given parameter value can become. Dropout is nevertheless an effective regularization technique because it prevents any single neuron from become excessively influential within the network: Dropout makes it challenging for some very specific aspect of the training dataset to create an overly specific forward­propagation pathway through the network because, on any given round of training, neurons along that pathway could be removed. In this way, the model doesn’t become over­reliant on certain features of the data to generate a good prediction. When validating a neural network model that was trained using dropout, or indeed when making real­world inferences with such a network, we must take an extra step first. During validation or inference, we would like to leverage the power of the full network, i.e., its total complement of neurons. The snag is that, during training, we only ever used a subset of the neurons to forward propagate x through the network and estimate ŷ. If we were to naïvely carry out this forward propagation with suddenly all of the neurons, our ŷ would emerge befuddled: There are now too many parameters and the totals after all the mathematical operations would be larger than expected. To compensate for the additional neurons, we must correspondingly adjust our neuron parameters downward. If we had, say, dropped out half of the neurons in a hidden layer during training, then we multiply the layer’s parameters by 0.5 prior to validation or inference. For a hidden layer in which we dropped out 33.3% of the neurons during training, we multiply the layer’s parameters by 0.667 prior to validation. Thankfully, Keras handles this parameter­adjustment process for us automatically. When working in low­level TensorFlow, however, you need to be mindful and remember to carry out these adjustments yourself. This paragraph is a Trilobite­Reading Sidebar. If you’re familiar with creating ensembles of statistical models (e.g., a single random forest out of multiple random decision trees), then it may already be evident to you that dropout produces such an ensemble. During each round of training, a random subnetwork is created and its parameter values are tuned. Later, at the conclusion of training, all of these subnetworks are reflected in the parameter values throughout the final network—in this way, the final network is an aggregated ensemble of its constituent subnetworks. END SIDEBAR.
Like learning rate and mini­batch size discussed in Chapter 8, network architecture options pertaining to dropout are hyperparameters. Here are our rules of thumb for choosing which layers to apply dropout to and how much of it to apply: œ If your network is overfitting to your training data (i.e., your validation cost increases while your training cost goes down), then dropout is warranted somewhere in the network. œ Even if your network isn’t obviously overfitting to your training data, adding some dropout to the network may improve validation accuracy—especially in later epochs of training. œ Applying dropout to all of the hidden layers in your network may be overkill. If your network has a fair bit of depth, it may be sufficient to apply dropout solely to later layers in the network (the shallowest layers may be harmlessly identifying features). To test this out, you could begin by applying dropout only to the deepest layer and observing if this is sufficient for curtailing overfitting; if not, add dropout to the next deepest layer, test it, and so on. œ If your network is struggling to reduce validation cost or to recapitulate low validation costs attained when less dropout was applied, then you’ve added too much dropout—pare it back! As with other hyperparameters, there is a Goldilocks­zone for dropout too. œ With respect to how much dropout to apply to a given layer, each network behaves uniquely and so some experimentation is required. In our experience, dropping out 20% up to 50% of the hidden layer neurons in machine­vision applications tends to provide the highest validation accuracies. In natural language applications, where individual words and phrases can convey particular significance, we have found that dropping out a smaller proportion—between 20% and 30% the neurons in a given hidden layer—tends to be optimal. Data Augmentation In addition to regularizing our model’s parameters to reduce overfitting, another approach is to increase the size of our training dataset. If it is possible to collect additional high­quality training data for the particular modeling problem you’re working on, then you should do so! The more data provided to a model during training, the better the model will be able to generalize to unseen validation data. In many cases, collecting fresh data is a pipe dream. It may nevertheless be possible to
generate new training data from existing data by augmenting it, thereby artificially expanding your training dataset. With the MNIST digits, for example, many different types of transforms would yield training samples that constitute suitable handwritten digits, e.g.: œ skewing the image œ blurring the image œ shifting the image a few pixels œ applying random noise to the image œ rotating the image slightly Indeed, as shown on the website of Yann LeCun (Figure 1.10), many of the record­ setting MNIST validation dataset classifiers took advantage of such artificial training 18 dataset expansion. FANCY OPTIMIZERS So far in this book we’ve only used one optimization algorithm: stochastic gradient descent. While SGD performs well, researchers have devised shrewd ways to improve them. Momentum The first SGD improvement is to consider momentum. Here’s an analogy of the principle: Let’s imagine it’s winter and out intrepid trilobite is skiing down a snowy gradient­mountain. If a local minimum is encountered (as in the middle panel of Figure 8.7), the momentum of the trilobite’s movement down the slippery hill will keep it sailing by and the minimum will be easily bypassed. In this way, the gradients on previous steps have influenced the current step. We calculate momentum in SGD by taking a moving average of the gradients for each parameter and using that to update the weights in each step. When using momentum, we have additional hyperparameter β (beta), which ranges from zero to one, and which controls how many previous gradients are incorporated in the moving average. Small β values permit older gradients to contribute to the moving average, which can be unhelpful—the trilobite wouldn’t want the steepest part of the hill to guide its speed as it approaches the lodge for the après­ski portion of the day. Typically we’d use larger β values, with β = 0.9 serving as a reasonable default. Nesterov Momentum
Nesterov Momentum Another version of momentum is called Nesterov momentum. In this approach, the moving average of the gradients is first used to update the weights and find the gradients at whatever that position may be—this is equivalent to a quick peek at where momentum might take us. We then use the gradients from this sneak­peek position to execute a gradient step from our original position. In other words, our trilobite is suddenly aware of its speed down the hill, so it’s taking that into account, guessing where its own momentum might be taking it, and then adjusting its course before it even gets there. AdaGrad While both momentum approaches improve SGD, a shortcoming is that they both use a single learning rate η for all parameters. Imagine, if you will, that we could have an individual learning rate for each parameter, thus enabling those parameters which have already reached their optimum to slow or halt learning, whilst those that are far from their optima can keep going. Well, you’re in luck! That’s exactly what can be achieved with the other optimizers we’ll discuss in this section: AdaGrad, AdaDelta, RMSProp and Adam. 19 The name AdaGrad comes from “Adaptive Gradient”. In this variation, every parameter has a unique learning rate that scales depending on the importance of that feature. This is especially useful for sparse data where some features occur only rarely: When those features do occur, we’d like to make larger updates their parameters. We achieve this individualization by maintaining a matrix of the sum of squares of the past gradients for each parameter, and dividing the learning rate by its square root. AdaGrad is the first introduction to the parameter ε (epsilon), which is a doozy: Epsilon is a smoothing factor to avoid divide­by­zero errors and can safely be left at its default −8 20 value of ε = 1 × 10 . An added benefit of AdaGrad is that it minimizes the need to tinker with the learning rate hyperparameter η. You can generally just set­and­forget­it at its default of η = 0.01. A considerable downside of AdaGrad is that, as the matrix of past gradients increases in size, the learning rate is increasingly divided by a larger and larger value, which eventually renders the learning rate impractically small and so learning essentially stops. AdaDelta and RMSProp AdaDelta resolves the gradient­matrix­size shortcoming of AdaGrad by maintaining a 21 moving average of previous gradients in just the same way that momentum does. 22
22 AdaDelta also eliminates the η term so a learning rate doesn’t need to be set. RMSProp (Root Mean Square Propagation) was developed by Geoff Hinton (Figure 23 1.17) at about the same time as AdaDelta. It works similarly except it retains the learning rate η parameter. Both RMSProp and AdaDelta involve an extra hyperparameter ρ (rho), or decay rate, which is analogous to the β value from momentum and which guides the size of the window for the moving average. Recommended values for the hyperparameters are ρ = 0.95 for both, and η = 0.001 for RMSProp or η = 1 for AdaDelta. Adam The final optimizer we’ll discuss in this section is also the one we’ll employ most often in the book. Adam—short for Adaptive Moment Estimation—builds on the optimizers 24 that came before it. It’s essentially the RMSProp algorithm with two exceptions: 1. An extra moving average is calculated, this time of past gradients for each parameter 25 (called the average first moment of the gradient, or just the mean) and this is used to inform the update instead of the actual gradients at that point. 2. A clever bias trick was used to help prevent these moving averages from skewing towards zero at the start of training. Adam has two β hyperparameters, one for each of the moving averages that are calculated. Recommended defaults are β = 0.9 and β = 0.999. The learning rate 1 2 default with Adam is η = 0.001. Since RMSProp, AdaDelta and Adam are so similar they may be used interchangeably in similar applications, although the bias correction may help Adam later in training. Even though these new­fangled optimizers are in vogue, there is still a strong case for simple SGD with momentum (or Nesterov momentum), which in some cases performs better. As with other aspects of deep learning models, you can experiment with optimizers and observe what works best for your particular problem. A DEEP NEURAL NETWORK IN KERAS We can now sound the trumpet as we’re reached a momentous milestone! With the additional theory we’ve covered in this chapter, we have enough knowledge under our belts to competently design and train a deep learning model. If you’d like to follow along interactively as we do so, pop into the accompanying Deep Net in Keras Jupyter notebook. Relative to our shallow and intermediate­depth model notebooks (refer to Example 5.1), we have a pair of additional dependencies—namely, dropout and batch
normalization: Example 9.4 Additional dependencies for deep net in Keras from keras.layers import Dropout from keras.layers.normalization import BatchNormalization We load and preprocess the MNIST data the same was as previously. It’s the neural network architecture cell where we begin to diverge: Example 9.5 Deep net in Keras model architecture model = Sequential() model.add(Dense(64, activation='relu', input_shape=(784,))) model.add(BatchNormalization()) model.add(Dense(64, activation='relu')) model.add(BatchNormalization()) model.add(Dense(64, activation='relu')) model.add(BatchNormalization()) model.add(Dropout(0.2)) model.add(Dense(10, activation='softmax'))
As before, we instantiate a Sequential model object. After we add our first hidden layer to it, however, we also add a BatchNormalization() layer. In doing this we are not adding an actual layer replete with neurons, but rather we’re adding the batch norm transformation for the activations a from the layer before (the first hidden layer). As with the first hidden layer, we also add a BatchNormalization() layer atop the second hidden layer of neurons. Our output layer is identical to the one used in the shallow and intermediate­depth nets, but to create an honest­to­goodness deep neural network, we are further adding a third hidden layer or neurons. As with the first and second hidden layers, the third hidden layer consists of 64 batch­normalized relu neurons. We are, however, supplementing this final hidden layer with Dropout, set to remove a fifth (0.2) of the layer’s neurons during each round of training. The only other change relative to our intermediate­depth network is that we use the Adam optimizer (optimizer='adam') in place of ordinary SGD optimization: model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) Note that we need not supply any hyperparameters to the Adam optimizer because Keras handily includes all the sensible defaults we detailed in the previous section automatically. For all of the other optimizers we covered, Keras (and TensorFlow for that matter) has implementations that can easily be dropped in in place of ordinary SGD or Adam. Simply refer to the documentation for those libraries to see exactly how it’s done. 26 When we call the fit() method on our model, we discover that our digestion of all the additional theory in this chapter paid off: With our intermediate­depth network, our validation accuracy plateaued around 97.6%, but our deep net attained 97.87% validation accuracy after the 15th epoch of training, shaving 11% of our already­small error rate away. To squeeze even more juice out of the error­rate lemon that that, we’re going to need machine vision­specific neuron layers such as those introduced in the upcoming Chapter 10. TENSORBOARD
When evaluating the performance of your model epoch over epoch, it can be tedious and time­consuming to read individual results numerically, as in Figure 9.7, particularly if the model has been training for many epochs. Instead, TensorBoard (Figure 9.8) is a convenient, graphical tool for: œ visually tracking model performance in real time, œ reviewing historical model performances, and œ comparing model performances. Figure 9.7 Our deep neural network architecture peaked at a 97.87% validation accuracy at the 15th epoch, besting the accuracy of our shallow and intermediate­ depth architectures. Due to the randomness of network initialization and training, you may obtain a slightly lower or a slightly higher accuracy with the identical architecture.
Figure 9.8 The TensorBoard dashboard enables you to, epoch over epoch, visually track your model’s cost (loss) and accuracy (acc) across both your training data and your validation val data. TensorBoard comes automatically with the TensorFlow library and instructions for 27 getting it up and running are available via the TensorFlow site. It’s generally straightforward to set up though. Here, for example, is a procedure that adapts our
Deep Net in Keras notebook for TensorBoard use on a Unix­based operating system, including Mac OS: 28 1. As shown in Example 9.6, change your Python code as follows: a. Import the TensorBoard dependency from keras.callbacks b. Instantiate a TensorBoard object (we’ll call it tensorboard) and specify a new, unique directory name (e.g., deep­net) that you’d like to create and have TensorBoard log data written into for this particular run of model­fitting: tensorboard = TensorBoard(log_dir='logs/deep­net') c. Pass the TensorBoard object as a callback parameter to the fit() method: callbacks = [tensorboard] 29 2. In your terminal, run: tensorboard ­­logdir='logs/deep­net' ­­port 6006 3. Navigate to localhost:6006 in your favorite web browser. Example 9.6 Code to use TensorBoard while fitting a model in Keras from keras.callbacks import TensorBoard tensorboard = TensorBoard('logs/deep­net') model.fit(X_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_valid, y_valid), callbacks=[tensorboard])
By following the above steps or an analogous procedure for the circumstances of your particular operating system, you should see something like Figure 9.8 in your browser window. From there, you can visually track any given model’s cost and accuracy across both your training and validation data sets in real time as these metrics change epoch by epoch. This kind of performance tracking is one of the primary uses of TensorBoard, though the dashboard interface also provides heaps of other functionality, like visual breakdowns of your neural­network graph and the distribution of your model weights. You can learn about these additional features by reading the TensorBoard docs and exploring the interface on your own. SUMMARY Over the course of the chapter, we discussed common pitfalls in modeling with neural networks and covered strategies for eliminating these pitfalls—or at least minimizing their impact on model performance. We wrapped up the chapter by applying all of the theory learned thus far in the book to construct our first bonafide deep learning network, which provided us with our best­yet accuracy on MNIST handwritten­digit classification. While such deep, dense neural nets are applicable to generally approximating any given output y when provided some input x, they may not be the most efficient option for specialized modeling. Coming up next in Part III, we’ll introduce neural network layers and deep learning approaches that excel at particular tasks, including machine vision, natural language processing, the generation of art, and playing games. KEY CONCEPTS Here are the essential foundational concepts thus far. New terms from the current chapter are highlighted in purple: œ parameters: œ weight w œ bias b œ activation a œ artificial neurons: œ sigmoid œ tanh
œ ReLU œ input layer œ hidden layer œ output layer œ layer types: œ dense (fully­connected) œ softmax œ cost (loss) functions: œ quadratic (mean squared error) œ cross­entropy œ forward propagation œ backpropagation œ unstable (especially vanishing) gradients œ Glorot weight initialization œ batch normalization œ dropout œ optimizers: œ stochastic gradient descent œ Adam œ optimizer hyperparameters: œ learning rate η œ batch size
1 . Recall from Chapter 4 that a neural network earns the deep moniker if it consists of at least three hidden layers. 2 . Also known as a Gaussian distribution. 3 . This is part of the magic of a library like TensorFlow. While there are other operations in Python already that perform matrix multiplication, such a numpy.matmul(), TensorFlow has implemented highly optimized versions of these operations. Deep learning performs such a staggering number of calculations like this one, that without some heavy optimization of the underlying code, the calculations might take a long time. Additionally, TensorFlow also implements methods to perform these calculations on a GPU rather than a CPU if needed. Wherever possible, it’s best to make use of TensorFlow’s operations over their NumPy counterparts. 4 . In an apparent affront to the previous footnote, we’re not using tf.random_uniform(), but in this instance it doesn’t matter—this method is only called once at the start of training to initialize out trivial (and random!) example, and so it won’t slow things down by a measurable amount. 5 . In case you’re wondering, the leading underscore ( _ = ) keeps the Jupyter notebook tidier by outputting the plot only, instead of the plot as well as an object that stores the plot. 6 . Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of Machine Learning Research, 9, 249­56. 7 . Select Kernel from the Jupyter notebook menu bar and choose Restart & Run All. This ensures we start completely fresh and don’t re­use old parameters from the previous run. 8 . It can be helpful to remember that some neurons should be saturated, i.e. their values should be very large or very small. We just endeavor to create a situation where the network learns where this is appropriate and does so intentionally, as opposed to starting off that way. 9 . The change is directly proportional to the negative magnitude of the gradient,
scaled by the learning rate η. 10. Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv: 1502.03167. 11. This is essentially a linear relationship, the simplest form of regression. 12. Recall the quadratic function from high school algebra. 13. Indeed, as early as Chapter 10, we’ll be encountering models with tens of millions of parameters. 14. Which can be annotated as n ≫ p, indicating the number of samples is much greater than the parameter count. 15. Least Absolutely Shrinkage and Selection Operator 16. Hinton, G., et al. (2012). Improving neural networks by preventing co­adaptation of feature detectors. arXiv:1207.0580. 17. If the phrase round of training is not immediately familiar, refer back to Figure 8.5 for a refresher. 18. yann.lecun.com/exdb/mnist 19. Duchi, J., et al. (2011) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121­59. 20. AdaGrad, AdaDelta, RMSProp and Adam all use ε for the same purpose and it can be left at its default across all of these methods. 21. Zeiler, M.D. (2012). ADADELTA: An Adaptive Learning Rate Method. arXiv:1212.5701 22. This is achieved through a crafty mathematical trick that we don’t think is worth expounding on here. You may notice, however, that Keras and Tensorflow still have a learning rate parameter in their implementations of AdaDelta. It is recommended to leave them at η = 1, i.e., no scaling and therefore no functional learning rate as we have come to know it in this book. 23. This optimizer remains unpublished. It was first proposed in Lecture 6e of Hinton’s Coursera Course “Neural Networks for Machine Learning”. The slides can be found at:
www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf. 24. Kingma, D.P, & Ba, J. (2014). Adam: A Method for Stochastic Optimization. arXiv:1412.6980 25. The other moving average is of the squares of the gradient, which is the second moment of the gradient or the variance. 26. This model.fit() step is exactly the same as for our Intermediate Net in Keras notebook, i.e., Example 8.3. 27. tensorflow.org/guide/summaries_and_tensorboard 28. This is also laid out in our Deep Net in Keras with TensorBoard notebook. 29. Note: we specified the same logging directory location that the TensorBoard object was set to use above. Since we specified a relative path not an absolute path for our logging directory, we need to run the tensorboard command from the same directory as our Deep Net in Keras with TensorBoard notebook.
--- End of content from file: (Addison-Wesley Data & Analytics Series) Krohn, J._Beyleveld, G._Bassens, A. - Deep Learning Illustrated_ A Visual, Interactive Guide to Artificial Intelligence-Pearson Education (2019).pdf ---


--- Start of content from file: (AK Peters Visualization) Christian Tominski_ Heidrun Schumann - Interactive Visual Data Analysis-CRC Press (2020).pdf ---
Interactive Visual Data Analysis
AK Peters Visualization Series This series aims to capture new developments and summarize what is known over the whole spectrum of visualization by publishing a broad range of textbooks, reference works, and handbooks. It will contain books from all sub!ields of visualization, including visual analytics, information visualization, and scienti!ic visualization. The scope will largely follow the calls of the major conferences such as VIS: techniques, algorithms, theoretical foundations and models, quantitative and qualitative evaluation, design studies, and applications. Series Editor: Tamara Munzner University of British Columbia, Vancouver, Canada Visualization Analysis and Design Tamara Munzner Information Theory Tools for Visualization Min Chen, Miquel Feixas, Ivan Viola, Anton Bardera, Han-Wei Shen, Mateu Sbert Data-Driven Storytelling Nathalie Henry Riche, Christophe Hurter, Nicholas Diakopoulos, Sheelagh Carpendale Interactive Visual Data Analysis Christian Tominski, HeidrunSchumann For more informa!on about this series please visit: h"ps://www.crcpress.com/AK-Peters-Visualiza!on-Series/book-series/CRCVIS
Interactive Visual Data Analysis Christian Tominski Heidrun Schumann
CRCPress Taylor&FrancisGroup 6000BrokenSoundParkwayNW,Suite300 BocaRaton,FL33487-2742 ©2020byTaylor&FrancisGroup,LLC CRCPressisanimprintofTaylor&FrancisGroup,anInformabusiness NoclaimtooriginalU.S.Governmentworks Printedonacid-freepaper InternationalStandardBookNumber-13:978-1-4987-5398-2(Hardback) 978-0-3678-9875-5(Paperback) Thisbookcontainsinformationobtainedfromauthenticandhighlyregardedsources.Rea- sonable efforts have been made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the conse- quences of their use. The authors and publishers have attempted to trace the copyright holdersofallmaterialreproducedinthispublicationandapologizetocopyrightholdersif permissiontopublishinthisformhasnotbeenobtained.Ifanycopyrightmaterialhasnot beenacknowledgedpleasewriteandletusknowsowemayrectifyinanyfuturereprint. Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means,nowknownorhereafterinvented,includingphotocopying,microfilming,andrecord- ing,orinanyinformationstorageorretrievalsystem,withoutwrittenpermissionfromthe publishers. For permission to photocopy or use material electronically from this work, please access www.copyright.com(http://www.copyright.com/)orcontacttheCopyrightClearanceCen- ter, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not- for-profit organization that provides licenses and registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, a separate system ofpaymenthasbeenarranged. Trademark Notice:Productorcorporatenamesmaybetrademarksorregisteredtrade- marks,andareusedonlyforidentificationandexplanationwithoutintenttoinfringe. Visit the Taylor & Francis Web site at http://www.taylorandfrancis.com and the CRC Press Web site at http://www.crcpress.com
To the Rostock visualization group.
Contents Foreword xiii Preface xv Authors xvii Chapter 1(cid:4) Introduction 1 1.1 BASICCONSIDERATIONS 2 1.1.1 Visualization, Interaction, and Computation 2 1.1.2 Five Ws of Interactive Visual Data Analysis 4 1.2 INTRODUCTORYEXAMPLES 5 1.2.1 Starting Simple 5 1.2.2 Enhancing the Data Analysis 8 1.2.3 Considering Advanced Techniques 10 1.3 BOOKOUTLINE 13 Chapter 2(cid:4) Criteria, Factors, and Models 15 2.1 CRITERIA 16 2.2 INFLUENCINGFACTORS 19 2.2.1 The Subject: Data 19 2.2.2 The Objective: Analysis Tasks 28 2.2.3 The Context: Users and Technologies 35 2.2.4 Demonstrating Example 38 vii
viii (cid:4) Contents 2.3 PROCESSMODELS 41 2.3.1 Design 41 2.3.2 Data Transformation 44 2.3.3 Knowledge Generation 47 2.4 SUMMARY 48 Chapter 3(cid:4) Visualization Methods and Techniques 51 3.1 VISUALENCODINGANDPRESENTATION 54 3.1.1 Encoding Data Values 54 3.1.2 Presentation 62 3.2 MULTIVARIATEDATAVISUALIZATION 67 3.2.1 Table-based Visualization 67 3.2.2 Combined Bivariate Visualization 69 3.2.3 Polyline-based Visualization 71 3.2.4 Glyph-based Visualization 73 3.2.5 Pixel-based Visualization 75 3.2.6 Nested Visualization 77 3.3 VISUALIZATIONOFTEMPORALDATA 82 3.3.1 Time and Temporal Data 82 3.3.2 Visualization Techniques 86 3.4 VISUALIZATIONOFGEO-SPATIALDATA 95 3.4.1 Geographic Space and Geo-spatial Data 96 3.4.2 General Visualization Strategies 99 3.4.3 Visualizing Spatio-temporal Data 106 3.5 GRAPHVISUALIZATION 111 3.5.1 Graph Data 111 3.5.2 Basic Visual Representations 113 3.5.3 Visualizing Multi-faceted Graphs 118 3.6 SUMMARY 124 Chapter 4(cid:4) Interacting with Visualizations 129 4.1 HUMANINTHELOOP 131 4.1.1 Interaction Intents and Action Patterns 132 4.1.2 The Action Cycle 135
Contents (cid:4) ix 4.2 REQUIREMENTSFOREFFICIENTINTERACTION 136 4.2.1 Interaction Costs 136 4.2.2 Directness of Interaction 138 4.2.3 Design Guidelines 143 4.3 BASICOPERATIONSFORINTERACTION 144 4.3.1 Taking Action 144 4.3.2 Generating Feedback 146 4.4 INTERACTIVESELECTIONANDACCENTUATION 148 4.4.1 Specifying Selections 149 4.4.2 Visual Emphasis and Attenuation 153 4.4.3 Enhanced Selection Support 156 4.5 NAVIGATINGZOOMABLEVISUALIZATIONS 159 4.5.1 Basics and Conceptual Considerations 160 4.5.2 Visual Interface and Interaction 162 4.5.3 Interaction Aids and Visual Cues 164 4.5.4 Beyond Zooming in Two Dimensions 168 4.6 INTERACTIVELENSES 173 4.6.1 Conceptual Model 173 4.6.2 Adjustable Properties 176 4.6.3 Lenses in Action 178 4.7 INTERACTIVEVISUALCOMPARISON 184 4.7.1 Basics and Requirements 184 4.7.2 Naturally Inspired Comparison 186 4.7.3 Reducing Comparison Costs 190 4.8 INTERACTIONBEYONDMOUSEANDKEYBOARD 194 4.8.1 Touching Visualizations 194 4.8.2 Interacting with Tangibles 197 4.8.3 Moving the Body to Explore Visualizations 202 4.9 SUMMARY 204 Chapter 5(cid:4) Automatic Analysis Support 207 5.1 DECLUTTERINGVISUALREPRESENTATIONS 209 5.1.1 Computing and Visualizing Density 209 5.1.2 Bundling Geometrical Primitives 212 5.2 FOCUSINGONRELEVANTDATA 214
x (cid:4) Contents 5.2.1 Degree of Interest 214 5.2.2 Feature-based Visual Analysis 220 5.2.3 Analyzing Features of Chaotic Movement 224 5.3 ABSTRACTINGDATA 231 5.3.1 Sampling and Aggregation 231 5.3.2 Exploring Multi-scale Data Abstractions 233 5.4 GROUPINGSIMILARDATAELEMENTS 238 5.4.1 Classification 239 5.4.2 Data Clustering 243 5.4.3 Clustering Multivariate Dynamic Graphs 250 5.5 REDUCINGDIMENSIONALITY 257 5.5.1 Principal Component Analysis 258 5.5.2 Visual Data Analysis with Principal Components 260 5.6 SUMMARY 263 Chapter 6(cid:4) Advanced Concepts 267 6.1 VISUALIZATIONINMULTI-DISPLAYENVIRONMENTS 268 6.1.1 Environment and Requirements 269 6.1.2 Supporting Collaborative Visual Data Analysis 270 6.1.3 Multi-display Analysis of Climate Change Impact 276 6.2 GUIDINGTHEUSER 277 6.2.1 Characterization of Guidance 278 6.2.2 Guiding the Navigation in Hierarchical Graphs 283 6.2.3 GuidingtheVisualAnalysisofHeterogeneousData286 6.3 PROGRESSIVEVISUALDATAANALYSIS 288 6.3.1 Conceptual Considerations 290 6.3.2 Multi-threading Architecture 294 6.3.3 Scenarios 297 6.4 SUMMARY 303 Chapter 7(cid:4) Summary 305 7.1 WHAT’SBEENDISCUSSED 305 7.2 HOWTOCONTINUE 307
Contents (cid:4) xi Bibliography 311 Index 339 Figure Credits 343
Foreword Big Data has attracted much attention. Many examples are given in the news how new data-driven applications and algorithms provide new insights, enable more effective and efficient operation, and support experts in a wide variety of application domains. This suggests that getting value out of data is easy, but practitioners in the field know better. Obtaining novel, actionable insights from large and complex data is hard work. In many cases, the human in the loop is indispensable to find unexpected relations, to judge if there is true value in the data and to check if results are valid, as a human is needed to bring in both deep domain expertise and common sense. How to bring the human in the loop? One route is to use visualization: translate data into images and take advantage of the incredible strength of the human visual system, enabling the viewer to detect patterns quickly and leading to deep insights. However, visualization has its limits and for instance larger datasets quickly lead to cluttered images, like the hairballs that result when networks with more than a hundred nodes are visualized. An important next level is to bring in interaction: enable people to select the data they are interested in. When carefully designed, such interactive visualization systems enable exploration of much larger datasets. But, how to find relevant relations in collections of millionsofitems,withhundredsofattributes?Statistics,machinelearning,and data mining provide many analysis methods geared towards finding patterns and correlations automatically, but such analysis methods inevitably also lead toreductionandpossiblythelossofvitalinformation.ThecredoofInteractive Visual Data Analysis, also known as visual analytics, is therefore to use a combination of visualization, interaction, and automated analysis methods to explore huge amounts of data and to obtain solid insights. Problem solved? Unfortunately not. The design of effective systems for visualdataanalysisisfarfromtrivial,andrequiresawidevarietyofknow-how, skills, and experience. Besides pure technical skills, many other aspects are rel- evant.Foreachelementofsuchasystemawidevarietyofalternativesolutions are available, selecting the most suitable requires a thorough understanding of their strengths and limitations. When standard solutions do not suffice, creativity is needed to find new methods or ingenious combinations of existing ones. And, above all, development of such systems is a design process, having a deep understanding of this process and dedication to the needs and wants of the prospective users is essential. xiii
xiv (cid:4) Foreword I teach students to develop interactive systems for visual data analysis. And, I struggle with this. Should I guide them through the complete zoo of all different visualization, interaction, and analysis techniques, for all kinds of different data? That would lead to a very lengthy and not really exciting course. What I really missed is a compact textbook that provides an overview of interactive visual data analysis: a book that guides through the design process, discusses the supporting disciplines and how they fit together, and gives examples of more advanced topics. When Christian Tominski and Heidrun Schumann wrote me that they had prepared such a book, I was very excited, and even more when I read it. This book really fills a gap. There are of course many texts on visualization, inter- action, and data analysis, all with their own angle, but an integrated overview is rare, whereas I think that the real magic comes from the combination. The authors have an extensive experience in doing research in visualization and developing real-world solutions for real-world problems, and that shows. Many examplescomefromtheirownworkandgiveusanopportunitytolookoverthe shoulders of experts. They have a broad experience with many different topics andareas,andhaveattackedmanycomplexcases,suchasdynamicgeo-spatial data. I therefore expect that many readers can take direct advantage of the ideas, solutions, and examples given. Besidestheirexperienceinresearchanddevelopment,theauthorsalsohave a solid track record in communicating their insight in the field: their book “Visualization of Time-Oriented Data”, together with Wolfgang Aigner and Silvia Miksch, is a classic. In the current book, they again show their mastery in handling a complex topic, and I am very impressed by their thoughtful treatment and analysis, their clear and compact writing style, and all clear and useful illustrations. In short, I highly recommend this book. It provides clear guidance, a thorough overview, and many ideas to students, teachers and everybody who aims to develop effective systems for interactive visual data analysis. I hope this book will inspire many to develop effective solutions for interactive visual data analysis that enable experts to extract great value and deep insights from huge, complex datasets. Jarke J. van Wijk Department of Mathematics and Computer Science Eindhoven University of Technology, The Netherlands October 2019
Preface In the year 2000, Heidrun Schumann co-authored the first German textbook on visualization. Christian Tominski was among the first students to use that textbook during their studies. Now, about twenty years later, we, Heidrun and Christian, are happy to present this new book on interactive visual data analysis. AboutthisBook TheideaforthisbookwasbornatIEEEVIS2014inParis during a discussion of Heidrun and Tamara Munzner, who had just published her own book Visualization Analysis and Design. The goal was to write a book that provides a comprehensive overview of the principles of interactive visual data analysis. It soon became clear to us that this was not an easy endeavor. Before us were five years of systematization, categorization, harmonization, and over and over again in-depth discussions on how the content could be best prepared, organized, and presented to our readers. The result is a book with five core chapters. What differentiates our book most from others is its visual analytics focus, that is, the synthesis of visuals, interactivity, and analytics. The book introduces criteria for designing interactive visual data analysis solutions, discusses factors influencing the design, and examines the involved processes. The reader is made familiar with the basics of visual encoding and gets to know numerous visualization techniques for multivariate data, temporal data, geo-spatial data, and graph data. A dedicated chapter introduces general concepts for interacting with visualizations and illustrates how modern interaction technology can facilitate the visual data analysis in many ways. Addressing today’s large and complex data, the book covers relevant automatic analytical computations to support the visual data analysis. The book also sheds light on advanced concepts for visualization in multi-display environments, user guidance during the data analysis, and progressive visual data analysis. In her review of this book, Tamara Munzner commented: “I was consistently impressed by how broad the set of things you’ve done is.” The entire book is richly illustrated with many examples and use cases. Here, we have chosen to rely primarily on our own visual analytics research. This allowed us to create many new schematic depictions and expressive visualizations to illustrate the discussed topics. A great advantage of creating new illustrations is that we could make them available free of charge under Creative Commons Attribution 4.0 International License (CC BY 4.0). xv
xvi (cid:4) Preface The multitude of topics described in the book can be of interest to a wide range of readers. Students can use the book to learn about interactive visual dataanalysis.Theywillbenefitmostfromthestructuredtop-downviewoffered by the book. Visualization experts can use the book as a reference and for teaching. The advanced concepts discussed at the end of the book may serve as inspiration for new research. Last but not least, domain experts will find the many examples and use cases interesting. Given the broad scope of the book, practitioners from many domains may gain from it. Acknowledgments This book might have been written by two people, but it representstheworkofmanypeople.Becausethisbookwouldnotexistwithout our own previous research, special thanks go to all of our former and current colleagues from the Rostock visualization group. Moreover, we would like to say many thanks to our friends and partners with whom we collaborated and co-authored papers that had a substantial impact on this book. We are also very grateful for support in creating images and transferring rightstouseimages.Manythanksgoto(inalphabeticalorder)MarcoAngelini, NicolasBelmonte,ThomasButkiewicz,SteveDübel,ChristianEichner,Steffen Hadlak, Helwig Hauser, Alexander Lex, Martin Luboschik, Thomas Nocke, Axel Radloff-Delosea, Martin Röhlig, and Sylvia Saalfeld. Feedback from reviewers greatly helped to improve the book. Many thanks go to Wolfgang Aigner for his review of the chapter on interaction. Martin Luboschikprovidedvaluablecommentsonthevisualizationchapter.Hans-Jörg Schulznotonlygaveususefulfeedbackonthechaptersonfundamentalaspects and automatic analysis support as well as on the summary and the preface, he also took part in many discussions and contributed in the early phase of the book project. Tamara Munzner deserves special thanks for her in-depth review of the entire almost finished book. Her comments helped us a lot in finalizing and polishing the book. Sunil Nair, the editorial director at Taylor & Francis, and his editorial assistants Kirsten Barr and Shikha Garg supported us on the publisher’s side. Finally, we would like to thank our families for providing the warm and heartfelt atmosphere that is necessary to successfully complete a book project. Thank you for your love and support! Christian Tominski Heidrun Schumann Institute for Visual & Analytic Computing University of Rostock, Germany October 2019
Authors Christian Tominski is a researcher and lecturer at the Institute for Visual & Analytic Computing at the University of Rostock, Germany. He received doctoral (Dr.-Ing.) and post-doctoral (Dr.-Ing. habil.) degrees in 2006 and 2015, respectively. His main research interests are in visualization of and interaction with data. He is particularly interested in effective and efficient techniques for interactively exploring and editing complex data. Christian has published numerous papers on new visualization and interaction techniques for multivariatedata,temporaldata,geo-spatialdata,andgraphs.Heco-authored twobooksonthevisualizationoftime-orienteddatain2011andoninteraction for visualization in 2015. Christian has developed several visualization systems and tools, including the LandVis system for spatio-temporal health data, the VisAxes tool for time-oriented data, and the CGV system for coordinated graph visualization. Heidrun Schumann is a professor at the University of Rostock, Germany, where she is heading the Chair of Computer Graphics at the Institute for Visual & Analytic Computing. She received doctoral degree (Dr.-Ing.) in 1981 and post-doctoral degree (Dr.-Ing. habil.) in 1989. Her research and teaching activities cover a variety of topics related to computer graphics, including information visualization, visual analytics, and rendering. She is interested in visualizingcomplexdatainspaceandtime,combiningvisualizationandterrain rendering, and facilitating visual data analysis with progressive methods. A key focus of Heidrun’s work is to intertwine visual, analytic, and interactive methods for making sense of data. Heidrun published more than 200 articles in top venues and journals. She co-authored the first German textbook on data visualization in 2000 and a textbook specifically on the visualization of time-oriented data in 2011. In 2014, Heidrun was elected as a Fellow of the Eurographics Association. xvii
1 CHAPTER Introduction CONTENTS 1.1 Basic Considerations ........................................ 2 1.1.1 Visualization, Interaction, and Computation ....... 2 1.1.2 Five Ws of Interactive Visual Data Analysis ....... 4 1.2 Introductory Examples ..................................... 5 1.2.1 Starting Simple ..................................... 5 1.2.2 Enhancing the Data Analysis ....................... 8 1.2.3 Considering Advanced Techniques .................. 10 1.3 Book Outline ................................................ 13 DATA have become a most valuable good. Doctors rely on rich databases about diagnoses and medications to give patients the best possible treat- ments. Enterprises generate profits based on data about needs and preferences of potential customers. Scientists make new discoveries and contribute to a vast body of scholarly data. Data are everywhere in the information age. Data are collected by huge numbers of devices equipped with various sensors. A smartphone with a dozen and more different sensors is not uncommon. Data are also generated computationally. Sophisticated models are constructed and simulated in an attempt to estimate how our climate may look like in a few centuries. And we as humans are sources of data as well. Social networks and messaging services record our interests and daily activities. Now with so much data available, the question is how can we make sense of them? Well, the data have to be explored and analyzed in order to derive valuable information. To this end, a channel has to be established for the data to enter into the human mind where insight can be generated. The classic way of ingesting data is to decipher alphanumerically encoded transcripts, or simplytexts.Yet,readingpilesofdocumentsistootimeconsuming.Therefore, data are often aggregated in reports, which may contain structured tabular information. This already helps in extracting the key messages. But complex relationships within the data may still be too difficult to identify. This is where interactive visual data analysis enters the stage. While text and reports are serial media, where one piece of data has to be processed 1
2 (cid:4) Interactive Visual Data Analysis after the other, visual methods aim for the human visual system at its full bandwidth. Humans are amazingly fast in extracting information from graph- ical depictions. Graphical means are not only beneficial for communicating information,theyalsoserveasscaffoldsforhumansensemaking.Mentalmodels can be established more easily with the help of visual abstractions and visually acquired information can be remembered better than textual descriptions. We all know the idiom: “A picture is worth a thousand words.” But this is not quite right. More correct would be “A picture can be worth hundreds of thousands of words.” This slightly provocative statement hints at two important aspects. First, can be suggests that there are not only good visual representations of data but also exemplars that are not so helpful. Second, the increase in the number of words is to indicate that, in the information age, we are facing big data. Thisbookisaboutconceptsandmethodsfortheinteractivevisualanalysis of large and complex data by jointly exploiting the power of humans and computers. In this book, you will learn that solutions for interactive visual data analysis are not created in passing. Careful design is necessary before expressive visual representation can be shown on a computer display. Useful interaction is essential to enable users to engage in a dialog with the data and the information contained therein. Especially in the light of big data, we need support from analytic computations to help us extract interesting features from the data. 1.1 BASIC CONSIDERATIONS Before we go into any details about interactive visual data analysis, let us briefly look at some fundamental terms, ideas, and concepts. 1.1.1 Visualization,Interaction,andComputation Visualization is a computational process that generates visual representations of data. A first definition has been established by visualization pioneers in 1987. Their definition reads as follows [MDB87]: “Visualizationisamethodofcomputing.Ittransformsthesymbolicintothe geometric,enablingresearcherstoobservetheirsimulationsandcomputa- tions.Visualizationoffersamethodtoseetheunseen.Itenrichestheprocess ofscientificdiscoveryandfostersprofoundandunexpectedinsights.” McCormicketal.,1987 This definition describes a transformation that involves several entities and steps. First, there are the data into which we seek insight. Second, there is the
Introduction (cid:4) 3 computer that transforms the data into visual representations. Finally, there is the human who is making sense of the visual representation. Visual representations are fundamental for visually driven data-intensive work, but they alone can hardly satisfy all the analytic needs we are facing in the information age. We need support from interaction mechanisms and computational analysis methods. Already in 1981, Bertin recognized the need for interactively adjustable visual representations [Ber81]: “Agraphicisnot‘drawn’onceandforall;itis‘constructed’andreconstructed untilitrevealsalltherelationshipsconstitutedbytheinterplayofthedata.The bestgraphicoperationsarethosecarriedoutbythedecision-makerhimself.” Bertin,1981 Interaction adds the necessary flexibility to visualization. It allows us to activelytakepartinthevisualdataanalysis.Wemaywanttofocusondifferent features of the data, look at the data from different perspectives, or adjust visual representations so as to crystallize the desired insights. While interaction incorporates human competences into the sense-making process, automatic computational methods utilize the power of the machine. Large and complex data usually cannot be visualized in their entirety. Auto- matic computational analyses crunch the data in search for characteristic features or meaningful abstractions that are easier to digest than the raw data. The important interplay of visualization, interaction and computational analysis is summarized in the Visual Analytics Mantra by Keim and col- leagues [Kei+06]: “AnalyseFirst– ShowtheImportant– Zoom,FilterandAnalyseFurther– DetailsonDemand” Keimetal.,2006 According to this mantra, visual analysis starts with an automatic analytic phase. The important features extracted in this phase are then visualized. Via interaction, the visual representation is adjusted, the data are filtered, and further analytic computations are triggered. Details are readily available upon request.
4 (cid:4) Interactive Visual Data Analysis This tight interleaving of computational and human efforts is the key benefit of interactive visual data analysis as a knowledge-generation approach. The computer can process large amounts of data quickly and accurately. The human has enormous pattern-detection abilities and is proficient in creative thinking and flexible decision-making. A direct consequence of the interplay of data, humans, and computers is that knowledge from different fields has to be brought together for a successful dataanalysis.Relevanttopicsincludevisualdesign,computergraphics,human- computer interaction, user interfaces, psychology, data science, and algorithms, to name only a few. The need to get diverse methods work in concert makes the development of practical solutions a non-trivial endeavor. 1.1.2 FiveWsofInteractiveVisualDataAnalysis In order to come up with helpful data analysis tools, their context of use needs to be taken into account in the first place. One way to describe the context is to follow a variation of the Five Ws: What, why, who, where, and when. What data are to be analyzed? There are many different types of data, such as player statistics, census data, movement trajectories, and biological networks. Each type of data comes with its own individual characteristics, including data scale, dimensionality, and heterogeneity. Why are the data analyzed? The objective is to help people accomplish theirgoals,forexample,findinggoverningfactorsinageneregulatorynetwork. Goals typically involve a number of analytic tasks, such as identifying data values or setting patterns in relation. Whowillanalyzethedata? Adoctorwhostudiesdatainday-to-dayclinical routine needs different analysis tools than a strategic investor who is exploring streamsofnewsdatainsearchfornewmarketopportunities.Individualabilities and preferences play a role as well. Where will the data be analyzed? The regular workplace is certainly the classic desktop setup with a display, mouse, and keyboard. Yet, there are also large display walls and interactive surfaces that offer new opportunities for interactive visual data analysis. Whenwillthedatabeanalyzed? Aswithanytool,visualization,interaction, and computation are means that must be at hand at the right time. A data analysis may follow domain-specific workflows where each step is associated with its own individual requirements.
Introduction (cid:4) 5 These Five Ws suggest that there are many factors influencing the develop- ment of data analysis tools, including data types, analytic tasks, user groups, display environments, domain conventions, and so forth. Factors related to the What and the Why are crucial for the practical applicability of data analysis tools. Certainly, any visually driven and interactively controlled tool has to consider human factors, the Who, with regard to perceptual, cognitive, and physical abilities, expertise, background, and preferences. The Where and When aspects become increasingly relevant when the data analysis runs on multiple heterogeneous displays, supports collaborative sessions, or follows domain-specific workflows. InthelightoftheFiveWsitisclearthataninteractivevisualdataanalysis solution, in order to be successful, has to be tailored for a specific purpose and setting.Giventhewealthofanalyticquestionswearefacingintheinformation age, a large variety of concepts and techniques is needed. Next, we look at a few introductory examples. 1.2 INTRODUCTORY EXAMPLES So far, we have sketched the basic idea of interactive visual data analysis on a rather abstract level. In the following, a series of examples will demonstrate the communicative power of visual analysis approaches, on the one hand, and the involved design decisions and challenges, on the other hand. The examples will take us from basic visual representations to advanced analysis scenarios. On the way, we will increase the degree of sophistication of the examples by enhancing the visual mapping, integrating interaction mecha- nisms and automatic computations, combining multiple views, incorporating user guidance, and considering multi-display environments. 1.2.1 StartingSimple The data we will analyze are graphs (the What). Graphs are a general model for describing entities and relations among them. They are universally useful in many different domains. Biologists model natural phenomena via gene regulation networks, climate researchers make use of climate networks to simulate weather on earth, and crime investigators sketch connections between suspects to solve complicated cases. Typical examples from our daily lives are computer networks and social networks. A graph generally consists of nodes, edges, and attributes. Nodes represent entities, whereas edges represent relationships between the entities. Nodes as well as edges can have attributes that store additional information. Before we start with visual examples, let us first take a look at the raw data to be visualized. Listing 1.1 shows our graph stored in the JSON format. Lines 2–11 contain three nodes, each associated with an id and a label. Lines 14–22 define two edges. Edges are specified between a source node (src) and a destination node (dst), both referenced by their id. An additional attribute stores the weight (or strength) of the connection between the two nodes.
6 (cid:4) Interactive Visual Data Analysis Listing1.1 A graph with nodes, edges, and attributes 1 { 2 "nodes": [ 3 { "id": 0, 4 "label": "Myriel" 5 }, 6 { "id": 1, 7 "label": "Napoleon" 8 }, 9 { "id": 2, 10 "label": "Mlle Baptistine" 11 }, 12 // More nodes here ... 13 ], 14 "edges": [ 15 { "src": 1, 16 "dst": 0, 17 "weight": 1 18 }, 19 { "src": 2, 20 "dst": 0, 21 "weight": 8 22 }, 23 // More edges here ... 24 ] 25 } Our listing with three nodes and two edges is only an abbreviated view as indicated by the comments in lines 12 and 23. The graph that we are about to visualize actually contains 77 nodes and 254 edges. It captures the co-occurrence of characters in the chapters of Victor Hugo’s Les Misérables. With only the listing of the graph, it is extremely difficult to make sense of the information hidden in the data. Therefore, we (the Who) will now perform a visual analysis to gain insight into the graph. In the first place, we are interested in the structure of the graph (the Why). A basic method for visualizing graphs is a node-link diagram. Figure 1.1a shows such a simple visual representation of the graph. Nodes are visualized as dots, and edges are represented as links between the dots. There are many different options for laying out the dots on the display. In our case, a force- directed layout algorithm has been applied. As we can see, the structure of the graph, that is, who is connected to whom, becomes quite clear, merely by drawing dots and links. Let us refine our interest in the data. We are now interested in who are the major characters with the most connections to other characters. We can already extract this information by looking at the number of edges that are connectedtoanode.Yet,itisabitcumbersometocountedges,andtheclutter of edges makes counting difficult anyway. So how can we make the desired information more readily visible?
Introduction (cid:4) 7 (a)Plain structure. (b)Encoding degree via color. (c)Encoding degree via color and size. (d)Encoding weight via line width. Figure1.1 Node-link diagram visualizing a graph’s structure and attributes. The number of edges per node is also called the node degree. This derived numeric attribute can be visualized alongside the graph structure. To this end, we assign to each dot a color that represents the node degree. Dark green nodes exhibit a large node degree, while light green nodes have a low degree. Figure1.1billustrates thecolor coding.With thisenhancednode-link diagram, we can immediately identify the dark green dot in the center of the figure as the major character in the network. But we are still not fully satisfied. Low-degree nodes are of the same size as theimportanthigh-degreenodes.Wewanttofurtheraccentuatetheimportant charactersinthenetworkandattenuatethelessrelevantsupportingcharacters. Complementing the color coding, we vary the size of the dots depending on the node degree. As can be seen in Figure 1.1c, it is now much easier to assess the importance of characters. Our visual representation is now quite expressive in terms of information about the nodes of the graph. However, the edges have received only little
8 (cid:4) Interactive Visual Data Analysis attention. To render a more complete picture of the data, it would be nice to visualize the edge weight as well. This can be achieved by varying the width of the lines connecting the colored dots. In Figure 1.1d, bold lines indicate strong edges, whereas thin lines stand for weak edges. With this additional visual encoding, we can easily see to whom a character is most prominently connected. Insummary,wehavenowvisualizedthegraphstructureandtwoassociated graph attributes. The structure is nicely visible as dots and links. The two attributes node degree and edge weight are encoded visually via color plus size and line width, respectively. The resulting visual representation enables us to see the key characteristics of the data. By reading the data’s textual representation we could have obtained the same characteristics, but it would have cost us much more time and painstaking brainwork. 1.2.2 EnhancingtheDataAnalysis The previous simple examples illustrated the potential of visualization. Yet, simplevisualrepresentationsaloneareoftennotenoughtosolvemorecomplex problems. The graph that we visualized consisted of 77 nodes and 254 edges only. However, it is not uncommon to work with graphs with thousands of nodesandedges,anddozensofattributes.Climatenetworksareanexampleof suchlargeandcomplexgraphs.Theyaregeneratedfromlarge-scalesimulations of meteorological phenomena with the goal to better understand and predict the development of climatic conditions on earth. With increasing size and complexity of the data, we have to go beyond the simple visual representations introduced earlier. When visualizing large data, we risk ending up with visual representations that are cluttered. Adequate countermeasures have to be taken. Moreover, it is hardly possible to encode all aspects of the data into a single image. It is rather necessary to provide multipleviewsonthedata,whereeachviewemphasizesaparticulardatafacet. The next two examples illustrate these lines of thought. Consider the climate network visualized in Figure 1.2a. It contains 6,816 nodes and 116,470 edges. Its visual representation is actually a mess; there are simply too many dots and links. What can we do about this? A standard approach in such situations is to focus on relevant subsets of the data. Subsets can be created dynamically using interactive filtering mechanisms that enable users to specify the parts of the data they are interested in. For the climate network we may be interested in those nodes that are crucial for the transfer or flow in the network. Such nodes are characterized by a high centrality, a graph-theoretic measure. An automatic algorithm can be used to calculate the centrality for each node of the network. Then it is up to the user to determine interactively a suitable threshold for filtering out low-centrality nodes and their incident edges. Figure 1.2b shows the climate network where nodes with a centrality below 65,000 have been filtered out. As a result the visual representation contains
Introduction (cid:4) 9 (a)Full graph with 6,816 nodes and 116,470 edges. (b)Filtered graph with 938 nodes and 5,324 edges. Figure1.2 Dynamic filtering to focus on relevant parts of a climate network. only those graph elements that are relevant with respect to the interest of the user. Now, there are no more than 938 nodes and 5,324 edges. This obviously reduces clutter and generates a better view on the structures hidden in the data. With dynamic filtering as described so far, it is possible to deal with problems caused by data size. Another challenge is data complexity. It relates to the many semantic aspects that may be linked to the data. We already mentioned the graph structure and the graph attributes as important aspects. Additionally, there can be spatial and temporal aspects. A climate network is usually given in a spatial frame of reference and it may also be subject to
10 (cid:4) Interactive Visual Data Analysis Figure1.3 Multiple-views visualization of a climate network. change over time. In order to understand data comprehensively it is necessary tounderstandtheindividualaspectsandtheirinterplay.Thisrequiresmultiple dedicatedvisualrepresentations,eachaddressingtheparticularitiesofaspecific aspect. Figure 1.3 depicts a system where multiple views work in concert to visualize our climate network. Without going into too much detail, there are a density plot (top left), a node-link view combined with a map (center), a globe view (right), a multivariate attribute plot (below node-link and globe), a filter slider (bottom), and a few auxiliary controls. All these views are linked. That is, picking a data item in one view will highlight that item in all other views. This linking among views is essential for integrating the different data facets and enabling the user to form a comprehensive understanding of the data. 1.2.3 ConsideringAdvancedTechniques In the previous paragraphs, we computed graph-theoretic measures, added interactive filtering, and combined multiple linked views to create a compre- hensive overview of the data. But how far can we get with interactive visual data analysis? Certainly, there are limits. The visualization has to fit into the available display space. Interaction should not overwhelm users with too many things to carry out manually. Analytic computations have to generate results in a timely fashion. As we try to push these limits, we have to consider advanced techniques. For the purpose of illustration, we briefly look at two examples. One aims to guide users during the data analysis and the other to expand the screen space for visualization. It has already been mentioned that interaction is crucial to creative sense- making. However, interaction can also be demanding. The user has to deal
Introduction (cid:4) 11 (a)Where should I go next? (b)Visual cues hint at candidates! Figure1.4 Guidance provides assistance during data navigation. with several questions: What can I do to get closer to my goal, which action sequence do I have to take, how are the individual interactions carried out? An advanced visual analysis system is capable of providing guidance to assist the user in answering such questions. We illustrate such guidance by the example of another common question: Which part of the data should be visited? A typical approach to study data more closely is to zoom in as shown in Figure 1.4a. Note that we visualize a different graph now, a graph about co-occurrence of search engine query keywords with 2,619 nodes and 29,517 edges. While zoomed in, it is possible to see details, but only for a fraction of the graph. So, the data analysis is an iterative process during which one part of the graph is visited after the other. This iterative process requires users to answer the question where to go next. Should the user be hesitant to continue the navigation of the data, this may indicate that guidance should be provided (the When). If this is the case, a computational method scans the immediate neighbor- hoodofthecurrentlyvisiblepartofthegraphinsearchfornodesoredgesthat are potentially interesting according to a user-specified degree-of-interest (DoI) function.Thevisualrepresentationisthenenhancedwithvisualcuesthatpoint at the most promising candidates. Figure 1.4b shows a few nodes emphasized withredcircles.Thesenodesareworthinvestigatingindetail.Moreover,arrows at the view border suggest directions in which further interesting nodes can be found. The user is free to follow the given recommendations or to continue the exploration otherwise. Of course, guidance is a delicate means of user support. Ifguidanceisobtrusive,usersmaynotacceptit.Ifitiswell-balanced,however, guidance can be a valuable tool. Foroursecondexampleofadvancedvisualdataanalysis,weareaddressing the screen space limit. When pushing this limit, a natural step forward is to
12 (cid:4) Interactive Visual Data Analysis Figure1.5 Visual data analysis in a multi-display environment. Reprinted from [RLS11]. go for bigger displays. Instead of working in the classic desktop environment, several displays are combined to form so-called multi-display environments (the Where). As shown in Figure 1.5, there are stationary public displays and dynamicprivatedisplays,whichcanenterandleavetheenvironmentasneeded. Each display may contain several views and the views can be focused and re-arranged to suit the analytic situation at hand. On the one hand, multi-display environments provide more space for dis- playing visual representations at high resolution. There can be even multiple users working collaboratively to analyze the data. On the other hand, new challenges need to be tackled. How to distribute views on the displays, how to deal with users occluding the displays, and how to interact with views at the greater scale? Finding answers to these questions and supporting the user in such potent workspaces is part of ongoing visualization research. In this section, we presented a series of visualization examples. We started with basic visual encodings, incrementally enhanced the visual representations, and finally looked at some advanced techniques. These examples are kind of a teaser of what to expect from this book. The next section will provide some more detailed information on the book structure.
Introduction (cid:4) 13 1.3 BOOK OUTLINE This book is structured into six chapters: the first introductory chapter you are currently reading plus five chapters on interactive visual data analysis to come. Figure 1.6 provides an overview of the chapter structure. Chapter 2 is concerned with fundamental aspects pertaining to interactive visual data analysis. We will look into design criteria, factors influencing the design, and models describing the involved processes. Chapter 3 is about visualization. You will learn about the basic methods of visual encoding and presentation, and about various visualization techniques for different types of data. Chapter 4 is dedicated to interaction. The chapter discusses general inter- action concepts and illustrates how interaction techniques can facilitate the visual data analysis in many ways. Chapter 5 deals with automatic computations to support the visual data analysis. The primary goal will be to reduce the complexity of the data and their visual representations. Chapter 6 sheds some light on advanced concepts for interactive visual data analysis, including multi-display visualization environments, user guidance, and progressive visual data analysis. Chapter 7 will briefly summarize the book and outline ideas for readers to continue with the topic of interactive visual data analysis. CH. 1 CH. 2 CH. 3 CH. 4 CH. 5 CH. 6 CH. 7 Introduction Criteria, Visualization Interacting with Automatic Advanced Summary Influencing Factors, Methods and Visualizations Analysis Support Concepts and Models Techniques Figure1.6 Chapter structure of this book. Icons by icons8.com. FURTHER READING General Literature: [SM00] • [Spe07] • [WGK15]
2 CHAPTER Criteria, Factors, and Models CONTENTS 2.1 Criteria ...................................................... 16 2.2 Influencing Factors .......................................... 19 2.2.1 The Subject: Data .................................. 19 2.2.2 The Objective: Analysis Tasks ...................... 28 2.2.3 The Context: Users and Technologies .............. 35 2.2.4 Demonstrating Example ............................ 38 2.3 Process Models .............................................. 41 2.3.1 Design ............................................... 41 2.3.2 Data Transformation ................................ 44 2.3.3 Knowledge Generation .............................. 47 2.4 Summary .................................................... 48 INTERACTIVE VISUAL data analysis is highly context-dependent. We will need different techniques for analyzing time-series data than for graph data.Wewillwanttousecompletelydifferentvisualrepresentationsforgetting an overview of the overall data distribution than for inspecting individual patterns and trends. And we will most likely interact differently when working with data on an interactive surface as compared to a standard desktop. There is no silver bullet solution that simply scales to all possible analysis scenarios. If there was such a universal approach, there would be no necessity for this book. The first step to designing context-dependent solutions is to know the fundamental requirements posed by interactive visual analysis scenarios. This aspect will be addressed by introducing corresponding criteria in Section 2.1. Second, we need to describe the influencing factors that characterize analysis scenarios. This primarily concerns the data to be analyzed and the tasks to be accomplished, but also the people who carry out the analysis and the environment in which it takes place. These influencing factors will be dealt with in detail in Section 2.2. Finally, we need to understand the fundamental 15
16 (cid:4) Interactive Visual Data Analysis processes behind interactive visual data analysis. These will be discussed in Section2.3,wherewecoverthedesignprocess,thedata-transformationprocess, and the knowledge-generation process. Throughout this chapter, we will use illustrating examples to convey the major points. 2.1 CRITERIA Ifyouwanttobesuccessfulinanalyzingdatawithinteractivevisualtools,you cannot just create an arbitrary visual representation, add some interactivity to it, and spice it with a little computational support. Much of the potential of interactive visual data analysis would be wasted. Even worse, you could end up with findings that are simply not true. Any follow-up decisions based on your analysis would be tainted. Consider for example the visual representation from Figure 2.1a. The data captures the Germans’ life satisfaction in 2017. Satisfaction is measured on a scale from 0 (very dissatisfied) to 10 (very satisfied). The map represents the average satisfaction per region with a red-yellow-green color scale. To the casual observer, Figure 2.1a suggests that people in the eastern parts of Germany are mostly dissatisfied. This is where the visual representation fails, because the visual impression is not backed by the data. A closer look at the legend tells us that the visualized satisfaction is between 6.83 and 7.43, which means people are closer to being satisfied than to being unhappy. 7.43 7.43 6.89 6.89 7.28 7.28 7.20 7.20 6.94 6.94 6.83 6.86 6.83 6.86 7.12 7.12 7.11 7.11 6.92 6.92 7.20 7.27 6.97 7.20 7.27 6.97 7.15 7.15 7.26 7.26 7.21 7.21 6.83 7.43 Very dissatisfied - 0 6.83 7.43 10 - Very satisfied (a)Failing visual representation. (b)Succeeding visual representation. Figure2.1 Visualization of life satisfaction in Germany.
Criteria, Factors, and Models (cid:4) 17 Figure 2.1b shows a successful visualization. The color scale is mapped appropriately to the possible range of values between 0 and 10. The visual representationnowsuggeststhatGermanpeoplearemostlysatisfiedwiththeir lives. There are only slight differences between the different parts of Germany. The take-home message of this example is that we always have to ask what makes a good visual representation that actually helps us analyze our data? In fact, we would like to be able to assess the overall quality of interactive visual analysis solutions. However, as indicated before, there are many influencing factors, including the properties of the data, the nature of the tasks, the characteristics of the human, the modalities of the output and input system, and the resources of the environment. These factors bear complex questions in themselves and are not easy to define formally. This makes it hard to come up with a coherent definition of quality. Instead, we establish three quality criteria that interactive visual approaches to data analysis should obey. Expressiveness In the first place, an interactive visual representation must be expressive. Expressiveness is a mandatory condition. The assessment of whether an interactive visual representation is expressive or not can only be made in relation to the data to be visualized and the task to be sup- ported. Apparently, we have to consider visual expressiveness and interactive expressiveness separately. Avisualrepresentationisexpressiveifitcommunicatesthedesiredinforma- tioncontainedinthedata,andonlythisinformation.Inotherwords,thevisual representation neither fabricates nor withholds information, but objectively reflects the information that we need to accomplish our task. Accordingly, an interactive representation is expressive if it allows us to carryouttheactionsneededtoacquirethedesiredinformation,andonlythese actions. Put differently, the user is enabled to do exactly what is necessary for the task at hand. Effectiveness Secondly, an interactive visual representation should be effec- tive. Effectiveness is a goal-oriented condition. It relates to the degree to which we as humans are able to achieve our analysis task. An interactive visual representation is effective if it is geared to the human sensory and motor systems, that is, our abilities to observe and interact with our environment. As we are dealing with visual representations, the properties ofthehumanvisualsystemareofprimaryrelevance.Inthissense,effectiveness captures how well we can extract the information needed for our task from a visual representation. In a similar vein, we can characterize effectiveness as a measure of how well we can convey an interaction intent to the computer. Most of the time, this concerns our physical ability to move our fingers, hands, or the whole body with different speed and accuracy, but also commands issued with our vocal tract.
18 (cid:4) Interactive Visual Data Analysis TABLE2.1 Quality criteria of interactive visual data analysis. Criterion Concern Expressiveness Faithfully map data and tasks Effectiveness Enable users to accomplish task Efficiency Balance benefits and costs Efficiency Finally, interactive visual analysis should be efficient. Efficiency is a desired property. It adds the economic aspect: The gains from using an interactive visual approach should outweigh the computational resources and human effort needed to carry out the analysis. Resource-wise,weareinterestedinhowmuchtimeandhowmuchmemoryit takestoperformcalculationsonthedataandtotransformthemintoexpressive andeffectiveimages.Thedisplayspaceneededisanefficiency-relevantresource as well. On the users’ side, effort goes into interpreting visual representations and carrying out interactions. Interpreting visual representations is primarily a mental effort, but also related to physical eye movements. On the other hand, carrying out interactions is mostly a physical effort, but requires mental activities as well for planning and coordinating the interaction. With expressiveness, effectiveness, and efficiency we have described three criteriathatcontributetoanotionofqualityofinteractivevisualdataanalysis. Table 2.1 summarizes the main points of our discussion. Note that the order in which we introduced the criteria bears an important message: Expressiveness first! No matter how hard we try to be effective and efficient, an interactive visualization is not useful at all if it misrepresents the data. One can also speak of representational primacy [AS05]. Already in 1983, Tufte defines this as a fundamental principle of good graphical representa- tions [Tuf83]: “Aboveallelseshowthedata.” Tufte,1983 It is important to realize that the introduced quality criteria are difficult to evaluate formally. While some aspects would be easy to quantify, such as, computation time or display space, others are not. For example, the benefit of a visual data analysis is hard to capture, but we would need it to determine efficiency. Moreover, we deal with aspects of the human user, whose cognitive processes are not yet fully understood and whose goals and tasks are hard to define on a formal level.
Criteria, Factors, and Models (cid:4) 19 Nonetheless, the described quality criteria provide us with a basis for critically questioning the visual representations, the provided interactions, and the involved calculations and transformations. 2.2 INFLUENCING FACTORS In order to build interactive visual analysis solutions that are expressive, effective, and efficient, we must take a closer look at the factors that influence the analysis. These factors pertain to the subject of the analysis, that is, the data, to the objective of the analysis, that is, the goals and tasks, and to the context of the analysis, that is, the human and technical resources involved. 2.2.1 TheSubject:Data Data on a computer are but sequences of zeros and ones. They are worth nothing without knowledge about how to decipher them. Consequently, if we want to make sense of data, we need both the data themselves plus a description of how to interpret them. This description of data properties is the first factor to be taken into account for choosing or building appropriate analysis solutions. DataDomain So, what characterizes an individual datum (or data value)? Any data value comes from a data domain. The data domain is the set of values that can potentially appear in the data. An important property of a data domain is its scale. The scale determines whatrelationsandoperationsarepossibleforthedatavaluesinthedomain.At the top level, we can differentiate qualitative (or categorical) and quantitative (or numerical) data. At a second level, we can further categorize qualitative data into nominal and ordinal data, and quantitative data into discrete and continuous data. The different data scales and the relations and operations they permit are summarized in Table 2.2. Next, we look a bit closer at these different data. TABLE2.2 Operations possible in different data domains. Qualitative Quantitative Nominal Ordinal Discrete Continuous Equality • Order • • Distance • • • Interpolation • • • •
20 (cid:4) Interactive Visual Data Analysis Nominal Data Fornominaldata,wecanassumetheexistenceofanequality relation =, which allows us to determine whether two values are equal. This is the most primitive insight that can be gained about data values. Additionally, we can count frequencies of nominal values. An example of nominal data would be identifiers such as names {Anika, Ilka, Maika, Tilo, ...}. Ordinal Data For ordinal data, an order relation < exists in addition to the equality relation. This allows us to determine whether a data value is smaller (or less, before, weaker, of lower rank) than another data value. With the help of the order relation, it is possible to sort or rank data values. An example of ordinal data would be age groups such as {children, youths, adults, elders}. With the order relation, we can say that children < adults. Nominal and ordinal data do not have an inherent notion of distances between data values. This changes with quantitative data, which are based on metric scales as we will see next. Discrete Data Discretedataarenumericdatawhosedomaincanbeequated to the set of whole numbers Z. This implies that we can count discrete values and determine the difference between any two data values by means of a distance function. An example of discrete data would be the number of people visiting a doctor. If one day 34 people seek treatment and another day 23, we can naturally derive a difference of 11 persons. Continuous Data Continuous data are numeric data whose domain can be equated to the set of real numbers R. As such, continuous data are uncountable. We can also say they are dense, that is, between any two data values, a third data value exists. This property is a necessary condition for being able to carry out interpolations on the data. Anexampleofcontinuousdatawouldbetemperaturevaluesasmeasured hourly by a weather station. From two data values 10.6℃ and 13.2℃ measured at 8:00 and 9:00 we may interpolate 11.9℃ for 8:30 (in the case of stable weather conditions). Note that it is not always obvious to which category a data value belongs. For example, when dealing with customer numbers or zip codes, we may think they are discrete data. However, they are actually categorical identifiers, for which neither order nor distance have a meaningful interpretation. Laterinthisbook,wewillseethatthedifferentdatascalesrequiredifferent visual encodings. For example, for ordinal data, the ordering of data values must be clearly communicated, yet without suggesting any notion of distances
Criteria, Factors, and Models (cid:4) 21 between data values. More details on how to appropriately visualize data depending on their scale will be discussed in Chapter 3. With the scale of data domains, we can characterize individual data values. If multiple numerical data values are combined, we can further distinguish scalar data, vector data, and tensor data. If we are dealing with a single numeric value, we speak of a scalar. A scalar datum contains but the value itself, such as the temperature values mentioned before. Multiple numeric values can be combined to form a vector. A vector datum defines a direction plus a magnitude based on the vector components. An example is velocity vectors to describe motion in physics. Still more information is captured by a tensor. A tensor datum consists of multiple directions and magnitudes. The order of a tensor defines how much information it bears. Scalars and vectors are special cases of tensors with order 0 and 1, respectively. A 2nd-order tensor can be represented by a matrix, tensorsofhigherorderbymulti-dimensionalarrays.Anexampleoftensordata are 2nd-order stress tensors whose 3×3 components describe the stress at a point inside deformed material. In this book, we focus on scalar data. The interactive visual analysis of vectordataandtensordataisachallengingproblemonitsown.Theinterested reader is referred to books specialized on these topics [HJ05; Tel14]. DataStructure In the previous paragraphs, we introduced data domains to characterize an individual piece of data. However, an interactive visual analysis of a single piece of data does not make much sense. We are interested in analyzing entire sets of data. In reality, data are often unstructured and contain various chunks of heterogeneous information. The analysis of such messy data is extremely difficult. Therefore, data should ideally be stored in or be transferred to structured formats. One such format that is universally applicable is the data table. A data tableconsistsofrowsandcolumns.Thecolumnsrepresentdatavariables.Each variable is associated with a data domain that specifies the values that can possibly appear in a column. The values that actually do appear in a column define the value range. The rows of a data table represent data tuples. A tuple consists of a set of data values and can be understood as a unit of data that describes the properties of an observed entity. There is one value for each variable, and the value is from the variable’s domain. Depending on the context of use, tuples are also called observations, records, items, or objects. We will often use the more general term data element. Particularly in scenarios where relations exist between data entities, these relations may define another layer of structural organization. Hierarchies are common to structure data in a top-down fashion according to different levels of detail (or abstraction). More generally, data can be modeled as graphs whose nodes and edges represent data entities and the relations among
22 (cid:4) Interactive Visual Data Analysis them, respectively. Examples of graph-structured data are social networks or biochemical reaction networks. Both hierarchies and graphs can be stored by using two data tables, one for the entities and one for the relations. Another option to store data in a structured manner is to use data grids. Data grids are particularly important in the realm of flow visualization and volume visualization. As these topics are not in the scope of this book, we again refer the reader to relevant specialized literature [HJ05; PB07; Tel14]. DataSpace A data table as introduced before serves to structure sets of data values. However, structure alone is not sufficient. We additionally need to consider the characteristics of the data space that is spanned by the variables. An important point is the distinction between independent and dependent variables. Independent variables correspond to the dimensions of the space where the data have been collected, observed, or simulated. In turn, the depen- dent variables describe the attributes of what has been collected, observed, or simulated.Moreformally,wemaymodelthisaspectasafunctionaldependency as follows: f :(D ×D ×···×D )→(A ×A ×···×A ) 1 2 n 1 2 m where D denote the dimensions (independent variables) and A the i i attributes (dependent variables). The definition of f implies that a point in the reference space is associated with exactly one data point in the attribute space. A schematic depiction is given in Figure 2.2. Reference Attribute Space Space Figure 2.2 Functional dependency between the reference space and the attribute space. For a point in the reference space, there is exactly one point in the attribute space. It is typically this functional dependency that has to be made visible and understood during interactive visual data analysis. Assume, for example, your dataconsistoftemperatureandairpressureobservedatdifferentlocationsand times: (latitude×longitude×time)→(temperature×pressure). You might be interested in studying how the measured attributes vary over time or where extrema are located. In order to support these and similar analysis questions, it is necessary to visualize the functional dependency between dimensions and attributes faithfully. Note that there are also analysis objectives for which the functional dependency plays only a minor role, for example, when acquiring a general overview of the data’s value ranges. As a graphical summary, Figure 2.3 collects the key terms that we have discussed so far.
Criteria, Factors, and Models (cid:4) 23 Dimensions selpuT Attributes D D A A A 1 2 1 2 3 t 2 1 t 5 2 t 0 3 t 54.32° 13.05° 5 4 97 Data Element 4 t 1 5 t 9 6 t 5 7 t 1 8 Data Domain -10 0 Value Range 9 20 Figure2.3 Key terms for characterizing data. DataSize Withthehelpofatabularmodelofdataandthedistinctionofdimensionsand attributes, we can now study another important characteristic of data: their size. To this end, let’s take a look at a data table with dimensions, attributes, and tuples as illustrated in Table 2.3. We can see that the size of a dataset is determined by n, the number of dimensions, by m, the number of attributes, and by k, the number of tuples. TABLE2.3 A data table. D D ··· D A A ··· A 1 2 n 1 2 m d d ··· d a a ··· a 1,1 1,2 1,n 1,1 1,2 1,m . . . . . . d d ··· d a a ··· a k,1 k,2 k,n k,1 k,2 k,m In general, we can say we are dealing with k tuples of n-dimensional m-variate data. Data that consist of more than one dimension are denoted as multi-dimensional data. If there are many dimensions, the term high- dimensional data is common. A similar notation is used for the attributes. If there is only one or two attributes, we use the terms univariate and bivariate data, respectively. The presence of several attributes is indicated by the term multivariate data. Obviously, the larger m, n, and k are, the more challenging and complex the interactive visual analysis will be and the more sophisticated tools will be needed to support the analysis. The question of whether a dataset can be regarded as large or not is often discussed controversially and typically cannot be answered unambiguously.
24 (cid:4) Interactive Visual Data Analysis One way to resolve this issue is to think of the different bottlenecks that data have to pass on their way from the computer to the human mind. This can be made clear by asking the following questions: • How much data fit in storage? • How much data fit in memory? • How much data fit on the display? • How much data can we digest? If for any of these questions our data do exceed the corresponding limit, we mayconsiderthemlarge.Theearlieralimitisexceededinthislistofbottlenecks, the larger the data typically are. In any case, we will need additional methods, such as interaction or computational support, to overcome the bottlenecks. These topics will be studied in Chapters 4 and 5. So far, we have discussed data properties that can, to a certain degree, be extracted from the data. We can look at the values and make assumptions abouttheassociateddatadomains.Wecaninspectthevariablesanddetermine which are independent and dependent. And of course, we are able to count the variables and tuples of a dataset to get its size. Next, we will deal with a property that cannot be derived, but must be provided to us: the data scope. DataScope In order to create expressive visual representations of data, we need to know thedata’sscope.Thedatascopecharacterizesthevalidityofdataatoraround the point of observation. We can distinguish between three types of scopes as illustrated in Figure 2.4: • Global Scope: Data are valid across the entire reference space. • Local Scope: Data are valid at a point of reference and its vicinity. • Point Scope: Data are valid only at a point of reference. Reference Space Reference Space Reference Space (a)Global scope. (b)Local scope. (c)Point scope. Figure2.4 The scope defines to which extent an observation is valid.
Criteria, Factors, and Models (cid:4) 25 Whether individual values are valid globally, locally, or point-wise cannot be determined from a dataset per se. Instead a data description is needed, which should include hints as to which scope is applicable. For specific applications, we may assume a certain data scope. One such example is geo-spatial data, which, according to Tobler’s first law, have a local scope, because attributes measured at proximal locations tend to be correlated[Tob70].Unfortunately,thelocalscopeisnotalwaysdefinedprecisely. From a visualization perspective, the question is how the local validity of data can be represented? Let us illustrate this with the three different representations of measure- ments of water quality as shown in Figure 2.5. The data are color-coded with a green-to-red color scale, where green represents high quality and red signals low quality. Figure 2.5a neglects the local scope and shows us just the data points. This kind of representation is rather sparse and does not really support building a good understanding of the data. (a)Data points only. (b)Voronoi partitioning. (c)Shepard interpolation. Figure2.5 Visualizing the local scope of measurements of water quality. In Figures 2.5b and 2.5c, interpolation comes to our help. By interpolation, wecanassignacolortoallpixelsinordertocreateadensevisualrepresentation thatcommunicatesthelocalscopeofthedata.Figure2.5busesnearest-neighbor interpolation, which corresponds to a partitioning of the space into discrete Voronoi regions. Now we get a much better impression of the water quality. However,thediscreteregionssuggestdiscontinuitiesattheirborders.Certainly, no such discontinuities exist in the water. In Figure 2.5c, we used Shepard’s interpolationmethod[She68].Thevisualrepresentationissmooth,whichmore closely corresponds to what we expect from the data. Note, however, that interpolating scattered data is a non-trivial problem. There are various methods and each comes with its own set of parameters and corresponding results. Which methods and parameterizations are appropriate must be decided on a case-by-case basis.
26 (cid:4) Interactive Visual Data Analysis Meta-data All of the previously discussed properties of data (domain, structure, space, size, and scope) should be described and provided along with the actual data. This data description is also called meta-data, or data about data. Ideally, the meta-data are curated with the same scrutiny as the data themselves, because the meta-data are the first to be consulted for an informed decision on how to visually analyze the actual data. Moreover,meta-datacan(andshould)containadditionalinformationabout the evolution of the data. This includes the data’s past, the data’s present, and the data’s future [Sch+17]. Intermsofthedata’spast,wemaythinkofinformationabouthowthedata have been collected, observed, or simulated. This so-called data provenance information can be quite important when it comes to understanding and replicating the results from analytic activities. An example of provenance meta-data is indicators of data quality to specify how much confidence we may have in the recorded data. A closely related notion is that of data uncertainty, which tells us if and to which degree the data may be uncertain. Meta-data about the data’s present state basically specify how the data are stored and how they can be retrieved. This covers all the data descriptors described earlier plus technical details about the data format. For example, the meta-data can tell us if a variable is allowed to include missing data (or null values), and if yes, how they are signaled. Thedata’sfuturemaybecoveredbymeta-dataaswell.Thismainlyincludes information about what operations may be performed on the data. Such data utility information is somewhat rarer and often not given explicitly. But it can be quite valuable. For example, we already mentioned data interpolation as difficult to deal with. Meta-data can give us information about whether interpolation is possible and under which assumptions. In summary, meta-data are as important as the data to be analyzed. Figure2.6providesanoverviewofthevariousaspectsdiscussedintheprevious paragraphs. While our considerations remained rather theoretical so far, the next section will be more practical by describing common classes of data. META-DATA Data Value Data Set Data Evolution Data Domain Data Structure Data Provenance Data Space Data Format Data Size Data Utility Data Scope Figure2.6 Meta-data to characterize the data to be analyzed.
Criteria, Factors, and Models (cid:4) 27 ClassesofData Let us next take a look at classes of data that are common in practice. We introduce the following abstract notation: A stands for (one or more) data attributes,T fortime,andS forspace,andRforstructuralrelationships among the data. Depending on which of these aspects are present, different classes of data can be distinguished. Figure 2.7 provides an overview. More details and example data are described below, where the arrow symbol → is used to indicate a functional dependency between reference space and attribute space. Time Multivariate Data A T Temporal Data Multivariate Graphs Space-time R S Space Graph Data Spatial Data Dynamic Graphs Spatio-temporal Data Multivariate Dynamic Graphs Spatio-temporal Graphs Multivariate Spatial Graphs Multi-faceted Graphs Spatial Graphs Figure2.7 Four-set Venn diagram illustrating different classes of data. Multivariate Data Multivariate data consist of several data attributes A. Typically, the data analysis will concern the distribution of data values and correlations among the attributes. An example is player statistics as maintained in many leagues of various sports. Temporal Data For temporal data, the reference space is defined by the dimension of time, and one or more time-dependent attributes are observed over time T →A. The data analysis typically seeks to under- stand how the attributes evolve over time. This includes the detection of trendsorcyclicbehavior.Stockpricesareacommonexampleoftemporal data. Spatial Data If data are given in a spatial frame of reference, we denote them as spatial data S → A. Two-dimensional and three-dimensional spatial data are common. The analysis of spatial data is centered around finding spatial patterns, clusters, or hot-spots. An example of spatial data could be the distribution and richness of mineral deposits. Spatio-temporal Data Quite often, spatial data do also include a temporal dependency, in which case, we speak of spatio-temporal data S×T →A. An analysis of such data combines spatial and temporal issues, such as the development of spatial clusters over time or cyclic re-occurrence of hot-spots. Spatio-temporal data are ubiquitous in the form of weather data.
28 (cid:4) Interactive Visual Data Analysis Graph Data Graphs consist of a set of data entities, the nodes, and a set of relations between the entities, the edges. In the first place, we are interested in understanding the structural information R inherent in graphs. Additionally, nodes and edges of a graph can also be associated withdataattributestobeanalyzedR→A.Graphscanevenbelinkedto a spatial context or vary over time, in which case different dependencies couldbedefined,suchasR→S×T×AorS×T →R×A.Anexample of so-called multi-faceted graphs is climate networks, which are employed to simulate and predict climatic phenomena on earth. The introduced classes of data are deliberately kept general. This allows us to map practical analysis problems to these classes. For example, the analysis of documents or document collections could be done by modeling documents as attribute vectors among which relations exist R → A. The analysis of medical images naturally maps to S →A, where S corresponds to the pixel grid and A is the pixel color. In a similar way, the analysis of flow data maps to S →A (or S×T →A, if the data are time-varying), where S is a 2D or 3D grid-structured space and A defines the components of vectors. If the visual analysis is about more complex artifacts such as software systems or distributed processes, it is typically necessary to break them down into smaller conceptual pieces before a meaningful mapping to A, T, S, and R is possible. With the introduction of general data classes, we end our discussion of data characteristics as the first influencing factor to be considered. Next, we continue with analysis tasks as the second influencing factor. 2.2.2 TheObjective:AnalysisTasks Intheprevioussection,weinvestigatedthesubjectofinteractivevisualanalysis, the data. In this section, we study the objective, that is, the analysis tasks. Tasks are important particularly for two reasons. First, they determine which portions of the data are relevant for the analysis, and second, they are crucial for meeting the effectiveness criterion as introduced in Section 2.2. Let us illustrate this with an example. Figure 2.8 shows two color-coded maps with identical geo-spatial data. Figure 2.8a uses a yellowish-to-green color scale where color varies in perceptually uniform steps to support the task of identifying the data values. The map in Figure 2.8b uses a color scale that emphasizes extrema, whereas intermediate values are attenuated by the use of gray tones. This encoding facilitates the task of locating minima (in blue) and maxima (in red) on the map. Our example clearly demonstrates that the generation of effective visual representations depends on the task at hand. In fact, we can state that a particular visual representation supports particular tasks: Data + visual representation 7→ tasks. In turn, this implies that specific tasks require dedicated visual representations: Data + task 7→ visual representation. The very same dependency on the task exists for the
Criteria, Factors, and Models (cid:4) 29 (a)Coloring suited to identifying values. (b)Coloring suited to locating extrema. Figure2.8 Different visual encodings to support different tasks. interactiondesignandtheinvolvedanalyticalcomputations.Aswesee,itisthe task that determines which design, technique, or method would be appropriate for an interactive visual analysis of data. However, tasks are notoriously difficult to conceptualize due to their multi- facetedcharacter.Ataskisusuallycarriedouttoachievealargergoal.Moreover, a task is typically focused on an analytic question that operates on a relevant part of the data, the task’s target. Finally, a task can be accomplished by employing different means. In the following, we will describe in more detail the goals, analytic questions, targets, and means that characterize tasks. Goals Goals describe the intent with which analysis tasks are pursued. Goals have an overarching character in that achieving a goal typically involves several steps of analytic activity. General goals are to explore, describe, explain, confirm, or present the data [Sch+13a; LTM18]. Exploration is geared towards making first observations, such as discovering trends in heterogeneous data or detecting outliers in homogeneous data. This also includes noticing the absence of something that was actually expected in the data such as a commonly known pattern or trend that is not there. In a sense, exploration follows an I-know-it-when-I-see-it approach to undirected search, or as pioneers of visual analytics put it [TC05]: “[...]detecttheexpectedanddiscovertheunexpected.” ThomasandCook,2005
30 (cid:4) Interactive Visual Data Analysis Description isallaboutcharacterizinganobservationbytheassociateddata elements, and thereby deriving a specification for an observation. For example, an outlier can be described by its characteristic values and, if available, its spatio-temporal context. A proper description may serve as a basis for configuring further analysis steps. In particular, a description allows for sharing first insights with other people, who can later be involved in verifying the analysis results. Explanation means identifying all contributing data and finding the main causes behind an observation. This involves investigating several ques- tions. Is the observation by itself significant or did we just interpret too much into the noise among the data? Does the observation re-occur throughout the data or are we looking at a singular outlier produced by unlikely circumstances? If the observation does re-occur, does it show up reliably under the same conditions, thus forming a pattern, or are its appearances seemingly random? Such investigations are necessary to understand the expected and recog- nizethereasonsbehindtheunexpected.Bytryingtoexplainobservations, we will obtain more and deeper insights, which will lead to hypotheses about the data. Confirmation aims to verify the hypotheses. Unlike exploration, confirma- tion is a directed search. We look for something concrete, some evidence to either back up or refute a hypothesis in terms of validity, generality, or reliability. For this purpose, we may study alternative visual represen- tations of the data or re-parameterize the analytic computations in order to check whether this leads to the same results. If available, related data may be consulted to test if they exhibit similar results. Presentation is to communicate confirmed analysis results. While explana- tion and confirmation were about convincing ourselves, presentation is about convincing others of what we have found in the data. This is best done by telling a story about the data, the analysis, and the results. Such a story can act at different levels of emphasis. We may inform an audience by letting the results speak for themselves, explicate the results to an audience, or even persuade an audience into agreement with the results. The audience in this context can be the listeners of a talk, the readers of an article, or colleagues participating in a scientific discussion. Exploration, description, explanation, confirmation, and presentation can be understood as subsequent phases in the data analysis process. We start exploring the data until we make an observation. Next we describe the finding and try to explain it. Then we confirm that our hypotheses about the data are valid. Finally, we are ready to present the confirmed analysis results. Note how this workflow proceeds with increasing specificity from not knowing that certain observations can be made in the data to knowing them well enough to present them.
Criteria, Factors, and Models (cid:4) 31 Thepreviousparagraphsoutlinedinbroadstrokesdifferentgoalsofanalytic activities, yet without detailing what is actually done. This is where concrete analytic questions enter the stage. AnalyticQuestions Analytic questions describe what specifically is sought in a particular analysis step. There are many different questions that are relevant in data analysis scenarios [KK17]. In general, we can distinguish between two fundamental categories: elementary and synoptic questions [AA06]. Elementary Questions Elementary questions refer to individual data ele- ments. This includes questions that concern a single data element, but also multiple data elements. What is essential is that each data element is studied individually. Elementary questions may be concerned with: • Identify: What is the value? • Locate: Where is the value? • Compare: Is it less or more? • Rank: Is there any order? • Connect: Are they related? • Distinguish: What makes the difference? Synoptic Questions Synoptic questions refer to groups of data. As such, synoptic questions are concerned with the characteristics of sets of data ele- ments, rather than individual data elements. The previously listed questions can operate on groups as well. Additionally, synoptic questions may ask: • Group: Do they belong together? • Correlate: Are there any dependencies? • Trends: Do they develop systematically? • Cycles: Do they re-occur periodically? • Outliers: Are they special with respect to the rest? • Features: What is characteristic for the data? Note that the two lists of analytic questions are not comprehensive. One can easily imagine many more questions that could be asked about data. The further readings collected at the end of this chapter provide additional information in this regard.
32 (cid:4) Interactive Visual Data Analysis To recap, analytic questions specify what we would like to know about the data. However, as indicated by the unspecific word they in the above lists, a question does not tell us where to look. This aspect is covered by the target of tasks as described next. Targets Typically, not all data are equally important in the context of a specific analytical question. The target of a task is about where in the data a task actually operates. But why is this important? Specifying a task’s target enables us to narrow down what we need to look at to accomplish the task. Are we interested in the data values just by themselves, or how they distribute in a particular spatial or temporal context? Do we want to identify individual data values of the whole data set, or only with respect to a certain subset? Do we search for global trends, or are we interested in more fine-grained variations? Data of Interest More specifically, the target defines the data of interest, that is, the subset of the data that is indeed relevant to the task. Conceptually, such a subset can be created with respect to the data variables and the data elements. A target can cover all variables of the data space or only a particular subset of variables. Restricting the view on particular variables is called data projection. Thedataofinterestcanbenarroweddownfurtherbyfocusingonparticular data elements. To this end, we may specify criteria that data elements must exhibitinordertobeconsideredrelevanttotheanalysis.Thiswayitispossible to restrict the data of interest to specific value ranges. Restricting the view on particular data elements is called data selection. Figure 2.9 illustrates how projection and selection constitute the target. Note that the target is not necessarily a closed chunk of data as illustrated in the figure. The general case is that the target is made up of several pieces of data that are distributed across the data space. Looking at Figure 2.9 we can see that the target is much smaller than the entiredata.Itcanevenbeassmallasasingledataelement,forexample,when working on elementary questions such as identifying a value in the reference space. If the target is a proper subset of the data, elementary or synoptic questions could be addressed depending on whether the data elements are considered individually or as self-contained groups. A target that refers to all data elements typically supports questions of synoptic character. Data Granularity For the case that the data are structured hierarchically according to different levels of detail (or abstraction), a target is further characterizedbyitsgranularity.Thegranularitydefinesthelevelofabstraction needed to accomplish a task.
Criteria, Factors, and Models (cid:4) 33 Projection V V V V V 1 2 3 4 5 d 1 d 2 d 3 d 4 d Target Selection 5 d 6 d 7 d 8 Figure2.9 A target as defined by projection and selection. Carrying out tasks at a low level of detail (high level of abstraction) is suited to get a general overview of the data. At this coarse-grained level, tasks are concerned with investigating fundamental data features such as general correlations or overall trends. On the other hand, tasks can operate on a high level of detail (low level of abstraction). Fine-granular targets typically permit insights into details that cannot be seen in an overview. An example would be small fluctuations in an otherwise monotonically developing trend. Weknownowhowtargetsdefinewhereinthedataataskoperatesinterms of the data of interest that are studied at a particular data granularity. Next, we briefly describe the means that can be employed to carry out tasks. Means There are many options for carrying out the tasks. For example, if we need to locate a particular data element whose specifics are known, we can query the datadirectly.However,ifthedataelementisonlyvaguelyspecified,formulating aqueryishardlypossible.Inthiscase,wehavetoscanthevisualrepresentation in order to find it. As we see, querying and scanning are both suitable means to accomplish the same task. In general, the means describe how a task is performed. From a conceptual point of view, one can differentiate between visual, interactive, and computa- tional means. Visual Means subsume all kinds of visual inspection. The task is carried out with the eyes, in fact, with the entire human visual system. For example, wemightvisuallyidentifyadatavaluebylookingupacolorinthevisual representation’s color legend. Interactive Means relate to interactive information gathering. In this case, tasks are carried out by the hands or other parts of the human motor
34 (cid:4) Interactive Visual Data Analysis system. To continue the previous example, a data value could also be identified by hovering over it with the mouse cursor to bring up a label that shows the exact value. Computational Means stand for calculations in general. In this case, the computer produces the desired results. For example, we could perform a cluster analysis using algorithmic computations on the input data in order to group similar data elements. Each category of means is based on its own concepts and methods, which will be detailed in Chapter 3, Chapter 4, and Chapter 5. Nonetheless, visual, interactive, and computational means are usually applied in concert to fully exploit the cognitive capabilities of the human and the computational power of the machine. Let’s illustrate this with the example of finding outliers. We might start with an initial visual inspection of the data followed by an interactive selection of relevant data. Then, a computation of distances could relate the selected data to other data elements in order to detect outliers. Finally, we may again want to do a visual inspection to examine whether outliers have been flagged correctly. In this section, we have characterized analysis tasks with respect to goals, analytic questions, targets, and means, which indicate why a task is pursued, what ataskseeks,where ataskoperatesinthedata,andhow ataskisactually carried out. Figure 2.10 collects the key terms that we dealt with. They allow us to formulate tasks like describe groups of data elements with low values by marking them interactively. Two more examples are given in Table 2.4. Such taskdescriptionscanthenbeusedtoinformthedesignorsupporttheselection of interactive visual analysis solutions [Sch+13a]. In conclusion, the characteristics of the tasks and the properties of the data are two important influencing factors. Together, they form the basis for some of the most influential early classifications of visual approaches to data analysis [KK93; Shn96]. However, data and task are not the only factors to be observed as we will see in the next section. TASKS Goals Questions Targets Means Elementary Synoptic Explore Data of Interest Visual Describe Granularity Interactive Explain Identify Group Computational Confirm Locate Correlate Present ... ... Figure2.10 Goals, questions, targets, and means characterize analysis tasks.
Criteria, Factors, and Models (cid:4) 35 TABLE2.4 Examples of tasks. Goal Question Target Means Explore locations of maximum values visually Describe groups of low-value elements by marking Confirm cyclic behavior of temperature by statistics 2.2.3 TheContext:UsersandTechnologies To enable an effective and efficient interactive visual data analysis, the context of the analysis has to be considered as the third important influencing factor. Primarily, we have to ask who is analyzing the data and where does the analysis take place. In this sense, the context subsumes aspects of the users who conduct the analysis and aspects of the technologies that constitute the analysis environment. It is beyond the scope of this section to comprehensively characterize human users and different technologies. Instead, we will briefly look into the key concerns that need to be taken into account. HumanUsers Concerning the question of who is carrying out the data analysis, the following aspects are relevant: Human Factors An important factor is the properties of human individuals. What are we able to see with our visual system and what are we able to do with our motor system? Interactive visual analysis solutions should becenteredaroundtheabilitiesofhumansingeneral.Forexample,when interacting at larger displays via touch gestures, our precision is usually limited for the majority of users. Moreover, no user is like the other. Therefore, it is important to consider not only general characteristics of humans, but also the properties of individual users. For example, we need to adapt visual encodings for people with color vision deficiencies so as to allow them to gain the same insight as people with normal vision. User Background and Expertise People who work with interactive visual analysis solutions come from different backgrounds and have different expertises. Visual analysis experts excel in creating interactive visual analysis solu- tions. They apply their tools routinely to examine various kinds of data. However, they are usually not trained in the field to which the analysis is supposed to bring new insights. On the other hand, domain experts are focused on application-specific problems and data, of which they often have a quite good mental model.
36 (cid:4) Interactive Visual Data Analysis Theyexpectanalysistoolstoadapttotheirworkflows,andnottheother wayaround.Domainexpertscouldpreferspecificrepresentationsorways of doing things, simply because they are widely used in their application background, even if other alternatives would be better suited. Casual users from the general public will typically favor basic interactive charts and maps over more sophisticated visual analysis systems. There- fore, casual users need to be provided with visual representations that are easy to interpret and interactions that are easy to carry out. It can even be necessary to include incentives and guidance to motivate people to take a closer look and to assist them in doing so. Application Domain Only if we have a sufficient understanding of the problemsthatneedtobesolvedinanapplicationdomaincanwedevelop suitable analysis solutions. In addition to addressing domain problems, we also need to consider domain conventions. For example, the U.S. Geological Survey suggests using specific colors for geologic maps. As a consequence, these colors cannot be employed for other visualization purposes than geologic characteristics in this particular context. Application domains can also pose hard constraints on the analysis system. For example, medical diagnosis systems certainly must be super reliable and heavily tested before they can be used in clinical practice. Single-user and Collaborative Analysis Dataanalysescanbecarriedout by a single user or by multiple collaborating users. In the former case, the applied means can be tailored to the needs and preferences of a single individual. In collaborative settings, however, we need to consider a larger pool of methods that additionally need to be flexibly parame- terizable in order to be able to attune them to a broader variety of user needs. Moreover, while single-user analysis can focus on the exchange of information between the user and the machine, collaborative analysis further needs to consider the interaction between users, for example, for sharing visual representations and discussing findings. Technology To address the question of where the data analysis is performed, the following technology-oriented aspects are relevant: Computational Resources Analyzing data with the help of computers implies the use of computational resources. In the case of interactive visual data analysis, we need resources to create and render visual rep- resentations, ideally in high quality and at interactive frame rates. For larger data, we need additional computational resources to carry out analytical pre-processing steps. Depending on the computational resources of the environment and in linewiththeactualanalysisobjective,itmustbedecidedhowtobalance
Criteria, Factors, and Models (cid:4) 37 betweenthedifferentdemands.Forexample,whenusersinteract,immedi- ate visual feedback is crucial. So, visualization quality could be sacrificed temporarily in favor of quicker response rates during interaction. Similar trade-offs are necessary in many analysis situations. Display Technologies Nowadays, there exist a variety of technologies for presenting data. It makes a big difference to visualize data on a small portrait-oriented display of a mobile phone, a large landscape-oriented mega-pixel display wall, or a professional designer monitor with high contrast and a wide color gamut. In general, display technologies are characterized by their physical size, their aspect ratio, their resolution, and their ability to reproduce colors faithfully. The physical size largely determines if a display is applicable in particular environments. The available pixel resolution indicates how much data can be represented. The display’s aspect ratio influences the layout of the presented data. The available colors naturally have an impact on the use of colors in a visualization. Input Modalities An analysis environment is also characterized by its input modalities. They decide about how the interaction is carried out and howpreciselyaninputcanbemade.Classicmouseandkeyboarddevices are common in regular desktop scenarios. Modern displays facilitate interaction via touch technology. This has the advantage that the interaction can take place exactly where the data are shown, that is, on the display. A disadvantage, though, is that smaller graphical objects are more difficult to pick accurately. In the case of large display walls, mouse and touch interaction alone are impractical due to their limited reach. Alternatives are to track the user’s position and gaze and utilize the tracking information to steer the visual data analysis. As we see, a variety of aspects characterize the context of interactive visual data analysis. Although we have described them one by one, the different aspects exhibit numerous interdependencies. For example, visualizing larger data on a display wall implies that we also have to think of the increased resources that we need to render the visualization and of the interaction modalities that will allow a human user to operate in a large-display setting ergonomically. For another example, if we intend to develop a color-intensive visual representation, we must make sure that it is used on displays with a wide color gamut, and we should also think of ways to circumvent the problem of defective color vision. In summary, we have now described the data, the tasks, and the context as the major influencing factors. Together, they not only inform the design of interactive visual analysis solutions, but they also determine whether a
38 (cid:4) Interactive Visual Data Analysis solution meets the criteria defined in Section 2.1, that is, how well and how fast users can interprete their data, and how balanced the use of resources is. The condensed take-home message of this section is: If you want an expres- sive, effective, and efficient analysis solution, you first need to know what data are to be studied, what tasks are to be carried out, and what the context is in which the analysis is conducted. The more comprehensive the answers to these questions are, the easier it will be to come up with an appropriate solution. 2.2.4 DemonstratingExample In the following, we will demonstrate how the data, the tasks, and the context may manifest in practice. In order to keep our example manageable, we will brieflycharacterizethedataandthecontext,whereasthetaskswillbedescribed in greater detail. The Data and the Context For our example, we will use a mete- orological time series, which conceptually is an instance of temporal data T → A. The data are multivariate in that they comprise several attributes, including air temperature, air pressure, wind speed, hours of sunshine,cloud cover,precipitation,andprecipitation type.The individual values are scalars with continuous, discrete and nominal scale. For instance,air temperatureiscontinuous,hours of sunshineisdiscrete,and precipitation type is nominal. We will be investigating more than 23,725 daily measurements, which amounts to about 65 years worth of data. Forthesakeofsimplicity,wewillassumethedataanalysisiscarriedoutby a single user and takes place on a standard desktop computer with a regular display and the typical mouse and keyboard input. Although our example has been supported by an expert from climate impact research, we do not include any application-specific conventions or constraints. This is again to keep the example simple. The Task Now, let’s assume our goal is to explore the data in search of something interesting. More specifically, we are interested if there are any patterns in the data corresponding to seasonal variations, which means, we are dealing with a synoptic analysis question for which we need all data elements.Nonetheless,inordertonarrowdownouranalysisproblemsomewhat, we begin by concentrating on the three attributes hours of sunshine, air temperature, and cloud cover as the target. The charts in Figure 2.11 suggest that we will be accomplishing our task with the help of visual means. There is one chart per targeted attribute. Each chart shows time along the horizontal axis and a time-dependent attribute along the vertical axis. For each pair of date and measurement, a dot is placed in the chart.
Criteria, Factors, and Models (cid:4) 39 18h 16h 14h 12h 10h 8h 6h 4h 2h 0h January February March April May June July August September October November December (a)Hours of sunshine. 30°C 25°C 20°C 15°C 10°C 5°C 0°C -5°C -10°C -15°C -20°C January February March April May June July August September October November December (b)Air temperature. 80/80 70/80 60/80 50/80 40/80 30/80 20/80 10/80 0/80 January February March April May June July August September October November December (c)Cloud cover. Figure2.11 Meteorological measurements over the course of the year. NotethatthetimeaxisspansonlythedaysoftheyearfromJanuary1ston the left to December 31st on the right. In fact, only day and month of a date determine where to place a dot horizontally, the year is ignored. Conceptually, this corresponds to a projection with the effect of mapping the data of several years onto a common one-year scale. As a result, we obtain charts that are well suited to get an overview of seasonal trends. For example, we can easily see the expected pattern that the hours of sunshine in Figure 2.11a peak around the day of the summer solstice on June 21st and reach the bottom around the day of the winter solstice on December 21st. In Figure 2.11b, we can see that the air temperature in principle follows the hours of sunshine, but with a delay of approximately one month. This offset is well known and called seasonal lag. While not showing anything exciting so far, Figures 2.11a and 2.11b still helped us to detect the expected.
40 (cid:4) Interactive Visual Data Analysis 1000 100 10 1 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 (a)Cloud cover value frequencies in Rostock. 1000 100 10 1 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 73 75 77 79 (b)Cloud cover value frequencies in Dresden. Figure2.12 Histograms of the distribution of cloud cover values. This gives credence to the validity of the data and the expressiveness of the visual representation. But we can also discover unexpected things. Looking at Figure 2.11c, the average daily cloud cover (with 0/80 and 80/80 meaning clear sky and fully covered, respectively) seems almost evenly distributed over the course of the year at a first glance, with just a slight tendency of denser sky cover in the winter months. But if you look closely at the chart, you might be able to spot a horizontal line that is lighter and less covered with dots than all others. Could it be that there are unusually few occurrences of a particular cloud cover value? As we reason about this question in more detail, we change our task from exploring seasonal trends in the data to describing the unexpected finding. Providing a description of the finding may help those who were unable to discern it on their own: The thin horizontal line is located a little above the 50/80 mark for an average cloud cover of 52/80. Next, we want to find out whether our finding is by itself significant, or if we just interpret too much into the noise among the data. This implies a slight shift in our task from describing the unexpected finding to explaining it. To this end, we study the frequency distribution of cloud cover values, that is, we look at how frequently individual values appear in the data. Again, we approach our analytic task with visual means, more concretely withhistogramsasshowninFigure2.12.Ahistogramshowsalongitshorizontal axis the individual data values whose frequencies are of interest to us. For each of the 81 possible cloud cover values from 0 through 80, there is a vertical bar whose height encodes how often a value occurs in the data. The histogram clearly shows that values of 52/80 and also of 12/80 are much rarer than all others. Hence, our observation was not a visual illusion, but is backed by the data. Finally, we want to confirm our finding and show that it is not specific to the time series at hand. For this purpose, we look for similar patterns in the
Criteria, Factors, and Models (cid:4) 41 time series from a different weather station as shown in Figure 2.12b. And indeed, we can see a similar phenomenon. The pattern does not only re-occur for our initial observation at 52/80 and 12/80, but it even seems to affect the data more broadly at 2/80, 22/80, 32/80, 42/80, and so forth. At this point, we can only speculate about the reasons behind this pattern. For a satisfactory explanation, further investigations would be necessary, which suggest that visualdataanalysisisoftenanopen-endedprocesswithmanytwistsandturns involving many analytic tasks. With the above example, we end the discussion on influencing factors. Next, we continue with conceptual considerations regarding the fundamental processes of interactive visual data analysis. 2.3 PROCESS MODELS In the previous sections, we have introduced criteria and influencing factors. The criteria help us to capture a notion of the quality of analysis solutions, whereas the influencing factors tell us what we need to consider in order to obtain high-quality results. What we have not talked about so far is the actual solution itself. How does it come about, how does it look like, and how is it employed? All these questions will be dealt with in this section. Inthefollowing,wewillshedlightontheprocessesthatthedifferentactors beinginvolvedinthecontextofinteractivevisualdataanalysishavetodealwith. In the first place, there are designers who are responsible for conceptualizing data analysis solutions. They need to know about the design approach of how to get from an application problem to an actual solution for that problem. Secondly, there are the developers who engineer and implement the solution to createarunningsystem.Developersneedaconceptualblueprintoftheprocess of transforming data artifacts through several stages into interactive visual representations. Finally, target users actually analyze data. In this regard, it is of interest how utilizing data analysis solutions leads to new insights. Understanding this aspect will enable us to deploy and embed our solutions successfully into real-world problem-solving workflows. Followingtheabovelinesofthinking,wewillnextstudyabstractmodelsof interactivevisualdataanalysisfromadesignperspective,adata-transformation perspective, and a knowledge-generation perspective. 2.3.1 Design Designing interactive visual data analysis solutions is a challenging endeavor. The key difficulty is to come up with an appropriate ensemble of visual, interactive,andcomputationalmeansthatactuallyhelpusersinaccomplishing application-specific analysis problems. To cope with this difficulty, it makes sense to follow a well-structured design process called the nested model [Mun09; Mey+15]. The model consists
42 (cid:4) Interactive Visual Data Analysis Domain Characterization Data and Task Abstraction Design Conceptualization Solution Implementation Figure2.13 Four nested levels outline how to design interactive visual data analysis solutions. Adapted from [Mun09]. offournestedlevelsthatdescribethewayfromadomainproblemtoanactual implementation of a solution. Here, we describe a variant of the nested model that has been slightly adapted, mainly to be consistent with the terminology used in this book. The model’s levels are illustrated in Figure 2.13 and are detailed below. Domain Characterization First, the visualization designers must familiar- ize themselves with the application domain. This primarily concerns understandingtheusersandtheirdomain-specificproblems.Moreover,it is necessary to characterize the context of the data analysis as discussed in Section 2.2.3. What are the typical workflows and what tools are employed? How does the working environment look like? It is mandatory that these questions be answered in cooperation with domain experts. Data and Task Abstraction Once the application domain is characterized, the next step is to abstract the essential data and tasks based on the notions introduced in Sections 2.2.1 and 2.2.2. This is to focus thedesign onconceptuallyrelevantaspects,ratherthandomain-specificdetails.For example, instead of speaking of biomedical signaling pathways, we would saywearedealingwithtime-varyingattributesingraphsT×R→A.The diffuse application problem of trying to understand signaling pathways, for example, would be translated to explore recurring pattern of peaks visually and confirm them computationally. The abstract description of data and tasks will then inform the design conceptualization on the next level. Design Conceptualization This level is about defining the appropriate means that constitute the overall solution. As discussed earlier, the means can be of visual, interactive, or computational nature. That said, the design conceptualization involves finding expressive visual encodings, useful interactive tools, and potent computing procedures. Thanks to the abstract data and task description established in the preceding step, we can identify approaches that have previously proven
Criteria, Factors, and Models (cid:4) 43 successful, and if necessary, adapt them to the application domain. Only if no prior art exists is it necessary to develop entirely new approaches. As we will see in Chapters 3 to 5, the design space for visual, interactive, and computational approaches is considerably large. This implies that substantial expertise is necessary in order to make appropriate design decisions that match the data, the tasks, and the context. Solution Implementation Finally, the conceptualized design has to be brought to life. To this end, the building blocks identified in the previous level are implemented as concrete algorithms. They perform all the com- putations and transformations that are necessary for the data analysis. Moreover, the algorithms provide parameters that enable us to flexibly adjust the data analysis. Following the nested model, as outlined above, has several advantages. First, the domain characterization suggests a user-centered design strategy where the needs of the users are central to later design decisions. Second, the abstraction of data and tasks bridges the gap between the different vocabu- laries of domain experts and the designer. The abstraction further allows us to categorize application-specific problems into classes of data analysis prob- lems, which in turn simplifies the selection and the development of reusable approaches. Finally, the separation of concerns between conceptualization and implementation is beneficial because it allows the designer to concentrate on identifying or devising appropriate techniques, and the developer to focus on efficient algorithms. Asanothermajoradvantage,thenestedmodelfacilitatestestingthevalidity of a data analysis solution. Why is this important? The problem is that the design process is prone to incorrect design decisions. Being aware of the potential threats will help us to carefully check and critically question the design choices we make. Next, we briefly describe what can compromise the validity of our design on the different levels. • Inaccurate domain characterization. The application domain has not been characterized accurately. For example, the identified problems are not actual problems of the target users, or their workflows and working environment deviates from what was understood. • Incorrect data and task abstraction. The abstraction fails to extract the correct descriptions of the data and the tasks. The data may contain different information than expected. The tasks are not appropriate to successfully approach the application problem. • Inadequate design conceptualization. The devised techniques are not adequate. The visual encoding might not be expressive or effective. The interaction techniques could not match the environment. Computational results could be invalid due to the unfitness of analytical procedures.
44 (cid:4) Interactive Visual Data Analysis • Inefficient solution implementation. The solution does not run efficiently. This could be due to the time complexity of the involved algorithms or their inefficient implementation. Moreover, the memory footprint could be too large for the available resources. It is important to realize that invalid design decisions on the upper levels typicallyleadtoacascadeofnecessarychangesatthelowerlevels.Forexample, a misunderstood application problem can void all later design efforts. On the other hand, an issue at the lower levels can often be remedied with moderate effort, for example, the implementation could be improved by replacing a computationally expensive exact calculation with a sufficient approximation. In summary, the nested model describes the fundamental design process and where problems could occur along the way from the application problem totheimplementationofalgorithms.Next,wewillswitchourperspectivefrom the overall design to the actual process of transforming data into images. 2.3.2 DataTransformation In this section, we are interested in what actually happens to the data when becoming expressive visual representations. Certainly, we begin with raw data as input and seek images1 as output. But how do we continue from there? What are the steps to be performed in order to transform data into images? From a most abstract point of view, we can think of this process as a parametric transformation v that takes data and parameters as its input and generates an image as its result [JMG07]: v :D×P→I By invoking I = v(D,P), some data D ∈ D are transformed to an image I ∈ I, where P ∈ P is a parameterization that controls the transformation. Let us next dig a bit deeper into what v does internally and what changes D will undergo. Models that can help us in this regard are the visualization pipeline [HM90] and the data state reference model [CR98]. They conceptually model the data-to-image transformation by defining different data stages and different types of operators. Data Stages From bits and bytes to images, data exist in various states. In order to abstract from particular state details, it makes sense to consider four basic data stages (note the difference between state and stage): • data values, • analytical abstractions, 1Forbrevity,weuseimage todenoteanyformofinteractivevisualrepresentation.
Criteria, Factors, and Models (cid:4) 45 • visual abstractions, and • image data. The data values are typically raw and unprocessed digital pieces of infor- mation.Analyticalabstractionsarewell-structureddatameaningfullyenriched withderivedcharacteristics.Thisincludesdatatables,hierarchicallystructured levels of detail, and higher-order abstractions such as classifications or clusters. Visual abstractions model the visual appearance of the data by means of geometric primitives and corresponding visual attributes such as fill color or stroke style. Finally, image data describe the colored pixels to be shown on the output device. Thedifferentdata stagesareillustrated asboxesin Figure2.14. Thecircles inthefigurerepresentoperatorsthatdotheactualdataprocessingasdescribed next. Preprocessing Mapping Rendering Operators Operators Operators Data Analytical Visual Image Values Abstractions Abstractions Data Value Analytical Visual Image Operators Operators Operators Operators Data-oriented Graphics-oriented Figure 2.14 Data-oriented and graphics-oriented stages and operators. Adapted from [Chi00]. Operators An operator processes some type of input in order to produce some type of output. In the light of the above data stages, we can distinguish two categories of operators, transformation operators and stage operators. As illustrated at the top of Figure 2.14, transformation operators take data from one stage and generate data of another stage. That is, the type of input an operator accepts and the type of output it produces belong to different stages. The data transformation happens in three fundamental steps: preprocessing, mapping, and rendering. These three steps form the so-called visualization pipeline. Along the pipeline, data-oriented operators carry over their results to graphics-oriented operators, as indicated by different colors in Figure 2.14. For example, relevant wind features are extracted from radar data (preprocessing),thencolor-codedontoacartographicterrainmodel(mapping), and then finally displayed on a stereoscopic projector (rendering).
46 (cid:4) Interactive Visual Data Analysis For stage operators, as illustrated at the bottom of Figure 2.14, the type of input and output are within the same stage (not necessarily of the same type). Operators that handle data values are usually responsible for data cleansing and format conversions. At the stage of analytical abstractions, further data processingcantakeplace.Forexample,metricsmaybecalculatedtodetermine thequalityofclusters,orahierarchicalstructureofdifferentlevelsofdetailmay be established. Stage operators on visual abstractions support the adaptation of the visual representation, for example, by re-arranging visual elements or by reducing their numbers so that they fit the display. Finally, image operators may modify the graphical output. They can blur certain parts of the image to attenuate them, or enhance contrast and lightness of other parts for emphasis. Withthehelpofoperatorsanddatastages,wecannowdefinetheinternals of our previously sketched mapping v :D×P→I as a network of interlinked processing steps that transform data from one state to another across several stages as illustrated in Figure 2.15. It is this network that needs to be set up when designing interactive visual analysis solutions. As mentioned earlier, parameters are used to adjust and control the data transformation.Whilenotbeingdirectlyvisibleinourexample,theparameters are essentially part of the operators. Each operator in the network can make parameters available. These can then be set interactively by the user via a suitable user interface, or they can be set automatically based on the output of operators of the network. Defining reasonable default parameterizations is also a task to be dealt with during the design phase. Finally, note that the presented model is not restricted to a single input being transformed to a single output. In practice, it is not uncommon to deal withseveraldatasources,andthedataoftencontainsomuchinformationthat multiple images are needed to represent them in an understandable way. In our example in Figure 2.15, D consists of two data sources and I comprises actually two visual representations. Data Values Analytical Abstract. Visual Abstractions Image Data Operator State Data-oriented Graphics-oriented Figure 2.15 A network of operators describes the data’s transformation through several stages from data values to image data.
Criteria, Factors, and Models (cid:4) 47 Action Analysis Hypothesis Solution Knowledge Insight Knowledge Finding generation Exploration loop Verification loop loop Figure2.16 Knowledge generation model. Adapted from [Sac+14]. We have learned how data analysis solutions can be modeled as operator networks that transform data to interactive visual representations. In Chap- ters 3 to 5 of this book, we will learn more about concrete visual designs, interactive techniques, and computational procedures. For now, let us make a switch from the internals of the data transformation to studying how using interactive visual data analysis solutions leads to new insight and knowledge. 2.3.3 KnowledgeGeneration Developing useful data analysis solutions is essentially a human-driven effort. The transformation from data to expressive images takes place in the machine. Finally, it is again the human’s turn to make sense of the depicted data. What we are actually interested in is the gain in knowledge generated by interactive visual data analysis. This gain does not pop up all of a sudden, but is the outcome of an iterative process [vWij06]. Here, we cannot delve into the intricate mechanisms of the human brain, which, as already mentioned, are not yet fully understood. Instead, we con- sider the knowledge-generation model of visual analytics, which conceptually describes the interplay of the human and the analysis solution [Sac+14]. TheanalysissolutionisshownasasingleblockontheleftsideofFigure2.16. Despite being depicted here rather abstractly, the solution internally consists of all the interactively parameterizable data abstractions, processing steps, and visual encodings as outlined in the previous section. In this section, we want to concentrate on the model’s human part as illustrated on the right side of Figure 2.16. The human is connected to the analysissolutionviathreeloops.Theseloopssuggestthatknowledgegeneration takes place at different levels of sophistication. The first loop is the exploration loop, which primarily supports exploratory goals. The loop starts with observing the output generated by the analysis solution.Interestingfindingsmadeduringtheobservationwillprompttheuser to take actions. Interesting findings can be trends or recurring patterns, but also the inability to detect anything useful can trigger actions. Actions can be understood abstractly as adjustments of the data transformation process
48 (cid:4) Interactive Visual Data Analysis in terms of configuring the network of operators or altering the operators’ parameters. At some point, when enough findings have been accumulated, the knowledge-generation process will transition into the verification loop. The verification loop models the confirmatory phase of the data analysis. It starts with describing and explaining the findings to constitute insight in the senseofameaningfulinterpretationintheapplicationdomain.Theapplication domain is also the source of the hypotheses to be tested. As we gain more and more insight, new hypotheses can be formed or existing ones be confirmed or rejected. This involves carrying out actions to collect sufficient evidence. For example, additional calculations can be performed to check whether they produce similar results, or alternative visual representations can be generated to see whether they support the same conclusions. Finally,thethirdloopistheactualknowledge-generationloop.Atthispoint, we are on the verge of turning the accumulated insight into new knowledge of the application domain. Making this final step usually requires consultation withdomainexpertstodeterminewhethersufficientlystrongevidenceexiststo trust the analysis results. If this is not the case, either additional insight must be gained from the data, or new or altered hypotheses need to be established and tested. The three loops of exploration, verification, and knowledge generation give usbutanideaofhowinteractivevisualrepresentationsofdataaretransformed into something trustworthy and valuable. In practice, the reasoning process is rather spontaneous and does not follow strict deterministic rules. During the analysis, the human constantly switches between the three loops as new findings and insight lead to the formulation of new questions. For example, new knowledge may give rise to new hypotheses to be verified. In turn, the rejection of a hypothesis might initiate further rounds of exploratory activities. Withtheknowledge-generationmodelasoutlinedabove,wehavecompleted our study of the processes behind interactive visual data analysis. The three perspectives we have taken, the design perspective, the data-transformation perspective,andtheknowledge-generationperspective,provideuswithasound overview of the interplay of human and computer efforts when it comes to setting up, implementing, and using data analysis solutions. The variety of topics touched upon in this section suggests that experts from several fields should work together in order to make the data analysis a success, including experts from cognitive sciences, visualization, human-computer interaction, data mining, and from the problem domain itself. 2.4 SUMMARY The development of interactive visual data analysis solutions is a challenging endeavor. It requires observing several aspects. In this section, we focused on criteria, influencing factors, and process models. Below, we briefly summarize the key messages of this chapter.
Criteria, Factors, and Models (cid:4) 49 Criteria We established three fundamental criteria that interactive visual data analysis solutions should satisfy. The expressiveness criterion tells us that interactive visual representations must communicate the relevant information and must afford the necessary actions to generate knowledge. The effectiveness criterion demands that analysis solutions be aligned with the human visual and motor systems in order to be able to extract information and perform interactions effectively. Finally, the efficiency criterion suggests balancing benefits and cost of the data analysis. Influencing Factors In order to develop solutions that meet the aforemen- tioned criteria, we must take several influencing factors into account. First of all, the analysis is influenced by the data we are dealing with. We discussed several characteristics of data. At the level of data values, we distinguished data domains with different scales. Entire datasets are characterized by the data structure, the data space (with dimensions and attributes as dependent and independent variables, respectively), the data size, and the data scope. We also considered additional meta-data that describe the data’s evolution. Anotherimportantinfluencingfactoristhetaskstobeaccomplishedduring the data analysis. We characterized tasks by four facets: goals, questions, targets,andmeans.Thegoalcaptureswhatthegeneralpurposeoftheanalysis is. The questions describe concretely what should be answered by the analysis. The targets tell us where in the data we should focus our analysis effort. The means suggest that tasks can be accomplished in various ways by visual, interactive, or computational methods, or combinations of them. Finally, the user and the technology are important influencing factors. With regard to the users, we must observe general human factors, individ- ual backgrounds and expertise, the application domain, and whether users work individually or collaboratively. Regarding the technology, we need to considertheavailablecomputationalresources,displaydevices,andinteraction modalities. Process Models Three fundamental processes are at work in the context of interactive visual data analysis: the design process, the data-transformation process, and the knowledge-generation process. The design process can be modeled along four nested levels, in which domainproblemsarecollected,abstractdataandtaskdescriptionsarederived, appropriate visual, interactive, and computational means are composed, and suitable algorithms are implemented. The data-transformation process describes the way of the data from their raw form to interactive visual representations. Along the path, the data pass the stages of input data, analytical abstractions, visual abstractions and image data, the former ones being data-oriented, the latter ones being graphics- oriented. A network of operators is responsible for the actual data processing and data transformations.
50 (cid:4) Interactive Visual Data Analysis Finally, we introduced a model that abstractly outlines the knowledge- generation process by means of three intertwined loops. These loops subsume exploratory and confirmatory analysis activities culminating in the creation of new domain knowledge. In summary, this chapter presented the fundamental know-how that is necessary to develop interactive visual data analysis solutions. In the following chapters, we will describe in detail visual, interactive, and computational approaches that are useful for analytic purposes. FURTHER READING General Literature: [Mac86] • [vWij06] • [War12] • [Gua13] • [Mun14] Visual Analysis Tasks: [Vic99] • [Sch+13a] • [Bre16] • [KK17] • [LTM18]
3 CHAPTER Visualization Methods and Techniques CONTENTS 3.1 Visual Encoding and Presentation .......................... 54 3.1.1 Encoding Data Values .............................. 54 3.1.2 Presentation ......................................... 62 3.2 Multivariate Data Visualization ............................ 67 3.2.1 Table-based Visualization ........................... 67 3.2.2 Combined Bivariate Visualization .................. 69 3.2.3 Polyline-based Visualization ........................ 71 3.2.4 Glyph-based Visualization .......................... 73 3.2.5 Pixel-based Visualization ........................... 75 3.2.6 Nested Visualization ................................ 77 3.3 Visualization of Temporal Data ............................ 82 3.3.1 Time and Temporal Data ........................... 82 3.3.2 Visualization Techniques ............................ 86 3.4 Visualization of Geo-spatial Data .......................... 95 3.4.1 Geographic Space and Geo-spatial Data ............ 96 3.4.2 General Visualization Strategies .................... 99 3.4.3 Visualizing Spatio-temporal Data .................. 106 3.5 Graph Visualization ......................................... 111 3.5.1 Graph Data ......................................... 111 3.5.2 Basic Visual Representations ....................... 113 3.5.3 Visualizing Multi-faceted Graphs ................... 118 3.6 Summary .................................................... 124 V ISUAL representations of data are at the heart of this chapter. Theoret- ically, there are myriad ways of mapping data to visual representations. Some will lead to nice depictions of the data and might even have some aesthetic value, whereas others might be less successful. 51
52 (cid:4) Interactive Visual Data Analysis This suggests that the visual mapping is most crucial. Different mapping strategiesleadtodistinctvisualrepresentationsthatcommunicatethefeatures of the data quite differently [Han09]: “Therepresentationeffect:Humanperformance variesenormously(10–100:1)withdifferentrepresentations.” Hanrahan,2009 Therepresentationeffect isillustratedinFigure3.1.Itshowsthreeplots,all visualize the same data, a daily time series of the number of people diagnosed with an influenza-related illnesses. Yet, the three visual representations were generated using different visualization techniques and different parameteriza- tions. Figure 3.1a shows a linear plot. We can clearly identify some peaks in time and periods of moderate and low number of cases. It seems there is no clear trend over time. But are there any cyclic patterns? To answer this question, a different visualization technique can be employed: a spiral plot. The spiral plot in Figure 3.1b uses a cycle length of 32 days. From this figure, no cyclic patterns can be discerned. So let us try a different parameterization. In Figure 3.1c, the cycle length has been set to 28 days, which amount to exactly four weeks. As the weekdays are aligned now, we can see a clear cyclic pattern: At the beginning of a week more people are diagnosed and hardly any diagnoses are reported on weekends. This pattern is impossible to discern from the linear plot. On the other hand, the exact peak times are not as easy to identify in the spiral plot. By this example with quite simple data and rather basic visualization techniques, we have seen how important the design of visual representations is. Depending on what features of the data are to be communicated to answer which questions (e.g., Where are the peaks? Or, do cyclic patterns exist?), different options may be appropriate. In this chapter, we are interested in visual representations that fulfill the criteria of expressiveness, effectiveness, and efficiency as introduced in the previous chapter. To this end, it is important to learn basic visual building blocks and understand how to compile them into visual data representations. The two key concerns when designing a visual representation is to decide how to encode data visually and how to present the data meaningfully to the user. The fundamental ideas behind visual encoding and presentation will be described in Section 3.1. Once a suitable design has been devised, it can be implemented as a visu- alization technique. Visualization techniques typically make some assumptions with respect to the data for which they are applicable. Parameters may be available to fine-tune the visual representation within reasonable bounds.
Visualization Methods and Techniques (cid:4) 53 (a)Line plot. (b)Spiral plot (cycle length 32 days). (c)Spiral plot (cycle length 28 days). Figure3.1 Illustration of the effect of different visual representations. Because there are different classes of data, there are different visualization techniques, of which this chapter offers a data-oriented overview. First, we discuss basic techniques for visualizing multivariate data in Section 3.2. Then our view will be extended in Sections 3.3 and 3.4 to techniques for temporal data and geo-spatial data, which require the communication of a temporal and a spatial frame of reference, respectively. Graph data define relations among data elements, which calls for dedicated visualization techniques as we will see in Section 3.5. In short, we broaden our view step-by-step from basic visual encodings to the visualization of multivariate data attributes A, to techniques for data with temporal context T and spatial context S, to relations among the data R. This also includes different combinations of the individual aspects.
54 (cid:4) Interactive Visual Data Analysis 3.1 VISUAL ENCODING AND PRESENTATION In1967,basedonanextensiveanalysisofvisualrepresentationsincartography, Jacques Bertin introduced the idea of data graphics expressed by marks and visual variables [Ber67; Ber83]. Marks serve as the carriers of information. They are distinguished by their dimensionality. There are 0D points, 1D lines, 2D areas, and 3D bodies. Marks are the basic building blocks of visual representations. The actual information is conveyed via visual variables such as position, shape, or hue, which control the marks’ visual appearance. In other words, visualizingdatameanscreatinggraphicalprimitivesandspecifyingtheirvisual appearance according to the underlying data values. This concept of data graphics is still the fundamental basis of visualization. 3.1.1 EncodingDataValues Letusdiscussthevisualencodingwithvisualvariablesinmoredetail.Avisual variable can be understood as a graphical property that can be varied within a reasonable range. Different variations of a visual variable can be perceived as different. For example, we can discern position, shape, or hue of a mark. Originally introduced by Bertin, the concept of visual variables has later been refined and extended in several ways [Mac86; Mac95]. Figure 3.2 illustrates a selection of visual variables commonly applied today. Position Lightness Length Saturation Area Hue Angle Shape Figure3.2 Visual variables. With the help of visual variables, we can define the visual encoding, that is, how the data are represented visually. In this regard, two questions must be addressed: • What to map? • How to map?
Visualization Methods and Techniques (cid:4) 55 Quantitative Data Ordinal Data Nominal Data Position Position Position Length Lightness Shape Angle Saturation Hue Area Hue Lightness Lightness Length Saturation Saturation Angle Length Hue Area Angle Shape Shape Area Figure3.3 Effectiveness ranking of visual variables. Adapted from [MW14]. WhattoMap? First,wemustdecidewhichdatavariablesaretobemappedontowhichvisual variables. For answering this question, we need to consider the analysis task at hand, in particular the task’s target as discussed in Section 2.2.2. It tells us which data variables are deemed relevant and hence should be mapped onto the most effective visual variables. The question that remains is which visual variables are the most effective? As we will see next, the answer depends on both, human perception and the data domains introduced in Section 2.2.1. With the help of perceptual studies, researchers found that visual variables are differently effective for nominal, ordinal, and quantitative data [CM84; HB10]. Visual variables that allow us to make precise distinctions are particu- larly suited to encode nominal data. Visual variables that facilitate a visual ordering are well suited for ordinal data. For quantitative data, we need visual variables that support the estimation of proportions or differences. An effectiveness ranking of visual variables is illustrated in Figure 3.3. Interestingly, encoding to position is effective for all data domains. On the other hand, a visual variable’s rank may change drastically across the different data domains, as it is for instance the case for hue and shape, which are best applied for nominal data but are less or not effective for ordinal and quantitative data. HowtoMap? Once we have decided what to map, we must think about how the data are to be mapped onto a visual variable. As already indicated in Chapter 2, the task and the data at hand influence the mapping strategy. Figure 2.8 back on page 29 already illustrated the utility of different color mappings for different tasks. In the following, we will briefly look at further ways of tuning the visual mapping to the task and the data. For the sake of brevity, we will concentrate on the use of colors. Identifyingandlocatingdatavaluesonthedisplayarefundamentalanalysis tasks. In order to support identification tasks, perceptual color maps should be
56 (cid:4) Interactive Visual Data Analysis Individual Values Classes Identify Locate Figure3.4 Color maps for identifying and locating values and classes. 2015 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2015 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2014 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2014 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2013 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 2013 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec (a)Color coding for identification tasks. (b)Color coding for location tasks. Figure 3.5 Applying the color maps from Figure 3.4 to temperature data. Adapted from bl.ocks.org/mbostock/4063318. used. Such color maps guarantee that perceived color differences are propor- tional to the underlying differences in the data [BRT95]. Ideally, there should be a clearly perceptible color for each value in the data. Certainly, the degree to which this ideal can be reached is limited by the output device and human perception. Forlocationtasks,thegoalistodeterminewhereinthevisualrepresentation a certain value or subrange of interest is located. This is supported by color maps that let the relevant data stand out. Again, the characteristics of human perception need to be considered. Particularly helpful are visual encodings that are pre-attentive [HE12]. For example, the relevant data are encoded with a signal color, whereas less-relevant parts of the data are visualized with dimmed colors. Another important factor relates to the question whether the analysis targets individual data values or classes of values. Continuous color maps are a sensible option for individual values. Segmented color maps are suitable for classes of values. Figure 3.4 provides a schematic illustration with color maps for identifying and locating individual values and classes of them. In Figure 3.5, we applied two of these color maps for the visualization of three years of daily temperatures. Figure 3.5a uses the continuous color map to supporttheidentificationofdatavalues,whereasFigure3.5busesasegmented color map to support the locating of a particular class of values, days with temperatures between ten and twelve degrees Celsius in our case. What we can see from these figures is how substantially different the data look like when using different color maps. What we can’t see is how the mapping works internally. This will be explained next.
Visualization Methods and Techniques (cid:4) 57 Data range d min d val d max Normalized range 0 t 1 Visual range v v v min val max Figure3.6 Basic mapping of a data variable onto a visual variable. Typically, the entire data range from the minimum value to the maximum valueismappedontotheentirespectrumofavisualvariable.Alinearmapping as expressed by the following formulas is common to assign a visual value v (a color in our case) to a data value d . In a first step, it is practical to val val calculate a normalized data value t∈[0,1]: d −d t= val min d −d max min This normalized t is then used in a convex combination to compute the actual visual representative v from the visual range [v ,v ]: val min max v =(1−t)·v +t·v val min max Figure 3.6 illustrates the basic mapping of a quantitative variable onto a continuous color map. Note that for qualitative data, it is necessary to first map categorical or ordinal values to numbers, for which dedicated methods exist [Ros+04]. The distinction between quantitative and qualitative data is not the only factor that influences the mapping strategy. In the following, we will illustrate this with additional examples. Asafirstexampleofadata-dependentadaptation,letusconsiderthescales showninFigure 3.7a.Scalesplayanimportantrolewhenitcomestointerpret- ingvisualrepresentations,thatis,whentheuserperformstheinversemapping of color to data. Plainly using the data’s actual minimum and maximum often leads to situations where the interpretation is unnecessarily complicated. As can be seen in our figure, expanding the mapping range [d ,d ] to multi- min max ples of two or powers of ten can significantly ease the interpretation [JTS08]. Several algorithms exist to create similar results [TLH10]. In addition to the data’s minimum and maximum, also the magnitude spanned by the value range influences the visual mapping. The basic mapping introduced before works well in many cases. However, for large value ranges that cover several magnitudes and include small as well as large values, a linear mapping could be ineffective. In such cases, it makes sense to use a non- linear mapping. Figure 3.7b compares a linear-scale mapping with a log-scale mapping. For the logarithmic variant, the calculation of t changes as follows: log(d −d ) t= val min log(d −d ) max min
58 (cid:4) Interactive Visual Data Analysis 294 300 10000 10000 400 400 197.8 200 8000 1000 320 320 280 101.6 100 6000 100 240 5.4 0 4000 10 160 120 -90.8 -100 2000 1 80 30 -187 -200 0 0 0 0 (a)Value range expansion. (b)Logarithmic mapping. (c)Box-Whisker mapping. Figure3.7 Enhanced data-dependent visual mapping. Variable A 0 20 Data ranges Variable B 10 50 Combined color map 0 50 Figure3.8 Combined color map for comparing two data variables. In situations where the analysis is focused on semantically meaningful ranges of values, it can even make sense to go beyond linear or logarithmic mapping.Figure3.7cshowsabasicmappingincontrasttoamappingthathas been adapted based on the data’s underlying distribution. In the example, the distribution is approximated via quartiles and inter-quartile ranges in the data as indicated by the box plot [Tuk77]. As a result, we obtain a visual mapping that emphasizes unusually low and high data values and outliers with shades of blue and red, whereas common values are represented by neutral white. As a last example, let us consider the visual mapping when the task is to compare data, for example, two data variables or data given at two time steps. The challenge for the visual mapping lies in balancing two conflicting needs. Ontheonehand,theindividualsetsbeingcomparedshouldbedistinguishable and values should be identifiable. This calls for a separate visual encoding per set being compared. On the other hand, the sets must be comparable. However, this is usually not the case when using separate visual encodings. Instead, a common encoding must be employed for all data, so that the same data value, no matter with which data variable or time step it is associated, is always visualized with the same visual stimulus. However, this could lead to local data variations being washed out in the visual representation. One way to address these conflicting needs is to combine multiple color maps into a dedicated comparison map [TFS08b]. Figure 3.8 illustrates this
Visualization Methods and Techniques (cid:4) 59 for comparing two data variables A and B. Variable A has lower values and is mapped using variations of green, whereas variable B has larger values represented by variations of blue. Shades of gray indicate the range of values thatbothvariableshaveincommon.Withthehelpofthecombinedcolormap, comparison is possible, while individual values are still distinguishable. Insummary,weseethatthedecisiononhowtomapdatavaluesontovisual variables must be done with care. What has been said here about color coding applies analogously to other visual variables. More details about color coding in general and specifically for different tasks and data characteristics can be found in the further readings at the end of this chapter. In the following, we continue with advanced mapping strategies in which multiple visual variables are employed. UsingMultipleVisualVariables A 1-to-1 visual mapping is the standard approach to visual encoding: A single data variable is mapped onto a single visual variable. Yet, it is also possible to utilize n visual variables simultaneously. There are two variants of this approach. A 1-to-n visual mapping takes advantage of the combined power of n visual variables for the encoding a single data variable. When multiple data variables are to be visualized, an n-to-n visual mapping has to be designed. Note that n-to-1 visual mapping makes no sense, because it would no longer be possible to unambiguously interpret a visual stimulus. 1-to-n Visual Mapping In general, we can use several visual variables to encode a data value. For example, we can set a mark’s position, size, and color according to one and the same data value. This typically makes the value easier to recognize. A particularly clever approach of combining n = 2 visual variables is two-tone coloring [Sai+05]. The goal of two-tone coloring is to achieve precise value recognition while keeping space demands low. Figure 3.9 illustrates the problem.Whenvisualizingdataasaplot,valuescanbereadprecisely.However, a certain amount of vertical display space is needed in order to make data values easily recognizable. Less display space is needed when the data are visualized via color coding. Yet, the data values are more difficult to read. With a continuous color map, it takes some time to figure out which value is associated with a color. With the segmented color map, only discrete classes of values can be identified. Let’s look at how two-tone coloring deals with these issues. The basic idea is to encode a data value with two colors and combine this withalengthencoding.Asshownintheenlargedviewontheright-handsideof Figure 3.9, a data value is visualized by two adjacent colors from a segmented color map, orange and red for the particular data value in our example. The colors tell us at a glance where in the value range the specific data value is
60 (cid:4) Interactive Visual Data Analysis 100 100 0 2/3 ≈ 83 0 20 40 60 80100 1/3 75 0 25 50 75 100 Figure3.9 Two-tone coloring explained. Adapted from [JTS08]. Figure3.10 Two-tone visualization of 20 years of daily temperatures. located. It is between 75 and 100. The precise value is encoded by varying the proportion of the colors, that is, how many pixels are colored with the first color and how many with the second color. In the example, 1/3 of the pixels are red and 2/3 are orange. This means the encoded value is one-third between 75 and 100, which is approximately 83. Figure 3.10 illustrates that the two-tone approach has several advantages. First, thanks to the combined use of color and length as visual variables, less display space is necessary, which allows us to overview more data. Second, colors make it easy to locate data of interest. Third, it is even possible to rather accurately identify individual values. In sum, a quite efficient visual encoding.
Visualization Methods and Techniques (cid:4) 61 05 05 150 10 0 50 0 50 (a)Mapping to position. (b)Mapping to area. 5 5 0 150 0 150 A B 10 10 C 2500 2500 0 0 0 50 0 50 (c)Mapping to color. (d)Mapping to shape. Figure3.11 Visual encoding via position, area, color, and shape. n-to-n Visual Mapping We already mentioned that it is often necessary to visualize more than one data variable. In such cases, the visual encoding can utilize several visual variables simultaneously. This shall be illustrated by the example of a scatter plot. A scatter plot consists of two orthogonally aligned axes that represent the valuerangesoftwodatavariables.Dotsareplacedinthespacespannedbythe axes in order to visualize the data elements. Conceptually, this corresponds to a mapping of data to position. A first data variable is mapped with respect to the horizontal x-axis, and a second variable with respect to the vertical y-axis. Figure 3.11a shows a basic example.
62 (cid:4) Interactive Visual Data Analysis In order to encode additional data variables, we can now employ additional visual variables. For example, we can vary the area of the dots to encode a quantitative data variable as in Figure 3.11b. With larger dots, it is also possible to use colors to visualize an ordinal data variable, which is illustrated in Figure 3.11c. We can even think of replacing the dots with different shapes to visualize a nominal data variable. In sum, Figure 3.11d now represents five data variables: two are mapped to position, one to area, one to color, and a fifth to shape. With each variable being added to the visual mapping, the richness of the visual representation is increased. Theoretically, we could add yet another visualvariable,forexample,bytexturingtheshapes.However,fromapractical point of view, there are limits. While a rich visual mapping opens up the possibility to make a wider range of analytic discoveries, the downside is that the mental effort required to digest the visual representation increases as well. Therefore, it is really important to balance the visual mapping according to the task and the data. When designing a visualization, the following quote can be useful to keep in mind [dSai39]: “Inanythingatall,perfectionisfinallyattainednotwhenthereisnolonger anythingtoadd,butwhenthereisnolongeranythingtotakeaway[..]” deSaint-Exupéry,1939 In fact, the visual encoding is one of the most crucial steps when designing visualdataanalysissolutions,becauseithasconsiderableimpactonexpressive- ness, effectiveness, and efficiency. However, a good visual encoding is only half the story. Of equal importance is an appropriate presentation of the encoding, as we will see in the next section. 3.1.2 Presentation The visual encoding produces marks whose graphical appearance meaningfully reflectsthedata.Themarksarecompiledintoviewstobedisplayedtotheuser. The question now is how to present the marks and views in such a way that thevisualizeddataareeasytounderstand.Inthefollowing,wewilldiscusskey questions that need to be taken into account when designing the presentation. Shouldthemarksbepresentedin2Dor3D?Whendealingwithlargenumbers of data elements, should we display the marks for all of them or should we concentrate on subsets of interest? And, how can we present multiple views each dedicated to showing a particular aspect, perspective, or facet of the data?
Visualization Methods and Techniques (cid:4) 63 PresentingMarksin2Dor3D The first design decision is whether to place the information-bearing marks in a two-dimensional (2D) or a three-dimensional (3D) presentation space. A 2D presentation space has two independent axes: the horizontal x-axis and the vertical y-axis. A 3D presentation space has an additional third z-axis. Both 2D and 3D representations have different pros and cons. Two- dimensionalvisualizationsarearguablymoreabstractandeasiertoexplore.On the other hand, human perception is naturally tuned to the three-dimensional world around us. Moreover, the third dimension can also serve as a carrier of additional information. However, the third dimension also introduces difficul- ties that are less problematic or do not even occur in 2D, such as occlusions and perspective distortions. There is no definite answer whether to use 2D or 3D. The decision has to be made carefully on a case-by-case basis, taking into account several factors, including the number of marks to be arranged, the analysis task, and the available display technology. Figure 3.12 shows an example 3D terrain visualization with a 2D overview map. We will return to the implications of 2D and 3D representations later in Section 3.4 when we talk about geographic visualization in more detail. Figure3.12 Terrain visualization with overview+detail. Courtesy of Martin Röhlig. PresentingAllDataorDataofInterest In order to facilitate a comprehensive understanding of the data being studied, two essential communicative goals must be supported by the presentation: 1. Convey an overall picture 2. Provide details where necessary
64 (cid:4) Interactive Visual Data Analysis The overall picture allows users to observe global patterns and general properties of the data. The details enable users to study local particularities of the data. For data of small or moderate size, overview and details for all data can be presented in a single view. However, with larger data, this becomes more and more difficult. For large data, conveying an overall picture usually requires neglecting details and giving preference to the presentation of the data as a whole. When details are to be presented, it is usually necessary to sacrifice completeness and concentrate on selected data. A basic strategy to address this problem is to distinguish between data of interest and less relevant data. The idea is that marks encoding the data of interest will be presented in full detail, while the others are reduced or simplified. Two approaches support this strategy: overview+detail and focus+context [CKB08]. Overview+Detail presents an abstracted overview of all data in one view and selected details in separate views. There can be a small overview being superimposed on a large detail view, as commonly seen for geo- graphic maps. Alternatively, a large overview can be superimposed by multiple detailed views. In Figure 3.12, the main view shows a detailed representation of the 3D terrain for a selected region, while a smaller window provides an abstracted 2D overview and highlights the region whose details are currently visible. Focus+Context integratesfocuseddetailswithinaglobalcontextinasingle view.Theintegrationcanbeachievedindifferentways[Hau06].Aclassic approach is to use distortions to magnify the focus, scale down the context,andcreateasmoothtransitioninbetween.Fish-eyedistortionis a prominent example [LA94]. Figure 3.13 illustrates a fish-eye distortion being applied to the rows of a table-based visualization of the Iris flower dataset.Ascanbeseen,thedistortionmakesindividualrowsinthefocus region easier to read and even allows text labels to be displayed. (a)Regular visualization. (b)Focus+context distortion of rows. Figure3.13 Illustration of focus+context for a table-based visualization of the Iris flower dataset. Focused rows are magnified to accommodate labels.
Visualization Methods and Techniques (cid:4) 65 Both overview+detail and focus+context are viable approaches to present- ing the overall picture and details of larger data. For overview+detail, the user has to establish links between the separate views mentally. The integrative approach of focus+context relieves the user of this task. However, interpreting a densely packed focus+context presentation might require some training, especially when distortions are involved. PresentingMultipleViewsinSpaceorTime When the data to be analyzed become more complex, it is no longer feasible to indiscriminately present each and every aspect of the data in a single view. When we reach this point, it makes sense to create several dedicated visual representations, each focused on communicating a particular aspect or facet of the data. The question is how several such views can be presented to the user in order to convey a comprehensive picture? There are two basic answers to this question: The views are presented either in parallel side-by-side or sequentially one after the other. Conceptually, this corresponds to an arrangement of views in space or a sequencing of views in time. What this means concretely will be described next. Arranging Views in Space Showing several faceted visual representations in parallel side-by-side is commonly referred to as multiple views visualiza- tion [WWK00]. Later, the term coordinated has been prepended to indicate that the views are not used in isolation but in concert [Rob07]. In particular, coordination means that interactions performed in one view are automatically propagated to all other views. An example of three different views for visualizing the same multivariate dataisshowninFigure3.14.Detailsaboutthedepictedvisualizationtechniques will be given very soon in Section 3.2. How multiple views can be employed for graphvisualizationwillbeexplainedattheendofthischapterinSection3.5.3. For the time being, we are interested in the spatial arrangement of views. Therearetwoextremalpositionswhenitcomestodefiningthespatialarrange- ment.Ontheonehand,onecoulduseafixedandprovablyefficientarrangement designedbyanexpert.Ontheotherhand,userscouldbeprovidedwiththefull flexibility to arrange views arbitrarily. Both extremes have their advantages and disadvantages, and both are actually applied in practice. An interesting intermediate alternative is to allow flexibility only within certain limits. For example, it can make sense to prohibit partial overlap of views. In other words, a view should be either visible or not. This can be achieved by partitioning of the available screen space into regions, each of which may contain one or more views. While views are allowed to be flexibly resized and moved from one region to another, the overall arrangement is constrained to remain a partition, which is free of partial overlap. Arranging multiple views on a single display is usually a task that can be accomplished with little or moderate effort. However, when views have to be
66 (cid:4) Interactive Visual Data Analysis Figure3.14 Multiple coordinated views for analyzing multivariate data. arranged on multiple displays, the costs for defining a suitable spatial layout increase significantly. In such scenarios, users should be supported by means of algorithms that automatically distribute and lay out views on the available displays. Later in Chapter 6, we will describe such an automatic arrangement approach in detail. Sequencing Views in Time The alternative to arranging views in space is to sequence them in time. That is, instead of showing multiple views simultaneously,theyarepresentedstep-by-step,oneaftertheother.Depending on how many views are shown to the user per time unit, a sequencing of views may generate the impression of a slide show or an animation. The advantage of sequencing views in time is that each view can fully utilize the display space. There is no need to divide the space among views. Obviously, sequencing views in time is particularly suited to convey temporal characteristics of data. It can also be helpful to take the user on a journey from one data facet to another. However, presenting views in quick succession to the user also has some limitations.Forexample,itcouldbedifficulttomakesenseofalltheinformation provided during a sequence of views. Especially when sequences take a long time, users may be unable to follow and could drown in an indigestible flood of visual representations. Therefore, it is mandatory to provide interactive controls to pause, slow down, reverse, and advance the presentation. In summary, this section dealt with the basics of visual encoding. We discussed how data can be mapped to visual variables of marks, and how the marks can be presented to the user. With the general aspects of visualization being clear now, we will next study how specific techniques utilize visual variables and arrange visual marks for visualizing different kinds of data. We
Visualization Methods and Techniques (cid:4) 67 will start with techniques for multivariate data, that is, data with several data attributes. 3.2 MULTIVARIATE DATA VISUALIZATION We have already seen several visualization examples in this chapter that are univariate in the sense that they show the data values of only one dependent data variable, for instance, the line plot and the spiral plot in Figure 3.1 or the temperature representation in Figure 3.10. However, inspecting only one dependent variable is often not enough. In many application contexts, it is necessary to discern and understand the interrelations and dependencies between multiple or even all of the dependent variables. To this end, we need multivariate visualization techniques. Multivariate data visualization concentrates on the depiction of the depen- dent variables of a dataset, that is, the attributes A. In the following, we will introduce fundamental classes of multivariate visualization techniques. Each class is characterized by its own fundamental visualization strategy. We will consider: • table-based visualizations, • combined bivariate visualizations, • polyline-based visualizations, • glyph-based visualizations, • pixel-based visualizations, and • nested visualizations. Independent variables, including time T and space S, and relationships R between data elements will be taken into account in later sections of this chapter. 3.2.1 Table-basedVisualization As detailed in Section 2.2.1, multivariate data are generally modeled as data tables where columns accommodate attributes, and rows represent data tuples. It is common to represent such data using tabular spreadsheets, which show data values as text in the spreadsheet cells. As such, spreadsheets are well- suited for reading and editing data values precisely. However, understanding the multivariate relations of the data is hardly possible. Moreover, the textual representationofdatavaluesrequiresconsiderabledisplayspacepercell,which limits the number of data tuples that can be shown. The simple, yet very effective idea of table-based visualization is to retain the tabular layout of spreadsheets, but to replace the textual representation of
68 (cid:4) Interactive Visual Data Analysis Figure3.15 Two-tone colored table-based visualization of the Cars dataset. Figure3.16 Table Lens with textual labels for focused data tuples. data values by a visual representation. A visual representation will not only maketheinterpretationofthedatamucheasier,itwillalsorequirelessdisplay space. Depending on the data domain of an attribute, different visual encodings are practical. A universally applied strategy is to color-code the table cells. An alternative for quantitative data is to embed bars into the table cells and to vary the bars’ length depending on the data values. It is also possible to combine length and color analogous to the two-tone approach introduced in Figure 3.9 back on page 60. An example of a table-based visualization with two-tone coloring is given in Figure 3.15. The image shows seven properties of about 400 cars in the Cars dataset sorted by miles per gallon (MPG). As we can see, table-based visualization can provide us with a general overview of the distribution of multivariate data. On the other hand, while we gain an overview, we lose details: The exact data values are no longer available as textual labels. The Table Lens is a classic focus+context technique to cope with this problem [RC94]. For a focused subset of the data, the Table Lens shows larger rows and inserts exact values in textual form. The remainder of the table, that is, the context remains unchanged. Figure 3.16 shows an example visualization of demographic data of 111 countries. The height of table rows is varied by means of a fish-eye transformation function so as to create a smooth transition between focus and context.
Visualization Methods and Techniques (cid:4) 69 A table-based visualization is also compatible with the common operations available with spreadsheets. One can reorder the table columns and sort the rows according to attribute values. While this is quite helpful for studying individualattributes,findingmultivariaterelationshipsisstillnotan easy task. The data must be sorted successively on a column-by-column basis, and the obtained results have to be integrated mentally by the user. To reduce the required effort, it makes sense to incorporate methods that sort rows based on multivariate similarities. One such method will be presented in Section 5.4.2 of Chapter 5, which is dedicated to automatic analysis support. 3.2.2 CombinedBivariateVisualization An alternative to table-based approaches is to visualize multivariate data by combining several bivariate displays. To this end, individual bivariate displays are created for all pairs of attributes (a ,a ):a ,a ∈A,i=6 j. The bivariate i j i j displays are then combined to facilitate an overview of the entire data. The combination can be a spatial arrangement or a temporal sequence of views. Spatial Arrangement Among the earliest and most widely used combi- nations of bivariate displays is the scatter plot matrix [Cle93]. For m attributes, the matrix is composed of m2−m individual scatter plots, as illustrated in Figure 3.17. There are m−1 plots per row and per column. Each scatter plot consists of two orthogonal axes that represent two attributes, and dots visualize the data tuples with respect to the two data attributes. The scatter plot at (i,j) visualizes the attributes a and a . The scatter plot at (j,i) shows the same data but with the i j axes swapped. There is usually no scatter plot at (i,i), because it hardly makes sense to plot attribute a against itself. Instead, the space is often i used to show the value distribution of a or simply just a label. i The rows and columns of a scatter plot matrix can be re-arranged to support different tasks. For example, it can make sense to bring relevant attributes to the top left or to show correlated attributes next to each other. Temporal Sequencing Instead of using a spatial arrangement as a matrix, it is also possible to create an animated sequence of scatter plots, which is known as the Grand Tour [Asi85]. The idea is to show the individual scatterplotsoneaftertheother.Thesequenceofplotsiscarefullychosen sothatthemostimportantmultivariaterelationshipsarerevealedduring the animation. When working with combined bivariate visualizations, it is important to realize that each bivariate display communicates the data values only with respect to two attributes. Hence, finding an outlier or a correlation in either bivariate display does not tell us whether the finding is indeed a multivariate
70 (cid:4) Interactive Visual Data Analysis Figure3.17 9×9 scatter plot matrix of meteorological data. Color is used to ease the recognition of data variables. feature.Inordertohelpusersdrawtherightconclusions,additionalmechanisms need to be integrated. Onesuchmechanismisinteractivebrushing & linking.Itenablestheuserto mark dots in a bivariate display, and in this way to select data tuples. All the dots that represent data values of the selected tuples are then automatically highlighted in all bivariate displays. So, if the user marks an outlier in one displayandseesthatthesubsequentlyhighlighteddotsareoutliersintheother displays as well, then it can be assumed that indeed a multivariate outlier has been found. More details about data selection and visual highlighting are provided in Section 4.4 of Chapter 4.
Visualization Methods and Techniques (cid:4) 71 3.2.3 Polyline-basedVisualization A difficulty with combined bivariate visualizations is that the connection between the individual displays has to be established by the observer mentally. That is, as the eyes move from one bivariate display to the next, the observer hastokeeptrackofthevisiteddotsinordertoformacompleteunderstanding of data tuples. Visualization techniques based on polylines aim to tackle this difficulty. The basic strategy is to create m axes, one for each attribute, and n polylines, one for each data tuple. The polyline of an m-variate data tuple is constructed as follows. For each attribute value of the data tuple, a position is computed at the corresponding attribute axis. The m positions that we obtain are then connected to form the polyline that represents the entire tuple. Thequestionthatremainsishowtheaxesarearrangedonthedisplay.Par- allel coordinates plots are known for their parallel arrangement of axes [Ins09]. For radar charts, the axes are not parallel, but emanate from a central point. Figure 3.18 illustrates both the parallel and star-shaped arrangement of axes for the same multivariate data. Axes can also be arranged in 3D, and in Section 3.3, we will see that dedicated arrangements exist for temporal data. (a)Parallel coordinates plot. (b)Radar chart. Figure3.18 Visualization with polylines across parallel and star-shaped axes. It is worth mentioning that the order in which polylines connect the axes is important, because patterns in the data can be best interpreted between neighboring axes. Let us look at some of the visual patterns that are created by polyline-style visual representations. Consider the pairs of parallel axes in Figure 3.19. The visualization shows several distinct patterns synthetically created for the purpose of illustration. The first pair of axes shows a positive correlation. How a negative correlation looks like can be seen in the second pair of axes. The third and fourth pair show a peak and a valley, respectively. Finally, there are a few local outliers in the right-most pair of axes. It is interesting to think about how the same data would look like when being visualizedasindividualscatterplots.Ifyouwanttocheckifwhatyouimagined is correct, Figure 3.20 provides the answer.
72 (cid:4) Interactive Visual Data Analysis 100 100 200 100 100 200 100 90 90 180 90 30 90 190 90 45 80 250 80 160 80 22 05 80 11 78 00 80 40 70 200 70 140 70 15 70 160 70 35 10 60 60 120 60 60 150 60 30 5 50 150 50 100 50 0 50 140 50 25 40 40 80 40 -5 40 130 40 20 30 100 30 60 30 -- 11 50 30 11 12 00 30 15 20 50 20 40 20 -20 20 100 20 10 10 10 20 10 -25 10 90 10 5 0 -30 80 Reference Posi�ve Reference Nega�ve Reference Peak Reference Valley Reference Outlier Figure3.19 Visual patterns between pairs of parallel axes. Posi�ve Nega�ve Peak Valley Outlier 200 200 1122 0505 0000 11111 02468 6800000 00 -- 1111223 - 5005050 505 111111111 123456789 000000000 1223344 5050505 50 40 -20 100 10 0 20 -- 32 05 89 00 5 102030405060708090100 102030405060708090100 102030405060708090100 102030405060708090100 102030405060708090100 Reference Reference Reference Reference Reference Figure3.20 The same data as in Figure 3.19 visualized as scatter plots. Axes-basedVisualization When thinking more deeply about scatter plots, scatter plot matrices, and parallel coordinates plots it becomes clear that they all follow the same under- lying strategy: They are all based on axes with respect to which data points are projected [CvW11]. Many other types of diagrammatic representations follow this strategy [Har96]. As such, axes-based visualization is a universal concept that is applicable to a wide range of multivariate analysis questions. A difficulty though is that tuples with the same data values are projected to the same positions. This leads to over-plotting. The problem is that we typicallycannottellhowmanytuplesarerepresentedbyalinesegment(ordot or pixel in general). This problem can be remedied in different ways. A simple option is to use transparency. Another option is to incorporate additional visual cues to visualize the frequency of data values. Histograms are quite usefulinthisregard.Theycanbeeasilyattachedtotheaxesofscatterplotsor parallel coordinates plots. Figure 3.21 shows an example. Later in Chapter 5, we will return to the problem of reducing over-plotting and clutter in visual representations by means of additional automatic computations.
Visualization Methods and Techniques (cid:4) 73 Figure3.21 Parallel coordinates with histograms showing demographic data. 3.2.4 Glyph-basedVisualization Axes-basedvisualization,asdiscussedbefore,emphasizesrelationshipsbetween two attributes. In contrast, glyph-based visualization aims to emphasize indi- vidual data elements. To this end, data elements are visualized as small self-contained graphical objects called glyphs. Each glyph visually encodes a multivariate data element, more precisely, it encodes the attribute values associatedwithadataelement.Settingupaglyph-basedvisualizationrequires three steps: 1. Glyph design 2. Attribute-to-glyph mapping 3. Glyph placement Glyph Design The design of glyphs is an intricate process [Bor+13]. The particular challenge is to encode several data attributes, although there is only very little display space available per glyph. Severalquestionsneedtobeanswered:Howmanyattributesshouldaglyph represent? How many individual values should be discernible per attribute? Which visual variables should be used? How should the visual encodings be intertwined and the overall glyph be compiled? When answering these questions, it must be ensured that a glyph can be easily perceived as a whole, and that multiple glyphs can be easily separated from each other. Only then can a glyph-based visualization be interpreted as individual data elements to be studied as such and compared to others. Figure 3.22 illustrates a couple of classic glyph designs. The autoglyph in Figure 3.22a consists of a grid of color-coded cells, one cell per attribute. Stick figures as in Figure 3.22b vary the length, thickness, color, and angle of limbs in order to visualize data. The Chernoff faces in Figure 3.22c encode data by varying the features of a human face, such as the expression of the mouth or the eyes or the shape and size of the nose.
74 (cid:4) Interactive Visual Data Analysis (a)Autoglyph. (b)Stick figures. (c)Chernoff faces. Figure3.22 Examples of classic glyphs for visualization. Good Regular Bad Figure3.23 Corn glyph for representing six ordinal data values. Attribute-to-Glyph Mapping The second step is to carefully decide which data attributes should be visualized with which visual encoding offered by a glyph. We already discussed this mapping problem in general in Section 3.1.1. For the case of glyph-based visualization, there is one aspect that needs to be considered in particular: Usually, glyphs can encode only a few distinct values due to their limited size. As a consequence, it can be necessary to reduce the attributes’ value ranges. For example, a continuous attribute may need to be discretized to only a few values before it can be mapped to a glyph. What this means shall be illustrated by a glyph for visualizing factors influencing maize harvest. The design is based on three pictorial corncobs: a particularly well-developed exemplar, a regular cob, and a withered cob as shown in Figure 3.23. These three depictions represent three ordinal values: good, regular, bad. By cutting the corn glyph into segments, it is possible to encode several data values. However, any attribute that we would like to map to a segment of the corn glyph must first be made compatible with the ordinal scalethattheglyphcandisplay.Thiscanbedoneinaclassificationstepwhere numeric data values are assigned to classes. Glyph Placement Finally, the placement of glyphs is important. Placing glyphs on grids is useful in general. Figure 3.24a provides an example with data about the resistance of bacteria against eight antibiotics. The binary data (resistent, non-resistent) are represented as a grid of autoglyphs, where black
Visualization Methods and Techniques (cid:4) 75 (a)Autoglyphs as grid (aka. shape coding). (b)Corn glyphs on a map. Reprinted from [SM00]. Reprinted from [NSS05]. Figure3.24 Examples of different placements of glyphs. means resistant and white means non-resistant. Such representations provide a nice overview of the data, which makes it possible to spot patterns, as for example, the glyphs showing no resistance at all (all cells are white) or glyphs showing a T-shape. Glyphs can also be placed more flexibly according to data criteria. For example, the glyph placement can be computed based on a semantic grouping of data elements [War02]. If spatial dependencies exist in the data, glyphs are typically arranged so as to communicate these dependencies. Figure 3.24b showsanexamplewithcornglyphsplacedonamap[NSS05].Theuseofglyphs for data with spatial dependencies will be discussed further in Section 3.4 3.2.5 Pixel-basedVisualization In contrast to glyph-based visualization, pixel-based visualization appears to be a minimalistic approach. The key idea is as simple as to create a representation where each pixel visualizes exactly one data value by its pixel color [KK94]. Yet, what appears to be minimalistic is quite the opposite. Pixel- based representations are very compact, which makes it possible to display millions of data values. Three design questions need to be answered for pixel-based multivariate data visualization: • How should individual data values be mapped to color? • Where should the pixels for multiple attributes be located? • How should the individual pixels be arranged? The first question is particularly important. A pixel is the smallest possible graphical primitive on a computer display. In fact, a pixel is so small that identifyingitscolorcanbedifficult.Moreover,apixelistypicallynotperceived
76 (cid:4) Interactive Visual Data Analysis Avg. temp Min. temp Max. temp Wind speed Pressure Precip. January February March April May June July August September October November December 1893 2000 1893 2000 1893 2000 1893 2000 1893 2000 1893 2000 -21.4 -26.8 -16.6 0.0 9554.0 0.0 -10.3 -14.5 -6.3 1.1 9747.5 0.5 0.9 -2.2 4.0 2.2 9941.1 1.1 16.5 11.9 22.3 7.0 10123.7 5.4 23.1 16.7 30.7 16.2 10267.4 55.6 29.6 21.5 39.1 25.3 10411.0 105.7 Very low Low Mean High Very high Figure 3.25 Pixel-based visualization of daily values of six meteorological attributes collected for more than hundred years in the city of Potsdam. Courtesy of Thomas Nocke. in isolation, but together with its surrounding context. As a consequence, our perception is influenced by contrast effects. Therefore, pixel-based approaches should employ color maps with well-discriminable colors. The second question is about deciding where the pixels of a particular attributeshouldbeshown.Acommonapproachistopartitionthedisplayinto regions, each being associated with one attribute. Figure 3.25 demonstrates this for meteorological data with 6 attributes. For each attribute, there is a separate rectangular region filled with colored pixels. Finally,thethirdquestionregardsthearrangementofpixelswithinaregion. In general, there are different options for arranging the pixels: • Element-wise: The pixels are simply arranged in the same order as data tuples appear in the dataset. • Pre-determined: The pixel arrangement is determined by an independent variable of the dataset. For example, temporal dependencies can be revealed by arranging pixels with respect to time. • Attribute-based: The pixels are sorted according to the data values of a particular attribute. Multivariate correlations can be revealed by such attribute-based arrangements.
Visualization Methods and Techniques (cid:4) 77 Selec�on Year Quarters Months Weeks Days Figure 3.26 Exploding a pixel-based visualization by applying a stepwise separation of a time-oriented recursion pattern. Adapted from [LS07]. • Query-based: The pixels are arranged according to the distance of their associated data tuple to a query specification. This way, relevant values (with respect to the query) are grouped and appear at the front of the arrangement. OurexampleinFigure3.25usesapre-determinedarrangementwithrespect totime.Wecaneasilyseethatthefirstthreeattributesarestronglyinfluenced by the seasons of the year. The fourth and fifth attributes show a dependency on seasons as well, though less pronounced. The last attribute seems to be independent of time. Apparently,usingpixelasinformation-bearingprimitivesisverywellsuited to create overviews of multivariate data. Yet, details are not so easy to access. One way to remedy this is to use exploded views, which are widely applied in the context of technical drawings. The idea is to add details to complex graphics by separating individual parts, showing these parts by enlarged views, and enriching them with annotations. As illustrated in Figure 3.26, the idea of exploded views can also be applied to pixel-based visualization [LS07]. In the example,auser-selectedpartofarecursivetime-orientedarrangementofpixels isexploded.Asteppedanimationshowsasequenceof“explosions”thattransfer the compact and dense pixel selection into a calendar-style representation. Details are now much easier to discern. 3.2.6 NestedVisualization The final class of techniques for multivariate data visualization to be described in this section is nested visualization. The previous example from Figure 3.26 already involved a nested recursive arrangement of pixels, whereas the other visualization techniques discussed so far had a flat arrangement. The basic idea of nested visualization is to divide the attribute space into subsets and to spatially nest the subsets on the display. This involves two steps:
78 (cid:4) Interactive Visual Data Analysis 1. Definition of attribute subsets and nesting order 2. Embedding of subsets within subsets Definition of Attribute Subsets and Nesting Order Inthefirststep,three aspectshavetobeconsidered.First,thedatadomainoftheattributesmustbe madecompatiblefornesting. Nested visualizations require nominal, ordinal,or discrete data domains of small cardinality. Otherwise, a nesting is impractical or cannot be carried out at all. Second,theattributesubsetshavetobespecified.Allsubsetsmusthavethe same size. Typically, there are not more than three attributes per subset. It is importanttonotethatdifferentsubsetsleadtodifferentvisualrepresentations, which in turn, communicate different aspects of the data. Third, the nesting order of the subsets must be defined. In other words, it has to be declared which subset should be nested into which other subset. The nesting order determines which attributes are primary in the visual representationandwhicharesubordinate.Usually,thenestingisrathershallow with hardly more than four levels. The reason is that the deeper the nesting, the harder it is to interpret the visual representation. Embedding of Subsets within Subsets Once the attribute subsets and their nesting order have been set up, the next question is how to embed them spatially on the display? Usually, the embedding depends on the size of the attribute subsets. Different techniques exist for subsets with one, two, and three attributes. Mosaic plots are suited for subsets with only one element, for example (a ),(a ),.... For the first step, the display area is split along the horizontal 1 2 axis into rectangular segments. The number of segments corresponds to the numberofdistinctvaluesofa ’sdomainandthesizeofthesegmentsrepresents 1 the frequency of the values in a ’s value range. For the second step, each of 1 the rectangular segments is split along the vertical axis with respect to a . 2 The procedure of splitting along the horizontal and vertical axes continues for all attributes in the nesting order. At the end, the rectangular segments of a mosaic plot represent the value distribution of the data. Figure 3.27 shows an example with the distribution of surviving passengers of the Titanic with respect to class and sex. Ifthenestingorderconsistsofpairsofattributes(a ,a ),(a ,a ),...,then 1 2 3 4 the embedding scheme of dimensional stacking can be used [LWW90]. The basic idea is to embed grids within grids. The dimensional stacking starts with dividing the display into a grid of |a |×|a | uniformly sized cells, where 1 2 |a | denotes the number of distinct values in a ’s domain. The next step is i i to subdivide each grid cell into a new grid of dimensionality |a |×|a |. The 3 4 procedure continues for all pairs of attributes. In order to actually visualize multivariate data, the grid cells are color-coded according to the frequency of data tuples that exhibit the value combination of the corresponding cell.
Visualization Methods and Techniques (cid:4) 79 ts1 dn2 dr3 werC Sex Male Female ssalC oN seY oN seY oN seY oN seY devivruS Figure3.27 Mosaic plot visualizing the survival of passengers of the Titanic. As an example of dimensional stacking, Figure 3.28 visualizes data about resistance to antibiotics as mentioned earlier. The data consist of eight binary attributes indicating non-resistance or resistance. We can easily see that most of the cells are empty, meaning that no data tuples exist that exhibit the correspondingresistancepattern.Wecanalsoeasilyidentifythefrequentvalue combinations indicated in red and the most frequent resistance pattern in the top left corner, which represents no resistance at all. Withmosaicplotsanddimensionalstackingwehavediscussedstrategiesfor attributesubsetswithoneandtwoattributes,respectively.Iftheattributesub- sets are triples, the worlds-within-worlds approach can be used for embedding data into a 3D display space [FB90]. A nesting order with triples could look like this: (a ,a ,a ),(a ,a ,a ),.... The three attributes of the first subset 1 2 3 4 5 6 definethethreeaxesofa3Dcoordinatesystem.Withinthiscoordinatesystem, a particular point is fixed by user selection. This point serves as the origin for embedding a new 3D coordinate system spanned by the second triple of attributes. The process of fixing a point and embedding a coordinate system is repeated for all triples. The coordinate system defined in the last step serves to display the actual data values of the attribute subset at the end of the nesting order. In other words, only three attributes are visualized directly, while the values of the
80 (cid:4) Interactive Visual Data Analysis a a a a a 7 n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. n-r. r. 8 6 4 2 a non-res. resistant non-res. resistant non-res. resistant non-res. resistant 5 a non-resistant resistant non-resistant resistant 3 a non-resistant resistant 1 Figure3.28 Dimensional stacking of eight binary data attributes. Adapted from [SM00]. remaining attributes are expressed by the nesting process. As such, the worlds- within-worldsapproachrepresentsonlyaparticularpartofthedataasspecified by the sequence of coordinate origins. From the previous paragraphs we see that nested visualization of multi- variate data is a powerful approach. However, the visual representations are not trivial to construct, and hence, require some training in order to be able to interpret them. Summary With the description of nested techniques, we conclude this section on visu- alization of multivariate data. As we discussed quite different visualization strategies for multivariate data, let us briefly summarize them. • Table-based visualization transforms common spreadsheets into visual representations. Sorting the table rows allows us to detect multivariate correlations, outliers, and clusters. A focus+context display of the table rows makes it possible to identify individual data values. • Combined bivariate visualization is suited to communicate two- dimensional value distributions, bivariate correlations, clusters, and out- liers. Multivariate relationships can be explored with interactive linking & brushing. Additional visual encodings can be used to represent the frequency of data elements as well.
Visualization Methods and Techniques (cid:4) 81 • Polyline-based visualization shows data elements as polylines across axes. The exploration of two-dimensional value distributions between two neighboring axes is well supported. The analysis of all bivariate relationships in the data requires rearranging the axes. Data frequencies can be visualized by embedding histograms. • Glyph-based visualization encodes data tuples as small glyphs. The goal is to provide an overview of the data, while details are typically omitted. As such, glyphs facilitate a rough estimation and comparison of the properties of data elements. Glyphs are also suitable for visualization of data in a spatial frame of reference. • Pixel-based visualization approaches encode each data value by a single color-coded pixel. Thanks to this very space-efficient encoding, even very large datasets can be visualized. Yet, details cannot be discerned easily, unless additional interactive or visual means are employed. • Nested visualization is based on a recursive embedding of attribute subsets on the display. The resulting visual representations are very well suited to communicate frequencies of the different value combinations in the data. Yet, the interpretation of nested visualizations is not easy, particularly when the nesting is deeper. Intermsofthebasicvisualdesignoptionsdiscussedinthebeginningofthis chapter, we can conclude the following. The described visualization strategies for multivariate data typically use spatial arrangements in 2D. There are techniques that use 3D spatial arrangements, such as the worlds-within-worlds approach, and also techniques that generate an animated sequence of views, such as the grand tour, but in general 2D spatial arrangements are much more widely used. Already at this point, we can further conclude that visualization requires interactiontotakefulladvantageofthepowerofvisualrepresentations.Interac- tionallowsus,forexample,toidentifydataondemandortoexplorealternative correlations through rearrangements. In fact, interaction is so important that the entire Chapter 4 is dedicated to this topic. Moreover, we have mentioned that certain visualization strategies can only be applied when the data domain contains not too many values. We also briefly mentioned the problem of over-plotting when there are very many data elementstobevisualized.Inthesecases,additionalcomputationsarenecessary to condense the data before they can be transformed into meaningful images. We will discuss this topic in more depth in Chapter 5. All in all, this section on multivariate data visualization focused on the dependentvariables,theattributesA.Inthenexttwosections,wewillconsider the independent variables time T and space S.
82 (cid:4) Interactive Visual Data Analysis 3.3 VISUALIZATION OF TEMPORAL DATA Time is an exceptional dimension. Virtually everything around us is governed by the steady progress of time. So it comes as no surprise that much of the data that people seek to understand are connected to time. In this section, we add the dimension of time T to our considerations. In other words, we are interested in methods and techniques for visualizing time and temporal data, where the latter primarily means communicating the dependency of data attributes on time T → A. As we will see in a moment, time and data that depend on time are quite special and require dedicated techniques for their visualization. 3.3.1 TimeandTemporalData Time is not just another data attribute. Time has several properties that need to be taken into account when visualizing data that are connected to time. In the pages to come, we will briefly characterize time and temporal data. CharacterizingTime Philosophers have pondered the concept of time for ages. Here we want to concentrate on aspects of time that are relevant for the visual analysis of temporal data. Before going into the details of time, there is a more general aspect to be mentioned: Analog to the different data domains discussed in Section 2.2.1, the time domain can be characterized as ordinal, discrete, or continuous. For ordinal time, only equality and partial order relations are defined. For discrete time,amappingexitsfromthetimedomaintothesetofintegers,whichmakes it possible to measure distances in time. Continuous time conceptually maps tothesetofrealnumbers.Assuch,continuoustimeisdense,which,ingeneral, complicates the data handling and visualization. Let us next look at the specifics of time in more detail. In particular, we will deal with four aspects: primitives, arrangement, granularity, and structure of time. Primitives In the first place, we have to think about temporal primitives. Temporal primitives serve as anchors to pinpoint certain events or phenomena in time. Two different types of temporal primitives can be distinguished: instants and intervals. A time instant is a singular point in time. An instant is assumed to have no temporal extent. An example of a time instant would be 19:30 o’clock, and we could use it, for example, to agree on a specific time to meet for dinner with friends. A time interval is a temporal primitive with an extent. An interval can be defined explicitly by two time instants: the interval’s start and end. Alter-
Visualization Methods and Techniques (cid:4) 83 A equals(A, B) B A A before(A, B) meets(A, B) B B A A A equals(A, B) overlaps(A, B) starts(A, B) B B B A A A before(A, B) during(A, B) finishes(A, B) B B B (a)Instant relations. (b)Interval relations. Adapted from [All83]. Figure3.29 Temporal relations for time instants and time intervals. natively, an interval can be defined with a start instant and a duration. For example, when we mark the dinner in our electronic calendar, we may reserve a slot that starts at 19:30 and lasts for 2 hours. The distinction of temporal primitives is important, because different rela- tions may exist between instants and intervals. As illustrated in Figure 3.29, particularly the relation between intervals can be versatile [All83]. Because understanding temporal relations is a relevant analysis objective, visual repre- sentations should be designed so as to enable the observer to identify them. Arrangement The arrangement of time basically describes how temporal primitive are laid out in time. The arrangement can be linear or cyclic. In correspondence with our natural perception of past, present, and future, time can be considered as a linear arrangement of temporal primitives. For linear time, we can clearly denominate a temporal primitive as being before or after another one. We may also quantify how much time has elapsed. Cyclic time is based on recurring temporal primitives. Many phenomena follow the natural cycles on Earth, such as the seasons of the year or the hours oftheday.Forcyclictime,anyprimitiveisprecededandsucceededatthesame time by any other primitive. For example, winter comes before summer, but winter also succeeds summer. A common data analysis objective for unknown data is to figure out whether there are recurring cycles in the data or not. Granularity Time is usually measured with respect to a smallest possible unit. If there is only the smallest possible unit, we can say that the time domain has a single granularity only. The granularity of time is also closely related to the size of temporal data. The finer the granularity is, the larger we can expect the data to be. A typical example would be simulation data that are given at sub-second granularity. Inmanyapplications,itispracticaltoconsidertimewithrespecttomultiple granularities, where smaller units are compiled into larger ones. Human-made
84 (cid:4) Interactive Visual Data Analysis calendar systems are an example for such multi-granularity time models. They can serve as a natural scaffold for multi-scale data analysis. Studying temporaldataatafinergranularitymayrevealsubtledetails,whereasacoarser granularity would convey overall trends or rough estimates. Structure Finally, the structure of time is relevant [Fra98]. Our regular understanding of time is that it progresses in an ordered fashion. There is a single thread of time on which things happen one after the other, and what has happened cannot be changed or undone. If the exact path through time is unknown, for example, in planning and predictionscenarios,abranching timestructureunfolds.Eachbranchdescribes a possible development in time, of which only one will eventually become true. If multiple paths through time co-exist in parallel, the conceptual structure of time is denoted as multiple perspectives. This type of structure helps, for example, when analyzing eyewitness reports, where each person has their own perspective on the observed reality. It is obvious that branching time and multiple perspectives require more resources for their visualization than ordered time, simply because more information needs to be encoded visually. The uncertainty inherent in the different time structures has to be communicated as well. As we see, time is more than a simple linear succession of consecutive moments. It makes a difference if we are dealing with instants or intervals in time, if time is linear or cyclic, if a single or multiple granularities are given, or if time runs in an ordered, branching, or parallel fashion. All of these aspects need to be considered when designing visual representations of time and temporal data. A schematic overview of the discussed aspects is given in Figure 3.30. After this brief characterization of time itself, we will next take a closer look at how data can be linked to time. TIME Primitives Instant Interval Arrangement Linear Cyclic Granularity Single Multiple Structure Ordered Branching Multiple perspectives Figure3.30 Aspects of time to be considered when visualizing temporal data.
Visualization Methods and Techniques (cid:4) 85 CharacterizingTemporalData In general, temporal data are data with references to time. Depending on whether the values in the data vary with respect to time T, the data are denotedastemporal ornon-temporal.Ifadatasetasawholechangesovertime, that is, there are several versions V of the data, then the data are denoted as dynamic. If the dataset is fixed, meaning there is only one version, it is said to be static. These denominations lead to a categorization of different types of data: Static, non-temporal data do not vary over time at all. The well-known Iris flower dataset is an example of data that are completely independent from time. Static, temporal data can be considered a historical view of how an observed phenomenon evolved during a certain period of time. Common time series are a typical example. They contain time-varying data values, but the dataset itself does not change after it has been created. Dynamic, non-temporal data are data that change over time, but only a single snapshot of time is available. In other words, we have a continuous stream of data, but without a history. Such data can often be found in monitoring scenarios, for example, when the current state of some machinery is continuously visualized in a control room. Dynamic, temporal data are data whose values vary over time, and the state of the dataset changes over time as well. Such bi-temporally depen- dent data constantly evolve and also maintain a history. An example is meteorological data, which contain time-dependent measurements and are updated regularly as new measurements become available. An illustration of the different types of temporal dependencies of data is given in Figure 3.31. Non-temporal data can be visualized with general techniques for multivariate data as introduced in the previous section. A peculiarity of dynamic data is that the visualization has to keep up with the changes in the data. If dynamic data additionally preserve a history (dynamic, temporal data), often only a small time window of it can be visualized due to the flood of information that needs to be processed. Itshouldbementionedthatthedistinctionofthedifferenttypesoftemporal dependenciesofdataisinspiredbydatabaseterminology[Ste98].Inthecontext ofvisualization,suchaclearnotationhasnotyetgainedwidespreadacceptance. Hence, one may find a mix of alternative terms, such as time-varying data, time-series data, or dynamic data, but most of the time the data are in fact static, temporal data. In the following, we will concentrate on the visualization of such static, temporal data, because this is the class of data that is most relevant in many application domains.
86 (cid:4) Interactive Visual Data Analysis laropmet-non laropmet static dynamic T T t t 0 0 V V now now T T t t 2 2 t t 1 1 t t 0 V 0 V now now Figure3.31 Types of data with references to time. Adapted from [Ste98]. 3.3.2 VisualizationTechniques In general, any visual variable can be used to encode time and its associated temporal data, where the visual design choices apply as introduced in Sec- tion 3.1. A key characteristic of visualization techniques for temporal data is if the visual representation is static or dynamic. A static representation does not change over time. It remains fixed and providesanoverviewofthetimeaxisinasingleimage.Themostcommonvisual encoding is based on mapping time to space, which means that the temporal primitivesareplacedatdifferentlocationsonthedisplay.Aprominentexample of a static time-to-space visualization is small multiples [Tuf83]. The idea is to visualize the data as a series of miniaturized, high-density, non-overlapping views that are arranged according to time. The views are of equal size and use the same visual encoding, yet each view shows the data for a different temporal primitive. Figure 3.32 illustrates the small-multiples approach. The figure makes clear that static representations can provide a nice overview of the data. In contrast to static representations, a dynamic representation changes over time. This corresponds to a mapping of time to time, more precisely, of time in the data to time of the presentation (or wall-clock time). In other words, the temporal data are shown as an animated sequence of individual images. Dynamic representations can communicate the time-varying behavior of the data very well. However, at any time during a dynamic representation only a single moment of the temporal data is visible, and it is immediately overwritten by the next moment. This can make it more challenging to spot details and to get an overview of the data, particularly when the data are
Visualization Methods and Techniques (cid:4) 87 Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Figure3.32 Small multiples visualization of the number of people diagnosed with problems of the upper respiratory tract. larger. Therefore, dynamic representations are usually equipped with controls to pause, rewind, speed up, or slow down the representation. Knowing about the fundamental distinction between static and dynamic representations, we next introduce concrete visualization techniques for time and temporal data. Because time and temporal data are important in so many applications domains, there exist a plethora of techniques to visualize them. A comprehensive survey is available in specialized literature [Aig+11]. Here, we can only describe a few selected examples, and because this book is a print medium, we concentrate on static representations. To cover a broad range of techniques nonetheless, the examples are organized according to the key aspects of time as discussed before. First, we deal with techniques for time instants and time intervals and then for linear and cyclic time. We will illustrate the utility of multiple granularities and finally take the structure of time into account. RepresentingInstantsandIntervals A key difference when visualizing temporal data is whether we are concerned withinstantsorintervalsintime.Instantscorrespondtopoints.Theybearonly asinglepieceoftemporalinformation.Intervals,ontheotherhand,correspond to ranges. They bear two pieces of information (start and end, or start and duration). Let us next look at two visualization techniques, one that is suited for instants and one for intervals. The Time Wheel Intheprevioussectiononthevisualizationofmultivariate data, we introduced axes-based techniques with different layouts of axes and different types of data representations between pairs of axes. The Time Wheel is an axes-based technique particularly for multivariate temporal data [TAS04]. It uses a central time axis around which several attribute axes are arranged in a radial fashion as shown in Figure 3.33.
88 (cid:4) Interactive Visual Data Analysis Figure3.33 Time Wheel visualization of human health data. The actual data representation is done via lines that connect an instant of the time axis to its corresponding data values at the attribute axes. To this end, time and attribute values, which may be continuous, are projected to their associated axes and a line is drawn between them. As such, the Time Wheel implements a visual encoding based on the screen positions of the lines’ start and end points at the axes. The Time Wheel in Figure 3.33 visualizes temporal data that contain the number of people diagnosed with certain health problems. The time axis uses dates as time instants. Each of the attribute axes shows a different diagnosis. As can be seen, the top and bottom axes can be best interpreted with respect to time. By rotating the attribute axes around the central time axis, it is possible to bring any pair of attributes into this focal position. In its depicted state,theTimeWheelshowsmoderatelylowvaluesthroughouttime,butthere are also dates with unusually high numbers of diagnosed people. Triangular Model The triangular model is a technique particularly for visu- alizing intervals [Kul06]. It is based on two coordinate axes, the horizontal one representing time and the vertical one representing duration. In the triangular model, an interval is represented as a dot with two attached arms. The dot is placed so that the arms connect the time axis exactly at the start and the end of the represented interval. The point’s height corresponds to the interval’s duration. Figure 3.34 provides a schematic illustration. The triangular model is useful when it comes to reasoning about prop- erties and the relationships of multiple intervals, because it generates easily distinguishable visual patterns for all possible interval relations. There is even
Visualization Methods and Techniques (cid:4) 89 I 1 I 2 I 4 I 3 I 3 I I I 2 1 5 I 5 1 2 3 4 5 6 7 8 9 Time Time notiaruD 10 I 4 5 0 1 2 3 4 5 6 7 8 9 (a)Standard interval representation. (b)Intervals in the triangular model. Figure 3.34 Visual representation of intervals using the triangular model. Adapted from [Qia+12]. room for visualizing data that might be associated with the intervals. The dot-based encoding would allow for resizing or coloring the dots based on some attribute values. Yet, the triangular model is only of limited use for multivariate attributes. LinearandCyclicVisualization Different analytic questions can be answered with linear and cyclic visu- alizations of time. Temporal trends can be communicated well with linear representations. Recurring patterns, on the other hand, are easier to spot with cyclic representations. Below are three example techniques, the first has a linear time axis, the second a cyclic time axis, and the third combines linear and cyclic encoding. Stream Graph A stream graph is a technique for visualizing multivariate temporal data with a linear arrangement of time. As in the previous two examples, time is shown along the horizontal display axis from left to right. The multivariate data attributes are visualized as stacked streams, there is one stream for each attribute. The actual visual encoding is based on varying the thickness of the streams along the horizontal axis. That is, the vertical height of a stream at a particular horizontal position represents the underlying data value at the corresponding time. Various alternatives exist for ordering the streams and shaping the overall stack of streams. As illustrated in Figure 3.35, a stream graph provides a nice overview of thedataevolution.Ontopofthat,astreamgraphundeniablyhasanaesthetic value, which is the reason why it has gained widespread popularity [BW08b]. A downside is that individual streams can be difficult to interpret, because we intuitively perceive a stream’s thickness as being perpendicular to its flow, whereas the actual visual encoding is always done with respect to the vertical axis irrespective of the ups and downs of the streams.
90 (cid:4) Interactive Visual Data Analysis Figure 3.35 Stream graph with randomly generated data. Adapted from bl.ocks.org/mbostock/4060954. Spiral Representation In case cyclic temporal behavior is central to the analysis task, a spiral representation of the data can be created [TS08]. The basic idea is to map the time axis to the shape of a spiral along which the time-dependent data values are shown. The cycle length determines how many values are shown per spiral loop. The data values can be visualized in different ways, for example, by color-coding or by bars of varying length. We already saw a color-coded spiral at the beginning of this chapter in Figure 3.1. Another example is depicted in Figure 3.36. It shows four years of daily temperature data encoded with the two-tone pseudo-coloring technique. The spiral nicely reflects the recurring seasonal pattern of lower temperatures (blues and greens) in winter and higher temperatures in summer (reds and oranges). Figure3.36 Spiral display with four years of daily temperatures in Rostock.
Visualization Methods and Techniques (cid:4) 91 Such cyclic characteristics become visible, only if the cycle length of the spiral matches the length of the recurring patterns in the data. Particularly for data with unknown periodicities it is therefore necessary to enable the user to interactively search for suitable cycle lengths. How the user can be guided in the exploration process will be illustrated later in Section 6.2 of Chapter 6 on advanced visual analysis methods. Inadditiontocyclicpatterns,thespiralcanalsocommunicatelinearaspects of the data. By looking along a line from the center to the outer loop of the spiral, we can see how the data evolves from one cycle to the next. In our example from Figure 3.36, we can easily compare summers and winters from the past (inner loops) with the current year (outer loop). Cycle Plot The cycle plot is a technique particularly designed for the com- bined visualization of linear and cyclic components of temporal data [Cle93]. Thebasicideaistoshowthecycliccomponentasalineplotintowhichseveral smaller plots are embedded to visualize the linear component. As such, the cycle plot is a kind of nested visualization. For the purpose of illustration, Figure 3.37 compares a standard line plot against a cycle plot. The line plot is shown as a reference at the top. The actual cycle plot is depicted at the bottom. Its horizontal axis encodes the 100 80 60 40 20 0 1st Week 2nd Week 3rd Week 4th Week 100 80 60 40 20 0 Mon Tue Wed Thu Fri Sat Sun Figure3.37 Comparisonofaregularlineplot(top)andacycleplot(bottom).
92 (cid:4) Interactive Visual Data Analysis days of the week. The dashed line visualizes the average values per day to form the overall representation of the weekly cycle. Per day of the week, we see a smaller plot embedded into the main chart. Each of these smaller plots represents the linear development over four weeks. The cycle plot allows us to see things at a glance that are not so easy to derivefromastandardlineplot.Forexample,itisimmediatelyclearthatdata values are generally lower on Saturdays and Sundays, and that data values have increased on Mondays, whereas Tuesdays are in a decline. ConsideringGranularity Severalofourpreviousexamplesvisualizedtemporaldataatthegranularityof days.Yet,temporaldatacanbegivenatdifferentgranularities:years,quarters, months,weeks,days,hours,minutes,seconds,orevensmallertimeunits.Next, we look at a visualization technique that considers multiple granularities to create a multi-scale representation of temporal data. Cluster and Calendar-based Visualization In order to understand how certain resources (human, energy, computing time, etc.) are utilized over time, itmakessensetocollectconsumptiondataandvisualizethem.Oftensuchdata are measured at fine granularity, say minutes or hours. Hence, consumption data can quickly grow to a size that poses a considerable challenge. Cluster and calendar based visualization addresses this problem by imple- menting a multi-granularity visualization [vWvS99]. The approach starts out with representing the daily evolution of the consumption. To this end, a classic line plot is used. As shown in the right part of Figure 3.38, the line plot has a granularity finer than hours along the horizontal axis. The vertical axis represents the resource consumption, in our case the number of employees. This kind of representation reveals patterns with respect to the time of the day. But are the patterns the same for all days, or are there characteristic patterns for different days of the year? To answer this question, the data are clustered. Details on clustering as a generaldataabstractionstrategywillbediscussedinSection5.4.2ofChapter5. For the time being, we only need to know that days are grouped with respect to the similarity of the daily consumption curve. A calendar representation as in the left part of Figure 3.38 is then used to visualize cluster affiliation. In other words, the days of the calendar are color-coded according to the cluster to which a day belongs. With this combined representation of calendar and line plot, the user can see the typical pattern of workdays (orange), days that deviatefromthisregularpattern,forexampleaseriesofFridaysduringsummer vacation (green), and when in the year unusual behavior can be observed, for example, on December 5 or December 31.
Visualization Methods and Techniques (cid:4) 93 Figure 3.38 Cluster- and calendar-based visualization of temporal data. Reprinted from [vWvS99]. VisualizingTimewithaBranchingStructure Last but not least, we said that the structure of time needs to be taken into account. All the examples presented before assume an ordered structure of time. That is, the data are understood as a single unique series of values. Branching time and multiple perspectives are different in that there can be variability in when things happen(ed) in time. Next, we look at a technique that supports the visualization of such variable time structures and also the temporal uncertainty they involve. Planning Lines Particularly in the context of scheduling, it is not always certain when activities start or end. The Planning Lines technique has been designed to support planning and clearly communicate the involved uncertain- ties[Aig+05].Forthispurpose,horizontalglyphsencodesixpiecesoftemporal information: the earliest and latest start of an activity, the earliest and latest end of it, and its maximum and minimum duration. A detailed view of a glyph is provided in the upper right box in Figure 3.39. For the actual planning, multiple glyphs can be placed on a calendar tablature where several activities are allowed to form a branching structure or run in parallel. Dependencies between activities are indicated via links. More precisely, if the start of an activity depends on the successful completion of another activity, the two activities are connected by a link. The links can split and also join paths through time. How a hypothetic planning scenario could look like is illustrated in Figure 3.39.
94 (cid:4) Interactive Visual Data Analysis Excavation Max / min duration Foundation Activity Walls / Ceilings Earliest / latest start Earliest / latest end Windows / Doors Roof Screed Mar 26, 2018 Apr 2, 2018 Apr 9, 2018 Apr 16, 2018 Apr 23, 2018 Week 13 Week 14 Week 15 Week 16 Week 17 Figure3.39 Visualization of uncertain time intervals for planning purposes. Adapted from [Aig+05]. Note that the Planning Lines technique is primarily for visualizing the structure of time and uncertainties, and not so much for the temporal data associated with branching time. In fact, visualizing branching time structures togetherwithmultivariatetemporaldataisachallengingproblemthatrequires further research to solve it. The Planning Lines are the last example in our series of visualization techniques for time and temporal data. The previous pages could cover only a fraction of the rich body of existing work. A comprehensive overview with many more examples is provided by the TimeViz Browser. TheTimeVizBrowser There is a great number of valuable approaches for visualizing time and associated data. A problem is how to find a technique that fits a user’s particular needs. The TimeViz Browser aims to provide a solution to this problem. The idea behind the TimeViz Browser is to provide in a single place a greater overview of what visualizations are possible. The TimeViz Browser is an illustrated survey with a searching and filtering function that allows users to narrow down the scope of techniques according to their needs. To reach a wide audience, the TimeViz Browser is available as a web site accessible at http://browser.timeviz.net. It enables practitioners and researchers to explore, investigate, and compare more than a hundred techniques. The design of the TimeViz Browser is shown in Figure 3.40. The main view consists of thumbnails and provides a compact visual summary of the available visualization techniques. Selecting a technique opens up the detail view that offers a brief abstract for the technique, a larger figure, and a list of relevant publications. The filter interface to the left covers several criteria, for example,
Visualization Methods and Techniques (cid:4) 95 Figure3.40 The TimeViz Browser provides an illustrated overview of more than a hundred techniques for visualizing time and temporal data. if a technique is suited for instants or intervals, or for linear or cyclic time. Using the filters it is possible to narrow down the collection of thumbnails presented in the main view, for example, to focus on techniques for a cyclic time arrangement of temporal intervals. The TimeViz Browser also offers a filter that is related to an aspect that we have left out of consideration so far: the presence or absence of spatial dependenciesinthedata.Inotherwords,inadditiontotemporaldependencies of data, there can also be a dependency of data on space S →A, often on the geographic space. This brings us to the next section, where we will discuss the visualization of spatial and also spatio-temporal data in more detail. 3.4 VISUALIZATION OF GEO-SPATIAL DATA Similartotherolethattimeplaysforunderstandingdata,thespaceintowhich the data are embedded is crucial for gaining insight. Given the ubiquity of modern sensory technology, it can be taken for granted that a considerable amountofthedatacollectedtodayareofspatialnature,typicallyofgeo-spatial nature. In this section, we are going to visualize data with a geo-spatial frame of reference. Our primary interest will be the communication of the dependency of the data attributes on the geographic space S →A. Tightly connected to this dependency are two of the analytic questions discussed in Section 2.2.2: What is the value at a given position (identification task), and where are the positions with a given value (location task)? Yet, before we introduce visualization methods that help us answer these questions, we will first look at the aspects being specific to geographic space and geo-spatial data.
96 (cid:4) Interactive Visual Data Analysis 3.4.1 GeographicSpaceandGeo-spatialData Geo-spatial data are data that are tagged with a position in geographic space. The probably most well-known geo-spatial data are weather data including measurements of temperature, precipitation, wind speed, and also wind direc- tion. Moreover, traffic data and demographic data are associated with the geographic space around us. The aspects that need to be considered when visualizing geographic space and geo-spatial data will be discussed in the following. CharacterizingGeographicSpace Geographic space is special in that it addresses the spherical shape of planet Earth.Here,weconsidertwoimportantaspectsthatcharacterizethegeographic space: dimensionality and scale. Dimensionality The geographic space is three-dimensional. A position in geographic space is typically defined by three coordinates: latitude, longitude, and elevation. The latitude is the angle between the equator and the poles. The latitude is 0° at the equator, +90° at the north pole, and −90° at the south pole. The longitude is the angle with respect to the prime meridian in Greenwich. Eastward angles have a positive sign, whereas westward angles are negative. The maximum absolute value of the longitude is 180°. Because the surface of the Earth is not perfectly planar, elevation is used to measure the distance of a geographic position above or below the sea level. Depending on the application, the two coordinates latitude and longitude may be fully sufficient for the visual analysis of geo-spatial data. For example, visual representations for weather forecasts typically are based exclusively on plotting the data with respect to latitude and longitude. Yet, there are also applications where the elevation part is crucial. Air traffic planning and analysis immediately comes to mind as an example. Scale In terms of scale, the geographic space is continuous and infinite with respecttoresolution.Essentialfordatacollection,processing,andvisualization is the definition of spatial units at a certain scale. In the simplest case, the spatial units are points given at a certain spatial scale. Yet, the spatial units may also be given as complex regions at multiple scales, such as federal states, districts, and zip-code regions. In such cases, the spatial units are typically organized in a hierarchical structure (similar to what we said about temporal granularity in the previous section). Figure 3.41 shows the German federal stateMecklenburg-Vorpommerncomposedofdifferentspatialunitsatdifferent scales. The spatial scale significantly affects the results of the visual data anal- ysis. Relationships detected at one scale may not be observable at another scale[And+10].Hence,determininganadequatespatialscalethatcorresponds
Visualization Methods and Techniques (cid:4) 97 Figure3.41 Spatialregionsatdifferentscales:Federalstate,districts,zip-code regions. to the subject under consideration is crucial. For example, investigating the trafficataspecificcrossingrequiresadifferentspatialresolutionthanexploring major routes between districts. Often,findingthespatialscalethatbestmatchesthetaskathandisatrial- and-error procedure. It may even be necessary to create further spatial scales by subsuming or subdividing spatial units. Coarser scales can be derived from the original scale by means of a suitable aggregation strategy. This includes the application of aggregation functions such as average, sum, or count. For thecreationoffinerscales,asuitabledistributionstrategyisrequiredtoassign data values to the newly specified sub-regions. Usually, additional context information is necessary to arrive at semantically meaningful aggregations and distributions. Both dimensionality and spatial scale are important characteristics of the geographic space. They have a decisive influence on the results that can be achieved with visual data analysis. Let us next take a closer look at what characterizes geo-spatial data.
98 (cid:4) Interactive Visual Data Analysis (a)Points. (b)Lines. (c)Areas. (d)Volumes. Figure3.42 Geo-spatial data can refer to different spatial units. CharacterizingGeo-spatialData Geo-spatial data (also called spatial data or geographic data) have explicit referencestothegeographicspace.Inotherwords,thedataareassociatedwith spatial units. As illustrated in Figure 3.42, there are four types of spatial units: points, lines, areas and volumes. For example, data collected at a measuring station are associated with the very position of the station. Highway traffic data would refer to lines. Census data typically refer to discrete administrative spatial areas. Weather phenomena such as thunderstorms are an example where geo-spatial data have a volumetric anchor in space. It is the existence of a relationship between the data and spatial units in geographic space that characterizes geo-spatial data in the first place. The relationship itself is further characterized by the first law of geography as formulated by geographer and cartographer Waldo R. Tobler [Tob70]: “[...]everythingisrelatedtoeverythingelse,butnearthingsaremorerelated thandistantthings.” Tobler,1970 This law has immediate consequences for the visualization of geo-spatial data. The law’s first part implies that geo-spatial data can be interpolated and extrapolated. The second part tells us about the weighting of near and distant data points. Pictorially speaking, the visual representation of a geo-spatial data value can spread in a certain neighborhood. The scope of the data, as explained in Section 2.2.1 of Chapter 2, defines how far the data may spread. At this point, we know about the specific character of the geographic space and geo-spatial data. In addition to these specifics, the characteristics of multivariate data apply as described in Section 2.2.1. Next, we will introduce strategies for visualizing the geographic space and the associated data.
Visualization Methods and Techniques (cid:4) 99 3.4.2 GeneralVisualizationStrategies Visualizing geo-spatial data means showing univariate or multivariate data attributesAinrelationtothegeographicspaceS.Thisallowsustoinvestigate spatial dependencies S×A. If the data have an additional temporal dimension T, a spatio-temporal visualization can be created to support the analysis of spatial and temporal dependencies S×T ×A. A prerequisite for geo-spatial data visualization is the representation of the geographic space. RepresentationofGeographicSpace Geographic space is typically represented by 2D maps. Maps are universally useful.Theyallowustodevelopanunderstandingoftheworldanditsnaturalor manmadephenomena.Cartographyisthedisciplinedealingwiththeproduction of maps. The cartographic literature provides a wide range of design concepts, guidelines and conventions for creating maps [Mac95]. One such convention is to use blue colors for water, green for lowlands, and brown and white for mountains and their icy caps. Anessentialstepwhencreatingamapistoprojectthesphericalgeographic coordinatestotheplanarcanvas.Therearevariousprojectionmethods,someof whichevendatebacktoancienttimes.Thedifficultythatallprojectionmethods have to deal with is to find a trade-off between preserving several properties of the Earth’s surface, including area, shape, direction, and distance. For example, the Equirectangular projection, attributed to Marinus of Tyre, who lived around 70–130 CE, preserves distances. The famous Mercator projection, which preserves angles and shapes, was invented in 1569. Even today, new projection methods are devised, for example the Natural Earth projection by Tom Patterson from 2011. Figure 3.43 illustrates these three projections. A particularly interesting class of projections is myriahedral projections, which (a)Equirectangular. (b)Mercator. (c)Natural Earth. Figure 3.43 Different map projections preserve different spatial proper- ties. Adapted from bl.ocks.org/mbostock/3757119, .../3757132, and .../4479477.
100 (cid:4) Interactive Visual Data Analysis Figure3.44 Myriahedral projections of the Earth. Adapted with permission by Nicolas Belmonte from philogb.github.io/page/myriahedral/. unfold the globe by cutting and folding the faces of polyhedra [vWij08; BW18]. The three examples in Figure 3.44 illustrate the new perspective on planet Earth offered by these projections. Another important step when creating maps is cartographic generaliza- tion. It addresses the issues of spatial scale as indicated before. Cartographic generalization aims at reducing the complexity of the visual representation of spatial features by depicting them at lower resolution. The difficulty is to preserve both the geometric characteristics (graphic generalization) and the essential data characteristics (semantic generalization). Figure 3.45 gives an example where the map of Mecklenburg-Vorpommern, as shown earlier, is displayed at different resolutions. The map data have been reduced with the classic line generalization algorithm by Visvalingam and Whyatt [VW93]. At 50% reduced data size, the visual representation of the map is almost identical to the original data. At 10%, the reduced map resolution is fairly obvious, but still the major features of the map can be discerned. (a)Original data 100%. (b)Data reduced to 50%. (c)Data reduced to 10%. Figure 3.45 Map representation at different resolutions. Generated with mapshaper.org. While2-dimensionalmapsarepowerfulingeneral,therearealsoapplication scenarioswhereitisnecessarytofaithfullyrepresentthe3-dimensionalfeatures of the geographic space. Sophisticated terrain rendering techniques have been developed to create almost photo-realistic 3D images of the Earth’s surface. Terrain rendering is a complicated matter in itself and involves several steps including the generation of 3D meshes, definition and selection of appropriate resolutions, hardware-accelerated shading and texturing, and advanced post-
Visualization Methods and Techniques (cid:4) 101 4392m 0m Figure3.46 Terrain rendering of the Puget Sound region. Courtesy of Steve Dübel. processing.Here,wewillnotgointothedetailsofterrainrendering,butinstead refer the interested reader to the dedicated literature on the topic [Ruz+12]. An example of a 3D terrain rendering of the Puget Sound region is shown in Figure 3.46. Color is used to visualize the terrain elevation. In the following pages, we will see further examples, where geo-spatial data are visualized in 3D scenes. Yet, before that, we will introduce basic visualization strategies for geo-spatial data. VisualizationofGeo-spatialData Theessentialtaskwhenvisualizinggeo-spatialdataistoconveytherelationship between the geographic space S and the data attributes A. In particular, we will concentrate on two key questions: First, how to combine S and A visually, and second, whether to draw S and A in 2D or 3D? Direct and Indirect Visualization There are two baseline options for the visual combination of S and A. We can directly show the geo-spatial data withinthepresentationofthegeographicspace.Thisshallbedenotedasdirect visualization. Alternatively, we can pursue an indirect visualization. In this case, the geo-spatial data and the geographic space are depicted in separate views, and their connection is communicated indirectly via visual cues. Direct Visualization For direct visualization, geo-spatial data are embed- ded directly into the visual representation of the geographic space. A prominent example is choropleth maps, for which the spatial units of the geographic space are color-coded according to their associated data. While such choropleth maps are intuitive and easily understood, they usually depict only one or two geo-spatial attributes. Multivariate geo-spatial dependencies can be communicated by placing glyphs, as described in Section 3.2.4, directly on the map. To this end, suitable glyph positions must be determined. Ideally, glyphs should be locateddirectlyattheirassociatedspatialunit.Atthesametime,glyphs
102 (cid:4) Interactive Visual Data Analysis (a)Straightforward placement. (b)Overlap-optimized placement. Figure3.47 Reducing overlap of stream graph glyphs on a map. should neither overlap each other nor occlude important geographic features.Theserequirementsmakeglyphplacementanon-trivialproblem whose solution may involve sophisticated optimization algorithms [FS04]. Figure 3.47 illustrates the difference between a straightforward and an overlap-optimized placement of stream graph glyphs. Overall, direct visualization allows us to easily identify the data values at certain positions and to locate positions with certain data values. However, with an increasing number of spatial units and data values, direct visualization reaches its limits, because it becomes increasingly difficulttoembedallgeo-spatialdatadirectlyintothemap.Thisiswhere indirect visualization can help. Indirect Visualization For indirect visualization, geo-spatial data and geo- graphic space are represented in distinct views that are linked through visual cues. For example, Figure 3.48a shows a choropleth map and a parallel coordinates plot. The map visualizes a single attribute by color for the entire geographic space. The parallel coordinates plot shows all fiveattributesforalldatatuples.Yet,theconnectionbetweengeographic space and the multivariate data is possible only for a single location, which is marked in the map via cross-hairs, and whose associated data tuple is highlighted in red in the parallel coordinates view. One way to increase the number of locations for which multivariate geo-spatial data can be visualized is to use geo-spatial probes [But+08]. The goal of probes is to achieve a more flexible combination of the map and several views showing the geo-spatial data. To this end, the user can place a number of probes on the map. For each probe, a separate view is created that shows the data associated with the space around the probe. The connection between the probed spatial locations and the data views is established via visual links. A probe-based indirect visualization is illustrated in Figure 3.48b.
Visualization Methods and Techniques (cid:4) 103 (a) Univariate choropleth map plus (b) Flexible visualization via probes. multivariate parallel coordinates plot. Courtesy of Thomas Butkiewicz. Figure3.48 Indirect visualization of geo-spatial data. A major advantage of indirect visualizations is that we can show larger amounts of geo-spatial data. Yet, the user must mentally connect two or more views to explore the data’s spatial dependencies. In fact, both direct and indirect visualization are viable options for com- municating geo-spatial data. When the spatial aspect S is in the focus, direct visualization tends to be better suited. When the data attributes A play a stronger role, indirect visualization can be more practical. Let us next continue with the question of whether geographic space and geo-spatial data should be depicted in 2D or 3D. 2Dand3DVisualization Wealreadydiscussedthegeneraldesigndecisionofusing2Dor3Dpresentations in Section 3.1.2. When visualizing geo-spatial data, this decision is more intricate, because we have to distinguish between the presentation of the geo-spatial data A and the presentation of the geographic space S [Düb+14]. A systematic view on this concern is given in Figure 3.49. The vertical axis schematically depicts the difference between 2D and 3D presentations of the geo-spatial data, while the horizontal axis compares 2D and 3D presentations of the geographic space (map or terrain). The four possible combinations of 2D and 3D can be characterized as follows. 2D Data Visualization on 2D Map Showing both data and space in 2D is the quasi-standard for geo-visualization. Many established techniques
104 (cid:4) Interactive Visual Data Analysis D2 D3 2D 3D A atad latiaps-oeG Geographic space S Figure3.49 Systematic view of 2D and 3D representations of geo-spatial data and geographic space. Adapted from [Düb+14]. belong to this category, including choropleth maps, contour maps, dot maps, flow maps, cartograms, or glyph maps, as shown earlier in Fig- ure3.47.Thelevelofvisualabstractionachievedbypurely2Dapproaches usually makes the resulting images easy to interpret. 2D Data Visualization in 3D Terrain In this category, 2D and 3D com- ponents are combined. A 3D terrain rendering represents the geographic space in its full extent. The geo-spatial data are mapped onto the 3D terrain via 2D billboards or 2D textures, as already illustrated in Fig- ure 3.46. This makes it possible to study how the data behave in relation to specific landscape characteristics, such as mountains or valleys. 3D Data Visualization on 2D Map For this category, the geographic space is abstracted as a 2D map, whereas 3D graphical elements are used for the data. In a sense, the third dimension is exploited for a richer depiction of the data, for example for showing more data values, providing clearer arrangements, or incorporating more dimensions, for example the dimension of time, as we will see a little later. 3D Data Visualization in 3D Terrain If three-dimensional spatial rela- tionships play an important role for the data analysis, it makes sense to showbothdataandspacein3D.AnexampleisgiveninFigure3.50.The visualization shows how an aircraft decreases its speed (color changes fromdarkbluetobrightblue)whilemaneuveringthroughthemountains near Sion airport during the approach.
Visualization Methods and Techniques (cid:4) 105 Figure3.50 3D visualization of the trajectory of an aircraft approaching Sion airport. Courtesy of Steve Dübel. In general, both 2D and 3D representations are suited for the visualization ofgeographicspaceandgeo-spatialdata.Whichcombinationtochoosedepends again on the specific data and tasks. In the case that 3D components are involved in the visualization, it is definitely necessary to address the problems due to occlusion and perspective distortion. In the first place, it is important to make the user aware of the fact that information may be obscured. This awareness enables the user to act and resolve potential problems by changing the viewpoint on the 3D scene. Visibility widgets as illustrated in Figure 3.51 can help the user in this regard [RS17]. The panoramic view in the bottom right corner allows the user to look with an extremely wide angle onto the 3D scene. The central circularlensvisualizesthedataotherwisehiddenonthebacksideoftheterrain. The color-coded bands at the bottom and to the left indicate where and at Figure3.51 Visibility widgets help users identify obscured information in 3D geo-visualizations. Courtesy of Martin Röhlig.
106 (cid:4) Interactive Visual Data Analysis what distance the majority of obscured information is located. While such sophisticatedtoolsmightnotalwaysbeavailabletotheuser,itisimportantto provide at least rudimentary support for an effective exploration of 3D visual representations. In summary, direct and indirect visualization, as well as 2D and 3D visu- alization, are the basic options when designing visual representations of geo- spatial data. Next, we will put these basic strategies to use for the combined visualization of data in space and time. 3.4.3 VisualizingSpatio-temporalData So far, we considered the visualization of geo-spatial data and temporal data separately.Yet,spaceandtimearetightlyconnected.Spatialphenomenaoften include a temporal aspect, and evolving phenomena are often embedded in space. This leads us to the question: how to visualize spatio-temporal data? Spatio-temporal data have references to both space S and time T, a fact that makes the visual data analysis more difficult. It is no longer sufficient to understand in isolation how data values are distributed in space S → A and how they evolve over time T → A. It is rather necessary to combine both perspectives to enable a comprehensive understanding of spatio-temporal dependencies S×T →A. Designing corresponding visual representations is challenging. Particularly for the case of spatial-temporal data visualization, it isimportanttobalancethedesigner’swishtocommunicatealotofinformation and the users’ capacity to conceive it. Probably the most widely used solution is to show an animated map where the visualization changes with each frame. We also mentioned small multiples as a suitable approach and gave an example in Figure 3.32 on page 87. In general, we could combine and link two dedicated views, one visualizing the temporalaspectofthedataandtheothershowingthespatialaspect.However, in all these cases, it would be up to the user to mentally integrate S and T. In the following, we want to illustrate how 3D data visualization on 2D maps can help us obtain visual representations where S and T are already integrated.Ourfirstexamplewillbeaboutthevisualizationofspatio-temporal movement data. The second example is concerned with the visualization of spatio-temporal health data. StackingMovementTrajectoriesin3D Movement data capture how some observed objects move through space and time. Here, we consider movements where each data point is defined as a tuple with latitude and longitude coordinates, a timestamp, and several data attributes, including speed, acceleration, or sinuosity. The standard way of looking at such data is to study 2D maps where movement trajectories are visualized as 2D paths. Figure 3.52a shows an example with trajectories of cars moving along roads. Color is used to encode
Visualization Methods and Techniques (cid:4) 107 (a)2D map with 2D paths. (b)2D map with stacked 3D bands. Figure3.52 Visualization of movement trajectories. the cars’ speed. This basic form of representation suffers from overplotting which makes it difficult or even impossible to see any details of the spatio- temporal behavior of individual trajectories. Atthispoint,thethirddisplaydimensioncanhelpusuntanglethesituation. As illustrated in Figure 3.52b, the simple, yet powerful idea is to tilt the map in 3D and to stack 3D trajectory bands along the third dimension, where each band is assigned a separate z-coordinate [Tom+12]. The3Dtrajectoryvisualizationhastwoadvantages.First,wegetacompre- hensiveoverviewofalltrajectories.Second,itisnowpossibletoinvestigateeach individual trajectory in detail. The spatial aspect can be studied in relation to the base map. For the selected trajectory, a dynamic auxiliary map layer is displayed. It eases the interpretation of the spatial embedding, particularly for the data at the top of the stack. Moreover, the bands in the stack can be grouped according to their similarity in terms of their shape. This way, we can see quite well where in space the data exhibit certain characteristics, for example, where the speeds are higher (green) and where they are lower (red). But what about the when as captured by the temporal aspect of the data? Again, the third dimension comes to our help. In contrast to 2D paths, the 3D bands offer enough space to include arrows that indicate the direction of the movement. This allows us to see the temporal sequence of places visited during a movement. Moreover, the stacking order can be altered with respect to time. Trajectories from the past could be put at the bottom of the stack, whereas the most recent trajectories could go to the top, or vice versa. Such a temporal ordering of the trajectory bands can reveal long-term changes in the overall movement data.
108 (cid:4) Interactive Visual Data Analysis Both the arrows in the bands and the ordering of the stack allow us to draw qualitative conclusions about temporal dependencies. We can tell that one data point is temporally before or after another one. But quantitative statements about the temporal distance between data points cannot be made. In Section 4.6.3 of Chapter 4, we will learn how an interactive lens technique can help us remedy this problem for movement data. Next, we will continue with an approach where time is granted more visual prominence by design. 3DSpace-timeCubeVisualization Under the assumption that the geographic space is defined by two coordinates, spatio-temporal data can be mapped directly to the three display dimensions. This general visualization approach is called a space-time cube. The x-axis and the y-axis of the cube are used to map the two spatial coordinates, the third z-axis shows time. The space-time cube (STC) has been developed long before visualization became an independent field of research [Häg70]. The primary purpose of STC visualizations is to show the paths that objects have taken through space and time [Kra03]. In the following, we will use the STC concept to visualize spatio-temporalhealthdata.Thespatialframeofreferencewillbevisualizedas a basic 2D boundary map. The actual data attributes, in our case the number of people diagnosed with certain diseases, will be visualized via different 3D graphical representations. (a)Pencil glyphs for linear trends. (b)Helix glyphs for cyclic patterns. Figure3.53 Visualizing spatio-temporal data using 3D glyphs on a 2D map. 3D Glyphs in a Space-time Cube Let us start with 3D glyphs. As in 2D, glyphs are created and placed at a suitable location for each spatial unit of the map. Figure 3.53 shows 3D pencil glyphs and 3D helix glyphs [TSS05]. Both types of glyphs are oriented along the z-axis, which represents time. Colored glyph segments visualize individual data values. Pencil glyphs facilitate the visual analysis of linear temporal trends. Map- ping different attributes to the different faces of the pencil allows us to see correlations in the evolution of multiple data attributes. The example in Fig- ure 3.53a shows the monthly number of people with different infections of the upper and lower respiratory tracts.
Visualization Methods and Techniques (cid:4) 109 Helixglyphsemphasizecyclicpatternsinthedata.Ahelixglyphisbasically a 3D band that winds up along the z-axis. Sub-bands visualize different data attributes. Figure 3.53b shows the same data as Figure 3.53a. Each helix glyph is configured to show 12 months per helix loop with the seasonal peaks currently facing toward the viewer. Both pencil and helix glyphs have the advantage that spatial and temporal aspects are shown within a single STC image, which allow us to study mul- tivariate spatio-temporal dependencies. Yet, while the temporal evolution of the data is nicely visualized along the individual glyphs, the spatial evolution is more difficult to extract. Because of the empty space between glyphs, the user has to mentally link the information displayed in one glyph to what is shown in another glyph. How an alternative visual representation can resolve this problem will be explained next. 3D Wall in a Space-time Cube The question is, how can we avoid gaps in thevisualrepresentationofthedatasothatthespatialcharacteristicsareeasier to interpret? An interesting solution is to create a non-planar slice through the three-dimensional space-time continuum onto which the spatial-temporal data can be projected [TS12]. The creation of the slice is based on three steps as shown in Figure 3.54: 1. Define topological path through neighborhood graph 2. Construct geometrical path through spatial units 3. Extrude slice for data visualization In the first step, a topological path has to be defined through the neigh- borhood graph of the spatial units. The topological path guarantees that the data visualization is free of gaps. In a second step, the topological path is transformed into a geometrical path. This path is constructed in the x-y-plane taking into account the geographic characteristics of the spatial units. A good geometricalpathshouldhavelowcurvatureandfollowthespatialunits’shapes without passing through other territories. Finally, the geometrical path is Time Space (a)Topological path. (b)Geometrical path. (c)Extruded slice. Figure3.54 Creation of a non-planar 3D slice through space-time.
110 (cid:4) Interactive Visual Data Analysis Figure3.55 Spatial-temporal visualization along a wall on a map. extruded in the direction of the z-axis to form a non-planar slice with a wall- like 3D shape. This wall acts as a kind of canvas onto which we can project visual representations of the spatio-temporal data. Figure 3.55 shows an example with the health data that we have already visualized via glyphs earlier. Per wall segment, colors visualize the number of sick people for 36 months of observation. As there are no gaps in the visualization, it is now easier to follow the data’s evolution through space and time.However,wearerestrictedtothepathdefinedinthefirstplace.Therefore, itisimportanttoprovideflexiblemeanstoadjustthepathinteractivelyorbased on the characteristics of the data. For example, data-driven paths could be definedbypursuingagradientdescentalongspatio-temporaltrendsinthedata. In conclusion, we see that designing visualizations of spatial and spatio- temporaldatainvolvesnumerousdesignchoiceswhichallmustbeweighedand decided carefully. The primary goal must be to clearly represent the spatial dependencies of the data. This goal can be achieved on the basis of suitable 2D map or 3D terrain representations. The actual data representation can be doneinavarietyofways,wheredifferentcombinationsof2Dand3Dgraphical elements are possible. Conceptually, the data can be visualized directly in the spatial frame of reference or indirectly as a dedicated view that is linked with the map or the terrain. When the dimension of time is added to the visualization, matters become still more complex. We have seen that using the third display dimension is a viable option to integrate time. Yet, when time, space, and multivariate data attributes are to be analyzed in concert, we usually have to make compromises. In our last example, we used a wall-like 3D visualization without gaps to ease the spatial interpretation of the data, but the solution shows only a single data attribute. 3D glyphs are capable of visualizing multiple spatio-temporal attributes, but some information may be invisible at the glyphs’ back face. Becauseitisnecessarytomakecompromises,acomprehensivevisualanalysisof
Visualization Methods and Techniques (cid:4) 111 spatio-temporaldatashouldideallyprovidetheuserwithdifferentvisualization techniques and easy ways to configure them. Next, we will learn that the visualization of graphs, which typically do not have a natural embedding in space or time, is a considerable design challenge as well. 3.5 GRAPH VISUALIZATION In the previous sections, we have learned how to visualize multivariate data attributes A, the temporal context T, and the spatial frame of reference S. Last but not least, this section will deal with the visualization of relations R among the data. Following the same pattern as in the previous sections, we will first look at the specifics of the underlying data model and then introduce dedicated visualization solutions and illustrate them with selected examples. 3.5.1 GraphData The data model that interests us now is graphs. A graph G=(V,E) consists of a set of vertices V (or nodes) and a set of undirected or directed edges E (or links) between the nodes. Roughly speaking, nodes represent pieces of data, whereas edges correspond to the relations among the data. We can also say that the edges define a structure among the data. NetworksandTrees Graphs may come in a variety of different forms. Depending on the charac- teristics of the structure of a graph as defined by its set of edges, one can distinguish different classes of graphs. If there are no particular constraints imposed on the graph, it is commonly called a network. As such, networks capture the intuitive notion of binary relations between entities. Social net- worksandcomputernetworksareprominentexamples.Butalsotransportation systems and biological systems are often modeled as networks. Incontrasttobasicnetworks,atreeisagraphthatobeysspecificconstraints. In particular, a tree is a connected, acyclic graph. Connected means that for any two nodes in the graph a path exists connecting the two nodes. Acyclic means that the graph does not contain paths where start node and end node are the same. Moreover, being an acyclic graph also implies that the path that exists between any two nodes is unique. As in nature, a tree’s root and leaves are special. A tree with a designated root node r ∈V is called a rooted tree. The root serves as the center of the tree from which edges are directed away describing cascades of parent-child relations. A node that has no edges to child-nodes, but only a single edge to its parent-node is called a leaf. A managerial hierarchy of a company is an example for a rooted tree with the CEO as the root, and team leaders being the leaves.
112 (cid:4) Interactive Visual Data Analysis Time (a)Structure (b)Attributes (c)Time (d)Space (e)Groups Figure 3.56 Facets to be considered when visualizing graphs. Adapted from [HSS15]. Networks and trees are the most commonly encountered graphs in the context of visualization. There are other, less common graphs. For example, in bipartitegraphs,thenodescanbepartitionedintotwosets,andedgesrunonly between the two sets, not within the sets. For another example, hypergraphs are graphs where an edge is allowed to connect more than two nodes. Many more types of graphs are known in the literature [BLS99; GYZ14]. Yet, in this book, we want to focus on the visualization of networks and trees. FacetsofGraphs Figure 3.56 provides an overview of the facets that are most relevant in the contextofgraphvisualization. Thestructure inFigure3.56a isaprimaryfacet. Yet, additional facets can also play a role when analyzing graph data. These additional facets lead to distinct types of graph data: Multivariate graphs have additional data attributes associated with their nodes and/or edges as illustrated in Figure 3.56b. The data attributes provide additional information about nodes and edges, for example, importance or weight. Dynamic graphs vary over time. As shown in Figure 3.56c, for each point in time, the set of nodes and the set of edges can be different. Between subsequent time steps, nodes and edges may continue to exist, leave the graph or enter it. Spatial graphs are associated with coordinates in space as in Figure 3.56c. The coordinates define the layout of nodes and sometimes even the routing of the edges. A network of flights connecting airports is an example of a spatial graph. Compound graphs partition the nodes and the edges into groups that are typically nested hierarchically, which is depicted in Figure 3.56e. The groups can be expanded or collapsed to create views of the graph with different levels of abstraction.
Visualization Methods and Techniques (cid:4) 113 As mentioned before, the structure of graphs is fundamental, while attributes, time, space, and groups may provide additional context infor- mation to complete our understanding of the data. Yet, these additional facets also make the visualization more difficult to design. Therefore, we will start simple and introduce basic visual representations for the structure of graphs in the next section. Later in Section 3.5.3, we will discuss how the additional facets can be incorporated. 3.5.2 BasicVisualRepresentations There are three fundamental categories of graph representations: node-link representations, matrixrepresentations, andimplicit representations. Addition- ally, there are hybrid representations, which are basically combinations of the fundamental representational paradigms. Node-linkRepresentations Node-link representations (or simply node-link diagrams) depict nodes as dots and the edges between them as lines or arcs. This representation is by far the most common one for networks and trees. A key question is, where should nodes and edges be drawn? Solving this problem is the task of graph layout algorithms [Bat+99; Tam13]. The literature defines a number of conventions, aesthetic criteria, and constraints that graph layouts should obey and fulfill. For example, nodes shouldnotoverlapandthenumberofedgecrossingsshouldbeminimal.Curved edges may be aesthetically more pleasing than straight lines or orthogonal edges. In general, it is not possible to observe all of these requirements, as they may be contradictory to each other. Therefore, the graph layout problem is typically formulated as an optimization task. Depending on the degree of freedom that an algorithm has to find suitable node positions, three classes of layouts can be distinguished [SS06]. There are: • free layouts, • fixed layouts, and • stylized layouts. Forafreelayout,therearenoparticularconstraintswithrespecttothenode positions. Such layouts are typically generated with force-directed approaches. Thegoalistoplaceconnectednodesneartoeachotherwithoutclutteringthem inasinglespot.Tothisend,force-directedalgorithmssimulaterepulsiveforces between all nodes and attractive forces between adjacent nodes [FR91]. With an adequate configuration and a suitable number of iterations, the simulation should converge in an equilibrium that represents the final layout. In contrast to free layouts, fixed layouts are based entirely on predefined node positions. Only the edges can be routed flexibly by the visualization
114 (cid:4) Interactive Visual Data Analysis Figure 3.57 Node-link diagram of flights connecting US airports. Created with gephi.org. method. Fixed layouts are typically encountered when the graph is embedded into some spatial frame of reference. Figure 3.57 shows an example where nodes are placed according to the positions of US airports. Inbetweenfreeandfixedlayouts,therearetheso-calledstylizedlayouts.On theonehand,theyarenotasrestrictedasfixedlayouts,andontheotherhand, they are not entirely free. A typical scenario is that possible node positions are confined to a certain predefined scheme. For example, the nodes may be required to be located on a circle or be arranged in an axis-aligned fashion. Tree layouts often aim to place the nodes of individual hierarchical levels on distinct horizontal or vertical lines. Once a layout has been computed, it can be post-processed to improve its appearance and readability. For example, the graphical representations of the nodes may overlap, because many layout algorithms neglect the fact that space is needed to actually draw a node. Therefore, node overlap removal is a commonly applied post-processing step [Nac+16]. Another option is to route edges in compact bundles. The bundling of geometrical primitives is a general approach to de-clutter visual representations. It will be explained in more detail in Section 5.1.2 of Chapter 5. Overall, node-link representations are well suited to display graph nodes and edges in a balanced way. Their widespread use testifies to the universal utility of node-link diagrams. Yet, for graphs with very many edges, node- link representations can get cluttered so much that they resemble hairballs prohibiting any visual analysis. Moreover, the time complexity of optimizing the layout can be high. Matrix representations deal time complexity for space complexity and avoid the edge clutter as we will see next.
Visualization Methods and Techniques (cid:4) 115 To morF 1 2 3 4 5 6 7 8 4 1 8 2 2 5 3 4 7 5 1 6 6 3 7 8 Figure 3.58 Node-link diagram and corresponding matrix representation. Adapted from [vHSD09]. MatrixRepresentations Matrix representations are a graphical interpretation of a graph’s adjacency matrix. There is exactly one row and one column for each node v ∈ V. If there exists an edge (v ,v ) ∈ E between the i-th and the j-th node, the i j corresponding matrix cell at position [i,j] (and [j,i] for undirected graphs) is marked. Otherwise, the cell remains empty. The marks placed in the cells can also be utilized to visualize the edge weight or other edge attributes, for example, by varying the color or the size of the marks. Figure 3.58 depicts a simple node-link diagram and a corresponding matrix representation. As we can see, a matrix is visually focused on the graph’s edges, whereas nodes are merely represented as labels. This makes it possible to detect structural patterns such as those illustrated in Figure 3.59. However, such patterns appear only if the nodes are ordered appropriately along the horizontal and vertical axis. There are many ways to order the nodes of matrices [Beh+16]. A simple approach is to sort them according to a given 1 2 3 4 5 1 1 2 3 4 5 1 . 1 . 5 X 2 2 5 2 X 3 . 4 . 4 3 5 4 3 1 2 3 4 5 1 2 3 4 5 3 3 1 1 2 1 2 1 3 4 3 4 4 4 2 2 5 5 5 5 Figure3.59 Graph patterns represented as matrices and node-link diagrams. Adapted from [SM07].
116 (cid:4) Interactive Visual Data Analysis M MMMl ML l MeC Mt MmmmMo..h C oGGGC F M tla M meo ee tl oh haB iiiC eB hm C lll ...C u MG nu mllla T e To THC P PeB lC .o eee Sa eB o lnl BGm thM hc Nru hpu o oP.ea o G JG Tl Fh um cm nnn Pra WWec pa M Ita h o aA e ee e oS om q M .l rPn na anCu E aOe c Ze r eah oE ooB VF r LMp .r IaB mac LC p oGeF nb Vna niBbn u gv s u luau t l t ooh pp nMro Gr eain a vs rnB me rrr at aho tB oruMlFa a aDCC so a �l m muaat df oe aC u a aJeo dn u pz lB s ffl se mr me m aJ ee mmoro o pgn e nmi ujp brer e ta eesh t mr n ta gv bhhl ue su vo ad lsf o r sra ru peo rtr Mr ebr os ia lu .lvdh l hn cae e yas je eye bb a u amv vee db doo a � o� n d aea aav aaaa J g aal c ee rii dehe ceo l rr lu teub e yel rj rsm s v llii iir r rorii r e r ioo ii ��� ai ii i iiii ir ii nn oi on uo on edd nne n uhe nnn aaaul l il g cL l cuu la s ecs ca nne ee e nn ee rr r rrr ueee t tll l ee i l rl ii i Rd hddo pnuu n nndn nu12 12ee eee e ee ee eee eee ee eeee eTa a i y yy yc ss ssss sr rr rr r tt tttt tt fl ll dn pyr tad he ncuu em en i cr eeoe do n ed rr rat scnr li irs ae L � mn n emh soe oo ae ois ee ti s ae osre hatl rc l snc n yi n ure u g tu dl egl e rT �o a n etv d br m ee r cl a R nn cenee e o el lree te rii a eo s ms 12em eb pe Pl iru yrn ra urua u nooe rieni dl�e h Ihres nts i nM ea urst t l rfsf nneaey uvl ll ma a rreed ip Hsem a i ar i ni B ide Pe ie e e ce uee o� rpp oe scna aTr ee pa naeeru e aen te fflG mu aat itt a ly Br� a V12l r ane ie auttrn l loa . ... ..tu eb hut er e vn imm nee iaa oiu lt Mrra lhhf st iel h o o u o mml e eee eee gnn ln io rn ...le dd nplh h�e ni ee vr l i na udr io o utpo bm cs l sg ivlm e eeegov lq b e ttm i rrc j rm mmm mmeb pbh Gu uuue nhb vo oaa yll ez mc r pus jus anu b du ooo o ooer ndt av ra aaaal lll yii ja a v liuee aaan a l lllloo ooooo ohhhhh s oaaaa pn hhe a.rr ee oa aaa arr l WWr cri MM MM MM MMM M MMMM MMMooual l t iOi NsGG GG GGGDA VBB BBBB BB B CC CCCCC CC CCCC PPPEE TTT ZF FFF SSF LLLJ JJJI M M MM l MLl Me CM tM m mm M o. .h CoG GG CF Mtl a Mm e oee tl o hha Bi iiC e B hm Cl ll . .. C uMG nu ml ll aT eT oT HC PPeB lC.o e ee Sa e Bo lnl B Gmth Mh c Nruh p uooP. eao G JG TlF h um cmn nn P raWW ecp aM I ta h oa Aee eeo S omq M .lr P nna an Cu E a Oe cZ erea hoE ooB VF rL M p. r I aB mac LC po GeF nbV n an i B bnu gv sul uau tltooh pp nM roGr e ai nav srn B mer rr ataho tBo r uM lFa aa D CCso a� l mmu aat df oea Cuaa J eo dn upzlB s ffls em rmem a J ee mm or o op g ne nm i uj p bre re t aees h tm rn ta g v bhhlu es uvoa dl sfo rsr ar u p e o rt r Mreb ro s ia l u .lv dh lh n c a eey asj eeye bb au amvv ee d bd oo a� o� n da eaa ava aa aJ gaal ce er iideh ec eol rr l uteu b ey elr j rs m sv lliii ir rror iir e ri ooi i ��� aii ii ii i iir iin n oi on uo o n edd nnen uh en nna a au lli l g c Ll cu ula se csca nn ee ee n ne err rr rru ee et tl llee ilrl ii iRd hdd op nuu nn ndn n u12 12e ee eee ee ee eee e e eee e eee e Taa i yyyyc ssss sss rr rr rr ttt tt t t tflll (a)Ordered by name. dn p yr ta d he n cuueme ni c re eoed on ed r rrat s c nr liir sa eL� mnn e mhso eooae o is ee t is a e osr e ha ltr cls nc n yin ur e u g tu d leg le r T� oane tv d brem er cla Rnnce nee eoe l lre e te riia eo sm s12 eme bp el Pi r uy r nra ur ua un oo er ie nid l �eh Ih re snt si nMe a urs t tlr fsf nn e aey uvl l lm aa r r ee di pH se m ai arin i Bi dePe i ee ecue ee o �r pp oes c na aT ree pan ae er u eae n te fflG muaat itt aly B r� a V12l ran ei eau ttr nl lo a. .. . ..t ub e hu t er evn i mm ne ei aao iu lt Mr ral h hf stie lh o ou omml e ee e ee eg nnl nior n.. .le ddn pl hh� en ie e vr li n au dr i oo utpo bm cs l sg i lvme ee ego vl q be t tmi rr c jr e bm mm m mm p bh G uuuu e nh bv oo aay lle zmc rpusj us an u bdu o ooo e oo r ndt av r aa aa al ll ly iij a avl iu e ea aa nal ll ll oo ooo ooo hh h hhs oa aa a pn hh ea . rree oaaa a alrr WW r criM MM M MM M M MM M MM M MM M oo ua l l ti Oi N sG GGG G GGD AV B BB BBBB B B CCCCC C CCCC C CC PPPEE TT TZFFF F S SF LL LJ J JJ I MM M Ml MLlM e C Mt Mm m m M o. . h Co G GG C FM tlaM me oe et lo h haB i iiC eBh m Cl ll. . . C uMG n um l ll aT eTo THC P P eBl C .o e ee Sae B ol nl B Gmth Mh cN ru h pu o oP .e aoG JG T lF hum cmn nn Pr a WWe cpa M Ita ho aAee ee o So mqM .lrP n n a a nCu E a Oec Z er ea hoE ooB V Fr LM p. r IaB ma c LC po G eFnb Vn an iB bnu g v s ulua utl t ooh pp nM ro Gr e ai nav srn B m er rr at aho tBoru M lFa aaDCC so a � lm mu a atdf oe a CuaaJ e o dn u pzlB sffls emr m ema Jee mm or o op gn en m iuj pb re re t ae esh tm rnt ag vbhh luesuvoa dl sfo r sr ar u pe ort r Mr eb ros ia lu .l vdh lhn ca e e yas je eye b b au am vve e d bd oo a�o �n da eaa ava aa aJ g aa l ceerii de hec eol r r l ut eu b eye l rjr sms vll iiiir r ror iir e r io oi i � �� a ii iiii iii r iin no i onu oo nedd nn e nuh enn n a aaul lil gc Ll cu ula s ec s ca nnee e en ne err rr rru ee e tt ll lee il rl i iiRdh dd opn uu nn ndnn u12 12 ee ee e eee ee e ee ee e ee ee ee eT a aiy yy yc s sss sss rr rr rr ttt tt tt tf l ll (b)Ordered by frequency. dnp y r ta dh e ncu u em en ic r e eo ed oned r rr a ts cn rl i irsa eL�m nne m hs oe oo ae o i s eet isaeo sr eh al tr cls nc n y inu r e ug t u dl e glerT �o ane t vdb remer c la R nn ce nee e o e ll re et e ri i aeo sms 12 eme b p elP i ru yr nr aur u au n oo er ien idl� eh Ihr e sn ts i nMea urs t tlrf sf nn e aey uvl l lm a a rr ee di pH sem ai ar i niB i d ePe i e e ecue ee o �r ppo esc naa T re epa n aee rue ae n te fflG m u aa tit ta ly Br� aV12 l ra n ei eau t tr n l lo a .... .. t ub e hu t ere v n i mmn eeia aoi ult Mrr al hh f s t iel ho ou omml eeee ee eg n nl nio rn ...l edd n pl hh�en iee vrli n au dr io o ut pobm cs ls gil vme eeego vlq be tt mi rr cj re bmmmm mm pbhGu u uu enh bv o o aayll e z m crpu s jusa n ub du oo oo eoo rn dt av r aaaaa l lll yii j aav l iu ee aaa n al llll oo o oo oo o hhhhh so aaa apn hh ea. rr ee o aa a aa lrr WW rcr i MMMM MM MM MMMMMMM MMoo ua llt i Oi Ns GG GG GG G DA VB B BB BBB B B CCCCC CCCC C CCC PPP EE TTT ZFF F FSSF LLL J JJJ I (c)Ordered by community. Figure 3.60 Differently ordered matrix representations of the same data. Adapted from bost.ocks.org/mike/miserables/. nodeattribute.Morecomplexapproachesclusterthenodesbasedonstructural properties of the graph so that similar rows and columns are placed next to eachother.Asfindingtheright orderingthatrevealsthecharacteristicfeatures of the graph remains challenging, interactive rearrangement is often helpful. Figure 3.60 illustrates the impact that the ordering has. All three matrices showthesameco-occurrencegraphoftheplayLesMisérables,whichwealready visualized in the introduction of this book in Figure 1.1 on page 7. Different shadesofgreenvisualizethenumberofco-occurrencesoftwocharacters,where darker greens represented larger numbers. The shades of blue in the matrix diagonal visualize the overall frequency of a character’s occurrences. Three different ordering procedures are depicted. In Figure 3.60a, rows and columns are simply ordered by the name of the characters. The matrix in Figure 3.60b has been ordered according to the characters’ occurrence frequency. Finally, Figure 3.60c shows the matrix ordered by the result of a community detection algorithm. As we can see, the more sophisticated the ordering procedure is, the clearer are the patterns in the matrix representation. Taken together, the main benefit of matrix representations is their focus on the graph’s edges, whereas the nodes are merely indicated as labels along the rows and columns. The visual mapping of the graph data is straightforward. While there is no need for layout optimization, matrix ordering is an intricate problem. It should further be noted that a matrix’s drawing space grows quadratically with the number of nodes. ImplicitRepresentations Matrix and node-link representations as described before have in common that they visualize the relations in the data, that is, the edges by means of dedicated graphical primitives (marked cells or links). In this sense, we can call these representations explicit.
Visualization Methods and Techniques (cid:4) 117 1 4 2 1 2 4 4 5 1 5 1 2 5 3 2 3 4 5 3 3 (a)Node-link. (b)Inclusion. (c)Overlap. (d)Adjacency. Figure3.61 Node-link representation compared to implicit representations. Adapted from [SHS11]. In contrast to that, implicit representations lack explicitly drawn edges. Instead,relationsbetweennodesareencodedimplicitlybytherelativeposition of the nodes. However, this strategy is applicable only if the graph structure followssomeregularities.Generalnetworkscanhardlybevisualizedbyimplicit approaches. Yet, implicit representations are very well suited for trees with their regular parent-child relations. There are three options for the implicit encoding of parent-child relations. The encoding can be based on inclusion, overlap, or adjacency as illustrated in Figure 3.61. For inclusion, the child nodes are included in the parent node. For overlap, the children merely overlap their parent. For adjacency, children and parent are placed next to each other. The absence of explicitly drawn edges obviously puts a visual emphasis on the nodes. In fact, implicit representations often aim to be space-filling, meaning that they strive to place the graphical primitives representing the nodes as tightly as possible in the available drawing space. Depending on the applied layout strategy, this can be fast and simple, or time-consuming and complex. Examples of implicit representations are abundant [SHS11]. Figure 3.62 showsafewselectedtechniques,allvisualizingthesameclassificationhierarchy ofmammalswithabout3,000nodes.The2DsquarifiedtreemapinFigure3.62a is based on inclusion [BHvW00]. The information pyramids in Figure 3.62b use adjacency to encode the parent-child relations by stacking the levels of the hierarchy along the z-axis [AWP97]. The 3D sunburst in Figure 3.62c uses adjacency as well, but now the levels are organized as a kind of shells around the central root node [SHS11]. For many more examples, the reader is referred to the website http://treevis.net, which collects all kinds of tree visualization techniques. In summary, the upside of implicit graph representations is their efficient usageofdrawingspaceforthenodeprimitives.Adrawbackisthatoverplotting can hamper perception if nodes are not carefully laid out. Moreover, the directionality of edges may get lost, depending on the chosen layout strategy. Overlap can easily show directionality, as either node u overlaps node v or the other way around. However, adjacency is symmetric and thus requires additional conventions such as drawing from top to bottom or from left to right in order to encode the directionality.
118 (cid:4) Interactive Visual Data Analysis (a)Squarified Treemap. (b)Information pyramids. (c)3D sunburst. Figure3.62 Implicit visualizations of a classification hierarchy. Software cour- tesy of Steffen Hadlak. HybridRepresentations Node-link, matrix, and implicit representations are suited for different graph data.Node-linkdiagramsaregoodforsparsenetworks,whichhaveamoderate number of edges. Dense networks with many edges are best visualized using a matrix. Trees, as we just said, are nicely represented by implicit approaches. But what if our graph has sparse and dense parts and includes tree-like sub-structures at the same time? This is where hybrid representations can help. They combine different representational paradigms. The idea is to mix representations, where possible, inordertoutilizethepositiveaspectsandcompensateindividualdisadvantages. Yet,thisalsoaddsanewquestion:Whichpartofagraphshouldberepresented in which way? Some hybrid representations leave the decision completely to the user, who then has to style the different parts of the graph via interaction. Yet, it is also possible to use methods to detect densely connected, sparse, or tree-like sub-structures in graphs and to assign representational paradigms automatically [AMA07]. AparticularlyniceexampleofahybridtechniqueistheNodeTrix[HFM07]. As the name suggests, it is a combination of node-link and a matrix repre- sentation. Figure 3.63 shows an example of the Les Misérables co-occurrence network, where the dense communities are visualized as sub-matrices, whereas connections between these communities are depicted as curved links. To sum up, node-link, matrix, implicit and hybrid representations are the fundamental options that visualization designers have at their disposal when creating images of graphs. In the next section, we will look at advanced graph visualization approaches taking into account additional data facets. 3.5.3 VisualizingMulti-facetedGraphs With the visualization approaches described above, we are able to represent the nodes and edges that make up a graph’s relational structure. However,
Visualization Methods and Techniques (cid:4) 119 CMU - Roth et al. Bederson et al. PARC Eick et al. Shneiderman et al. Berkeley Plaisant et al. Figure 3.63 NodeTrix visualization of a co-author network. Adapted with permission by Jean-Daniel Fekete from www.aviz.fr/Research/Nodetrix. we also mentioned additional facets that might be relevant when analyzing graphs. In particular, we named multivariate data attributes, temporal and spatial references, as well as hierarchical groupings of nodes and edges. This section deals with incorporating these additional facets into the visualization of a graph. Multi-faceted graph visualization is a research topic on its own [HSS15]. The design challenge lies in finding a balanced visual representation of the involved facets that meets the analytic or communicative goals of the user. In order to arrive at a well-balanced representation, it makes sense to follow a two-step design procedure. First, a base representation has to be defined for the primary graph facet whose depiction governs the overall display. Second, the additional facet(s) will be incorporated into the base representation. For example, if we want to visualize the graph structure together with multivariate attributes, we could choose a node-link diagram as the base representation for the structure. The visual representation of multivariate attributes could be incorporated by varying color, size, or shape of the nodes and the color, width, or dash pattern of the links. Alternatively, if our focus is moreontheattributesandlessonthestructure,wecouldchooseatable-based visual representation for multivariate data elements. The structural aspect could then be incorporated by drawing additional links between the rows of the table. In general, analogously to what has been said about the combination of views in Section 3.1.2, a visual combination of different data facets can be performed in two different ways. On the one hand, a temporal composition can be implemented by utilizing display time to show one facet after the other. On the other hand, the combination of facets can be realized through a spatial composition, which utilizes the available display space. Thebasicoptionsforspatialcompositionarejuxtaposition,superimposition, and nesting. Figure 3.64 illustrates these options for the case of combining
120 (cid:4) Interactive Visual Data Analysis (a)Juxtaposition. (b)Superimposition. (c)Nesting. Figure 3.64 Spatial composition of graph facets in a single representation. Adapted from [HSS15]. structure and geo-spatial context. For juxtaposition, structure and geo-spatial context are shown side-by-side. This results in a balanced view of both facets. However,itisnecessarytolinkthefacetsviaadditionalvisualcues,forexample, by arcs as in Figure 3.64a. For superimposition, the facets are drawn on top of eachother.IntheexampleinFigure3.64b,thegeo-spatialcontextisgoverning the display of the structure in that the geographic regions prescribe the layout of the graph nodes. This is also the case for nesting, where the nested facet has to obey the positioningandspaceconstraintsofthebaserepresentationintowhichwenest. Depending on which facet is nested into which, the visual results can differ significantly. For example, both instances of nesting in Figure 3.64c show the samedata,butconveydifferentinformation.Nestinggeo-spacewithinstructure (left) tells us which geographic region belongs to a given node, whereas nesting structurewithingeo-space(right)communicateswhichnodesbelongtoagiven region. Letusnextlookattwoconcretesolutionsthatdealwithmulti-facetedgraph visualization. First, we will consider the visualization of dynamic spatial trees asa3Dlayeredmaprepresentation.Thesecondsolutionvisualizesmultivariate compound graphs in a multiple-views system. 3DLayeredMapVisualizationofDynamicSpatialTrees As defined earlier, dynamic graphs change over time T, and spatial graphs are connectedtospatialreferencesS.Similartotemporaldataingeneral,dynamic graphs are usually analyzed with respect to their evolution over time. Which nodes and edges enter or leave the graph, and which parts remain persistent over time? When dealing with spatial graphs, the analytic interest is focused on the interplay between the graph structure and the spatial context. Next,wewanttodemonstratehowtreestructurescanbevisualizedtogether with their temporal and spatial facets. The example we are going to discuss is based on a map display of the spatial frame of reference. The tree structures will be nested into the regions of the map. Superimposition of several map layers is then used to communicate the temporal evolution of the data.
Visualization Methods and Techniques (cid:4) 121 Figure3.65 Map with tree layouts embedded into selected regions. Spatial Nesting of Tree Structures As just said, we start with a display of the spatial facet as a map. The next step is to embed the structural aspect of the trees into the map regions. For the later integration of the temporal facet, it is necessary to use a layout algorithm that assigns fixed positions to the tree nodes. Moreover, the algorithm must be able to adapt the layout to the irregular shape and size of the geographic regions. Both requirements can be fulfilled by combining a point-based layout with a skeleton-based region subdivision [Had+10]. Figure 3.65 shows an example result. We can see how the trees fit nicely into the different map regions. Temporal Layering and Difference Encoding In order to visualize time, a separatemaplayeriscreatedpertimestep.Followingtheideaofthespace-time cube introduced in Section 3.4.3, the map is tilted into 3D and the layers are stacked along the third display dimension. Figure 3.66 shows three such layers. We can now easily see the structures per time step. Yet, what can be more difficult to detect are the changes between the indi- vidual layers. Therefore, the visualization includes an explicit visual encoding of differences. Red and blue spikes indicate where nodes left or entered the trees. Moreover, orange and brighter blue lines connect those nodes for which the associated data values change significantly. Spikes and lines are always perpendicular to the map layers thanks to the fixed node positions of the tree layout.
122 (cid:4) Interactive Visual Data Analysis Figure 3.66 Three map layers visualize the data of three consecutive time steps. Spikes and lines indicate differences between the layers. Note that the static figure cannot reproduce the experience of a live 3D visualization due to the lack of dynamic motion, which is an important depth cue.Becauseourhumanperceptionisverywelltunedtointerpreting3Dscenes, the actual association of nodes and edges with layers, spikes, and lines can be quite easily interpreted with the running system. But still, there is a clear limitation of the presented approach: Only a few time steps can be visualized simultaneously. If the number of time steps increases, alternative means must be applied. One option would be to scroll through time, which corresponds to a mapping of the time in the data to the display time. In general, multi-faceted graph visualization becomes more difficult when the graph itself is large and when the involved facets are extensive as well. In the next section, we illustrate how a multiple-views approach can help in coping with this challenge. Multi-viewVisualizationofMultivariateCompoundGraphs As the number of nodes and edges in a graph increases, it becomes more and more difficult to show all of them in a visualization. In such cases, it makes sense to consider groups of nodes and edges as in compound graphs. A compound graph G = (V,E,H) is a graph where a hierarchical grouping is given as a rooted tree H. The set of leaves of H corresponds to the nodes V of G. Non-leaves of H correspond to the groups and are sometimes called meta-nodes or cluster nodes. Groups can be expanded or collapsed in order to study a graph at different levels of abstraction. A multivariate compound graph G=(V,E,H,A) is a graph whose nodes V and edges E have additional data attributes A. The central objective when analyzing multivariate compound graphs is to understand the relationship of structural properties of the graph and the
Visualization Methods and Techniques (cid:4) 123 attributes and groups associated with it. For example, given a subset of nodes being similar in their attributes, do they exhibit similar structural properties? Or, given a certain sub-structure of the graph, do the nodes in that sub- structure exhibit similar attribute values? Or, are the nodes in one group similar to the ones of another group? To answer these and similar questions, multiple coordinated views can be employed as in the graph visualizations system CGV [TAS09]. From a conceptual perspective, the system solves the problem of multi-faceted graph visualization via juxtaposition of facets, while individual views may work with superimposition. Multiple Views Figure 3.67 shows eight different views as provided by the system. The graph structure (V and E) is visualized in the matrix at the left and in the node-link diagram in the center. Selected attributes of A are encoded in these views by varying colors and sizes. A dedicated multivariate representation of the data attributes is offered by the parallel coordinates plot at the bottom. The density of nodes in the graph layout is visualized in a splat view in the bottom-right corner. What remains to be visualized is the hierarchical organization H of the graph. This is done in the three views at the top. The graphical hierarchy view (top right) shows nodes as colored triangles. This nicely visualizes the sizes of the groups defined by H. The textual tree view (top middle) enables users to read the labels associated with the groups. The 3D tree representation on the left is called Magic Eye View [KS99]. It shows the compound hierarchy Figure3.67 Multiple coordinated views for multi-faced graph visualization.
124 (cid:4) Interactive Visual Data Analysis explicitly as red dots and blue links projected onto a hemisphere. Additional orange arcs span the hemisphere to visualize selected links of E in relation to H. Finally, a property view in the bottom-left corner lists the attributes A associated with a selected node or edge. As we can see in this example (and in Figure 1.3 on page 10), there are quite different representations of one and the same graph data. Yet, each view focuses on different facets of the graph. Coordinated Highlighting In order to make sense of such a multiple-views visualization, it is obviously necessary to mentally connect the different repre- sentations. This can be supported by coordinated highlighting, which means thatthefocusedgraphelementisconsistentlymarkedinallviews.Forexample, the node with the label “albert, einstein, life” is marked with a circle in the node-link diagram and with a blue background in the tree view. The parallel coordinates plot visualizes the polyline of the node in red, rather than the regular black. The graphical hierarchy view, the matrix view, and the splat view show crosshairs to mark the node. Changing the focus in either view will change the marks in all views simultaneously. Additionally, the blue line in the graphical hierarchy view on the top indicates the current cut through the grouping hierarchy. When the user descends or ascends in the hierarchy by expanding or collapsing groups, the blue line will automatically adjust its shape to make clear to the user how deep in the graph the ongoing analysis takes place. The different views of the graph together with the coordinated highlighting make it possible to study even large multivariate graphs. In general, we can conclude that multi-view visualizations are a good solution when the data are complex in the sense that they contain a diverse set of facets. With the two examples of multi-faceted graph visualization, the section on how to represent relations among the data comes to an end. We have learned thatdedicatedmethodsexisttovisualizedifferentclassesofgraphs.Visualizing multi-faceted graphs requires diverse and rather complex representations and considerable design effort is necessary to create them. In Chapter 5, we will see how graph visualization in general can be advanced further with the help of automatic analytical computations. 3.6 SUMMARY This chapter explained how data can be mapped to visual representations for interactive visual analysis. At the heart of visualization are two fundamental steps, the visual encoding and the visual arrangement. The visual encoding is about mapping data to graphical marks and visual variables. The visual arrangement describes the layout of the data-representing graphical marks in one or multiple views. For the most part of this chapter, we described how the fundamental visualization steps are implemented in the context of different classes of data.
Visualization Methods and Techniques (cid:4) 125 In fact, a key message of this chapter is that the characteristics of the data are crucialfordesigningtheirvisualization.Wedealtwithseveraldataaspectsthat need to be considered: Are the data related to time T, are the data embedded in the geographic space S, or do the data contain relations R to be exposed in the visualization? These aspects can also occur in combination. In particular, we discussed the visualization of: • Multivariate data A • Temporal data T ×A • Geo-spatial data S×A • Spatio-temporal data S×T ×A • Graphs R • Multi-faceted graphs R×T ×S×A Aswehaveseen,themoreaspectsareinvolved,themorewecanclaimthat designing expressive and effective visual representations is challenging. And therearestillmoreaspectsworthbeingconsideredinthecontextofinteractive visual data analysis. Additional Visualization Aspects What we have not addressed in this chapter is the aspect of data quality. In fact, most of the data in the wild are “dirty”. By dirty we mean there are missing values, there are uncertain values, there are inconsistencies, and there are errors [GS06]. These problems add a whole new layer to the visualization design. For example, the visualization of uncertain geo-spatial data not only involves the aspects of space S and data A, but also the aspect of uncertainty U. Considering uncertainty U leads to new questions one may want to investigate. Here is an incomplete list: • A→U – What data are uncertain? • T →A×U – How do data values and uncertainty evolve over time? • T ×S →U – Where in time and space are the data uncertain? • R→U – Which structural relations are uncertain? • ... Uncertaintyisbutoneaspectamongothersworthbeingintegratedintothe visualization. Another source of information is provenance P. This not only includes data provenance, which explains how the data came about, but also insight provenance, which explains how insights have been generated [Rag+16]. It is obvious that the inclusion of provenance P expands the space of possible analytic questions to an even greater extent.
126 (cid:4) Interactive Visual Data Analysis Prioritized Multi-aspect Visualization With such a multitude of aspects having an impact on the visualization design, a simple combination of visual- ization techniques is often not successful. Instead, it is necessary to prioritize certain aspects and adjust the visualization design accordingly [Düb+17]. To this end, scalable visual representations with different levels of sophisti- cation must be developed for each involved aspect. For example, the spatial aspect could be represented in full detail using sophisticated terrain rendering, or as a contour representation that only indicates the main features of the spatial frame of reference. If space plays an important role in an analysis scenario, the sophisticated terrain rendering is employed. If space plays a minor role, the graphically less-expensive contour representation can be used. Making such importance-driven design decisions for all involved aspects leads to a prioritized multi-aspect visualization. However, this is a topic that requires much more research to arrive at a mature state with a comprehensive consideration of all relevant analysis aspects. Future work in this direction can draw inspiration from a wealth of available visualization techniques, some of which are collected in visualizations surveys. Visualization Surveys Since the foundation of visualization as a field in computer science, a variety of visual representations have been developed for all kinds of data. Here, we could only describe the fundamental concepts and procedures and illustrate them with selected examples. Many more techniques are described in the visualization literature. To help potential users find visual solutions to their data analysis prob- lems, it makes sense to catalogue the available visualization techniques. We already mentioned browser.timeviz.net and treevis.net, which provide comprehensive lists of techniques for temporal data and tree data, respec- tively. The InfoVis Wiki curates a compilation of such interactive online sur- veys at infovis-wiki.net/wiki/Interactive_Online_Surveys, including, for instance, surveys on: • Text visualization: textvis.lnu.se • Biological data visualization: biovis.lnu.se • Financial data visualization: financevis.net • Visualization of scientific literature and patents: paperviz.org • Visualization of dynamic graphs: dynamicgraphs.fbeck.com Often, these online resources are based on books or state-of-the-art reports that study specific visualization issues in greater detail [ML17]. These works should provide the interested reader with plenty of material to delve further into the world of visual data representations. At this point, we close the chapter on visualization. In the next chapter, we will focus on the role of interaction in the context of visual data analysis.
Visualization Methods and Techniques (cid:4) 127 FURTHER READING General Literature: [Spe07] • [War12] • [WGK15] Color Coding: [BRT95] • [HB03] • [ZH16] Visualization of Temporal Data: [Aig+11] • [Wil11] • [Bac+17] Visualization of Geo-spatial Data: [Mac95] • [AA06] • [And+13] Graph Visualization: [vLan+11] • [KPW14] • [Nob+19]
4 CHAPTER Interacting with Visualizations CONTENTS 4.1 Human in the Loop ......................................... 131 4.1.1 Interaction Intents and Action Patterns ............ 132 4.1.2 The Action Cycle ................................... 135 4.2 Requirements for Efficient Interaction ...................... 136 4.2.1 Interaction Costs .................................... 136 4.2.2 Directness of Interaction ............................ 138 4.2.3 Design Guidelines ................................... 143 4.3 Basic Operations for Interaction ............................ 144 4.3.1 Taking Action ....................................... 144 4.3.2 Generating Feedback ................................ 146 4.4 Interactive Selection and Accentuation ..................... 148 4.4.1 Specifying Selections ................................ 149 4.4.2 Visual Emphasis and Attenuation .................. 153 4.4.3 Enhanced Selection Support ........................ 156 4.5 Navigating Zoomable Visualizations ........................ 159 4.5.1 Basics and Conceptual Considerations ............. 160 4.5.2 Visual Interface and Interaction .................... 162 4.5.3 Interaction Aids and Visual Cues ................... 164 4.5.4 Beyond Zooming in Two Dimensions ............... 168 4.6 Interactive Lenses ........................................... 173 4.6.1 Conceptual Model ................................... 173 4.6.2 Adjustable Properties ............................... 176 4.6.3 Lenses in Action .................................... 178 4.7 Interactive Visual Comparison .............................. 184 4.7.1 Basics and Requirements ........................... 184 4.7.2 Naturally Inspired Comparison ..................... 186 4.7.3 Reducing Comparison Costs ........................ 190 4.8 Interaction Beyond Mouse and Keyboard .................. 194 4.8.1 Touching Visualizations ............................. 194 129
130 (cid:4) Interactive Visual Data Analysis 4.8.2 Interacting with Tangibles .......................... 197 4.8.3 Moving the Body to Explore Visualizations ........ 202 4.9 Summary .................................................... 204 V ISUALIZATION techniques provide us with expressive visual repre- sentations of data. As humans, we can interpret and make sense of the visual representations and draw conclusions about the underlying phenomena. Yet, if we look only passively at visual representations, we waste much of the potential of visual data analysis. Ideally, we would like to actively engage in a dialogwiththedata.Thisincludesgeneratingdifferentviews,studyingspecific details, and fine-tuning the visual encoding. All these activities are enabled through interaction. This chapter elaborates on many different forms of interaction for visual data analysis. When discussing interaction, four key aspects are relevant: the human, the tasks, the data, and the technology. Visual representations are studied and interpreted by human users. Based on their impressions, they will interact with the visualization system. The interactions are typically related to analytic tasks to be accomplished with respect to the data being studied. Technology is the mediator in the process of interactive visual analysis. It displays visual output and accepts the user’s input. Wrapped into a single sentence, this means: The user solves analytic tasks on data using technology. This chapter touches upon all of these four aspects. The chapter’s first part willfocusonthehuman’sroleininteractivevisualdataanalysis.InSection4.1, we motivate the human-in-the-loop approach of interactive visual reasoning and consider conceptual perspectives on humans interacting with visualization systems.Therequirementstobefulfilledinordertoarriveatusefulandusable interactionforvisualdataanalysiswillbediscussedinSection4.2.Throughout this chapter, we will keep a keen eye on human aspects. For the chapter’s second part, our attention will shift to the tasks to be carried out interactively and to the data on which they operate. We start with basic low-level interactive operations in Section 4.3. Section 4.4 deals with interactive selection and visual accentuation as fundamental tasks in visual data analysis. In Section 4.5, we continue with multi-scale exploration and navigation in zoomable visualizations. Section 4.6 explains how flexible and light-weight adjustments of visual representations can be made via interactive lenses. As an example of a more complex analytic task, Section 4.7 focuses on supporting visual comparison with naturally inspired interaction. As we discuss interaction for these different tasks, we will also be considering the data aspect. We will see that interaction, very much as the visualization, has tobedesignedaccordingtothedataathand.Thischapterincludesinteraction techniques for multivariate data, temporal data, spatio-temporal data, and graph data.
Interacting with Visualizations (cid:4) 131 Thethirdandlastpartofthischapterisdedicatedtothetechnologyaspect. Section 4.8 goes beyond traditional paradigms and sheds some light on novel ways of interacting with visualizations. As we will see, modern technologies such as touch and tangible interaction or large high-resolution displays open up new possibilities for interactive visual data analysis. Yet, new designs are necessary to make the best of what the new technologies offer. 4.1 HUMAN IN THE LOOP As indicated, we will begin this chapter with some general thoughts on inter- action for visual data analysis and the role of the human within this process. Let us start with the purpose of interaction. Is interaction necessary at all, can’t we let the computer do all the work? Well, if the analytic problem can be formalized precisely and its solution be calculated exactly, then certainly no interaction is needed. Typically, however, data analysis activities are not so simple. Jacques Bertin made this clear even before visualization existed as a field [Ber81]: “Agraphicisnot‘drawn’onceandforall;itis‘constructed’andreconstructed untilitrevealsalltherelationshipsconstitutedbytheinterplayofthedata.The bestgraphicoperationsarethosecarriedoutbythedecision-makerhimself.” Bertin,1981 Bertin’sassessmentstillholdstruetoday.Analyticgoalsareoftenill-defined andexploratoryinnature.Sometimesitisevenunclearwhatthedesiredresults are or how they should look like. In Chapter 3, we have seen many different visualization techniques. Deciding for suitable ones and configuring them appropriately require human expertise. It is interaction that enables the user to experiment with different visual encodings and to look at the data from different perspectives. Interaction is not only helpful for specifying how to visualize data, but also for selecting what to visualize. Data often capture complex phenomena as an interplayofthousandsofmulti-facetedpiecesofinformation.Thehumanmind, however, can digest only a limited amount of information at a time. Therefore, the data analysis must be divided into meaningful chunks. It is interaction that enables the user to define such chunks, to navigate between them, and to combine them in order to form a comprehensive understanding [Spe07]. In fact, interaction is not only for pragmatic purposes bringing the user closer to a goal. Interaction also serves epistemic purposes helping the user form and scaffold a better mental model of the problem being investigated and a better understanding of the tools being used [KM94].
132 (cid:4) Interactive Visual Data Analysis In summary, visualization helps us see things that are otherwise not visible, and interaction allows us to do things that we would otherwise not be able to do. While visual representations may provoke curiosity, interaction provides the means to satisfy it. 4.1.1 InteractionIntentsandActionPatterns Interaction is clearly important for visual data analysis. But what exactly actuates users to interact and what interactions are common? To answer these questions, next we will look into a high-level categorization of interaction intents and a more fine granular list of action patterns. InteractionIntents Interaction intents capture why users interact with visual representations. Sevenbroadcategories canbe identified [Yi+07].Without goingintotoomuch detail, next we will describe the basic idea behind each category. Mark something as interesting. When users identify something interest- inginavisualrepresentation,theytypicallyhavetheintenttomarkitfor furtherinvestigation.Markingscanbetransient tohighlightintermediate findings, or permanent to memorize important analysis results over a longer time. Show me something else. Forlargeandcomplexdata,itisoftenimpossible to visualize all information in a single view. To develop a comprehensive understanding, users have to explore different parts of the data and experiment with different combinations of variables to be shown in the visualization. Show me a different arrangement. Generating different visual arrange- ments allows users to study data from various perspectives to obtain different insights. For example, arranging data according to time can reveal trends, while attribute-driven layouts may be better suited to communicate data distribution. Show me a different representation. The visual encoding is decisive for what can be derived from a visual data representation. Therefore, users want to adapt the visual encoding to suit their needs, be it to carry out different analysis tasks, or to confirm a hypothesis generated from one visual encoding by checking it against an alternative one. Show me more or less detail. As in real life, users want to look at certain things in detail. On the other hand, users need an overview to keep themselves oriented. During data exploration, the level of detail needs to be adjusted constantly to satisfy the conflicting demands of studying subtleties and seeing the big picture.
Interacting with Visualizations (cid:4) 133 Show me something conditionally. It makes sense to restrict the visual- izationtoshowonlythosedatathatadheretocertainconditionsorsearch criteria that are particularly relevant to the task at hand. Interactively filtering out or attenuating irrelevant data clears the view and allows users to focus on their task. Show me related things. When an interesting finding has been made, a logical next step is to ask whether similar or related discoveries can be made in other parts of the data. To find, compare, and evaluate such relations, users wish to be connected to them on demand, for example by means of visual links to the related data. These seven categories cover interactions that are particularly relevant for visual data analysis. Next, we describe two additional categories of more general intents, which are nonetheless quite relevant for visual data analysis as well. Let me go back to where I’ve been. Because visual data analysis is an exploratory process, it is usually necessary to try out new views on the data and experiment with alternative what-if scenarios. If exploratory actions do not yield the desired results, users intend to return to a previous state. A history mechanism can keep track of the interaction and allows users to undo and redo operations. Let me change the interface. In addition to tuning the visual representa- tion to the data and tasks at hand, users also want to adjust the overall visualanalysissystem.Thisincludesadaptingtheuserinterface(e.g.,the arrangement of windows or the items in toolbars), and also the general management of system resources (e.g., display resolution and amount of memory to be used). The presented interaction intents summarize the reasons for an active participation of users in the visual data analysis. The next section focuses on more concrete action patterns. ActionPatterns An alternative perspective on interaction is offered by action patterns [SP13]. They are more oriented toward what users actually do to support the creation of insight during visual data analysis scenarios. Two types of action patterns can be distinguished. An action pattern can be: • unipolar or • bipolar. Unipolar patterns describe human actions that are performed in only one direction. For these actions, there is no natural opposite action. If semantically
134 (cid:4) Interactive Visual Data Analysis TABLE4.1 Examples of unipolar human action patterns [SP13]. Pattern Description Arranging changes ordering, either spatially or temporally Assigning binds features or values to be encoded Blending fuses visual representations together to form one entity Comparing determines similarities or differences Drilling brings out and displays interior, deep information Filtering displays subsets obeying certain criteria Navigating moves on, through, and around the data Selecting focuses on or chooses either individuals or groups TABLE4.2 Examples of bipolar human action patterns [SP13]. Pattern Description Collapsing/ fold in and compact visual items, or oppositely, fold them Expanding out or make them more diffuse Composing/ assembleandjointogethertocreateholisticrepresentations, Decomposing or oppositely, break up into separate components Linking/ establish relationships or associations, or oppositely, disso- Unlinking ciate and disconnect relationships Storing/ put aside for later use, or oppositely, bring stored items Retrieving back into usage meaningful at all, unipolar action can only be reversed by a generic undo operation. Selecting, navigating, and comparing are examples of unipolar action patterns. Bipolar patterns are characterized by the existence of two actions, where one action is the natural opposite of the other. Together, both actions can be used to make progress and to return to previous states. An example of a bipolar pattern is collapsing/expanding. The natural opposite action for collapsing multiple data elements into a single representative is expanding the representative to reveal its individual members. Therearemanymoreunipolarandbipolaractionpatternsbeingrelevantin thecontextofinteractivevisualdataanalysis.Tables4.1and4.2listprominent examples. We will return to some of them in particular in dedicated sections. In Section 4.4, we will look at selecting and filtering in more detail. Questions of navigating and drilling will be addressed when we talk about zoomable visualizations in Section 4.5. In Section 4.6, we will blend alternative visual representationsbymeansofinteractivelenses.Comparingasaquiteinteresting action pattern will be discussed in Section 4.7. At this point it should be clear that interaction is a multi-faceted concept forgivingmeaningtowhatisperceived,forcollectingrelevantinformation,and forextractingandstoringinterestingfindings.Inotherwords,thehumanisnot
Interacting with Visualizations (cid:4) 135 only a passive onlooker, but an active participant in a dynamic process. How this process can be modeled and understood conceptually will be discussed next. 4.1.2 TheActionCycle Fromaconceptualperspective,theideaofthehuman-in-the-loopcanbenicely explainedwithNorman’saction cycle.Theactioncycleisageneralmodelthat describes interaction via several stages of action [Nor88; Nor13]. As shown in Figure 4.1, the human and the system are connected via two phases: an execution phase and an evaluation phase. The execution phase is related to performing the interaction, whereas the evaluation phase is concerned with interpreting the visual response generated by the system. Goal EVALUATION PHASE Visual Evaluate Interpret Perceive Response HUMAN SYSTEM Intend Plan Execute EXECUTION PHASE Figure4.1 Stages of action forming the action cycle. Adapted from [Nor13]. Stages of Action Figure 4.1 suggests that a motivating goal is required before the human engages in some data analysis activities. The action cycle actually starts with intending some change that aims at making progress toward the analysis goal. The second stage consists of forming an action plan to satisfy the intent. The plan is then executed physically in the third stage. The system will then process the user’s request and generate a visual response to be presented to the user. This is where the evaluation phase starts.Thenewvisualrepresentationisperceived,andsubsequentlyaconscious interpretation takes place. In the final evaluation stage, the result of the interaction is compared to the original intent. If there is a mismatch of result and intent, the action cycle has to be re-run. Certainly, the user has to change the course of action to generate an alternative result. For our explanation, the action cycle ran only once. In practice, though, the action cycle is executed many more times. This is only partly due to discrepanciesthatmayoccurbetweentheintentandtheresultofaninteraction. A far more important reason for interaction is the dynamics of data analysis activities.Asindicatedearlier,interestsmayvarygreatlyashumansinvestigate
136 (cid:4) Interactive Visual Data Analysis visual representations of data. Spotting something interesting may raise the intent to study it in detail. Seeing the details may provoke curiosity to look at a different visual encoding. Doing so may lead to new insights or questions thatmayturnthedataanalysisinyetanotherdirectioncreatingyetnewideas and objectives. Levels of Interaction Depending on the number of runs typically necessary to achieve a goal, one can distinguish different levels of interaction: low-level interaction, intermediate-level interaction, and high-level interaction. At a low level, interaction is concerned with mapping the fundamental degrees of freedom offered by input devices to basic operations of pointing at andmanipulatinggraphicalobjects.Wemaythinkofsuchlow-leveloperations as the interaction alphabet or syntax. At an intermediate level, basic low-level operations are combined to form semantically meaningful data analysis activities. These include navigation in the data, adjusting the visual encoding, or filtering with respect to criteria of interest. Intermediate-level interactions can be understood as a kind of interaction vocabulary. Similartohowlow-leveloperationsarecombinedtocarryoutintermediate- level activities, so are the intermediate-level activities a precursor to high-level problem solving. At this level, interaction is considered more broadly as a catalyst for analytic thinking and discovery. The interaction vocabulary is employed to form longer action sequences that support the formation, refinement, and falsification of hypotheses. This includes setting data into relation, extracting high-level features, and organizing the derived knowledge artifacts in an analytic visual-interactive workspace. As we see, there is a cascade where high-level interaction builds upon intermediate-level interaction, which in turn is based on low-level interaction. Attheveryheartofthiscascadeistheactioncycle.So,thesuccessofinteraction essentially depends on designing the action cycle appropriately. To this end, we need to know more about the requirements for interaction in the context of visual data analysis. 4.2 REQUIREMENTS FOR EFFICIENT INTERACTION For visual data analysis to be productive, interactions must be designed prop- erly.Thissectiondealswiththecostsofinteractionanddiscussesrequirements that should be met to allow users to accomplish their analysis goals effortlessly. 4.2.1 InteractionCosts As a matter of fact, the phases of the action cycle occasion costs [Lam08]. This is the reason why they are also denoted as the gulf of execution (the intend,
Interacting with Visualizations (cid:4) 137 plan, and execute stages) and the gulf of evaluation (the perceive, interpret, and evaluate stages) [Nor88; Nor13]. The costs can be physical or mental. Physical Costs are related to performing physical actions, such as moving the fingers or forearms to control the mouse, and to physically sensing the visual response with the eyes. These are largely visceral activities, which happen without the human paying special attention to them. Physical costs can be significant when engaging in visual data analysis activities. Much of the interaction is of exploratory nature. For such trial-and-error procedures, many repetitive actions can be necessary. For example, functionality may be hidden in cascaded menu structures, which require long and accurate, and hence costly, pointer movements. Exploring the data typically requires constant navigation from one point of interest to another. During the course of an interactive data analysis, pointer mileage and click counts can accumulate and interaction can become a strain. Inasimilarvein,perceivingthevisualfeedbackresultingfrominteraction can be costly. If an interaction leads to much change in the visual representation, the eyes have to sense much information and transmit it to the brain. If visual changes are distributed across the display, the eye muscles will have to work a lot to capture each individual change. Mental Costs pertain to the stages of intending, planning, interpreting, and evaluating interaction. At these stages, conscious or subconscious activities require the user pay attention to the interaction. Just like the physical costs, the mental costs can be considerable. Visual- ization tools are often rich in functionality, and there are various ways to accomplish a task. How does a user know which graphical objects afford what actions upon the visualization? The more options there are, the more difficult it is to identify them, mentally weigh them, and make a decision for one or the other alternative. Similarly, the visual response can be costly to interpret and evaluate. It is not easy to contrast a refreshed visual representation against how it haslookedbeforetheinteraction.Thedifficultyliesincomparingagainst animagethatisnolongerbeingdisplayed,butinsteadisonlytransiently present in the human’s short-term memory. On top of that, visual data representations are usually packed with information, which further complicates the evaluation. If the entire layout of the visual representation changes due to interaction, the user may not be able to follow up and comprehend the effect. On the other hand, the visual response could be hardly visible, because it affects only a few pixels on the screen. This could leave the user wondering if the interaction is ineffective or if the system was not properly notified of the interaction intent.
138 (cid:4) Interactive Visual Data Analysis In the light of interaction costs, we may wonder if we should really try to tackle each and every data analysis problem by means of interaction? No! While interaction is a powerful tool, it is not a silver bullet. Interaction can be a burden when seemingly simple tasks are cumbersome to accomplish due to bad interaction design. Moreover, users may feel uncomfortable with being responsible for even the most basic parameter settings of a visualization. Another problem can be the arbitrariness of visual representations generated through interactive adjustments. It is no longer clear if a feature visible on the screen actually corresponds to a finding in the data or if it is just an artifact caused by an inappropriately set parameter. This points us to think of interaction in a less-is-more way. The system should be responsible for doing its best to relieve the user of unnecessary work. Only as a last resort should input be requested from the user. Fromthepreviousparagraphs,wecanconcludethatitisessentialtonarrow the gulf of execution and the gulf of evaluation: Each stage of the action cycle should require as little human effort as possible. Or put differently, interaction has to be designed such that its costs are minimal. In the following, we will see that the directness of interaction is an important factor in this regard. 4.2.2 DirectnessofInteraction The directness with which interaction is carried out largely determines how smoothly and efficiently the action cycle can run and how deeply the user can immerse in an interactive dialog with the data. The importance of what is called direct manipulation has been recognized quite early in human-computer interaction research. Hutchins and colleagues advocate direct manipulation and use visual data analysis as their motivating example [HHN85]: “Areweanalyzingdata?Thenweshouldbemanipulatingthedatathemselves; orifwearedesigningananalysisofdata,weshouldbemanipulatingthe analyticstructuresthemselves.” Hutchinsetal.,1985 Nowadays, direct manipulation is the preferred paradigm for interacting withdataandtheirvisualrepresentations.Thebasicideabehinddirectmanip- ulationisthattheuseroperatesdirectlyonthevisualrepresentationofthedata using physical actions, and the system provides feedback immediately. Accord- ing to its classic definition, a direct manipulation interface is characterized by three key properties [SP09]:
Interacting with Visualizations (cid:4) 139 • Objects and actions of interest are presented continuously using mean- ingful visual metaphors. • The user’s requests are expressed through physical actions, rather than complex syntax. • Actions are rapid, incremental, and reversible, and their effect is imme- diately visible. Thesepropertieshaveanimmediateconsequenceforthedesignofinteractive visualizations.Avisualrepresentationisnolongeronlyameanstocommunicate data to the human. Additionally, a visual representation has to provide the meanstoenablethehumantointeractwiththedataandthesystemingeneral. Knowing the basic idea behind direct manipulation, we can next study the question: What does directness actually mean? SeparationasanInverseMeasureofDirectness In order to develop an understanding of directness, it is helpful to look at directness from an opposite point of view. In fact, directness is inversely proportionaltothedegreeofseparationofthehuman’sactionandthesystems’s response. In other words, we do not achieve directness if there is a high degree of separation. Different types of separation can be detrimental to directness. There are: • conceptual separation, • spatial separation, and • temporal separation. Conceptual Separation relates to the different models involved when inter- actingforvisualdataanalysis.Therearetheuser’smentalmodel,thesystem’s implementation model, and the interface’s represented model [CRC07]. The mental model comprises the analytic problem being dealt with and the con- cept a user has about the system. It abstracts from details and focuses on goal-relevant aspects. Visualization software adheres to an implementation model. This is a formal model full of technical details, algorithmic conventions, and parameterized procedures. The mental model and the implementation model exhibit a large conceptual separation, as illustrated schematically in Figure 4.2. Therefore, there is the third model, the represented model. This model captures what the user can actually see and interact with on the display. The closer the represented model is to the mental model, the more directly can users interact.
140 (cid:4) Interactive Visual Data Analysis MENTAL REPRESENTED IMPLEMENTATION MODEL MODELS MODEL Reflects user’s vision Reflects technology Better Worse Figure 4.2 Conceptual separation across different models. Adapted from [CRC07]. Spatial Separation concerns distances to be covered during the interaction. Large distances can increase interaction costs considerably. Costs accrue when users have to move the pointer across large distances in order to execute certain actions. Spatial separation is also problematic when the eyes have to switch frequently between different parts of the screen when evaluating the system’s response. Consider, for example, the scattered data visualization and thecorrespondinggraphicaluserinterfaceinFigure4.3.Theuserinteractswith the controls to the right, whereas the visual response becomes visible in the main view. The problem is that action and effect are spatially separated. This can make it more difficult to understand the action-effect causality. The user may have to shift the attention back and forth between the user interface and the main view several times to comprehend how certain parameters influence the visual representation of the data. Figure4.3 Spatial separation between the graphical user interface (right) and the visual representation in the main view (center). Temporal Separation is about the latency between the user’s action and the system’s visual response. Ideally, a response should be provided within 50–100ms [Shn94; Spe07]. As a matter of fact, if the latency is too high, the efficiency of interactive visual exploration degrades [LH14]. However, the computations involved in processing the visualization transformation and generating the visual feedback can take a considerable amount of time. For
Interacting with Visualizations (cid:4) 141 example, the visualization in Figure 4.3 consists of scattered data points, their corresponding Voronoi diagram, and a smooth coloring in the background. Withrespecttolatency,theindividualdatapointscanberenderedvirtuallyat no time cost. Even the Voronoi diagram can be computed quite quickly given the fact that there are only ten data points to be processed by a Θ(nlogn) algorithm.Forthesmoothcoloring,however,acolorvalueneedstobecomputed for every pixel of the main window. Already for a moderately sized window with a 1,280×1,024 resolution, 1,310,720 pixels need to be colored. On a 4K display, it will be 8,294,400 pixels. This can cause a noticeable delay leading to the adverse effects of efficiency degradation. All three types of separation impair directness and hinder smooth and efficient visual data analysis. Therefore, reducing separation should be a top priority throughout the visualization development process. This concerns both the design and the implementation. Interaction and visual feedback should be designed so as to minimize conceptual and spatial separation. Implementation- wise, algorithmic efficiency is important to reduce temporal separation. ScenariosofDifferentDirectness We previously discussed directness on a theoretical level. But how does direct- ness manifest in practice? Next, we sketch five scenarios illustrating different degrees of directness (or separation) when interacting with a visualization. For these scenarios, we assume a user has spotted an interesting group of nodes in a graph visualization and wants to zoom in to look at them in detail. There are several alternatives to accomplish this by interacting with the system. 1. Source Code Editing On the implementation level, a visual represen- tation is defined by source code. Only a few lines of code need to be edited to set the visualization view to where the nodes of interest are located. The altered code is compiled and run to see if the visual result is as expected. If not, the procedure is re-run until the user is satisfied. Changing code lines, re-compiling, and test-running the visualization is the least direct form of interaction, as it exhibits large conceptual, spatial, and temporal separation. 2. Scripting Commands Alternatively, the visualization may offer a script- ing interface allowing the user to enter commands to zoom the view. Once issued, the commands take effect immediately while the visualiza- tion is running. In this scenario, no separate compilation is necessary, which reduces the temporal separation. But still the interaction is rather indirect, and several commands may be necessary before the view fits as desired. 3. Graphical Interface Thefieldofviewisdisplayedinagraphicalinterface alongsidethevisualization.Standardcontrolssuchasbuttonsandsliders
142 (cid:4) Interactive Visual Data Analysis allow the user to easily shift the view and control its zoom factor. Any changes are immediately reflected in the graphical interface and the visualization. Given the fact that the graphical interface represent the view status and at the same time serves to manipulate it, the conceptual gap is narrowed. Yet, the interaction (with the controls) and the visual feedback (in the graph visualization) are still spatially separated. 4. Direct Manipulation The user zooms the view directly by drawing an elastic rectangle around the nodes to be inspected in detail. This is a rather simple press-drag-release operation when using the mouse. During the interaction, visual feedback constantly indicates the frame that will make up the new view once the mouse button is released. Any necessary fine-tuning can be done using the mouse wheel. In this scenario, the manipulation of the view takes place directly in the visualization. There is no longer a spatial separation between the interaction and the visual feedback. Or is there? 5. Direct Touch Indeed there remains some degree of separation. The inter- action is carried out with the mouse, whereas the feedback is shown on the screen. To obtain a yet higher degree of directness, the interaction can alternatively be carried out using touch input on the display. The basic principle of sketching an elastic rectangle to specify a new view is maintained, but the action is performed using a finger directly on the display. Now, the interaction takes place exactly where the visual feedback is shown. A truly direct way of zooming in on a node-link diagram. Thesefivescenariosillustratedifferentdegreesofdirectnesswheninteracting with a visualization. Certainly, our list is not exhaustive, but it does contain some general ideas on how interaction can work in practice. From the examples, we may be inclined to think that direct touch is the best solution. But that is not quite true, we have to differentiate. Each scenario involves its own benefits and drawbacks. Direct touch, for example, tightly links interaction and visual feedback. However, the hand being used for the touch interaction may occlude the user’s view on important information. Scripting commands, for another example, can be difficult to specify for novice or casual users. Yet, expert users may be faster and more precise issuing commands than using a graphical form of specification. Moreover, commands are easily reproducible. Even the least direct scenario of modifying lines of source code has some benefits. It enables a visualization developer to test- drive several alternative visual representations without the need to set up and implementdedicatedinteractionmechanisms.Thiscanbehelpfulwhenrapidly prototyping visualization software. From our discussion we can see that it is important to carefully weigh the means of interaction that a visualization should exhibit. It can very well be
Interacting with Visualizations (cid:4) 143 that several alternatives are necessary to be able to adapt the interaction to different user audiences. In any case, we need to design the interaction such that it is useful and usable. The next section provides some guidelines that can help us achieve this goal. 4.2.3 DesignGuidelines Usability and user experience are important factors for the development of usable and useful interaction. They subsume several objective and subjective quality criteria for interaction, including efficiency, predictability, consistency, customizability, satisfaction, engagement, responsiveness, and task confor- mance, to name only a few. GeneralRules Guidelines can inform the design of efficient interaction. Here, we list the golden rules by Shneiderman and Plaisant [SP09]. They are not specifically related to visual data analysis, but still offer valuable advice. 1. Striveforconsistency.Consistentactionsshouldberequiredinsimilar contexts and be responded to consistently. 2. Cater to universal usability.Thesystemshouldbeusablefornovices, casual users, and experts alike. 3. Offer informative feedback. For each possible action, there should be informative feedback appropriate to its importance. 4. Design dialogs to yield closure. Action sequences should have a well-defined beginning, middle, and end. 5. Prevent errors. The system should prevent serious errors and be able to recover from minor problems. 6. Permiteasy reversalof actions.Actionsshouldbereversibletoallow for undoing accidental actions and to encourage exploration. 7. Support internal locus of control. Users should be given the feeling that they are in charge, not the computer. 8. Reduce short-term memory load. Short-term memory load should be limited to seven plus minus two chunks of information. FluidInteraction Inthecontextofvisualdataanalysis,onecanoperationalizetheaforementioned general rules to define what is called fluid interaction [Elm+11]. The idea of fluid interaction includes three guiding principles:
144 (cid:4) Interactive Visual Data Analysis • Promote flow • Support direct manipulation • Minimize the gulfs of execution and evaluation We already talked about direct manipulation and minimizing the costs of interaction. The first principle, promote flow, though is worth a few words of explanation. By “promoting flow” it is meant that interaction should be designed so that users can totally immerse in the data analysis activities. This requires balancing the difficulty of analytic tasks and the skills of the user, giving the user a sense of control over what is going on, having the right tools available at the right time, and providing an overall rewarding experience. Heer and Shneiderman underline the significance of the fluent, direct, and human-centered character of interaction in visualization [HS12]: “Tobemosteffective,visualanalyticstoolsmustsupportthefluentandflexible useofvisualizationsatratesresonantwiththepaceofhumanthought.” HeerandShneiderman,2012 With this statement, we end the first part of this chapter, in which we considered interaction mostly at a conceptual level, focusing on the role of the human-in-the-loop. The upcoming second part will be more concrete in terms of interaction tasks performed when analyzing different data. Based on a brief look at low-level interaction in the next section, later sections will introduce specific interaction concepts for interactive visual data analysis. 4.3 BASIC OPERATIONS FOR INTERACTION We already mentioned that visual analysis involves high-level interaction and builds upon intermediate-level interaction techniques, which in turn are based on low-level operations. These basic operations are the subject of this section, which is intended to bridge our previous conceptual considerations and the interaction techniques described in the sections to come. As visual data analysis is largely based on interacting with visual repre- sentations, we will focus on fundamentals of graphical interaction, that is, the procedure of performing physical actions in the real world that change virtual graphical objects on a computer display.
Interacting with Visualizations (cid:4) 145 OUT OF RANGE POINTING MANIPULATING Exit Release 0 1 2 Enter Press Move Move Move Figure4.4 Three-state model of graphical input. Adapted from [Bux90]. 4.3.1 TakingAction When interacting with visual representations, there are two basic questions. We have to define, first, where our interaction should take effect and, second, what the effect should be. Consequently, we distinguish two basic operations: • point and • manipulate. Pointing allows us to define which graphical objects we want to interact with and the manipulation defines what should happen with the objects. For example,wemaypointatanodeinanode-link-diagramtomarkitasrelevant, orwepointatasliderhandleandthenmanipulateittoadjustafilterthreshold. Physically, pointing and manipulation can be carried out in different ways. The necessary actions are typically related to movements that are tracked with an interaction device. Moving our hand to control a computer mouse is a prominent example. Moving our fingers allows us to press down a button or rotate the mouse wheel. We are also used to moving fingers and hand across our mobile phone’s surface. Conceptually, graphical interaction can be described with a three-state model[Bux90].Figure4.4showsthestatesasboxesandthetransitionsbetween them as arrows. Consider state 1 first. This is the state we are in when moving the mouse to point at something on the screen. As soon as we press a button, we transition from state 1 to state 2. This is how manipulations are triggered. For example, we can move the mouse to drag a filter slider. Upon releasing the button, we will return to state 1. If the mouse is out of reach, for example, if we move it beyond our application window or lift it from the table, we fall back to state 0, in which movements of the input device do not have an effect. Note that the previous paragraph describes only one simplified instance of the three-state model. Different configurations exist for different input modali- ties. For example, touch interaction does not support state 1 out of the box. Once our finger is on the surface, we immediately begin manipulating what is underourfingertip.Cleverinteractiondesignandadditionaltiminginformation are necessary to compensate for the missing state. On the other hand, a mouse with multiple buttons will lead to additional states and transitions. These can be exploited to provide more ways of interaction, but also complicate design and use.
146 (cid:4) Interactive Visual Data Analysis ModesofInteraction With the three-state model, we now have a conceptual understanding of how interaction is performed on a low level. We can further utilize the model to distinguish two characteristic modes of interaction. There are: • discrete (or stepped) interaction and • continuous interaction. Triggering a short state change can be considered a discrete interaction. Examples would be pressing and immediately releasing the mouse button or briefly tapping on a touch-enabled surface. In both cases, we temporarily enter state 2 to trigger a manipulation. We may have clicked a button to change the visual encoding or tapped a data element to mark it. Discrete interaction is most useful for occasionally making a selection when there are only few alternatives to choose from. However, in visualization scenarios, we often face very many alternatives. This not only concerns the many data elements that we may want to study in detail. There are also many parameters with potentially large value ranges to control the visualization transformation. Exploring the data and browsing different parameterization in a purely discrete fashion would certainly be cumbersome. This is where continuous interaction comes into play. For continuous inter- action, we remain in a manipulating state for a longer time while continuously movingtheinputdevice.Forexample,whenadjustingafilterslider,eachincre- mentalcursormovementresultsinaseparatevisualresponse.Theadvantageis that a larger space of options can be scanned in a short period of time using a singlecontinuousgesture.Thismakescontinuousinteractionparticularlyuseful for exploratory visual data analysis, where testing many what-if alternatives is a common task. 4.3.2 GeneratingFeedback So far, we have considered the fundamental low-level actions that users may perform. Yet, to complete the action cycle, a user request always requires a visualresponsebythesystem.Fromasystemperspective,twooperationsneed to be carried out: • update and • refresh. The update operation is concerned with changing the internal state of the visualization based on the action performed by the user. The refresh operation, on the other hand, is responsible for presenting a new visual representation that reflects the internal change.
Interacting with Visualizations (cid:4) 147 Perceive Manipulate Refresh Update VIEW MODEL CONTROLLER Figure4.5 Model-view-controller pattern. Again, a conceptual point of view can help us clarify the update and refresh operations. This time, we refer to the model-view-controller (MVC) pattern [KP88]. MVC is a software design pattern for graphical user interfaces. Figure 4.5 illustrates a variant simplified for the context of this text. In our case, the model consists of the data and all parameters of the visualization transformation. The views correspond to visual representations of the data and auxiliary information. The controllers conceptually capture all different ways of interaction. As we see from the figure, once the user performs some manipulation, the controller sends an update request to the model. This update can be as simple as a mode switch, but may also be as complex as performing some analytical processing of the entire data. Once the model has updated its internal state, the views are notified to refresh themselves to reflect the new model state. Again this could require a simple repaint using different colors, but also a complete rebuild of the visual data representation. In terms of temporal separation of action and response as discussed earlier in Section 4.2.2, both update and refresh are critical. Ideally, they run so fast that the user has the feeling of having a system that reacts immediately. TypesofVisualFeedback Update and refresh are the operations that generate the visual feedback. But how can the feedback itself be characterized? Analog to the modes of interaction, we can distinguish two types of visual feedback: • static feedback and • animated feedback. For static feedback, the system creates a single new visual representation to replace the old one. The new image immediately corresponds to the system’s new state. Such an abrupt visual change can be useful to draw the user’s attention toward the significant differences caused by the interaction. However, there is also a drawback: With static feedback, it might be difficult to com- prehend how the new visual representation has evolved from the old one. For example, when switching to an alternative graph layout algorithm, the graph nodes may be located at completely different positions. In such situations, users will have a hard time maintaining their mental model of the data.
148 (cid:4) Interactive Visual Data Analysis With animated feedback, the visual representation changes gradually. The system generates a series of responses to smoothly interpolate the display from its current state to the new one. Each intermediate step represents only an incremental update, which makes it easier for users to follow the changes and tounderstandtheeffectoftheiractions.Forourpreviousexampleofswitching the graph layout, nodes can be moved smoothly from their original locations to the new ones. However, a difficulty with animated feedback is that it takes timetocompleteandhencedelaystheactioncycle.Therefore,animatedvisual feedback has to be designed with care, but it is worth the effort. To summarize, pointing and manipulation are the basic operations to be carried out by the user. As we have learned, interactions can be discrete or continuous,wherethelatterisparticularlyusefulinthecontextofvisualization. Updating the data model and refreshing the visualization views are the basic operations to be carried out by the system in order to create visual feedback. The feedback can be static or animated, where the latter can help users better understand the effect of their actions. Together, the basic operations as described in this section are the fundamental building blocks for interactive graphical systems. In the following sections, we add the necessary semantics to create useful interaction techniques for visual data analysis. Next, in Section 4.4, we start with interactive selection as a preparatory step for many other operations such as highlighting, filtering, or even modifying data. How data can be explored interactively at multiple scales will be explained later in Section 4.5. 4.4 INTERACTIVE SELECTION AND ACCENTUATION From our regular work with computers, we know that selecting something on thedisplayisafundamentalandfrequentlyusedactionpattern.Inthecontext of visual data analysis, interactive selection takes a similarly central role. In fact, selection is a door opener for visual data analysis. It allows us to divide the analysis into smaller manageable subtasks simply by marking parts of the data to be relevant to the question at hand. Conceptually, selection can be described as follows. Assume we want to analyze a visual representation of some dataset V(D) that contains a lot of information. As we cannot digest all information at once, we successively concentrate on different subsets of the data. Let D+ ⊂D denote what we are currentlyinterestedin.ThedataD− =D\D+ arenotinourfocus,andhence, are considered less relevant for the time being. D+ and D− are constantly in flux as the user’s attention and interests change during the data analysis. The distinction of D+ and D− implies two things. First, interactive visual analysisrequiresmeanstoenableuserstospecifywhatisrelevanttothem.This aspectwillbediscussednextinSection4.4.1.Second,thevisualrepresentation of the data must be adapted in such a way that the relevant data stand out. Corresponding accentuation strategies will be introduced later in Section 4.4.2.
Interacting with Visualizations (cid:4) 149 4.4.1 SpecifyingSelections There are two points of departure for specifying a selection: Data can be selected based on their location on the display and based on the actual data values.Intheformercase,theselectiontakesplacedirectlyinthevisualization. This is also known as brushing [BC87; MW95]. In the latter case, selection criteria are defined on the data-level using dedicated controls, which is also referred to as dynamic querying [AS94; Shn94]. Next, we take a closer look at the two alternatives. InteractiveBrushing Interactive brushing works by marking a part of the visual representation and the data shown in that part get selected. With interactive brushing it is possibletospecifythreecategoriesofselections:pointselection,rangeselection, and composite selection. Point Selection A very basic form of brushing is to point at a location of interest followed by a discrete interaction, such as a mouse click or a tap on the screen. But how do we know which data reside at the selected point? Toanswerthisquestion,thevisualizationtransformationhastobeinverted. Thisisalsocalledpicking.Whilethevisualizationtransformationmapsdatato graphics, picking reverses this mapping in order to get from graphics to data. Depending on the underlying graphics, different methods can be employed for picking, including lookup-buffers, geometrical tests, or plain mathematical calculations. Range Selection There are two classic techniques for marking entire 2D ranges in a visualization: rubberband selection and lasso selection. Both are continuous interactions that employ a press-drag-release gesture for defining the selection. The rubberband selection uses an elastic shape, for example a rectangle, which makes it easy to mark orthogonal parts of the visualization. If the layout of the data is rather irregular, a free-form lasso selection is more practical. It offers more flexibility, but comes at higher interaction costs, because more (and more accurate) physical movements of the pointer are necessary. Rubberband and lasso define a geometric shape (rectangle or free-form shape) based on which the actual selection is made. There are two alternatives for that. For data to be selected, they must either be included in or intersect with the shape. Inclusion and intersection have different characteristics in terms of accuracy and effort. Which alternative is more suitable depends on the graphical objects to be selected. For smaller regular objects like the nodes in the graph layout in Figure 4.6a, inclusion is practical. We can easily and quite accurately include them in a rectangle or lasso. In contrast, larger irregular objects such as the geographic regions of the map in Figure 4.6b
150 (cid:4) Interactive Visual Data Analysis (a)Selection by inclusion. (b)Selection by intersection. Figure4.6 Rubberband selection for marking multiple data elements. are difficult to mark without adding too many unwanted objects. In this case, an intersection-based mechanism is more practical because it is easier to just touch the data we want to mark. Enclosing the data entirely is not necessary. With the techniques described so far, we can select the data from one particular point or area. But how can we select data from multiple origins? Marking them with a lasso selection is cumbersome at best, if not impossible. Therefore, we need interactions that enable us to combine and edit selections. Composite Selection There are various ways to compose multiple select operations. In fact, with 524,288, the number of theoretically possible com- binations of add, remove, subtract, intersect, union operations on selections is extremely high [Wil96]. Therefore, it makes sense to follow established standards. Multiple selections can typically be composed using modifier keys. Holding down the control key will usually toggle the data’s selection state. That is, unselected data become selected and vice versa. When data are arranged in a linear order, as for example top to bottom for the trajectories in Figure 4.7, thenthecontrolandshiftkeyscanbeusedincombinationtoselectmultiple subranges. Similar variations of the selection behavior can be designed for area selec- tions via rubberband or lasso. The shift and control keys can be used to add and remove data to and from a selection, respectively. Holding both keys simultaneously enables the creation of intersections. In summary, interactive brushing together with the options for composing selections enables us to specify our interest directly in the visualization. The advantage is that we can easily select what we see. On the other hand, the selection is based solely on the data’s visual arrangement. This can lead to situations were selections are unnecessarily complex to define. Therefore, it makes sense to complement interactive brushing with dynamic queries.
Interacting with Visualizations (cid:4) 151 1 2 SHIFT 3 CTRL 4 SHIFT Figure4.7 Four steps of selecting multiple trajectories using modifier keys. DynamicQueries Data-based selection via dynamic queries is helpful when visually based selec- tion is impractical. Consider, for example, the trajectory visualization in Figure 4.8a and suppose we are interested in selecting the low-speed segments in red and yellow. Brushing them is hardly possible. Rubberband selection is infeasible due to the irregular arrangement of the segments, and lasso selection would require a costly circumvention of the high-speed segments. Composite selection would be possible, but it would take very many individual operations. So why not select low-speed segments based on the speed values directly? To this end, we need a dedicated representation of the value range of the speed attribute and a mechanism that allows us to mark what we are interested in. Interactive Legends Interactive legends are a helpful tool for specifying dynamic queries [RLP10]. The example legend shown in Figure 4.8b consists of multiple sections, each is associated with a specific data interval. Users can simply click on a section to (un)select all trajectory segments that fall into the corresponding data interval. In our example, the segments with high speeds have been unselected to concentrate on the data where movements are slow. This would have been very hard to accomplish with interactive brushing, whereas querying via the legend took but a few clicks. Query Sliders Dynamic queries can also be done in a continuous fashion. For an illustration, let’s assume we want to select the red high-degree nodes from the graph visualization in Figure 4.9a. As we can see, the graph layout does not correspond to the characteristics based on which we want to select, which rules out interactive brushing as a suitable selection method. Moreover, we are not quite sure about what ‘high-degree’ means precisely and need a bit of flexibility to experiment with potential thresholds.
152 (cid:4) Interactive Visual Data Analysis (a)How to select slow-speed segments? (b)Select via an interactive legend! Figure4.8 Selecting segments based on their color, which represents speed. (a)How to select high-degree nodes? (b)Select via slider handles! Figure4.9 Selecting nodes based on their data attributes. This is where query sliders enter the stage. They are commonly applied in visual analysis scenarios [Eic94]. The exemplars in Figure 4.9 consist of a scale representing the value range of the node degree and two handles enabling the user to specify, in a continuous fashion, a data interval of interest. In Figure 4.9b, we have adjusted the slider so as to select all nodes with a degree between 23 and 29. The advantage of dynamic query sliders is obvious: We can precisely define our interest based on the characteristics of the data. Observing the visual representation while specifying the selection further enables us to evaluate on the fly if the result suits the task at hand. Adding more handles allows for selecting multiple distinct data ranges. It is even possible to combine multiple sliders to create sophisticated multi-criteria selections. However, a disadvantage of dynamic queries is that the interaction no longer takes place directly in the visual representation of the data, but with a dedicatedinterface,alegendoraslider.Thismayleadtospatialandconceptual
Interacting with Visualizations (cid:4) 153 separation. Therefore, it is important to strive for a tight integration of the selection interface and the visualization. In our previous examples, color has been used as the link that connects interface and visualization. This makes it easier to comprehend how a selection at the interface will affect the visual representation. The interactive mechanisms described above enable users to specify their interest in the data. The next step is to adapt the visualization such that relevant data stand out, allowing the user to fully concentrate on them. We havealreadyseenseveralexamplesofhowthiscanbedoneinthefiguresofthis section. Next, we will systematically discuss different strategies of emphasizing or attenuating certain elements of a visualization. 4.4.2 VisualEmphasisandAttenuation Given a visual representation of the data V(D) and a selection of relevant data D+, the question that we deal with now is how to visually distinguish D+ from the rest of the data D−? What is needed are additional encodings that either visually emphasize V+ or visually attenuate V− certain parts of the data. An alternative is to suppress parts of the data altogether. UsingD+,D,D− andV+,V,V− asaconceptualbasis,onecanimplement different strategies: Highlighting Highlighting emphasizes the data of interest V+(D+) in con- trast to a regular visualization of less-relevant data V(D−). Dimming Through dimming, we can attenuate less-relevant data V−(D−), while data of interest are visualized regularly V(D+). Filtering For filtering, only the relevant data are visualized regularly V(D+), whereas all other data are omitted. A decision for either strategy should be made carefully. As a general rule of thumb, the visual feedback should be effective, but with only minimal side effect on the regular visualization. There are two important influencing factors: the frequency of change of the selection and the size of the selection. Highlighting and dimming contrast the regular visualization V against an alternative encoding (either V+ or V−). If D+ can be assumed to be small, as for example when hovering the cursor over individual elements of a visualization, then highlighting is a sensible choice. If D− is small, as for example when we want to get rid of a few outliers, then dimming makes sense. Filtering effectively clears the view and allows users to fully focus on D+. A disadvantage, though, is that the user is no longer aware of the presence of D−. Therefore, filtering is typically applied only when a selection is stable for a certain subtask during the visual analysis. Theoretically, further strategies are possible. For example, one could redundantly emphasize relevant data and dim or filter less-relevant data
154 (cid:4) Interactive Visual Data Analysis REGULAR VISUALIZATION V(D) D– D+ D– HIGHLIGHTING V+(D+) + V(D–) DIMMING V(D+) + V–(D– ) FILTERING V(D+) Figure4.10 Strategies for visual emphasis of relevant data and attenuation of less-relevant data. (V+(D+)+V−(D−) or V+(D+), respectively). While this would generate a stronger contrast, the regular encoding V would not be used anymore. As this violates the rule of thumb not to interfere too much with the visualization, such redundant encodings are hardly applied in practice. Let us next illustrate our conceptual considerations with the example in Figure 4.10. Our starting point is a simple visual encoding that shows a time series as bars of varying size in neutral gray, our V. In order to emphasize and attenuate selected parts of the data, we vary color in this example. For V+, we use a dedicated highlighting color that can be easily distinguished from the neutral gray. Dimming is accomplished by using a lighter gray for V−, which blends well with the white background. Note that filtering is indicated via dashed outlines, even though the data are actually invisible. Figure 4.10 shows but a simplified example to illustrate how selected data canbeaccentuated.Inreal-worldapplications,findingsuitablevisualencodings isconsiderablymoredifficult.Usually,theregularencodingV alreadyoccupies much of the visual resources (e.g., color, size, position) for the purpose of effective and efficient visualization of the data. The difficulty is to come up with adequate V+ and V− to achieve emphasis and attenuation. The actual design challenge is that V+ and V− should not (or only mini- mally) interfere with V, but need to be sufficiently expressive to contrast D+ against D−. This design challenge has to be solved depending on application needs. A general strategy though is to use visual variables that are not yet used for the regular visualization. If this is impractical or impossible, it can even be necessary to embed additional graphical elements, such as outlines or halos into the visual representation. Of course this has to be done sparingly to avoid cluttering the visual representation of the data. Figure 4.11 provides a few more practical examples for visual emphasis and attenuation for graph visualization. Figure 4.11a shows a node-link diagram where node size and color encode certain node attributes. We want to concen-
Interacting with Visualizations (cid:4) 155 (a)Original visual representation. (b)Highlighting by encircling nodes. (c)Dimming nodes and edges. (d)Filtering nodes and edges. Figure4.11 Visual feedback for selections in visual representations of graphs. trate on the four larger red-orange nodes. It is not possible to highlight the nodes using a special color, because the nodes are already color-coded. Instead, Figure 4.11b shows the selected nodes highlighted via additional graphical primitives, circles in this case. While the nodes are now clearly marked as relevant, our attention may still be distracted by the presence of too many nodes and edges. Therefore, let us further draw the user’s attention to the relevant informa- tion by dimming all unselected nodes and their incident edges. In Figure 4.11c, the data of interest stand out clearly now, and it is easier to see how they are interconnected. For an even more focused view, we can filter all irrelevant information as in Figure 4.11d. While this removes any distraction, it also obliterates awareness of the filtered information. Looking at the figures, one can easily imagine alternative solutions, and indeed the design space for visual emphasis and attenuation is large [Hal+16]. Any visual variable can theoretically be used as a visual cue, including color,
156 (cid:4) Interactive Visual Data Analysis size, position, texture, blur, and even pulsing animations or blinking [WH04]. Particularly promising are visual cues that are perceived pre-attentively, which means they immediately draw our attention [HE12]. Deciding for either of these options is not an easy task, even for the binary distinction of D+ vs. D− discussed so far. Matters quickly become complicated when it comes to encoding multiple selections simultaneously in a single visualization image. In conclusion, the discussion in this section makes clear that expressive visualaccentuationisasimportantaseffectivemeansforinteractivelyspecifying selections. 4.4.3 EnhancedSelectionSupport So far, we have discussed basic means of interactive selection. In the following, we will introduce methods to enhance the work with selections in visual analysisscenarios.Smoothbrushing willallowustogobeyondbinaryselections. Brushing & linking helps us propagate selections across multiple visualization views. We will further see how automatic methods can be used to reduce the costs of interactive selection. SmoothBrushing Up to now, our selections define a binary distinction of data into selected and unselected. Smooth brushing is a concept that breaks the barrier of discrete binary selection [MW95; DH02a]. The idea is to assign to each data point a continuous selectedness from the interval [0..1]. The extremal values 0 and 1 stand for unselected and selected, respectively. Data points with a selectedness greater than 0 and smaller than 1 are somewhat selected. In other words, we are working with a kind of fuzzy selection. Obviously, a fuzzy selection requires dedicated interaction and visual feed- back. Let us illustrate this with the parallel coordinates plots in Figure 4.12. As with regular brushing, the user marks the data range to be assigned a selectedness of 1. This is shown in Figure 4.12a. For smooth brushing, the selectedness automatically spreads beyond the initially defined range until it gradually fades out to zero, as shown in Figure 4.12b. The automatic gradual spread is key to capturing features that by nature are not crisp and clear, and hence, costly to define. In order to provide visual feedback for smooth selections, it makes sense to consider selectedness as an additional attribute to be visualized alongside the data. The binary selection in Figure 4.12a shows selected and unselected data tuples as black and gray lines, respectively. The smooth selection in Figure 4.12b encodes selectedness by varying the lightness of the gray tones. This allows us to see even subtle differences in selectedness.
Interacting with Visualizations (cid:4) 157 (a)Binary selection in the range. (b)Fuzzy selection beyond the range. Figure4.12 Brushing a range (red) of an axis for binary and fuzzy selection. Brushing&Linking Regardlessofwhetherweuseregularorsmoothbrushing,itisalwaysexecuted on a single visual representation. However, visual data analysis often requires workingwithmultiplerepresentations.Thequestionthatarisesishowselections can be propagated across several visualization views? Consider for example the graph visualization system in Figure 4.13. It consists of four views: a textual tree view to the left, a hierarchy view on the top, a node-link view in the center, and a parallel coordinates view at the bottom. Each view allows us to select data that interests us. But how can the selectionbekeptconsistentacrossallviews?Manuallyreplicatingtheselection in the other views is certainly impractical. What is needed is a concept to automatically link all views. Or, as Buja and colleagues put it [Buj+91]: “Multipleviews,however,shouldnotberegardedinisolation.Theyneedtobe linkedsothattheinformationcontainedinindividualviewscanbeintegrated intoacoherentimageofthedataasawhole.” Bujaetal.,1991 Brushing & linking (or focusing and linking) provide the answer to the aforementioned questions [BC87; Buj+91]. Brushing is what the user does manually. The linking is done automatically by the system. The basis for the linking is a dedicated selection model where views are registered. As all views share the same model, a selection made in one view is automatically available inallotherviews.Theonlyresponsibilityoftheindividualviewsistorepresent the selection visually. For the example in Figure 4.13, the tree view shows a bluelabelbackgroundforselectednodes,thehierarchyviewusesdarkoutlines, the node-link view encircles selected nodes, and the parallel coordinates view dims data that are not selected.
158 (cid:4) Interactive Visual Data Analysis Figure4.13 Brushing & linking in a multiple-views graph visualization. So, with brushing & linking, we only need to perform the selection in one view instead of several. Moreover, we can revise the selection in any view, no matter where the selection has been triggered, and the selection consistency is maintained automatically. This gives us much flexibility in specifying our interest in selected parts of the data. AutomaticSelectionSupport Interactive selection can particularly benefit from integrating automatic meth- ods that take over otherwise manual selection steps. The general idea is that only an initial mark has to be placed by the user. From there, an automatic procedure completes the selection. We have already seen instances of this idea in the previous sections. For smooth brushing, selectedness is automatically spread beyond the initially brushed data interval. For brushing & linking, selections are automatically propagated across multiple visualization views. Developing dedicated selection algorithms is sensible if manual selections areverycostly.Thisisthecase,forexample,whenselectingclustersin3Dpoint clouds. Purely interactive selection in 3D point clouds is notoriously tedious and time-consuming work. Automatic selection procedures can facilitate this work[Yu+12].Basedonasimple2Dlassoselection,analgorithmautomatically expands the selection along analytically determined structures in the 3D point cloud. This effectively relieves the user of many incremental and costly 3D selection steps. The idea of automatic selection support can be generalized in various ways[HAW08].Forthispurpose,itiscommontotransformgraphicalselections intoabstractmodelsthatdescribeselectionrulesorconstraintsratherthanthe selection state of individual data elements [Che04]. Such declarative models
Interacting with Visualizations (cid:4) 159 help us apply the same selection to different datasets, maintain selections even when the data vary over time, or derive relaxed selections when broadening the view on the data is necessary. With this outlook on automatic support, we end the section on interactive selection and accentuation. We have learned that being able to mark and emphasize data according to varying interest is of fundamental importance for visual data analysis. In the next section, we will see that it is just as important to be able to examine data at varying scales, especially when dealing with large amounts of data. 4.5 NAVIGATING ZOOMABLE VISUALIZATIONS Data are usually explored, because it is not clear upfront where interesting patterns are located in the data. Interactive selection as described in the previous section enables us to investigate different parts of the data. Yet, this is only one part of the data-exploration equation. Asecondreasonfordataexplorationisthatitisinmanycasesalsounclear how deeply valuable information is buried in the data. This makes it necessary to analyze the data at different scales, a concern that is particularly relevant for larger data that span several magnitudes. An interactive solution to this problem is multi-scale data exploration via zoomable interfaces [Bed11]. Zoomable visualizations enable the user to explore different parts of the data at different scales. Shneiderman formulated the visual information seeking mantra as a fundamental guiding principle for studying data at varying scales [Shn96]: “Overviewfirst,zoomandfilter,thendetails-on-demand.” Shneiderman,1996 The mantra suggests to start from an overview. The overview offers a big picture by visualizing as much data as possible. Yet it is only a coarse big picture,asthereisnospacefordetails.Detailedinformationcanbegatheredby zoomingintosubsetswithlessdata.Lessdatameansmorespacefordetailsand closer inspection. With a zoomable visualization, the user can freely explore information at variable scales. The findings made during the exploration are compiled into a comprehensive understanding, very much like putting together the pieces of a puzzle. Superficially, zooming sounds very much like successively focusing on data subsets D+ as discussed in the previous section. The Shneiderman mantra, though, explicitly distinguishes between zoom and filter, since there is a big
160 (cid:4) Interactive Visual Data Analysis REGULAR VISUALIZATION V(D) D– D+ D– FILTERING V(D+) D+ ZOOMING V=(D+) Figure4.14 Using V= to scale relevant data to fit the display space. difference in how D+ is represented visually: Zooming works with a visual encoding V= that is capable of scaling the graphical objects representing D+, whereas with filtering, the focused subset is visualized only regularly V(D+). To illustrate the difference, Figure 4.14 compares a filtered view to a zoomed view.Filteringandzoomingaresimilarinthatnoirrelevantdataaredisplayed. Yet, for zooming V=(D+), the relevant data are scaled up so as to fit the available display space, the bars are enlarged and distributed evenly across the horizontal axis. Obviously, the size of D+ determines the scale of the visual representation. Discarding only a small part of the data such that |D+| ∼ |D| will lead to only a marginal change in scale. Focusing on a very small portion of the data |D+| (cid:28) |D| will bring the data analysis to a much finer scale. At any time, though, the user will see only a particular subset at a particular detailedness. It is the flexible interactive adjustment of focus (which part) and scale (which granularity) that enables users to engage in multi-scale data exploration. 4.5.1 BasicsandConceptualConsiderations Conceptually, zoomable visualizations build upon the notions of a world, a viewport defined in the world, and a screen. The world corresponds to the spatial arrangement of the visualized data. The viewport acts as a window into the world, and as such, determines what information is to be projected onto the screen. By moving the viewport, different parts of the world can be made visible. By resizing the viewport, one can adjust how much of the world is shown. The duo of move and resize operations of the viewport is commonly denoted as pan and zoom. Figure 4.15 illustrates a basic visualization world and how three different viewports lead to three different representations on the screen. AttheveryheartofzoomablevisualizationsisthefunctionV= thatdefines how the data representation gets adjusted when the scale is changed. This function can be implemented in different ways to accomplish:
Interacting with Visualizations (cid:4) 161 Viewport WORLD Viewport Viewport SCREEN SCREEN SCREEN setanidrooc neercS setanidrooc dlroW Figure4.15 Illustration of the conceptual model of zoomable interfaces. • geometric zooming or • semantic zooming. For geometric zooming, scale adjustments are considered at a geometric level only. That is, the visual representation is scaled according to the defined viewport. This can be done with a basic mathematical projection of the graphical objects. Semantic zooming goes beyond graphical scaling and allows for any kind of adjustments of the visualization depending on the scale. The additional semantics can be very valuable for visual data analysis. A simple example from graph visualization can illustrate how dramatic the difference between a purely geometric zoom and a semantically enhanced zoom can be. Figure 4.16 contains a sequence of three zoom operations toward acentralnodeofagraph(shownindarkgreen).Thezoomispurelygeometric: Everything becomes bigger as we zoom in. However, this does not help us much from a data analysis perspective. Figure 4.17 shows a semantically enhanced zoom. The scaling takes effect only on the nodes’ positions, but deliberately not on their size. The benefit of not scaling node sizes is that dense parts of the graph layout are untangled, creating an unobstructed view on the edges between the nodes. There is a second reason for keeping node size constant. In our case, node size encodes the node degree. To ensure a consistent interpretation of the data, the node size should not be changed during zoom operations. Since many visualizations use size as a visual variable to encode the data, it is important to pay close attention to what should and should not be scaled when zooming. The basic ingredients of zoomable visualizations are clear now. Next, we will look at the visual interface and the interaction that facilitate multi-scale data exploration.
162 (cid:4) Interactive Visual Data Analysis Figure4.16 Geometric zooming of a node-link visualization. Figure4.17 Semantically enhanced zooming of a node-link visualization. 4.5.2 VisualInterfaceandInteraction At any time, a zoomable visualization shows only a particular view of the data. Therefore, zoomable visualizations have to be designed such that users can orient themselves and take navigation steps easily, allowing full concentration on the data analysis objectives. This requires a suitable visual interface as outlined below. The interface should support the user in coping with three questions in particular [Spe07]: • Where am I? • Where can I go? • How do I get there? Where Am I? To facilitate the data exploration, it is essential to clearly communicate where the current view is located in the global context. Scroll barsasillustratedatthebottomandtotherightof Figure4.18indicatewhere in x-direction and y-direction the current view is located in the world. From the size of the scrollbars, we can further infer how much of the visualization is covered by the current view. Scroll bars require relatively little display space, but it takes some mental effort to extract the information they bear.
Interacting with Visualizations (cid:4) 163 Figure4.18 A zoomable graph visualization and its controls. Theoverview+detailconcept,asalreadydescribedinSection3.1.2,allocates more visual resources to convey information more explicitly. As can be seen in the left part of Figure 4.18, an overview shows a miniature version of the visualizationinadedicatedwindow.Withintheoverview,thecurrentviewport ismarkedwitharedrectangle.Thismakesitquiteeasytointerpretthecurrent view’s location and scale in relation to the entire visualization. However, a disadvantage of embedded overviews is that they obscure parts of the actual visualization. Where Can I Go? Similar to how scrollbars and overviews communicate locationandscaleofthecurrentview,theyindicatewhereonecouldpotentially go. The free space left and right as well as above and below the scrollbars and in the overview stand for parts of the data that can be accessed. Yet, indicating where one could potentially go is only a first step. As a second step, it makes sense to hint at where one could usefully go. The more information about the world is provided, the easier it is for the user to decide on the destination to explore next. Therefore, overviews often embed a highly abstracted visual representation of the data. In our case, the overview depicts the global graph layout only via tiny colored dots. But even though there are neither edges nor any details in the overview, we can still see accumulations of nodes that may be worth visiting. The size of these accumulations suggests to us at which scale we should look at them. How Do I Get There? Knowing where to go, the next question is how to actually get there? Getting to a new view may be accomplished by moving and scaling the current view, or by defining a completely new one. Conceptually, there are two basic options for users to actually carry out these operations. The first is to interact directly on the view. The second is to manipulate graphical objects of the interface. Table 4.3 lists some common
164 (cid:4) Interactive Visual Data Analysis TABLE4.3 Interactions for moving, scaling, and defining a zoomed view. Direct interaction on the view Move view Drag world in view, also known as panning. Scale view Mouse wheel, mouse drag, or pinch gesture on view. Define view Draw elastic rectangle in view. Manipulation of interface objects Move view Drag scrollbars or red frame in overview. Scale view Drag edges of scrollbars or red frame in overview. Define view Draw elastic rectangle in overview. implementations for both options. Interacting directly on the view is good for making smaller changes to explore the immediate vicinity of the current view. However, substantial changes are more difficult to make because they would require many repetitive interactions. This is where the manipulation of interface objects can be more practical. Yet, it is important to realize that the precision with which can be interacted is limited. This is due to the fact thattheinterfaceobjectsrepresenttheentirevisualizationspaceinarelatively small display space. In summary we see that a suitable interactive visual interface is indispens- able for multi-scale data exploration with zoomable visualizations. Yet, it does not stop there. Additional interaction aids and visual cues can further improve the utility of zooming and facilitate particular analysis tasks. 4.5.3 InteractionAidsandVisualCues With the techniques introduced so far, the navigation in the data is based solelyonthevisuallayoutofthedata.Forexample,whenwepaninanode-link diagram, we will get to see the data that is spatially near in terms of the graph layout. But what if the data analysis involves questions regarding the neighborhood in terms of the graph structure. The structural neighbors are not necessarily in the vicinity of the current view, but might be anywhere in the graph layout. The question is where are they and how can we get there? Another common task is to look for data that are similar to those shown in the current view. Again, the similar data are not necessarily spatially near the current view, but could be in a completely different part of the visualization. Again we ask ourselves, where should we go and how can we get there? Both scenarios outlined above have in common that the data we wish to inspect may not be visible on the screen. Therefore, our first goal is to make users aware of off-screen data. As a second goal, we want to enable users to actually navigate to these data. Third, we are interested in making the view change understandable. Next, we will see how off-screen visualization, navigation shortcuts, and animated feedback can help us achieve these goals.
Interacting with Visualizations (cid:4) 165 Arrow Halo Wedge Proxy Enriched wedge neercs-ffO neercs-nO Figure4.19 Visualcuesforpointingtooff-screendata.Adaptedfrom[GST13]. Off-screenVisualization Off-screen visualization is a form of focus+context representation where the zoomed view (the focus) is enhanced with additional context information. The idea is to embed additional visual cues into the display to make residue of otherwise invisible data elements visible. Figure 4.19 illustrates examples of visual cues for off-screen nodes (dashed lines) in a node-link diagram. From left to right, there are five alternative visual cues. An arrow is a most simple mark to point at off-screen data. While arrows only indicate a direction, halos and wedges visualize direction plus distance [BR03; Gus+08]. By mentally completing the halo to a full circle, we can infer the full circle’s center and so the location of an off-screen node. With a similar mental effort, we can determine a node at a wedge’s tip. Proxiesaimatrepresentingtheactualoff-screendatamorefaithfully[FD13]. Note how the proxy in Figure 4.19 allows us to see the off-screen node itself and also its connection to the visible part of the graph layout, rather than indicating direction and distance. Enriched wedges strive to balance several communicativegoals[GST13].Theyvisualizedirection,distance,andproperties of the off-screen object. Even additional meta-information can be embedded, for example, to explain why a node is considered relevant. Apparently, pointing to off-screen data has to be done sparingly to avoid cluttering the display. Therefore, an off-screen visualization is typically backed byamechanismthatautomaticallyinferswhichpartsofthedataarepotentially relevant.Suchmechanismscanbebasedondegree-of-interest(DoI)functionsor analyticalcalculationsonthedata.MoreinformationonDoI-basedexploration of graphs can be found in Section 5.2.1. NavigationShortcuts The next question to be addressed is how to get to data that are far from the current view. While basic pan and zoom interactions can of course be used for this purpose, we are now interested in making the navigation more cost-efficient. This can be achieved by means of navigation shortcuts.
166 (cid:4) Interactive Visual Data Analysis Off-screen nodes Proxy nodes Radar Screen Figure4.20 Bring & go with radar view and proxy nodes. A navigation shortcut has a target in the data and is further associated with a viewport. The viewport is typically centered on the target, and its extent defines how much context around the target will be shown. In order to actually take advantage of navigation shortcuts, we need to make them visible. A particularly useful approach is to employ the off-screen visualizationsdescribedbefore.Theonlythingthatneedstobedoneistoturn the otherwise passive visual cues into active interface elements that can be interacted with. Navigating to an off-screen target is then as easy as clicking or tapping a visual cue. The idea of combining off-screen visualization with navigation shortcuts is also known as Bring & Go. It is a very effective means to reduce navigation costs when exploring large data [Mos+09; TAS09]. Let us briefly illustrate the bring and the go part with an example. Figure4.20showsagraphvisualizationwithmanynodesoutsidethecurrent view. A so-called radar tool brings in the otherwise invisible graph nodes by placing proxy nodes at the screen boundaries. The proxies act as visual cues and also serve as navigation shortcuts. In order to go to a particular node, the user simply activates its associated proxy. Using navigation shortcut like this is much easier than repeatedly perform- ing pan and zoom operations. Note, however, that navigation shortcuts only work when potential targets are known. For exploring uncharted terrain, the basic pan and zoom operations are still important. We are now familiar with several interactions that allow us to navigate from one portion of the data to another. Closing the action cycle, we will next consider animated visual feedback to make transitions between different views easily comprehensible.
Interacting with Visualizations (cid:4) 167 AnimatedViewTransitions Themostsimpleformofvisualfeedbackistoinstantlyrefreshthevisualization once a new viewport is set. This corresponds to static visual feedback as introduced in Section 4.3.2. For small incremental changes of the viewport, as they occur when panning the view directly or when dragging a scrollbar, this type of visual feedback is a suitable option. However, when the viewport has changed substantially, for example, after activating a navigation shortcut, simply replacing the old view with the new one may confuse the user. The problem is that the user has no chance of creating a mental connection between old and new view. This problem can be addressed by providing animated visual feedback. A smoothly animated transition makes it possible for users to comprehend how one view evolves into another, and hence, to stay oriented within the data. Smooth viewport animations are based on the following general idea: First, zoom out from the current view, second, pan toward the new view’s location, and third, zoom in to the reach the new view’s scale [vWN04]. But instead of taking these steps one after the other, they are smoothly intertwined. It turns out that the math involved in a smooth viewport transition is not trivial. The interested reader is referred to the excellent mathematical derivation in the original paper [vWN04]. Here, we can only sketch the basic idea and illustrate it with an example. For this purpose, let us take a look at a sequence of snapshots of a zoom animation. Figure 4.21 provides an overview with all intermediate viewports marked as gray rectangles. The animation starts with the smallest viewport in thebottom-leftpartofthevisualization.Thedestinationisasmallsubgraphat thetop-right.Inthefirstphaseoftheanimation,wearetakentoazoomed-out Destination Origin Viewports Figure4.21 Viewports during an animated transition.
168 (cid:4) Interactive Visual Data Analysis Origin Destination Figure4.22 Snapshots of the viewport animation outlined in Figure 4.21. view. Then, the view is smoothly panned and zoomed toward the destination. TheresultshowninFigure4.22isnotonlyaneye-pleasinganimation,butalso helps users understand the view transition. Try for yourself to comprehend it without the overview and the intermediate steps of the animation. Looking at the origin and the destination alone, it is hardly possible to reach the same level of understanding as when following the animation. We now have reached the point where the action cycle is closed. We have described several interactive mechanisms and corresponding visual means that allow us to explore data at varying scales. Common to all our previous considerations is that we worked with a two-dimensional zoom, which is a perfect match for the many two-dimensional visualization techniques in existence. In the following, though, we will see that one-dimensional or n- dimensional zooming is also important in data analysis scenarios. 4.5.4 BeyondZoominginTwoDimensions Multi-scale exploration via zooming as described above inherently relies on a two-dimensional visual representation of the data. What we actually explore is the 2D view space. Yet, there are situations in which it makes sense to make the exploration more independent of the view space, that is, to provide specific pan and zoom functionality depending on the data characteristics. In this section, we will look at the particular case of multi-scale exploration of univariate (1D) and multivariate (nD) data by the example of time series. Exploring1DTimeSeries Understanding data in their temporal context is a particularly important analysis objective [Aig+11]. As we explore time series, we need to flexibly adjust where to look in time and also at how much we look. Range sliders are commonly applied to support one-dimensional navigation along the time axis. A range slider consists of a scale representing the time domain and two handles that define the time period to be visualized. Figure 4.23 shows a range slider in combination with a spiral visualization. Theslider’shandlescanbeadjustedtonarroworwidenthetimeperiodvisible
Interacting with Visualizations (cid:4) 169 Figure 4.23 A range slider controls the time period mapped to a spiral visualization of the daily average temperature for the city of Rostock. at the spiral. This corresponds to zooming in and out. A rather wide period facilitates an overview of the data (left spiral), whereas a rather narrow period allows us to see details (right spiral). Moving through time is possible by draggingtherangemarkedinbetweenthetwosliderhandles.Thiscorresponds to a pan operation. As the user manipulates the slider, different or more or less time steps are visualized, while the general spiral layout is maintained. It is left as an exercise to the reader to imagine what visual results a normal 2D zoom would produce. Inourexample,zoomandpanaredeliberatelyone-dimensional.Bymaking the zoom independent from the 2D visual representation and hooking it up more tightly to the dimension of time, it is possible to define a time period of interest more directly. However, there is a problem with sliders when used for navigating in time. Time-orienteddataoftencontainthousandsoftimesteps.Thetimeseriesfrom Figure4.23consistsofabout25,000daysworthofdata.Wewouldneedaslider that has a width of 25,000 pixels to be able to guarantee that any date in the range can be accessed. If we had the 25,000 pixels, each pixel would represent exactly one date. Typically, however, we do not have so many pixels. More realistic are scenarios where our slider has a width of 1,000 pixels, meaning that 25,000 dates are mapped to only 1,000 pixels. So, by moving a slider handle by one pixel, we will not get to the next day, but to the next 25th day, essentially skipping 24 dates in between. There is no way we can access any of these 24 dates by direct manipulation of the slider. In short, when the data dimension we wish to navigate with a slider is reasonably large, some values could be inaccessible. This problem can be tackled by considering multiple scales not only for the visualization, but also for the interaction. Multi-scaleInput The question is how to explore a larger range of values quickly and precisely by using a single continuous interaction gesture? With standard controls we
170 (cid:4) Interactive Visual Data Analysis (a)Regular range slider with global scale. (b)A slider with increased precision is dynamically added to the range slider. Figure4.24 Adjusting a time period at different input scales. can either be fast (cover large distances) or precise (get to an exact spot), but not both. The reason is that the scale at which the interaction takes place is fixed. The time slider from Figure 4.23 facilitates quickly browsing through thousands of days of data. Yet, how can we navigate with precision, say to an exact date? Interacting at dynamically changing scales is an answer to this question. Addingdynamicscalingtotheinputequationallowsustobeswiftandaccurate at the same time. At a coarser scale, large distances can be covered, while at finer scales, interaction precision is increased. We will illustrate this with a simple example. Let’s assume our data domain covers the dates between January 1, 2000 and December 31, 2010. The goal is to focus on the subrange from August 8, 2006 to October 8, 2010. Figure 4.24a shows a regular range slider, where the upper limit has already been set as intended. In order to set the lower limit, the user performs a continuous gesture as illustrated in Figure 4.24b. The gesture starts by grabbing the left slider handle, which is then dragged coarsely to the desired lower limit. As the exact date cannot be accessed directly, the user carries out a downward movement orthogonaltotheslider.Thistriggersthedynamicappearanceofanon-demand slider, where the gesture continues with a horizontal movement. Theimportantdetailaboutthenewslideristhatitoffersahigherprecision. This is accomplished by mapping a smaller range of values to the slider. It covers only a local interval around the value where the cursor left the regular slider. Thanks to the increased precision, the new slider’s handle can be moved exactly to the desired date. As this happens, the lower limit at the regular slider is updated automatically. Once the exact date has been reached, the gesture can be released, which dismisses the on-demand slider. As we can see, a two-scale mechanism enables us to interact coarsely (with theregularslider)andprecisely(withtheon-demandslider)inonegesture.By
Interacting with Visualizations (cid:4) 171 incorporating additional increased-precision sliders, it would also be possible to interact at more than two scales. Such a general approach would be useful for exploring very large time series. The example in this section started out with the goal to adjust a time period to specific limits. Some readers may argue that knowing the limits in advance, we could have set them simply by using the keyboard or a calendar widget. Yet, when exploring unknown data, it is often unclear where to look for interesting findings. They appear and disappear as we form and falsify hypotheses about the data that unfold before us. It can very well be that we spotsomethingpromisingwhilemovingtheslider.Thismaychangeourcourse of action and we decide on the fly that studying a different subrange could be more rewarding. This is a major advantage of continuous interaction: With a single gesture, we can dynamically explore the data at flexible pace and precision. Such a fluid exploration is hardly possible by querying the data in a discrete fashion, be it via keyboard input or a calendar widget. In the next section, we will continue to explore zoom concepts for data exploration. Yet, we will no longer navigate along the dimension of time alone, but rather expand zooming and panning to any dimension in the data. ZoomingMultipleDataVariables Previously, we considered sliders as useful elements of the graphical interface. Next,wewillseehowatightintegrationofslidersandvisualizationcansupport multi-scale exploration of nD time series. Let us illustrate this for the example of axes-based visualizations. As you may recall from Section 3.2.3, the basic visual component of axes-based visualizations are axes, each being associated with a specific data variable and arranged according to a specific layout. The actual data representation is achieved by lines or dots placed between pairs of axes. With this general approach, a variety of visualizations can be generated, including parallel coordinates plots, scatter plot matrices, line charts, and TimeWheels. An example of a TimeWheel is shown in Figure 4.25a. The central axis represents time, whereas the axes in the periphery represent time-dependent data attributes. Colored lines connect the time axis to all dependent attribute axes. Different colors are used to differentiate the attributes. The question we want to investigate is how can users flexibly explore not only time, but also the time dependent attributes via nD panning and zooming? An answer is to make the axes of the TimeWheel interactive [TAS04]. As axesalreadyrepresentrangesofobservablevalues,weonlyneedtoincorporate facilities that allow us to adjust what and how much of the value ranges gets visualized. To this end, a slider is integrated with the axis as shown in Figure 4.26. The slider body and its two handles to the left and the right can be manipulated directly via drag gestures as usual. Adjusting the slider has two complementary effects. First, the slider body marks the subrange of interest D+ = [D ,D ] within the global value low high
172 (cid:4) Interactive Visual Data Analysis (a)Plain non-interactive axes. (b)Axes with integrated sliders. Figure4.25 Integrated sliders for nD pan and zoom in the TimeWheel. Value range D D min Subrange of interest max D D low high D D low high Visual mapping range Figure4.26 Integrated range slider for per-axis pan and zoom. range D =[D ,D ]. Second, the actual visualization mapping of the axis min max is altered. Instead of showing the global min-max range, only the subrange of interest is visible. Note that the subrange extends over the entire length of the axis. This way, we obtain a per-axis pan and zoom functionality. Narrowing the slider allows us to look closer at details, whereas widening the slider will take us back to a broader view. Moving the slider will bring us to a different part of the value range. An example with four zoomable axes is shown in Figure 4.25b. The time axis has been zoomed to the first half of 1999, the two greenish axes have been set so as to get rid of outliers, and the red axis focuses on a user-chosen value range. With these per-axis zoom interactions, we conclude our excursion into the area of zoomable visualizations. As we have seen, there exists a rich set of interactive tools and corresponding visual cues for comprehensive multi-scale dataexploration.Inthenextsection,wewillstudyinteractivelensesasanother
Interacting with Visualizations (cid:4) 173 powerfulconceptforinteractivedataexploration.Incontrasttopanandzoom, which typically affect the visualization globally, interactive lenses are tools for lightweight local adjustments of the visualizations. 4.6 INTERACTIVE LENSES Theinteractiontechniquesdiscussedintheprevioussectionenableustoexplore different parts of the data, that is, to change what is shown on the screen. Anotheraspectofvisualdataexplorationistoexperimentwithdifferentvisual encodings of the data, that is, to change how the data are visualized. In a sense, we broaden our view of interaction from exploration of the data space to exploration of the visualization space. This includes adjusting the mapping of data values to visual variables and the arrangement of visual marks on the display as explained in Chapter 3. Thestandardwayofsupportingexploratoryvisualizationadjustmentsisto provideagraphicalinterfacewithallkindsofcontrolcomponents.InFigure4.3 back on page 140, we already saw such an interface. Altering parameters in the interface leads to a global and permanent change in the visualization. For example, when we switch the color scale, our visualization will be freshly painted overall. An elegant alternative to the aforementioned standard approach are inter- active lenses [Tom+17]. Lenses are lightweight exploration tools that can be added to a visualization on demand. Lenses can be used for various visualiza- tion adjustments, be it to encode data differently, to reconfigure the data’s visualarrangement,tofilterdataaccordingtocertainconditions,ortoconnect related findings. A key characteristic is that lenses produce local and transient changes in the visualization. That is, the visual representation is adjusted only inselectedpartsanditsoriginalstateisrestoredoncethelensisdismissed.For example, a lens could be used to enhance the color coding to inspect details of local accumulation of data elements. 4.6.1 ConceptualModel AschematicdepictionofaninteractivevisualizationlensisgiveninFigure4.27. A lens is defined by its position, size, shape and orientation and divides the visualrepresentationintoan interiorand anexterior part. Conceptually,lenses combine two interactions in a single tool: (i) interactive selection and (ii) adjustment of the visualization. A corresponding model can be defined based on the visualization pipeline. As you may remember from Chapter 2, the visualization pipeline describes how data are transformed to a visualization image via analytical and visual abstractions. A visualization lens can be understood as an additional lens pipeline that is attached to a standard visualization pipeline as illustrated in Figure 4.28. The standard pipeline (bottom) produces a regular visualization. The lens pipeline (top) implements a lens function that generates a lens effect. There
174 (cid:4) Interactive Visual Data Analysis Position VISUALIZATION Regular representation e Adjusted representation z Si Orientation LENS Exterior Shape Border Interior Figure4.27 Schema of an interactive lens. Adapted from [Tom+17]. Lens function 01100011 01110100 Selection Join 01000001 01101110 01101010 01100001 Figure 4.28 Model of a lens pipeline attached to a standard visualization. Adapted from [Tom+17]. are two points of information exchange between the standard pipeline and the lens pipeline. The first is a selection. It defines what is to be processed by the lens function. The second is a join, which specifies how the result of the lens function is to be integrated back into the standard pipeline. Next, we will describe these main ingredients of lenses in more detail. The Selection The selection corresponds to the content shown underneath the lens. Any type of content that is available along the visualization pipeline canbeselected.Alenscandirectlyselectpixelsfromtheimagespace.Content from other stages of the visualization pipeline can be selected as well, for example,asetof2Dor3Dgraphicalobjects,agroupofdataelements,arange of values, or any combination thereof. Typically, the selection will be a proper subset of the data that is signifi- cantlysmallerthantheoriginaldata.Thisallowsalenstoperformcalculations that would take too long for the entire dataset or would not be possible at all. As we will see a little later in this section, some lenses install mechanisms that automatically restrict the selection to maintain their operability.
Interacting with Visualizations (cid:4) 175 Christian Vincent Anja Thomas Arvid Grit Olivia Vincent (a)Alteration. (b)Suppression. (c)Enrichment. Figure4.29 Fundamental effects of lens functions. The Lens Function While the selection specifies what is to be affected, the lens function defines how the visualization is modified. For example, when our goal is to re-color parts of the visualization, the lens function may achieve this effect by altering selected graphical objects. A selection that contains raw data opens up the possibility to create an entirely different visual representation, but restricted to the selected values. The output generated by the lens function will lead to an alternative or modified visualization. In general, the goal is to improve the visualization with respect to the task at hand. To this end, a lens function can alter existing content, suppress irrelevant content, or enrich with new content. Figure 4.29 illustrates the different options. In Figure 4.29a, the visual encoding inside the lensisalteredtoemphasizethesmalldots.Incontrast,thelensinFigure4.29b suppresses the small dots deemed less relevant. Finally, Figure 4.29c shows a lens that enriches the visualization with labels for dots within the lens interior [BRL09]. The lens function usually depends on parameters that control the lens effect. A magnifying lens, for example, may expose the magnification factor as a parameter. A filtering lens may be parameterized by a threshold to control the amount of data to be filtered out. In general, a lens may be parameterized by an alpha value used for blending lens and visualization when both are joined. The Join To create the final image, the lens effect needs to be joined with the base visualization. Traditionally, the visible effect of a lens is confined to the lens interior. In the context of visualization, though, it can be practical to allow lenses to affect the visualization beyond their interior or even show their effect separately. Yet,mostlensesfollowthemetaphorofconventionallenseswherethevisual effect manifests exclusively in the lens interior. This can be accomplished with the following generic three-step procedure. First, render the base visualization,
176 (cid:4) Interactive Visual Data Analysis optionally sparing the interior of the lens. Second, fetch the lens effect to the lensinterior,optionallyblendingwiththebasevisualization.Third,incorporate suitable visual feedback and optional user control elements. In our examples, a thick outline makes clear to the user that a lens is in operation. Conceptually, the join can be done at any stage of the pipeline. If the join takes place at the early stages of the visualization pipeline, the visual effect may go beyond the lens. For example, a lens may adjust the positions of selected nodes in a node-link diagram. As a side effect, the incident edges of the altered nodes will take different routes as well, which in turn introduces (limited) visual change into the base visualization outside the lens. In summary, selection, lens function, and join describe the key components of lenses. Conceptually modeling lenses as secondary visualization pipelines makes it not only possible to use multiple lenses in the same visualization, but also to combine different types of lenses to create composite lens effects. Later in Section 4.6.3 we will see an example. Next, our attention shall first be drawn to the properties of lenses and means of adjusting them. 4.6.2 AdjustableProperties From the perspective of a user who is actually working with a lens, two questions are relevant: What properties of lenses exist and how can they be adjusted to suit the user’s data analysis objectives. LensProperties Looking at lenses, their geometric properties are the first to catch the eye. Position and size of a lens are most relevant. They determine where and to which extent a lens takes effect. Another prominent property is the lens shape. Followingtheclassicprototypeofreallenses,manyvirtuallensesareofcircular shape as shown in Figure 4.30a. Rectangular lenses are common as well. For non-circular lenses, also the orientation is relevant. Orientable lenses can be better adapted to the underlying data as illustrated in Figure 4.30b. In addition to the geometric properties, there are the parameters that control the inner workings of lenses. We already mentioned magnification factors and filter thresholds as examples of lens parameters. In general, lens parameters are often used to balance the strength of the lens effect, where strength can have different meanings, for example, how much more detail is added, how much irrelevant data are suppressed, or how substantially the base visualization gets altered. InteractiveandAutomaticAdjustmentofLenses A great deal of the flexibility attributed to lenses pertains to the possibility to adjust them interactively via direct manipulation. Complementary automatic mechanisms may support the adjustment of lenses.
Interacting with Visualizations (cid:4) 177 (a)Circular. (b)Rectangular orientable. (c)Content-adaptive shape. Figure4.30 Lenses with different shapes and orientation. (a)Move and resize. (b)Adjust parameters. Figure4.31 Direct manipulation of lenses. Direct manipulation as shown in Figure 4.31a is the preferred way of interactively adjusting position and size. Both properties can also be set automatically.Forexample,onecanpositionalensautomaticallyatinteresting data tuples. Automatic adjustments of the lens size can help to cope with the computationalcostsforproducingthelenseffectandalsothecognitivecostsfor makingsenseofit.AnexampleisthelabelinglensfromourearlierFigure4.29c. Whenbeingmovedintodensepartsofthedata,thelensautomaticallyreduces its size to limit the number of labels. This way, the lens can keep the labels readable and the algorithmic runtime low [BRL09]. Interactively switching between different lens shapes is possible, but rather uncommon. More interesting are lenses that adapt their shape automatically based on characteristics of the data [Pin+12]. Such self-adapting lenses are particularly useful in cases where the lens effect needs to be confined to complicatedgeometricfeaturesinthevisualization.Asanexample,Figure4.30c illustrates a lens that has adjusted itself to a cluster of data elements. Finally, we need to think about the internal parameters of lenses. For occasional adjustments, it is sufficient to rely on standard widgets. Parameters
178 (cid:4) Interactive Visual Data Analysis that need to be fine-tuned more frequently are preferably adjustable via dedicatedmechanisms.Figure4.31bshowsanexamplewherecustominterfaces elements are provided directly at the lens [KRD14]. There are also lenses that can adjust their parameters automatically to the data. An example is to tune a sampling rate parameter to the data density underneath the lens [ED06]. As we see, the adjustable properties of lenses (geometry plus parameters) make them very flexible data exploration tools. Lenses can be easily controlled via direct manipulation or through automatic procedures that adapt the lens totheunderlyingdata.Howlensescanbeappliedtoactuallyaccomplishvisual analysis tasks will be demonstrated next. 4.6.3 LensesinAction So far we have considered lenses on a rather abstract conceptual level. In this section, we will illustrate the versatility and utility of interactive lenses in the context of visual data analysis scenarios. We will consider four practical problems and corresponding lenses to solve them. We will start with the quite common task of studying specific details in the visualization. Second, we will explain how lenses can support the exploration of structural relationships in graphs.Addingspaceandtime,thethirdlenswillhelpusunderstandtemporal aspects of geo-spatial movement trajectories. Finally, we will make a step from altering the visualization to altering the actual data with an edit lens. ExploringDetailswithaFish-eyeLens Magnifying glasses have an ancient history as tools allowing us to look at details that cannot be seen with the human eye alone. On a computer screen, interactive magnifying lenses serve the same purpose: They are positioned on the screen where a more detailed view is needed. The lens will then transform the content underneath it according to a mathematical specification. A prominent example of such a mathematical specification is the fish-eye distortion [SB94]. It gradually pushes content from the lens center outward. As shown in Figure 4.32, this effectively magnifies the content near the mouse cursor and allows us to see the details there. Because the fish-eye distortion smoothly embeds the details within the global context, it is conceptually a form of focus+context representation as introduced in Section 3.1.2. The fish-eye lens already demonstrates the utility of lenses for altering a visualization in a dynamic and lightweight manner. Next, we will elaborate on lenses that are particularly useful for exploring structures in graph data. ExploringStructuralRelationshipswithGraphLenses When exploring graph data, structural relationships play an important role. Node-link diagrams, as introduced in Section 3.5 allow us to see how nodes are
Interacting with Visualizations (cid:4) 179 (a)Regular map visualization. (b)Details magnified with a fish-eye lens. Figure4.32 Magnifying details in a map visualization with a fish-eye lens. connected and if there are any communities or clusters. Next, we will apply graph lenses to enhance exploratory work with node-link diagrams [Tom+06]. Let us take a look at the zoomed-in graph visualization in Figure 4.33a. We are interested in the edges that connect the node in the center. As we can see, edge clutter is a problem in our example. There are many edges and we cannot really say which edges do actually hook up to our node of interest and which are just passing by. An interactive lens can help us out. In Figure 4.33b, we use a local-edge lens to clear the visualization of irrelevant edges. The lens suppresses edges that do not connect to nodes inside the lens. We can now easily see that our node of interest has seven incident edges. What we cannot see, though, are the adjacent neighbor nodes, which are beyond the current view. We could pan and zoom to each one of them, but this could take a while and we cannot really be sure what we will find. Using a lens can be more efficient in this situation. This time, we apply a bring-neighbors lens. As its name suggests, the lens will bring to us the neighbors of the nodes inside the lens. As the lens is moved toward the node of interest, its neighbors will be gradually drawn toward the lens. When the lens is exactly on top of the node, all its neighbors will be inside the lens as we can see in Figure 4.33c. The lens effectively produces a local overview of the neighborhood of the nodes covered by the lens. There is no need to manually visit the neighbors, the lens brings them in for us to inspect them. The bring-neighbors lens works well when the neighbors are evenly dis- tributed across the graph layout. However, if this is not the case, we could end up with the majority of the nodes occluding each other at the lens center. To solve this problem, we can exploit the fact that lens effects can be combined to create composite lenses. In our case, we combine the bring-neighbors effect with a fish-eye distor- tion. The combined effect will bring in the neighbors, but those that would accumulate too tightly at the lens center will be pushed outward to loosen
180 (cid:4) Interactive Visual Data Analysis (a)Node-link diagram without lens. (b)Local-edge lens. (c)Bring-neighbors lens. (d)Composite lens. Figure4.33 Graph lenses for exploring structural relationships. the clutter. Figure 4.33d shows the result. Looking closely, you will realize that the figure actually shows a composite lens that combines all three effects mentioned before: local-edge effect plus bring-neighbors effect plus fish-eye effect. We have just seen lenses in action for the specific task of exploring graph data.Thelenseshelpedustidyupedgeclutterandpeekatneighborhoodsthat are otherwise not visible at a glance. Next we will present a lens particularly designed for exploring temporal dependencies of movement data.
Interacting with Visualizations (cid:4) 181 Interior Scale Links Ring Lens Figure4.34 A lens to query temporal characteristics of movement data. ExploringTemporalAspectsofMovementDatawithaLens BackinSection3.4.3,wediscussedthechallengeofvisualizing spatio-temporal data. Showing space, time, and data attributes simultaneously at full detail is typically difficult. A more practical solution is to depict two aspects in full detail and add the third aspect interactively on demand for a selected part of the data. Here, we will enhance a visualization that focuses on space and attributes with an interactive lens to integrate the temporal aspect on the fly. Our point of departure is a 2D visualization of spatio-temporal movement data as shown in Figure 4.34. A map provides the spatial context. Trajectories of moving cars are visualized as lines, where colors encode speed. What we can see from this kind of representation is where cars drive at certain speeds. The time lens will help us to also see when [Tom+12]. The lens effect is shown in an auxiliary circular display to the left in Figure 4.34. The interior shows a scaled copy of the trajectory points selected withthelens.Thetime-dependentspeedattributeisvisualizedinthehistogram ring aroundtheinterior.Ourparticularexamplerevealsthatmovementsinthe selected region occur mostly around 9–10 and 18–19 o’clock, with the speeds being evenly distributed. Moreover, links connect the points in the interior with a finer-grained minute scale. This allows us to see additional details. For example, the trajectory marked in gray represents a movement that took place around 18:15 o’clock as indicated by the gray links accumulating at minute 15 of the 18th hour. The insights gained with the time lens could not be obtained with the plain trajectory visualization alone. This demonstrates quite nicely how useful lenses can be when it comes to accessing additional information and details of complex data on demand. All described lenses, the time lens, the graph lenses, and the fish-eye lens help users explore the data. Next, we go one step further and study a lens that supports editing the data.
182 (cid:4) Interactive Visual Data Analysis Figure4.35 Orthogonal node-link diagram of a biological network. EmployingaLensforSemi-automaticGraphEditing During the exploration of data, one may stumble upon findings that make it necessary to correct the data, for example, to insert missing elements, update erroneous data values, or delete obvious outliers. The goal in this section is to demonstrate how a lens can support such an on-the-fly data editing. Our particular example is about editing graphs. Suppose you have to insert a node with say a dozen of edges into a network such as the one shown in Figure 4.35. It is obvious that positioning the node and routing each of the edges by hand would be tedious work and take a lot of time. A graph layout algorithm could do the math and compute a high quality layout. But most algorithms would recompute the layout globally, which would harm the mental map that we or others might already have about the data. What we need is a tool that lets us edit the data locally without intensive manual labor and no global changes of the layout. Again,weemployalenstoaddressthisproblem.Fittingtoitspurpose,the lens is called edit lens [Gla+14]. It supports three fundamental edit operations: insert, update, and delete. Figure 4.36 illustrates the lens being used for inserting,updating,anddeletingagraphnode.Theonlymanualworkrequired is to place the lens within the layout to specify where an edit operation is to take effect. In a sense, the adjustable lens acts as a coarse human-specified solution to be refined with automatic methods.
Interacting with Visualizations (cid:4) 183 Delete Update Insert (a)Place lens to insert. (b)Adjust lens to update. (c)Flick lens to delete. Figure4.36 Editing using the edit lens. Adapted from [Gla+14]. The automatic part consists of two steps. The first step determines a suitable unoccupied area in the lens interior where the edited item can be placed.Second,theprecisespotwithinthatunoccupiedareaiscomputedbased on heuristics for different graph aesthetics criteria, such as maximum distance to other nodes, short overall edge length, or low number of edge bends. During the editing, the user is free to move and resize the lens and to choose a different heuristic. The lens will compute and suggest suitable node positions and edge routes on the fly. Only if the user agrees with a suggested solution is the result of the edit operation committed to the data. Tosummarize,theeditlenssimplifiesfullymanualeditingtosemi-automatic editing.Thiseaseseditingoperationsconsiderably,becausetheuseronlyneeds to define a coarse region interactively, rather than precise positions or routes. The algorithmic part of the lens computes suggestions for precise solutions, which the user can customize on-the-fly. Finally, the lens is integrated into the regular visualization so that data analysis and data editing can go hand in hand. The edit lens concludes our journey into the world of interactive lenses in the context of visualization. In addition to the conceptual ideas behind lenses, we have described several exemplars of lenses for a number of different tasks, including looking at details, exploring graphs, incorporating temporal information, and even data editing. For more information on the described lenses, the interested reader is referred to the list of references collected at the end of this chapter. This section discussed lenses as a versatile interactive approach to support data exploration and analysis. The focus was on one tool for many different tasks.In thenextsection,wewill fliptheperspective. The focuswill beonone taskforwhichacomprehensivesetofinteractiontechniqueswillbeintroduced. The particular task we will be dealing with is visual comparison.
184 (cid:4) Interactive Visual Data Analysis 4.7 INTERACTIVE VISUAL COMPARISON Visual comparison takes a central role during data analysis activities. By inter- actively comparing different parts of the data, users may formulate, confirm, fine-tune, or reject initial hypotheses, draw corresponding conclusions, and thus can gain a better understanding of the data. Elementary comparison is often a predecessor to a more in-depth data analysis.Forexample,wecancomparesuccessivedatavaluesinatimeseriesof stock prices to identify trends. Comparing the trends enables us to find groups with similar trend behaviors. Still more insight can be gained by comparing the groups, for example, to study if certain behaviors occur in specific periods of the fiscal year. It is also common to derive quantitative statements about the compared data to capture their degree of relatedness. A corresponding notion is that of similarity (or dissimilarity), which plays an important role in many higher-level knowledge generation activities. Thissectiondealswithdedicatedinteractiontechniquesspecificallydesigned to support visual comparison tasks. But before we look at these techniques in detail, we need to understand what comparison is and how visual comparison takes place. 4.7.1 BasicsandRequirements What precisely do we mean by comparison? Given individual data values p and q (or sets of values P and Q), comparison tasks are defined as the search for a relation r such that p r q (or P r Q). When comparing numerical values, order relations r ∈ {<,≤,=,≥,>} are of great practical relevance. Specific relations exist for comparing temporal data (e.g., before, during, after) and spatial data (e.g., inside, overlap, touch). There are three fundamental visual designs specifically for comparison tasks: juxtaposition, superposition, and explicit encoding [Gle+11]. Figure 4.37 provides a comparison (pun intended). Juxtaposition shows the data side- by-side, that is, in separate spaces. In contrast, superposition stacks data in a unified visual space. Explicit encoding means calculating and visualizing numeric differences of the data being compared. However, comparing data visually without dedicated interaction support is typically non-trivial. For example, suppose you have spotted two interesting JUXTAPOSITION SUPERPOSITION EXPLICIT ENCODING Figure4.37 Visual designs for comparison tasks.
Interacting with Visualizations (cid:4) 185 patterns in different parts of a color-coded spreadsheet visualization. For comparing them, you first have to visit one pattern and memorize it. Then you have to navigate to the second pattern and compare that to the stored mental image of the first one. This procedure is inefficient, because it requires you to scroll over and over again, and is also error-prone, because the actual comparison is carried out based on a mental image from your short-term memory. The previous statements suggest that visual comparison involves multiple actions working in concert to reach a higher level of analytic thinking. From an interaction perspective, visual comparison is a procedure that comprises three phases: 1. Select the information to be compared. 2. Arrange the selected information for comparison. 3. Compare the arranged information visually. From these phases, we can infer important requirements. First, a set of comparison candidates has to be selected and maintained interactively. The number of candidates is usually small because our visual working memory is limited [PW06]. Yet, data may enter or exit the set of candidates on the fly as user interests change during the data investigation. Second, the data to be compared have to be rearranged dynamically to facilitate their comparison. This is necessary because many standard visualiza- tionsareoblivioustocomparisontasksandarrangethedataaccordingtosome fixed layout algorithm or some naturally given mapping such as geographic positions.Asaconsequence,theremightbelargergapsbetweenthedatatobe compared, which make comparisons more difficult. The eyes have to look back and forth between different parts of the display frequently. Moreover, when studying larger data with zoomable visualization interfaces as introduced in Section 4.5, it is not guaranteed that all relevant data are visible at all. Many manualnavigationstepsmightbenecessaryinordertosuccessfullyaccomplish a comparison task. At the same time, the short-term memory has to store not only the locations of the data, but also their visual representations. Finally, the actual comparison is performed. Juxtaposition, superposition, and explicit encoding form the visual basis for the comparison. However, it is not clear upfront which is the best strategy for the data at hand. Therefore, the user should be able to interactively choose and parameterize comparison strategies as needed. In summary, we see that visual comparison is a highly dynamic procedure. Interactiontechniquesareneededtoflexiblylinkthephasesofcomparisonand to account for the changing interests during comparative analytic activities. In the following, we introduce dedicated interaction designs for visual comparison. NextinSection4.7.2,ourmainprioritywillbenaturalnessofvisualcomparison. Later in Section 4.7.3, we will additionally consider aspects of cost efficiency.
186 (cid:4) Interactive Visual Data Analysis (a)Side-by-side. (b)Shine-through. (c)Folding. Figure4.38 Natural behavior of people comparing information on paper. 4.7.2 NaturallyInspiredComparison Naturalinteractionisathemeofinteractionresearchthatfocusesonenhancing interactive experiences by drawing inspiration from natural human behavior. Following this thinking, we first observe how people compare information natu- rallyandthendeveloptechniquesthatmimicpeople’snaturalbehavior[TFJ12]. Figure 4.38 illustrates the three basic strategies that can be observed when humans compare information printed on paper: (a) Side-by-side comparison: Sheets of paper are moved on a table until they are arranged side-by-side to facilitate comparison. (b) Shine-through comparison: Sheets of paper are stacked and held against the light to let information shine through and blend. (c) Folding comparison: Sheets of paper are stacked and individual sheets are folded back and forth to look at them in quick succession. Our goal is to replicate these natural strategies. To this end, we have to design corresponding virtual counterparts for the involved visual components and interactive procedures. Intermsofthevisualcomponents,weneedavirtualcomparisonworkspace and a virtual equivalent for sheets of paper. The workspace will be a zoomable visualization space based on the ideas discussed in Section 4.5. Sheets of paper are replicated as visualization views that reside in the zoomable space. Within the zoom space, views can be moved freely in analogy to arranging pieces of paper on a table. How the visual components are put to use for naturally inspired comparison will be explained in the following. SelectingWhattoCompare The comparison procedure starts with the selection of data to be compared. When users spot something interesting for comparison, they can simply mark
Interacting with Visualizations (cid:4) 187 Sub-views Main view Origin highlight Figure4.39 Creating sub-views for comparison. A red frame indicates where the left sub-view has been detached from the main view. it with an elastic rectangle. The system then creates a new view corresponding to the marked region, a sub-view of the entire visual representation so to say. Once created, sub-views exist as independent views in the visualization space. The real-world analog would be to create a copy and cut it out to contain only the information of interest. Yet, in the real world, we would lose the connection between the cutout and its origin. On the computer, the parent- child relationship of views can be preserved in a view hierarchy. As a result, users no longer need to mentally keep track of what they want to compare becausethisinformationisnowexternalizedintheformofdynamicallycreated views collected in a view hierarchy. The view hierarchy further makes it possible to embed visual cues for highlighting a sub-view’s origin on demand. Figure 4.39 illustrates this for a matrix visualization from which two sub-matrices have been created. As the user points at the left one, a red frame indicates where the sub-matrix has been extracted from its parent matrix in the background. ArrangingforComparison The second comparison phase consists of arranging the views to be compared. Using simple drag gestures, views can be arranged in the visualization space similar to shifting paper on a table. In the real world, people use edges of papers or patterns on the table surface to guide the arrangement. In the virtual world, snapping can be used to assist in arranging views. Snapping automatically aligns views with respect to certain features in the visualization. This way, costly pixel-precise adjustments can for the most part be avoided. In our example from Figure 4.39, snapping helps to maintain the alignment of matrix cells.
188 (cid:4) Interactive Visual Data Analysis Side-by-side B A B A A B Shine-through A B B AB AB AB Folding A B B A A A Figure4.40 Overview of natural interaction techniques for visual comparison. Adapted from [TFJ12]. ComparingViews Phase three is the actual comparison. Here, we take a simplified perspective on the underlying problem and abstract from the specific details of the data being compared. Chapter 3 made clear that plenty of visualization techniques are available to generate expressive visual representations of data. Under this assumption, we can simply resort to our views and sub-views as the objects to be compared visually. Next, we introduce the virtual analogs for the natural comparison strategies: side-by-side, shine-through, and folding. They are also summarized in Figure 4.40. Side-by-side Comparison Side-by-sidearrangementsprovideuswithacom- plete sight of the data, which is helpful as an overview. On the other hand, when comparing details, resolving spatial references among the views requires some cognitive effort. In particular, we have to move our eyes to check if a feature in one view can be found in the very same spot in another view. For example, when we compare matrix cells at position [i,j], we can follow along the i-th row easily from the left sub-matrix to the right. However, as we do so, we have to take care to stop correctly at the j-th column, which may be doable for our small example, but would certainly require some counting for a larger matrix. For more complex visualizations, the eyes might need to move to and fro multiple times to be sure of looking at the same reference spot.
Interacting with Visualizations (cid:4) 189 Folding point P Candidate points P • Folding origin O Folding axis O A Folding anchor A Figure4.41 Folding geometry. Adapted from [TFJ12]. Shine-through Comparison As with natural comparison of paper, an alter- nativeistoletviewsoverlap.Inreality,peopleoftenpurposelystackpaperson top of each other in order to create a unified comparison space. While spatial references are aligned then, mechanisms are needed to look through or behind the paper sheets. One way to resolve the occlusion is to hold the paper against the light. The degree to which information shines through can be controlled by altering the viewing angle with respect to the light source. On the computer, shine-through comparison can be realized via alpha- blending. That is, views are made partially translucent, where the level of transparencycanbevariedbytheuser.Shine-throughmakesiteasytocompare shapes and sizes in the graphical depiction of the data. On the other hand, blending the views implies that we mix colors as well, which is hindering the comparison of color-coded visual representations. Using shine-through, it can also be difficult to figure out which view contributes its data to a particular feature in the blended image. Or put differently,shine-throughfavorsamergedviewonthedataatthecostoflosing separabilityofindividualdataelements.Thefoldinginteractiondescribednext addresses this aspect. Folding Comparison Folding paper back and forth allows us to compare information shown on different sheets. The same is possible with our visu- alization views: To uncover occluded information, the user can fold away or peel off views as if they were virtual paper. The folding resolves the occlu- sion temporarily, while otherwise keeping the views in place to preserve their arrangement. In a data analysis setting, the fold should appear directly where the user’s focus is, which is typically the location of the pointer cursor P. Knowing P, a heuristic can be applied to determine from a set of candidate points a folding origin O and a folding anchor A. For real paper, the folding origin O corresponds to the spot where we would grab a page for folding it. The anchor A represents the fixture around which the paper is folded, such as a staple or the binding. Finally, the folding axis is constructed as a line originating at P and being perpendicular to the line PA as illustrated in Figure 4.41. The folded visualization view can be rendered in different styles. As shown in Figure 4.42, the styles are to vary in naturalness, information-richness, and degree of occlusion. For example, the visual effect resembling natural paper
190 (cid:4) Interactive Visual Data Analysis Information-rich Natural Occlusion-free Figure 4.42 Information-rich, natural, and occlusion-free folding styles. Adapted from [TFJ12]. folding leaves the folded backside blank. A more information-rich style would enhancethebacksidewithadditionalinformation.Anocclusion-freestylecould be restricted to only showing a subtle shading gradient. Foranaturalfeelingandrealisticappearance,thefoldingisanimatedusing a simple physically based spring-mass system. The animation starts with a springforcesmoothlypullingthefoldfromtheoriginO tothepointerposition P. When the user adjusts the pointer, P is updated so that the fold smoothly follows up. When the user releases the fold, the spring force is inverted to pull the fold back from the pointer P to the folding origin O, effectively unfolding the view. With the animated visual feedback, the virtual folding interaction for comparison tasks is complete. It is time for a recap. We studied a comprehensive repertoire of techniques for interactive visual comparison. By dynamically extracting sub-views, it is possibletoflexiblydefinewhatshouldbecompared.Side-by-side,shine-through, and folding interaction enable users to carry out the actual comparison in differentways.Thenaturalinspirationbehindtheinteractiondesigncontributes to making the comparison intuitive and easy. However, a pressing question remains: Under which circumstances can we apply which technique most effectively?Findinganswerstothisquestionrequiresfurtherextensiveresearch and elaborate user studies. So far, we left an important advantage of interactive systems (compared to paper) out of consideration: the ability to do automatic computations, for example, to calculate differences and show them directly. In the next section, we will continue with interaction specifically for visual comparison. However, our focus will shift from naturalness to reducing the interaction costs for comparing data, also by including automatic calculations. 4.7.3 ReducingComparisonCosts Before taking measures to reduce costs, we need to know where they accrue. In Section 4.2.1, we learned that interaction costs in general can be attributed to executing the interaction and to evaluating the visual representation. For comparison tasks, the following specific costs can be identified:
Interacting with Visualizations (cid:4) 191 • Costs for selecting the data to be compared • Costs for carrying out the comparison • Costs for understanding the data in context To reduce these specific comparison costs, we will now combine several of thetechniquespresentedintheprevioussections,includingautomaticselection mechanisms from Section 4.4, off-screen visualization and navigation shortcuts from Section 4.5, and the idea of local and lightweight adjustments as we know themfromthelensesfromSection4.6.Let’sstartwithreducingselectioncosts. Semi-automaticSelectionoftheDatatobeCompared For printed non-interactive data visualizations, much of the costs for com- parison are related to memorizing where relevant data are located and what characteristics they exhibit. On a computer, interactive selection enables the user to mark and highlight interesting data (see Section 4.4), which effectively off-loads otherwise mental effort to the machine. As user interests change frequently during data exploration, it makes sense to invest in reducing the selection costs by integrating automatic mechanisms. The idea is to reduce the manual selection of n data elements (or data subsets) to be compared to only a single manual selection plus an automatic selection of the remaining n−1 elements (or subsets). Theutilityofthisidealargelydependsondefiningasemanticallymeaningful automatic selection. A sensible approach in the context of comparison tasks is to rank the data according to their similarity (or dissimilarity). In other words, when the user selects a first data element, the n−1 most similar (or dissimilar) elements are added to the selection automatically. There are a number of similarity measures for different types of data that can be used for this purpose. Euclidean or Manhattan distance work well for numerical data, Edit or Hamming distance operate on string data. Categorical data require dedicated distance measures [BCK08]. In cases where the similarity of complex data subsets must be captured, it makes sense to extend to multi-dimensional or subspace measures [Tat+12]. As an alternative to similarity-based automatic selection, one could con- sider mechanisms that traverse the internal (graph) structure of the data or utilize flexible degree-of-interest (DoI) functions, as for example explained in Section 5.2.1 of Chapter 5. Irrespective of the method being employed to drive the automatic selection, the benefit for the user is significant: A single click (or tap) is enough to create a selection of n comparison candidates. This corresponds to a reduction of the costs from O(n) down to O(1). Of course, additional manual steps may be taken to complement or refine the automatic selection.
192 (cid:4) Interactive Visual Data Analysis DynamicRearrangementandVisualCuesforComparison Once data have been selected for comparison, the next step is to compare them in detail. However, typically it will take a number of navigational steps until we have acquired enough information to draw a comparative conclusion. The goal is to reduce the time-consuming navigation between the data to be compared. Instead of us collecting the required information, the system should automatically bring it to where we need it. To this end, the data have to be rearranged dynamically. The basic idea is to create a juxtaposition arrangement to facilitate the comparison on the fly. One option to do this is to form a ring, a so-called CompaRing [Tom16]. As shown in Figure 4.43, the CompaRing is a circular arrangement of slots to be filled with the data for comparison, regions of a choropleth map in our case. When the CompaRing is activated, the previously selecteddataaredynamicallyrelocatedfromtheiroriginalpositiontotheslots. With all relevant data being now displayed at the ring, the comparison can be carried out more directly and more easily. However, the relocated data are now detached from their original spatial context,whichcouldbedetrimentaltootherdataanalysisobjectives.Therefore, indicator arcs point in the direction where a slot’s data are originally located. Wide arcs (max. 90◦) stand for far-away origins, whereas narrow arcs (min. 10◦) suggest the origin is close. Theindicatorarcscanfurtherbeexploitedtoaugmentthecomparisonwith additional details. In order to make even subtle variations visible, the pairwise differences between a selected slot and all other slots of the CompaRing are calculated and color-coded into the indicator arcs. In Figure 4.43, a diverging red-blue color scale indicates negative and positive deviations from the slot under the mouse pointer. ThekeybenefitoftheCompaRingisthatitisnolongernecessarytocollect and memorize data characteristics, as the CompaRing brings the required information to us. Moreover, the explicit visual encoding of calculated local differences grants us insight into details that are not evident in the base visualization. This naturally reduces the costs for comparison tasks. UnderstandingDatainContextwithNavigationShortcuts So far, the CompaRing facilitates the plain comparison of the selected data. Formoreprofoundinsight,wealsoneedtounderstandthedataintheirspatial context. This still requires us to navigate to individual places in the data manually. In order to reduce the cost for navigation, the CompaRing picks up the idea of navigation shortcuts as introduced in Section 4.5.3. Each slot of the CompaRing serves as a trigger for a smooth animation that takes the user (and the CompaRing) to the corresponding data’s original position. Manual navigation steps are thus reduced to a minimum. On top of that, using the navigation shortcuts in combination with similarity-based automatic selection as described earlier enables a whole new
Interacting with Visualizations (cid:4) 193 Slot Indicator Arc Data Minimum Maximum Negative Positive CompaRing Difference Figure4.43 Relocating selected regions to form a ring for easier comparison. The map background has been desaturated for the purpose of illustration. typeofdata-drivenbring & go navigation.Wecanuseanavigationshortcutto go to the context of some data of interest. If we find some interesting data in the context, we can mark them and the automatic selection will bring related data to the CompaRing and hence to our attention. Each of the newly brought data elements can then be used to go to yet another destination to continue the data exploration. The interesting thing about this type of navigation is the combined use of semantic (data similarity) and spatial (data context) relationships as paths through the data. This concludes the section on interactive visual comparison. We learned thatvisualcomparisonisahigh-levelactivitythatinvolvesseveralintermediate steps.Inthefirstpartofthissection,wereplicatedhumancomparisonbehavior on the computer to facilitate natural comparative data analysis. Secondly, we presented several strategies to reduce comparison costs by complementing interactive methods with automatic mechanisms. As a result, we obtain a comprehensive repertoire of dedicated techniques for visual comparison. A take-home message of the described techniques is that visual comparison requiresahighdegreeofflexibilitywhenitcomestoselectingdata,rearranging their visual representation, and actually collating them. In this section and for the most part of this chapter, we have focused on specific data analysis tasks that are to be accomplished visually and interactively by a human user in one way or the other. However, we largely ignored the question regarding the technology used for interaction and visual output. We implicitly assumed to be working with a normal display, mouse,
194 (cid:4) Interactive Visual Data Analysis and keyboard. In the next section, we will leave these behind and study how interactivedataanalysiscanbecarriedoutinalternativedisplayenvironments and with different interaction modalities. 4.8 INTERACTION BEYOND MOUSE AND KEYBOARD What most of the existing visualization and interaction approaches have in common is that they are targeted for regular desktop workplaces. Yet, already in 1985, researchers recognized the importance of considering new technologies for the interface between humans and computers [HHN85]: “Butifwerestrictourselvestoonlybuildinganinterfacethatallowsustodo thingswecanalreadydoandtothinkinwayswealreadythink,wewillmiss themostexcitingpotentialofnewtechnology:toprovidenewwaystothinkof andtointeractwithadomain.” Hutchinsetal.,1985 EGA, MCGA, and VGA graphics cards and the computer mouse were the new technologies at that time. Today, new display technologies are related to largedisplaywallsorsmallmobiledisplays,bothwithhigh-resolutiongraphical output. New form factors and the increased pixel density made it necessary to adapt existing visualization approaches or to devise new ones. Modern interactiontechnologies,suchasmulti-touchinteractionortangibleinteraction haveconsiderablybroadenedthespectrumofwhatispossibleand,atthesame time, created a need for rethinking existing visualization solutions with regard to interaction. In this section, we will shed some light on how interactive visualization solutionscanworkwithmoderninputmodalitiesandoutputenvironments.We willstartwithbasictouchinteraction.Fromthere,wecontinuewithinteraction for tangible visualization views. Finally, we illustrate how proxemic interaction can facilitate visualization on a large high-resolution display wall. 4.8.1 TouchingVisualizations Touch interaction has become very popular, especially for hand-held devices. Bytouchingthevisualizationdirectlyunderourfingertips,alsotheexploration and analysis of data seems to be particularly well supported and promising. With touch interaction, direct manipulation becomes truly direct because the interaction takes place where the visualization is shown: on the display. Yet, in order to actually obtain directness, we have to address a few challenges.First,wehavetodealwiththefactthatonlytwostatesofBuxton’s three-statemodelofgraphicalinput(seeSection4.3.1)areavailablewhenusing
Interacting with Visualizations (cid:4) 195 current touch technology. That is, there is no way we can hover a visualization as we can do with a mouse. We can either touch it or not. Second, touch interaction is less precise. While practiced users can position a mouse pointer at a pixel-precise location, we are typically unable to touch down exactly at a certain pixel. This is due to our limited motor skills and the fact that our fingertips are several times larger than a pixel. Moreover, as the hover state is lacking, there is no way of correcting the initial touch point before triggering an action. Once the finger is down on the display surface, the touch is registered. Another problem is the occlusion that is introduced when parts of our hand or arm cover information on the display. Moreover, dealing with multiple touches is non-trivial from an interaction design perspective. An in-depth discussion of all of the aforementioned challenges of touch interaction for visualizationisbeyondthescopeofthischapter.Formoredetails,theinterested reader is referred to the further readings collected at the end of this chapter. Here, we would rather briefly demonstrate the differences of mouse-based and touch-based interaction for a simple visual representation of time-oriented data, a SpiraClock visualization. Touch-enabledSpiraClockVisualization The SpiraClock is a technique for visualizing collections of temporal events, suchasthe personalagendaor busschedules [DH02b].The SpiraClock’s hands display the current time, whereas its interior provides a spiral view onto the time to come. Along the spiral, future events are marked as spiral segments, each spiral cycle represents an hour in the future. Figure 4.44 shows a simple example with the current time being almost 3:55 o’clock. In 5 minutes, the next appointment will start and it will take 15 minutes. Following the spiral inward, we see that after a half-hour break there will be another appointment lasting 25 minutes. Still further inward we see more future events. As time goes by, events gradually move outward and eventually exit the spiral, while future events enter the spiral at the center. InordertoenableuserstoexploreeventsintimeandadjusttheSpiraClock, it makes sense to support the following basic interactions: • Navigate in time • Adjust future view • Query details The question that interests us is how these actions can be carried out with the classic mouse and alternatively with touch interaction. Let us first look at the mouse-based approach and then at the necessary changes to create a touch-enabled SpiraClock.
196 (cid:4) Interactive Visual Data Analysis Figure4.44 Visualizing future appointments with a SpiraClock. Mouse Interaction For navigation in time, users can temporarily set a different time by rotating the clock hands via drag gestures. The view into the future can be narrowed or widened by dragging the spiral. Dragging toward the center will reduce the number of cycles, dragging outward to the rim will bring more cycles to the SpiraClock’s interior. Finally, textual details can be queried simply by hovering an appointment, which will display a tooltip with the corresponding information. Touch Interaction In order to take advantage of touch interaction, the mouse-based design has to be cast into a touch-based one. On first sight, the transition from mouse to touch seems trivial: Instead of operating the mouse, fingers will carry out the drag gestures directly on the display. Yet, our users may soon stumble upon the difficulty of grabbing the clock hands due to the limited touch precision. One way to address the precision issue would be to enlarge the clock hands. Yet,thickerclockhandsnotonlylookawkward,theyalsoincreaseocclusion.An alternative is to decouple graphics and interaction [Con+08]. That is, regular clockhandsareusedfordrawingtheSpiraClockandaseparate,slightlythicker invisible geometry for handling the interaction. This way, the SpiraClock can maintain its appearance, while the clock hands are easier to pick thanks to their invisibly increased thickness. Unfortunately, the enlarged interaction geometry of the clock hands makes it more difficult to perform a drag gesture on the clock interior, as we might accidentally touch the hands. To solve this conflict, we can use a dedicated
Interacting with Visualizations (cid:4) 197 TABLE4.4 Mouse-based vs. touch-based interaction for the SpiraClock Action Mouse Touch Navigate Drag gesture on Drag gesture on in time clock hands enlarged clock hands Adjust Drag gesture on Pinch gesture future view spiral interior anywhere Query Hover appointments Tap on appointments details touch gesture: the pinch gesture. While clock hands are rotated with a single finger, manipulations of the spiral will be performed by a two-finger pinch gesture.Pinchingisthequasi-standardforscalinginteractionsontouchdevices. In our case, we scale the number of spiral cycles as soon as a second finger touches the display. Finally, we need to design the interaction to query details about the displayed events. As hovering is incompatible with touch interaction, we have to resort to a different approach. This time, we use a simple tap gesture, which corresponds to just a brief touch. When an event is tapped, a tooltip will display its details. A second tap on the same event (or on the background) will dismiss the tooltip. But what about the increased thickness of the clock hands? Doesn’t it lead to problems when the clock hands overlap with events? Yes, indeed. But the tap gesture is a discrete interaction, whereas the dragging of clock hands is continuous. This means, the tap is a very short interaction that is easily detected. Once we know that a tap occurred, we simply ignore the clock hands and consider only the events as potential interaction targets. Aswehaveseen,alreadyforthethreesimpleinteractionsoftheSpiraClock, we had to carefully think about the transition from mouse to touch interaction. Table 4.4 provides a comparison of both designs. We cannot claim that either of the designs is better or even optimal. Yet, they served well the purpose of demonstrating a few issues when designing touch interaction for visual representations of data. In the next section, we will continue with modern ways of exploring data. Yet, we will go one step further from fingers touching visualizations on a display to tangible interaction with the display. 4.8.2 InteractingwithTangibles Tangibleuserinterfacesisabroadfieldofresearch[SH10].Thegoaloftangible interaction is to narrow the gap between the virtual world on the computer and the real world in which the interaction takes place. The link between both worlds is physical objects, so-called tangibles. Physical manipulations
198 (cid:4) Interactive Visual Data Analysis of tangibles are transferred to virtual objects on the computer. In this sense, mouse-based interaction is already a form of tangible interaction, yet a rather indirect one. More direct tangible interaction can be achieved by using tangibles such as discs or cubes directly on horizontal touch-sensitive surface displays. To minimizeocclusion,thetangiblesaretypicallymadefromtranslucentmaterials, such as acrylic glass or foil. Various physical gestures can be performed with tangibles. Tangibles can be placed and moved across the display. Additionally, they can be rotated or positioned with different sides facing upwards. These tangible interactions expand the possibilities for interactive data exploration on touch-sensitive displays. Yet, interaction and visualizations remain in the two-dimensional horizontal plane of the display. Next, we present an approach that extends tangible interaction to the three-dimensional space above the display to provide enhanced visualization and interaction functionality. TangibleVisualizationViews Let us first introduce the basic setup. As before, a horizontal surface display will serve to visualize data and to receive touch input from the user. What we add to this basic setup are so-called tangible views [Spi+10]. Tangible views are lightweight “devices” that act as additional displays in the space on or above the horizontal surface. In a most simple instantiation, a tangible view can be a piece of cardboard onto which a projector transmits visualization content. In this case, the tangible view is passive. A tangible view can also be active, in which case it is capable of displaying graphical content on its own, for example, a tablet device. A key characteristic of tangible views is that they are spatially aware. Throughconstanttracking,thesystemalwaysknowsatangibleview’sposition and orientation. This opens up whole new possibilities for interaction as illustrated in Figure 4.45. The extended capabilities of tangible views include basictranslationandrotationinthreedimensionsaswellasgesturesofflipping, tilting, and shaking. By providing tangible views that are distinguishable by shape or appearance it is possible to create an interaction toolbox, where users can infer interaction functionality from the look of a tangible view. Multiple tangible views can be used simultaneously for advanced interaction and for adding display space for visualization purposes. Yet, offering extended interaction capabilities is only one part of the story. The second part is to utilize them to create a semantically meaningful interac- tion vocabulary for visualization scenarios. This has to be done depending on the characteristics of the data and their visual representation, and in line with the tasks to be carried out. Next, we take a closer look at two selected applications of tangible views. In the first example, we will use a tangible view as a magnifying lens for a scatterplot visualization. For the second example, we apply two tangible views
Interacting with Visualizations (cid:4) 199 TRANSLATE ROTATE GESTURE Vertical Horizontal Vertical Horizontal Tilt Shake FREEZE APPEARANCE GESTURE Vertical Horizontal Color Shape Flip Wipe Figure4.45 Extendedinteractionwithtangibleviews.Adaptedfrom[Spi+10]. to visually compare graph matrices. For both examples, we have already seen non-tangible implementations earlier in this chapter. It will now be interesting to see how tangible interaction creates a more physical data exploration experience. ATangibleMagnifyingLens For our first example, the main visualization is a scatter plot. In order to de-clutter dense parts of the plot, we can use a magnifying lens as described in Section 4.6.3. In a standard mouse-plus-keyboard setting, the lens can be moved across the visualization by drag gestures to define where it should take effect. The degree of magnification is typically adjustable via standard or custom-made sliders. Now let’s make the virtual lens tangible and truly direct. To this end, a circular tangible view is inserted into the space above the surface display as illustrated in Figure 4.46. The tangible view’s horizontal position determines whichpartofthevisualizationistobemagnified(dashedcircleonthesurface). The actual lens effect is projected onto the tangible view. This already yields a very tight coupling of display and interaction. The extended interaction vocabulary also makes it possible to control the magnification factor in a tangible way. There are several options how this can be implemented. The tangible view can be raised and lowered along the vertical axis to increase and decrease magnification. Figure 4.46 illustrates a second alternative: The tangible view is rotated around the vertical axis while a circular gauge visualizes the current magnification factor. As a result, we obtain a tangible magnifying lens that can be adjusted by physical manipulation, and the visual feedback is immediately visible where the interaction takes place. In this first example, one or two hands control a single tangible view. Next, we add a second tangible view and operate both simultaneously to accomplish comparison tasks.
200 (cid:4) Interactive Visual Data Analysis Figure4.46 A circular tangible lens for magnification purposes. Figure4.47 Comparing matrix data with two tangible views. TangibleVisualComparison Now, the base visualization on the surface shows a graph. It is represented in matrix form as described in Section 3.5.2 of Chapter 3. We will be using two rectangular tangible views as depicted in Figure 4.47. In order to select a sub-matrix to be compared, the user moves a tangible view horizontally above the surface. To fix the selection, a freeze gesture is carried out. This involves no more than swiping the thumb through a particular spot at the border of the tangible view. Once frozen, a tangible view maintains its visualization content, effectively ignoring any horizontal movements. This allows the user to arrange both tangible views side-by-side for closer inspection and comparison. Once the views are sufficiently close to each other, the system recognizes the user’s comparison intent and adds an explicit encoding of the overall aggregated similarity of the matrices to the tangible views. In our case, the green frame suggests that the matrices are quite similar to each other.
Interacting with Visualizations (cid:4) 201 (a)Parallel coordinates. (b)Node-link diagram. (c)Space-time cube. Figure4.48 Tangible views for different visualizations. In order to continue the visual comparison elsewhere in the data, the tangible views need to be awakened from their frozen state. A natural gesture to stir up a stationary system is to shake it. This is what the user does to un-freeze the tangible views: Simply shake them horizontally and start over with selecting different parts of the matrix. In summary we see that tangible views offer novel and interesting ways of interacting with visual representations of data. This is not only the case for magnification and comparison tasks, but for a broad range of visualization problems.Figure4.48illustratesfurtherapplicationsexamples.InFigure4.48a, the tangible view can be raised or lowered to control the sampling rate with whichthemultivariateparallelcoordinatesplotissampled.Raisingandlowering thetangibleviewabovethenode-linkdiagraminFigure4.48benablestheuser to access different levels of abstraction of a hierarchical graph. The tangible view in Figure 4.48c serves as a slice in a space-time cube visualization of spatio-temporal data. From a conceptual perspective, tangible views advance interaction and visualization in several ways. Tangible views integrate display and interaction device, allowing us to interact directly with the visualization. They also extend common two-dimensional interaction by tangible three-dimensional interaction above a horizontal base visualization. The resulting enhanced interaction and the extended physical display space create a tangible experience of otherwise purely virtual actions. Results of controlled user studies suggest that tangible spatial interaction is indeed a promising alternative when working in and with layered zoomable information spaces, which are common in visualization scenarios [SMD12; Spi+14]. Yet, developing an enhanced interaction and providing evidence of its extended expressive power is only a beginning. It remains to be investigated what will be the most suitable interaction designs for a broad range of data analysis scenarios, taking into account different types of data and different user tasks.
202 (cid:4) Interactive Visual Data Analysis Another interesting observation is that tangible views, touch-sensitive devices, and the classic mouse, all require us to use our hands for interaction. Next, we will look at a hands-free form of interaction where a visualization is controlled by physical body movements in front of a large display wall. 4.8.3 MovingtheBodytoExploreVisualizations Sofar,wehaveconsideredvisualizationonregulardisplays,horizontalsurfaces, and tangible views. These and other output devices with conventional pixel resolution are typically limited in the amount of information that can be displayed. Thanks to technological advances, large high-resolution displays are now becoming available to a broader range of users. The larger physical size and the increased pixel resolution offered by such displays have obvious advantages for visualization applications. Particularly in light of big data, being able to visualize much more information is an exciting prospect. Yet, with larger size and more pixels there also come new challenges. It is no longer feasible to interact with a mouse or touch alone, because it is simply too difficult, if not impossible to reach across the entire display. Therefore, new solutions are needed to support the visualization and interaction on large high-resolution displays. In this section, we look at a scenario where physical body movements in front of a display wall support the exploration of a graph with multiple levels of detail [Leh+11]. VisualizingaGraphonaDisplayWall ThesetupweareaddressingisatileddisplaywallasshowninFigure4.49.The wall consists of 24 individual displays covering an area of 3.7 m × 2.0 m with a total resolution of 11,520 × 4,800 which amounts to 55 million pixels. The data we want to explore is a graph that is described at three levels of detail. The visualization is based on a node-link representation which is augmented with textual labels and hulls for grouping. Individual nodes can be expanded or collapsed in order to get a finer or coarser view on the graph, respectively. On a desktop, expand and collapse operations are typically carried out by clicking or tapping. In our scenario, this is obviously impractical due to the large distances that would need to be covered. InteractingbyBodyMovements An alternative is to control the displayed level of detail by the user’s physical movement in front of the large display wall. This requires a tracking system to be set up to acquire information about the user’s position and orientation (6 degrees of freedom). Theuser’spositioncanthenbeexploitedtoadjustthelevelofdetailglobally. To this end, the space in front of the display wall is sub-divided into zones with increasing distance to the display, as illustrated in Figure 4.50a. Each
Interacting with Visualizations (cid:4) 203 Figure 4.49 Graph exploration on a large high-resolution display wall. Reprinted from [Tom15]. zone corresponds to a level of detail. When the user moves into a zone closer to the display, the graph is visualized at greater detail. Stepping backward into zones farther away from the display will lead to a coarser representation. This approach is also dubbed proxemic interaction [BMG10]. It is inspired by natural human behavior: Humans typically step up to the object of interest to study it in detail and step back to obtain an overview. Withthezone-basedinteraction,userscancontrolthelevelofdetailglobally. Local adjustments require a way to point at where the level of detail should be changed. One option to do this is to show more details exactly where the user is looking on the display. Based on the tracking information (position (a)Zones for global control. (b)Gaze plus lens for local control. Figure4.50 Interacting by physical movements. Adapted from [Leh+11].
204 (cid:4) Interactive Visual Data Analysis and orientation), the user’s gaze direction can be estimated. With the help of dedicatedeye-tracking,theprecisionoftheestimationcanfurtherbeimproved. At the spot being looked at, an interactive lens is embedded into the regular graphvisualizationasillustratedinFigure4.50b.Nodesthatareinsidethelens are automatically expanded to reveal more detailed information. By moving the head, the user can quickly scan the graph for details. Filtering the tracking input and smoothly animating node expand and collapse operations help to avoid flickering caused by natural head tremor and to maintain a reasonably stable visualization. The user’s physical movements can be used not only to control the level of detail, but also to derive a suitable layout for the labels shown in the graph visualization. For a user standing close to the display, the visualization will show more and smaller labels. When looking from a greater distance, the user will see fewer, but larger labels for groups of nodes. This way, the costs for producing the visualization and for comprehending it can be balanced. The results of a pilot study suggest that interaction through physical movement in front of a display wall can indeed be a valuable alternative in cases where classic means of interaction fail [Leh+11]. Physical movement not only better matches the scale of the display, it also is in line with natural interaction with real-world objects. Interacting via zones was reported as the approach that is easier to use, but on the other hand, the lens offered more control over where increased detail is to be shown. Here,weconsideredtherelativelysimpletaskofadjustingthelevelofdetail of a graph visualization. But physical movement has also proved useful in otherscenarios.Zoomingtasks,forexample,canbesupportedwellbyphysical navigation [Jak+13]. Other research results indicate that physical navigation can also be beneficial for higher-level analytic sensemaking [AN13]. In this last section of the chapter on interaction, we have focused on new technologies for interactive visual data analysis. We made a transition from mouse interaction to touch interaction and enhanced the latter with tangible interaction. Finally, we have illustrated how physical movements can be exploited to control visualizations. The presented techniques illustrate the prospect of utilizing modern technologies, but more research is necessary to make them reliable and evolve them to mature and tested ways of interacting with data and their visual representations. 4.9 SUMMARY The goal of visualization is to support people in forming mental models of otherwise difficult-to-grasp subjects, such as massive data, complex models, or dynamic systems. The term forming implies that visual output is not the end product of visualization. It is rather the human-in-the-loop process of interactively exploring the data and adjusting their visual representation that enables us to gain insight. This chapter elaborated on this very process.
Interacting with Visualizations (cid:4) 205 Welookedatthetopicfromdifferentangles,includingaspectsofthehuman user who interacts, the interaction tasks, the data being interacted with, and the technology used for interacting. In the first part of this chapter, we dealt with the question of why interaction is needed in visual data analysis scenarios. In the middle part, we discussed what users can actually accomplish with general interaction concepts and concrete interaction techniques. Finally, in the last part, we considered how interaction can be carried out using modern interaction modalities. Throughout the chapter, many examples illustrated how interaction can support data analysis activities. ConcludingRemarks Useful and usable interaction techniques are the result of careful consideration of the human user, the analytic tasks to be accomplished, the characteristics of the data, and the technological environment in which the analysis takes place. Several books would be needed to cover the design space for interaction comprehensively. Nonetheless, the following paragraphs will provide some high-level remarks on the design of interaction for visual data analysis. Duality of Input and Output A fundamental requirement is to consider the duality of visual output and interactive input right from the beginning. Hutchins and colleagues put it this way [HHN85]: “[...]thenatureoftherelationshipbetweeninputandoutputlanguagemust besuchthatanoutputexpressioncanserveasacomponentofaninput expression.” Hutchinsetal.,1985 Appliedtothecontextofthisbook,thismeansthatwhateverwedisplayin a visualization will most certainly be relevant for interaction as well. Whatever we desire to input into the system will typically require a suitable visual representation for direct manipulation. Adding interaction as an afterthought to a visualization is likely to cause trouble and increased development costs. Mapping Tasks to Interactions Given the complexity of exploratory and analytic activities and the wealth of interaction modalities, finding a good mapping of tasks to concrete interactions is a non-trivial endeavor. Two requirements are particularly important: Interactions must be conflict-free and should be cost-efficient. It is absolutely necessary that interactions be conflict-free, meaning that an interaction must be associated with a unique task. To this end, interactions
206 (cid:4) Interactive Visual Data Analysis must be unambiguously distinguishable. A distinction is typically made at the level of gestures, such as hovering, clicking or double-clicking. Also the space where the interaction takes place can be a differentiating factor. That is, we use the same gesture, but perform them in distinct spatial regions. In this case, clicking a map, clicking a data item, and clicking the background are all different interactions. Moreover, the order or timing of operations could be considered, but this complicates the interaction enormously. Speaking of complicated interaction, the second requirement is about interaction costs: Interaction should be cost-efficient. Each interaction costs. The more often an interaction is used, the higher the accumulated costs. Therefore, the mapping of tasks to interactions should consider a ranking of the relevance or frequency of tasks and an estimation of the costs of the available interactions. Based on that, a reasonably balanced mapping can be established. Frequent tasks should be mapped to interactions with the lowest costs. Only for infrequent tasks is it acceptable to use interactions with higher costs. InteractiveplusAutomatic Directlyrelatedtothecostaspectofinteraction is the question regarding what should be done interactively and what can be accomplished by automatic means. But this is not a question of interactive versus automatic means, rather it is a call for interactive plus automatic means. This chapter proposed several ideas of how interaction can be eased by integrating automatic mechanisms. After all, it is the responsibility of the system (and the designer of the system beforehand) to provide the information needed in a particular situation. Whenever we make functionality available via interaction we should carefully think about how interaction costs can be reduced by integrating automatic assistance. With these concluding remarks, we close the chapter on interaction. As we have seen, interaction is an integral component of visual data analysis approaches. In the next chapter, we will see that analyzing large data addi- tionally requires the integration of computational analytic components. FURTHER READING General Literature: [Dix+04] • [SP09] • [Tom15] • [SP16] • [DP20] Zoomable Visualizations: [BH94] • [FB95] • [Fur97] • [Bed11] Lenses for Visualization: [Bie+93] • [TFS08a] • [Tom+17] Visual Comparison: [Gle+11] • [vLan18] • [Gle18] Beyond Mouse and Keyboard: [Lee+12] • [Ise+13] • [JD13] • [Mar+18]
5 CHAPTER Automatic Analysis Support CONTENTS 5.1 Decluttering Visual Representations ........................ 209 5.1.1 Computing and Visualizing Density ................ 209 5.1.2 Bundling Geometrical Primitives ................... 212 5.2 Focusing on Relevant Data ................................. 214 5.2.1 Degree of Interest ................................... 214 5.2.2 Feature-based Visual Analysis ...................... 220 5.2.3 Analyzing Features of Chaotic Movement .......... 224 5.3 Abstracting Data ........................................... 231 5.3.1 Sampling and Aggregation .......................... 231 5.3.2 Exploring Multi-scale Data Abstractions ........... 233 5.4 Grouping Similar Data Elements ........................... 238 5.4.1 Classification ........................................ 239 5.4.2 Data Clustering ..................................... 243 5.4.3 Clustering Multivariate Dynamic Graphs .......... 250 5.5 Reducing Dimensionality ................................... 257 5.5.1 Principal Component Analysis ...................... 258 5.5.2 Visual Data Analysis with Principal Components .. 260 5.6 Summary .................................................... 263 IN THE PREVIOUS chapters we described fundamental approaches to interactive visual data analysis. In light of the potentially complex and very large datasets we are facing today, it is hardly possible to indiscriminately visualize and interact with all data. Our visual representations would simply be overcrowded and interaction would be cumbersome and costly. This is why computational analysis support is necessary. Visual analytics pioneer Daniel Keim puts it this way [Kei+06]: 207
208 (cid:4) Interactive Visual Data Analysis “Thevisualanalyticsprocesscomprisestheapplicationofautomaticanalysis methodsbeforeandaftertheinteractivevisualrepresentationisused.This isprimarilyduetothefactthatcurrentandespeciallyfuturedatasetsare complexontheonehandandtoolargetobevisualizedinastraightforward mannerontheotherhand.” Keimetal.,2006 The use of automatic analysis methods aims at extracting essential data characteristics. Showing key characteristics instead of the original data values reduces the complexity of visual representations and facilitates an initial overview of the data. Deeper insight can then be achieved by combining visualanalysis,interactivequerying,andfurtherautomaticcomputations.This procedure is nicely reflected in the visual analytics mantra, which we already mentioned in the introduction [Kei+06]: “AnalyseFirst– ShowtheImportant– Zoom,FilterandAnalyseFurther– DetailsonDemand” Keimetal.,2006 Theprimarygoalofthischapteristoprovideanoverviewofcomputational approaches that can support the analysis of large and complex data. While we will focus on the analyze step, we will also see that it is actually the tight interplay of automatic, visual, and interactive means that really advances the way data can be investigated and understood. Each section of this chapter will briefly explain a basic strategy for the analysis step and one or two selected techniques that implement the strategy. A common theme will be the reduction of complexity to make the visual analysis easier or enable it at all. What gets reduced will differ from section to section. In Section 5.1, our objective will be to reduce the complexity in visual representations. Section 5.2 considers the extraction of data and features of interest to narrow down the analysis on the parts that are relevant. How complexity can be reduced in the data space will be described in Sections 5.3 to 5.5. Section 5.3 is dedicated to the reduction of the cardinality of the data domain via data abstraction methods. In Section 5.4, we will see that grouping similar data is a powerful strategy to reduce the number of data
Automatic Analysis Support (cid:4) 209 elements. Finally, Section 5.5 will explain dimensionality reduction as a way to focus the analysis on the key information-bearing data variables. Let us now discuss the sketched options in more depth. We will start with methods that aim at reducing the complexity in visual representations. 5.1 DECLUTTERING VISUAL REPRESENTATIONS Over-plotting and visual clutter are common problems when large volumes of data are visualized. A first and important strategy facilitating the analysis of large data is to employ methods that can declutter the visual representation. We consider two basic approaches: computing and visualizing density as well as bundling of geometrical primitives. 5.1.1 ComputingandVisualizingDensity Density-based representations aim to communicate the distribution of data, rather than showing individual data values. The basic idea is to calculate how many data values fall within certain intervals. Alternatively, one can calculate how many graphical objects are within certain regions of the display. Both alternatives will next be illustrated by two well-known examples: con- tinuous scatter plots, which are based on data density, and outlier-preserving focus+context visualization, which is based on visual density. Data Density Traditional scatter plots visualize data elements as dots. For verylargedata,thiscanresultinsevereover-plotting,whichmakesitimpossible to discern how much data are represented by a dot. Continuous scatter plots solve this problem by visualizing a continuous density function [BW08a]. For this purpose, a mapping is performed from the data domain to the spatial domain that is spanned by the two axes of the scatter plot. While being conceptually continuous, the density is typically (a)Traditional scatter plot. (b)Continuous scatter plot. Figure5.1 Comparison of traditional scatter plot and continuous scatter plot. Reprinted from [BW08a].
210 (cid:4) Interactive Visual Data Analysis approximated by counting data elements in discrete intervals. The continuous visualization is then obtained by interpolation. Figure 5.1 compares a traditional scatter plot based on discrete dots with a continuous scatter plot based on data density. Both plots show the same “blunt-fin” dataset. With the density-based visualization, internal structures in the data are much easier to discern. VisualDensity Incontrasttoscatterplots,parallelcoordinateplotsvisualize data elements as polylines across parallel axes. For large datasets, a large number of polylines needs to be drawn, which can substantially clutter the visual representation. Again, a density-based approach can be employed to reduce visual clutter. Yet, now we are interested in the visual density of the graphics [NH06]. By computing the visual density, it is possible to accentuate not only general trends, but also details such as outliers. The rest of the data can be attenuated. Figure 5.2 schematically depicts how the visual density in parallel coordi- nates can be determined and how the visualization can be adapted accordingly. For the purpose of illustration, we start out in Figure 5.2a with the most basic (and unusual) case of only two parallel axes. In a first step, the axes are subdivided into bins b , where i denotes the i,j axis and j the bin per axis. The second step is to count how many lines emanate from the bins of one axis and arrive at the bins of the other axis. These pairwise frequencies are stored in a so-called bin map. We can see in Figure5.2bthatabinmapisagridofcells,whereeachcellrepresentsapairof bins.Forexample,therearefourlinesbetweenbinsb andb .Consequently, 1,1 2,2 the corresponding cell in the bin map contains the number 4. Repeating the counting for all pairs of bins completes the bin map. Thethirdstepistocategorizethecellsofthebinmapasbelongingtotrends ordetails,whichisillustratedinFigure5.2c.Thresholds,outlierdetection,and clustering methods can be involved in the categorization [NH06]. In the final step,trendsanddetailsaredrawntothedisplay,butdifferentlyso.Figure5.2d illustrates that the trends are represented in a graphically aggregated fashion, whereas the details get rendered as individual lines. b b b 5 1 b 5 1 b b 1,4 2,4 1,4 1,4 1,4 2,4 b 6 b 6 b b 1,3 1,3 b b 1,3 2,3 b 1 b 1 1,3 2,3 b b 1,2 1,2 b b 1,2 2,2 b 4 b 4 1,2 2,2 1,1 1,1 b b b b 1,1 2,1 b b b b b b b b 1,1 2,1 2,1 2,2 2,3 2,4 2,1 2,2 2,3 2,4 (a)Binned axes. (b)Bin map. (c)Categorization. (d)Drawing. Figure5.2 Procedure of determining visual density in parallel coordinates.
Automatic Analysis Support (cid:4) 211 (a)Standard parallel coordinates plot. (b)Density-based visualization. Figure5.3 Visualization of more than three million data elements. Reprinted from [NH06]. For general parallel coordinates plots with m axes, the above procedure is repeated for each pair of neighboring axes. As a result, m−1 bin maps are obtained based on which the visualization complexity is reduced globally. Figure 5.3 demonstrates the benefit of the described approach. A classic parallel coordinates plot is shown in Figure 5.3a. Due to clutter, it does not revealmuchofthedata’sinnerstructure.Thedensity-basedapproachisshown in Figure 5.3b. The general trends are visualized as parallelograms of different shades of green, where strong trends use brighter greens and weaker trends use darker greens. Data elements that have been categorized as details are explicitly visualized as green polylines. With the density-based visualization, trends and details can be seen much better. By interactively adjusting the number of bins and the thresholds for cat- egorizing trends and details, it is possible to further abstract or elaborate on the visualized structures. Moreover, red polylines can be superimposed on the visualization to facilitate comparing user-selected data against the automatically determined trends. This is illustrated in Figure 5.4. We have seen that density-based methods, no matter if they operate on data values or on graphical objects, are very well suited to declutter visual representations.Inthenextsection,wediscussanotherapproachthataddresses the problem of visual clutter, this time, by bundling geometrical primitives.
212 (cid:4) Interactive Visual Data Analysis Figure5.4 User-selecteddatainredcomparedagainstgeneraltrendsingreen. Courtesy of Helwig Hauser. 5.1.2 BundlingGeometricalPrimitives As we know from the previous section, visual representations that use lines or line segments as their basic geometrical primitives are prone to severe clutter. Many such visualizations exit, for example, the parallel coordinates plot as described before, node-link diagrams of graphs, or trails of movement data. In this section, we focus on decluttering such visual representations by summarizing geometrical line primitives into so-called bundles. As before, the goal is to highlight the fundamental structures in the visual representation. The general procedure of bundling is outlined in Figure 5.5. The starting point is line primitives. They are the basis for the specification of paths that consist of the lines’ original vertices plus additional control points. Such paths can be flexibly adjusted in their course in order to form visual bundles. To this end, it is necessary to decide which paths should belong to a bundle and how the individual paths should be transformed. Both decisions can be made based on either an explicit or an implicit bundling definition. Lines Paths Bundles Image Defining Paths Bundling Drawing Explicit Definition Implicit Definition Figure5.5 General procedure of bundling. Inspired by [LHT17].
Automatic Analysis Support (cid:4) 213 (a)Conventional representation. (b)Hierarchical edge bundling [Hol06]. Figure5.6 Visualizationofdependenciesinasoftwareclasshierarchy.Adapted from bl.ocks.org/mbostock/4341134. Explicit definitions resort to existing criteria. For example, paths can be bundled along given hierarchical structures [Hol06]. In parallel coordinate plots, polylines can be bundled according to pre-calculated clusters [Hei+12]. Bundling based on such predefined criteria produces predictable layouts, but it is less flexible in terms of bundling control. Implicit definitions make use of appropriate similarity metrics. Paths that are similar will be transformed to bundles. A prominent example is force- directed bundling methods [HvW09]. Compatibility criteria define which paths are eligible for bundling and the actual bundling transformation is done via spring forces between the control points of compatible paths. The implicit bundling strategy is more flexible, since it is not necessary to address pre- defined constraints. It can be adjusted easily, and bundles will be updated on the fly. In the last step, the bundles are visualized. There are two options: drawing individualcurvesordrawingentirebundlesascompactgeometricalshapes.The important thing is to ensure that bundles are recognizable and distinguishable. Thiscanbeachievedbyasuitablevisualencoding.Often,bundlesareassociated with a unique color. The visual encoding can be enhanced by shading the bundles based on a pseudo-surface that spans a bundle [LHT17]. Blending can be applied to resolve occlusion problems when bundles overlap. Figure 5.6 illustrates the positive effect of bundling. The treemap in the background visualizes the class hierarchy of a software framework. Links between the nodes of the treemap indicate dependencies between classes. The darker a link is, the larger are the classes involved in a dependency. As can be seeninFigure5.6a,manylinksleadtosubstantialclutter.Bytransformingthe links into curved bundles, in this case based on the explicitly given hierarchy, the visualization can be streamlined. Figure 5.6b reveals the major relations among the class dependencies. In summary, this section discussed methods to facilitate the analysis of large datasets by reducing the complexity in visual representations. In the following sections, we will continue with reducing the complexity in the data.
214 (cid:4) Interactive Visual Data Analysis 5.2 FOCUSING ON RELEVANT DATA A common and widely used strategy in terms of data reduction is to focus the visual analysis on data of interest. But what are the data of interest and how can a distinction between interesting and less-relevant data be made? Of course,userscouldinteractivelyselectthedatatheydeemimportant.Forlarge datasets, however, a purely manual procedure can be time-consuming and error-prone. How automatic computations can help in determining relevant data will be the topic of this section. The basic idea is to let users specify (not select) their interests and to employ automatic computations to select those parts of the data that match the specification. We discuss two concepts that implement this general idea. First, we study the concept of degree of interest and how it can help us to narrow down the analysis on relevant data. Second, we present the concept of feature-based visual analysis, which is about the specification and automatic extraction of meaningful data characteristics. 5.2.1 DegreeofInterest The degree of interest (DoI) is a concept to capture the relevance of data for solving an analysis task. The degree of interest can be expressed by DoI functions.ADoIfunctionassignstoeachdataelementarelevancevalue.Using a suitable threshold, it is then possible to distinguish between relevant and less-relevant data. The relevant data correspond to the target of an analysis task. In Section 2.2.2, we explained that particularly the targeted data need to bevisualizedfaithfully.Incontrasttothat,less-relevantdatacanbedimmedor even omitted. This substantially reduces the complexity of the visual analysis. BasicApproach Already in the 1980s, Furnas introduced a DoI function to express the degree of interest for the nodes of static hierarchies [Fur86]. The basic idea is that the degree of interest of a node n depends on the distance of n to a focus i i node n and an a priori interest of n . For the purpose of illustration, we set f i up a DoI function doi(n ,n ) as follows: i f doi(n ,n )=dist(n ,n )+api(n ) i f i f i Thefocusnoden isassumedtobeintheuser’scenterofattention,dist(n ,n ) f i f is the length of the shortest path between n and n , and api(n ) describes the i f i a priori importance of n . In the context of hierarchical data, it makes sense i to define api(n ) as the level at which n can be found in the hierarchy, that i i is, the distance of n to the hierarchy’s root. i Figure 5.7 illustrates how the DoI function assigns a relevance value to each node. In Figure 5.7a, we can see the dist(n ,n ) term, to which the i f api(n )termisaddedinFigure5.7b.AsourDoIfunctionisbasedondistances, i
Automatic Analysis Support (cid:4) 215 0 2 0 2 0 r = 3 1 3 3 1 1 4 4 2 1 r = 5 24 4 4 0 2 26 6 6 2 4 2 r = 7 3 5 1 1 1 3 8 4 4 4 3 4 6 6 2 2 2 4 10 10 6 6 6 4 (a)Distances to focus node. (b)Adding node levels. (c)Extracted subtrees. Figure5.7 Illustration of Furnas’ DoI function. lower values stand for higher degrees of interest. A user-defined threshold r determines where to draw the line between relevant and non-relevant nodes. Depending on the threshold used, relevant sub-structures of different size are automatically extracted from the hierarchy. Figure 5.7c provides examples for three different thresholds r =3, r =5, and r =7. Furnas’ idea of a DoI function is well established. It has been extended in different ways, for example, to handle multiple focus nodes [HC04] or to deal with general graphs [vHP09]. Most of the time, however, DoI functions are pre-defined and cannot be altered by the user. This is in contrast to the necessity of adapting the visual analysis to changing tasks and requirements. Next, we look into a more flexible approach that allows the user to construct DoI functions on demand. FlexibleConstructionofDoIFunctions The construction of a DoI function requires a modular design of the involved components[Abe+14].Withoutgoingintotoomuchdetailabouttheunderlying formalism, setting up a DoI function can be done in three steps: 1. Definition of relevance components 2. Combination of relevance components 3. Specification of relevance propagation With the first step, the user defines relevance components that compute a relevance value per data element. It makes sense to work with normalized values,where0signifiesnorelevanceand1isthehighestrelevance.Anexample ofarelevancecomponentwouldbeaGaussianfunctionthatproducesasmooth decline of relevance with increasing distance to a particular value of interest. It can make sense to define several components to capture different aspects of relevance. For example, one component could consider spatial proximity, another component could account for temporal dependencies. Also structural
216 (cid:4) Interactive Visual Data Analysis properties can be taken into account, as in the previous example, where we used distances in a hierarchical structure. ThesecondstepistoconstructacomprehensiveDoIfunctionbycombining the relevance components. Conceptually, a combination of relevance compo- nents can be modeled as a function that takes two or more relevance values and returns a combined relevance value. Furnas’ DoI function simply adds the components. Yet, more advanced combinations are possible. A common example is the combination as a weighted sum. It allows for balancing the influence of components: Heavily weighted components influence the overall resultmorethanlightlyweightedones.Furthermore,minormaxcombinations can be applied. A min combination requires all involved components to return high relevance in order for the combination to return a high relevance as well. A max combination works the other way around: If either component returns a high relevance, the overall combination will yield high relevance. We can now flexibly construct component combinations to compute the relevance of data elements. However, a relevance value characterizes only an individual data element. The immediate context of the data elements is not taken into account. For example in Figure 5.7c, a subtree with three nodes is extracted for r =3. Yet, structural properties of the focal node are lost. The subtree does not tell us whether the focal node has siblings or child nodes. Choosing a higher threshold of r =7 adds too much context information than would be needed specifically for the focal node. Only if the threshold is set to r = 5 is the user’s interest in siblings and child nodes properly reflected. However, finding such an appropriate threshold is notoriously difficult. Therefore, the third and last step is to decide on a propagation method that distributes relevance in the vicinity of high-relevance data elements. The propagation can be performed in different ways. Structural propagation distributesrelevancealongtheedgesinagraph,temporalpropagationconsiders neighboring time intervals, and spatial propagation spreads relevance in a spatial neighborhood. By specifying different types of propagation, the user can trigger the extraction of different context information. Taken together, users can now specify what data should be considered as relevant.Theflexiblecombinationofrelevancecomponentsmakesitpossibleto specify which data aspects should be taken into account and how they should impact the final decision. With propagation, it is possible to steer how high relevance can diffuse into neighboring data. Sofar,thedescribedprocedureprovidesbutanabstractconceptualscheme. Inordertomakeitapplicabletorealanalysisproblems,weneedasuitableuser interface that supports the three aforementioned steps. Such a user interface typically requires a design that is tailored to the data being studied. Next, we will consider this aspect in more depth by the example of an interface for DoI-based visual analysis of dynamic graphs.
Automatic Analysis Support (cid:4) 217 DoI-basedVisualAnalysisofDynamicGraphs Dynamic graphs are typically large and complex, which makes their visual analysis difficult. Consider, for instance, dynamically changing co-author net- works, where nodes represent authors, and edges exist between authors that published together. Node attributes provide additional information about the authors, for example, the number of publications. Edge attributes, such as the number of joint publications, characterize the co-authorships. Figure5.8showsfiveyearsofsuchadynamicco-authornetworkasextracted fromtheDBLPcomputersciencebibliographydatabase.Fromthevisualization, we can very clearly see how computer science has gained in importance as an academic discipline over the years. But we can also see that the network structuredrownsinafloodofnewauthorsandco-authorrelationships.Already fortheyear1990,thevisualizationissosaturatedthatnostructuralinformation can be discerned from it. 1970 1975 1980 1985 1990 Figure5.8 Five years of a dynamic co-author network extracted from DBLP. Courtesy of Steffen Hadlak. Inthefollowing,wewilldemonstratehowtheflexibleDoI-basedmechanism outlined before can help us in analyzing large dynamic graphs [Abe+14]. The graph that we will be dealing with covers 22 years of the DBLP database from 1990 to 2011 with an overall number of 914,492 nodes and 3,802,317 edges. The DoI-based visual analysis starts with the formulation of some interest in this big network. Let’s assume the user is interested in the top authors. Now, what makes an author a top author? Obviously the number of publications of an author plays a role. Moreover, top authors are likely to have many co-authors with whom
218 (cid:4) Interactive Visual Data Analysis Figure5.9 DoI specification via a nested graphical user interface. Reprinted from [Abe+14]. theypublishedtheirmanypaperstogether.So,tworelevancecomponentsneed to be defined: one based on the number of publications, the other based on the number of co-authors, where we would like to prioritize the number of publications a little more. Finally, it must be decided how the relevance values should be propagated. In order to enable users to specify interactively what we described verbally so far, a suitable graphical user interface is needed. Figure 5.9 shows an interface that is based on nested frames. The innermost frames represent our tworelevancecomponents.Theyaredesignedashistograms.Sigmoidfunctions maphighnumbersofpublicationsandco-authorstohighrelevancevalues.The inner components are encompassed in an intermediate frame representing a weighted sum. Sliders at the components facilitate the adjustments of weights to give the number of publications a slightly higher impact. Finally, the outermost frame represents the selected propagation method, in our case a structural propagation with a certain drop-off. Thanks to the tight integration of aggregated visual representations and user controls, the described interface makes it easy to extend and adjust the DoI specification as necessary. The next question is how the DoI approach can help us reduce complexity and support the visual analysis of our large dynamic graph? In Figure 5.10, we can see that the DoI interface is but a part of a larger framework. In addition to the DoI interface (a), there are a panel with graph statistics (b), a list of all authors and their co-author relationsships for manual selection (c), controls to adjust visualization thresholds (d), a time line with the aggregated relevance per year (e), and a node-link representation of the graph (f). The node-link representation visualizes the graph in a DoI-based fashion. Nodes of interest, that is, nodes with a relevance above the user-defined threshold are represented in full detail as individual dots. Subgraphs that are
Automatic Analysis Support (cid:4) 219 b c f a d e Figure 5.10 DoI-based visualization of the 2007 DBLP co-author network. The graphical interface controls (a)–(d) allow users to specify their interest in the data. The visualizations (e) and (f) show the relevant data and their associated relevance values. Reprinted from [Abe+14]. Number of nodes few intermediate many low medium high low medium high low medium high Edge density Figure5.11 Glyph design for representing collapsed subgraphs. Adapted from [Abe+14]. not of interest are collapsed and summarized by glyphs. The glyphs visualize two properties of the collapsed subgraphs: their number of nodes and their edge density. As illustrated in Figure 5.11, the number of nodes is encoded by the angle of the arc sector at the top of a glyph, whereas the edge density is mapped to the radius of the semicircle at the bottom. Throughout the node-link representation, the relevance values are encoded with variations of green, where darker green stands for higher relevance. Glyphs and edges leading to glyphs are shown in gray to indicate that they are less relevant. Comparing the representation of the network of 1990 in Figure 5.8 to the one of 2007 in Figure 5.10 makes immediately clear how the concept of degree of interest helps us to focus the analysis on relevant data. The DoI-based visualization facilitates a quick overview of those graph elements that are of
220 (cid:4) Interactive Visual Data Analysis particular interest to the user. In our case, the top authors are emphasized, while less-relevant data are represented in an aggregated fashion to provide some context. Here, we discussed the specific example of large dynamic graphs. Yet, the generalideaofdegreeofinterestcanbeappliedtootherdataclassesaswell.Its most important characteristic is that we can distinguish between relevant and less-relevant data. This allows for emphasizing those data elements that match the interests of the user, while the others are summarized or even omitted. Inthenextsection,wecontinuewithanotherapproachfornarrowingdown the analysis on relevant data, more precisely, on features. 5.2.2 Feature-basedVisualAnalysis The goal of feature-based visual analysis is to automatically extract features that capture meaningful data characteristics [RPS01]. The visual analysis will then concentrate on the features, rather than on individual data elements. This approach has two key benefits. First, we can reach a higher level of analytic abstraction. And second, the visual representations are clearer and less cluttered, because usually the number of features is much smaller than the number of data elements. The basic procedure of feature-based visual analysis consists of three steps: 1. Feature specification 2. Feature extraction 3. Feature visualization The first step is concerned with the specification of criteria that define interesting features. Based on this specification, features are automatically extracted from the data in the second step. This also involves the tracking of features over time and the detection of events in the evolution of features. The third step is to visualize extracted features and detected events. Next, we will explain these fundamental steps in more detail and illustrate them in the context of visual analysis of time-varying reaction-diffusion systems. FeatureSpecification The first step is the specification of meaningful criteria that characterize the features. What meaningful criteria are depends strongly on the data to be analyzed. For certain types of data, there are tailored feature definitions. For example,inflowvisualization,wherefeature-basedvisualanalysishasitsroots, features describe critical points, vortices, or shock waves [Pos+03]. However, such a priori definitions do not always exist. For simulations of reaction- diffusion systems, for example, domain experts are interested in features that describe 3D regions with high concentrations of certain particles. However, the
Automatic Analysis Support (cid:4) 221 (a)Specification of thresholds. (b)Formal feature definition. Figure5.12 Feature specification with an interactive interface. Courtesy of Christian Eichner. thresholds of what should be considered a particularly high concentration is not known beforehand, but varies depending on the type of particles and the system being studied. In such cases, features can be specified interactively. Forinteractivefeaturespecification,tworequirementsneedtobeaddressed. First,usersmustbeenabledtographicallyinputrelevantdatacharacteristicsas features,andsecond,thegraphicalinputmustmaptoasuitableformalfeature description to enable the later automatic extraction of features [DGH03]. Figure 5.12a shows an interface that supports the interactive specification of features [Eic+14]. The parallel histograms in Figure 5.12a facilitate an informed definition of suitable thresholds. The graphical input performed on the parallel histograms easily translates to a formal definition based on predicatelogic.Thecorrespondingformulasdescribeopenorclosedintervalsof value ranges of interest as combinations of comparison predicates and Boolean logic operators, as illustrated in Figure 5.12b. The formulas can be stored for later reuse and fine tuning. They are also the basis for the automatic feature extraction. FeatureExtraction Once the feature specification is complete, the next step is to extract instances of features (or simply features) from the data. To this end, the automatic feature extraction is carried out for each type of feature and for each time step in the data. This involves spatial and temporal aspects. Spatial Aspects In general, the goal is to determine where features are locatedinthedataspaceandtocharacterizethembytheirposition,size, shape,andorientation.Again,howtheextractionprocessisimplemented depends on the type of features being relevant in the application domain. Dedicatedmethodsexistforthecriticalpoints,vortices,andshockwaves in flows as mentioned earlier [Pos+03].
222 (cid:4) Interactive Visual Data Analysis For our running example of reaction-diffusion systems, the data consist of particles located in a 3D grid of volumetric cells. Each of the cells is testedwhetheritmatchesoneofthespecifiedcriteriaornot.Neighboring cells that do match are merged in order to generate coherent 3D regions representing the features. In a second step, the extracted 3D regions can be abstracted further to 3D ellipsoids [vWal+96]. The ellipsoid axes are oriented according to the eigenvectors of the covariance matrix of the positions of the matching cells, and the length of the axes is determined by the respective eigenvalues. In other words, the ellipsoids appear at locations and stretch out in directions corresponding to the interesting parts of the data. Temporal Aspects Inordertolearnaboutthetemporalevolutionoffeatures, theellipsoidsneedtobetrackedovertime.Thequestioniswhichellipsoid at time t corresponds to which ellipsoid at time t ? In other words, i i−1 which ellipsoid at time t represents an evolved version of an ellipsoid at i t ? i−1 By solving this correspondence problem for all time steps, we obtain paths of features over time. Typically, an ellipsoid simply continues to exist with varying position, volume, and orientation, which results in a linear path. Yet, features may also split and merge, or exit and re-enter the data, which might indicate interesting events in the data’s evolution. The detection of such events is an integral part of the feature extraction step. The result of the event detection is a layered graph in which layers are time steps and nodes represent features. Edges exist when there is a correspondencebetweentwofeaturesofconsecutivetimesteps.Thepaths through the graph describe the evolution of features, where particular connectivity patterns represent different events. For instance, a node with one incoming edge and multiple outgoing edges corresponds to a split event. In summary, the feature extraction produces two results: a set of ellipsoids that characterize features spatially for each time step and an event graph of the temporal evolution of features. FeatureVisualization The visualization of features is at the heart of feature-based visual analysis. Its goal is to communicate the spatial and the temporal aspects of features as captured in the set of ellipsoids and the event graph. The spatial aspects can be visualized by showing the 3D ellipsoids of a particulartimestepascolor-coded,three-bandcontours.Thecontourrendering helps in reducing 3D occlusion. The color-coding allows us to distinguish different types of features. In our case, different colors represent different proteins. The advantage of this encoding becomes clear in Figure 5.13. In
Automatic Analysis Support (cid:4) 223 (a)Volume visualization. (b)Ellipsoid visualization. Figure5.13 Comparison of direct volume visualization of the particle concen- tration of one protein and ellipsoid-based visualization of features representing high concentrations of two different proteins. Courtesy of Christian Eichner. Figure 5.13a, which depicts the concentration of just one protein by direct volume visualization, we can hardly locate and quantify regions with higher proteinconcentrations.Figure5.13bvisualizestheextractedfeaturesinsteadof the raw data. Red and blue ellipsoids mark regions with high concentration for twodifferentproteins.IncomparisontoFigure5.13a,thespatialcharacteristics of regions with high concentrations can be recognized more easily even for two proteins. But how do the features evolve? In order to create an overview of the evolution of features across all time steps, the event graph can be visualized as layerednode-linkdiagram.Figure5.14ashowsanexamplewithfeaturesoftwo different proteins. Time is shown from left to right. Each node represents a featureataparticulartimepoint,andtheedgesshowtheevolutionoffeatures. Symbols indicate interesting events. The size of the nodes corresponds to the extent of the ellipsoids, which makes bigger features easier to spot. Tracing paths of connected nodes allows us to see how features grow, shrink, split, or merge. By comparing sizes and numbers of nodes at a certain time point, it is also possible to estimate whether there are few bigger spots of higher concentration or several smaller ones. For a detailed comparison, one can visualize the ellipsoids of successive time steps, as shown in Figure 5.14b. It is immediately clear how positions and shapes of features change from one time step to the other. However, this type of visual representation only makes sense when the number of time steps and features is small. Otherwise, the display would be too cluttered to gain any insight.
224 (cid:4) Interactive Visual Data Analysis (a)Node-link diagram of the event graph. (b)Twofeaturesattwotimesteps. Figure5.14 Visualizing the temporal evolution of features. Courtesy of Chris- tian Eichner. Tosummarize,thissectionpresentedthebasicstepsoffeature-basedvisual analysis. We learned that specifying, extracting, and visualizing features can help us better understand data in their spatio-temporal frame of reference. In the following section, we will discuss the feature-based approach further in the context of a particularly challenging problem: the analysis of chaotic movement. 5.2.3 AnalyzingFeaturesofChaoticMovement Analyzing chaotic movement is challenging, because interesting patterns are usually covered by loads of irrelevant information. Here, we consider chaotic movementthatisgeneratedbyastochasticsimulationprocess.Suchsimulations are typically controlled by parameters whose influence on the simulation outcome is not clear upfront. More specifically, the data consist of r simulation runs. Each run R = i (P ,M ) : 1 ≤ i ≤ r is characterized by a parameter setting P (the input i i i of the simulation) and by a corresponding movement M (the output). Each i movement M consists of the trajectories T ,...,T of m moving entities. The i 1 m trajectoriesaresampleduniformlyattimestepst ,...,t .Eachtrajectorypoint 1 n stores various pieces of information, such as position, speed, acceleration, or distance to other entities. Inadditiontothespatialandtemporalcontextofthesimulatedmovements, there are also the dependencies on the simulation parameters. There can be thousands of different parameter configurations, each resulting in thousands of
Automatic Analysis Support (cid:4) 225 … R= (P, M) r r r R = (P, M) 2 2 2 R = (P, M) 1 1 1 Figure5.15 Drawing thousands of trajectories of chaotic movements for mul- tiple simulations leads to cluttered and indecipherable visual representations. Courtesy of Martin Röhlig. complex movements with chaotic trajectories. As can be seen in Figure 5.15, indiscriminately drawing all data will certainly lead to extreme visual clutter, which makes it impossible to study individual trajectories or the influence of parameter configurations. Now let’s see how the feature-based approach can help us gain insight into chaotic movement despite its size and complexity. Instead of dealing with individual movement trajectories, we will now work with features of movement. FeatureSpecification The literature on visual analysis of movement data provides various specifi- cations of features that can be interesting to consider [And+13; vLan+14; Lub+15]. Four fundamental categories of features can be distinguished: • Basic features describe aggregated values for all trajectories of the movement such as average speed or average acceleration of the moving entities. • Group features characterize groups of moving entities. Interesting group features are, for example, the number of groups per time step or the ratio of group members and entities not being in any group. • Region features take the spatial distribution of the moving entities into account. For example, it can be interesting to extract regions with high or low density of entities of certain type, and to consider the count, position, and size of these regions. • Advanced features can be derived by further analytical processing of the previous features. As such, they describe features of features.
226 (cid:4) Interactive Visual Data Analysis Figure5.16 From entities (dot marks) to density map (gray-scale image) to regions (colored image). Courtesy of Martin Röhlig. Advanced features may, for example, detect time points with significant changes of the movement behavior. A comprehensive visual analysis typically requires several features from all four categories. Which features concretely will be most helpful in a given case depends on the data and the analysis task. FeatureExtraction The different categories of features listed above require different extraction mechanisms. The extraction of basic features and group features operates on individualtrajectoriesorsetsofthem.Theextractionofregionfeaturesrequires the computation of 2D density maps for each time step of the movement. By quantizing the density maps, regions with different characteristics can be extracted. Figure 5.16 shows a collage for the purpose of illustration. Moving entities and groups are displayed as dot marks and circles, respectively. A density map is visualized as a gray-scale image. Extracted high-density and low-density regions are colored in red and green. Regions that overlap with groups are orange. Blue represents uninteresting parts of the data. Given the variety of movement features and the complexity of chaotic movement data, it is impossible to carry out the feature extraction on the fly. Instead, an extensive pre-processing step computes as many features as
Automatic Analysis Support (cid:4) 227 2D Movement 1D Time series Figure5.17 2D movement reduced to 1D time series of feature values. possible in advance. This enables users to include or exclude certain features on demand during the actual data analysis. As a result of the feature extraction, the data are reduced considerably. In fact, the complex 2D movements with thousands of entities are condensed down to 1D time series of feature values as indicated in Figure 5.17. These time series are to be represented visually for all simulation runs in the final feature visualization step. FeatureVisualization The visualization must depict not only the temporal aspects of features, but alsothespatialcontextofthemovementandthedependenciesontheparameter settings. Because not all information can be squeezed into a single image, an overview+detail design with several linked views makes sense. Overview The overview supports the exploration of movement features and parameter dependencies across all simulation runs. To this end, parameter con- figurations and feature values are shown in a matrix-like fashion as illustrated in the lower left part of Figure 5.18. Parameter values are arranged to the left and the values of a selected feature are shown in the main part of the matrix. A small gap separates parameter values and feature values. The i-th matrix row represents the parameter configuration and the feature time series of the i-th simulation run R . The matrix cells are color-coded, where darker colors i stand for lower values and brighter colors stand for higher values. Note that the visualization shows only one feature at a time, but interactively switching between different features is not a problem thanks to the feature extraction pre-process. Detail The detail views depict temporal and spatial dependencies in more detail, but only for selected parts of the data. As illustrated in the upper part of Figure 5.18, the temporal aspect is communicated by means of a chart view that shows the time series of selected simulation runs. This facilitates comparing the characteristics of the movements under different parametric conditions. The spatial aspect is detailed by showing selected trajectories for a selected time interval in a trajectory view, which is depicted to the right of Figure 5.18.
228 (cid:4) Interactive Visual Data Analysis Chart view R i R j Parameters Selected feature over time R i ... ... R j ... ... Matrix overview Trajectory view Figure5.18 Feature visualization with overview and detail views. The trajectory segments are color-coded, which allows us to see where the movement exhibits certain characteristics. Additionally, the trajectory view can be combined with the computed 2D density map of a selected time step. Until now, we have sketched the basic ingredients of feature-based visual analysis for chaotic movement data. Implemented and applied to real data, the approach creates visual representations as shown in Figure 5.19. The views are linked, and basic interaction mechanisms allow the user to select particular timeseriesordatarangestobeshowninthedetailviews.Next,wetakealook at some of the insights that could be gained with the feature-based approach. c d a b Figure 5.19 Visualization of parameter settings, feature values, and detail information for selected parts of the data. (a) Parameter settings as gray-scale matrix; (b) Feature values over time as color-coded matrix; (c) Chart with selected time series; (d) Trajectory view with selected trajectory segments. Courtesy of Martin Röhlig.
Automatic Analysis Support (cid:4) 229 Figure 5.20 Visualizing the parameter dependency of average group size. Courtesy of Martin Röhlig. ApplicationExample We already mentioned that the data we are dealing with come from stochastic simulations. The concrete application background is in the context of systems biology where researchers study chaotic movements on cell surfaces. In par- ticular, the researchers seek to gain insight into proteins that move on the cell surface, dock to so-called lipid rafts, move along with them for a short period of time, and then leave them to continue moving freely or dock to other lipid rafts. These dynamic interactions between proteins and lipid rafts play an important role in medicine, for example, in cancer-related research. In our case, the simulation of the movement is controlled by 8 parameters. Simulations have been carried out for about 2,000 different combinations of parameter values. In each of these simulations, the chaotic movement of about 1,000 lipid rafts and 5,000 proteins has been calculated stochastically over a period of about 4,000 time steps. As a result, the researchers have obtained 180 GB worth of chaotic movement data to be analyzed. To support different analysis goals for these large data, about 60 features have been computed, including the average speed of all proteins per time step, regions with high and low density of proteins, group characteristics of the lipid rafts, and time periods with relatively constant behavior [Lub+15]. It is beyond the scope of this section to discuss all the features and their visual interpretation in detail. Instead, we want to highlight two findings that could be made by applying the feature-based approach to the described data. Parameter Dependency Based on the visual representation shown in Fig- ure 5.20, the researchers could detect a dependency of the average group size on the parameters that control the size of the lipid rafts, the number of proteins, and the fluidity of the medium. To make the dependency visible, the rows of the visualization have been sorted with respect to the values of these parameters. The feature-part of the visualization then
230 (cid:4) Interactive Visual Data Analysis (a)Density map. (b)Average protein-raft distance. Figure5.21 Visualization of the average distance to free proteins reveals the sweeping effect. Courtesy of Martin Röhlig. clearly shows bands with lower group size (darker greens) and higher group size (lighter greens). Movment Behavior In the visualization shown in Figure 5.21, the researchers could find a behavior called the sweeping effect: Regions surrounding the lipid rafts have particularly low density of proteins. The density map in Figure 5.21a makes this obvious for a selected time point. To confirm that the sweeping effect is permanent throughout the simula- tion, the feature visualization in Figure 5.21b shows the average distance of free proteins to their nearest lipid rafts over time. As can be seen in the chart view and from the mostly bright colors in the overview matrix, the distance is rather large, and for the two selected runs it even steadily increases slightly toward the end of the simulation. These two example findings nicely demonstrate that feature-based visual analysis can be very helpful when studying data that are not only large, but also chaotic. The visualization of selected features allows us to concentrate on the important aspects of the data, while leaving aspects that are irrelevant to the task at hand out of consideration. At this point, we close the section on facilitating the visual analysis by focusingonrelevantdata.Wehavelearnedthatfourcomponentsarenecessary in this context. First, we need a way to formally specify what is relevant. For the degree-of-interest approach introduced at the beginning of this section, relevancy is described with respect to data values. The feature-based approach discussed in this section’s second part targets higher-level data characteristics. Second, an automatic computation step has to determine interesting data elements or extract features from the data. This can involve basic comparison operations, but also more complex tracking and event detection methods.
Automatic Analysis Support (cid:4) 231 The third component is the visualization of only the relevant parts of the data. Thanks to the reduced amount of information to be communicated, the visual representations are clearer and more focused, which makes gaining insight easier or possible at all. Fourth and finally, interaction is required to specify one’s interest in the data or to select features to be visualized. Interaction adds the flexibility that is necessary to adjust the analysis to changing requirements. In fact, it is again the interplay of computation, visualization, and interaction that makes feature-based approaches so powerful. There is one aspect that is critical though: In order to focus on relevant data, users must know the target of their tasks. Unfortunately, this is not always the case, particularly when studying unknown data. Therefore, we need further concepts to support the data analysis via automatic computations as we will discuss in the next section. 5.3 ABSTRACTING DATA We already know that for today’s datasets, the number of data values easily exceeds the number of pixels on the display. This might lead to severe over- plotting where very many data values are mapped onto one and the same pixel. Figure 5.22, for example, shows a time series with more than 1.7 million time points. Given the limited width of the chart, each pixel column must accommodate about 1,000 time steps. From a visual data analysis perspective, the question is how to appropriately determine the values that should be visualized in the available display space. Figure5.22 Visualization of a time series with more than 1.7 million time points, where each black pixel represents about 1,000 data points. Courtesy of Martin Luboschik. 5.3.1 SamplingandAggregation Samplingandaggregationaretwoapproachesthataimatsolvingthisproblem. Thebasicprincipleofsamplingistoselectparticulardatavaluestobedisplayed. Thatis,samplingmethodspickfromtheoriginaldataasubsetofselecteddata values. Aggregation methods, on the other hand, combine and reduce several data values to representative values to be displayed. That is, aggregation methods condense the original data down to a reduced set of aggregated
232 (cid:4) Interactive Visual Data Analysis values. Note that with sampling, original data values are visualized, whereas with aggregation, derived values representing a couple of original values are visualized. Sampling In signal processing, sampling aims at representing a continuous signal by a discrete one. To this end, signals are gathered at certain sample points. The Nyquist Shannon sampling theorem tells us that in order to be able to reconstruct the original signal, the number of sample points must be greater than twice the bandwidth of the continuous signal [II91]. In statistics, sampling is understood differently. Here, sampling aims at defining a subset of individuals that are representative of or characterize an entire statistical population [Loh19]. In visualization, both definitions are applied. Sampling refers to gathering data values of variables at certain sample points as well as to gathering data elements. In general, sampling aims at determining a particular data subset that enables the user to see major characteristics of the original dataset. To this end, different sampling strategies can be applied ranging from simple equidistant sampling of data values to sophisticated methods such as graph sampling [LF06]. The used sampling method determines the quality of the output. With simple equidistant sampling, essential data characteristics might be lost. On the other hand, sophisticated sampling methods can preserve essential data characteristics, but they are computationally expensive and time-consuming. Therefore, a balance must be found between the quality of the produced result and the needed computation time. Aggregation While sampling methods define a subset of data values, aggre- gation methods replace the data values within an interval by an aggregated value describing certain statistical characteristics of that interval: • Minimum: The smallest value in the interval. • Maximum: The largest value in the interval. • Count: The number of values in the interval. • Sum: The sum of all values in the interval. • Average: The arithmetic mean (the sum divided by the count). • Median: The geometric mean (the central value of the sorted interval). • Mode: The most frequent value in the interval. • Count unique: The number of distinct values in the interval. • Standard deviation: The amount of variation in the interval.
Automatic Analysis Support (cid:4) 233 Which aggregation function to use depends on the analysis objective. The demonstrating example in the next section replaces the original data values by maximum and average values. For very large datasets, a simple data abstraction might not be sufficient. In Figure 5.22, the time series was represented by only one sampling value per 1,000timepoints.Suchasmallsubsetofsamplescanhardlyrepresentthewhole datafaithfully.Therefore,itmakessensetoproducemultipledataabstractions at different scales. Next, we will discuss such a multi-scale approach in more depth. 5.3.2 ExploringMulti-scaleDataAbstractions Sampling and aggregation can be done repeatedly to produce multi-scale data abstractions. With each repetition, a new coarser scale is created with a decreased number of data points. The creation of multiple scales comes alongwithtwoimportantbenefits.First,multi-scaledataabstractionsallowus to generate scalable and less cluttered visual representations [EF10]. Second, multi-scale data abstractions facilitate the analysis of different characteristics of the data. In fact, coarser data abstractions communicate global trends, whereas less-abstracted scales support the detection of local trends and details. Hence, switching between the scales enables users to gain different insights. With a larger number of scales, however, it becomes increasingly difficult to inspect all data at all scales. This raises the question of where to drill down from a coarser scale to a finer scale to get to additional information? Because this question is often not easy to answer, it makes sense to guide users to those scales that might yield new findings [Lub+12]. The idea is to exploit data differences between successive scales as an indicator for the availability of additional information at the finer scale. In other words, if two successive scales are quite similar, studying the finer scale might not reveal much new. In contrast, if the two scales exhibit larger differences, it makes sense to drill down and search for local patterns that only take shape at the finer scale. As an example, Figure 5.23 shows two line charts of a time series at two successivescales.Thefinerscaleatthebottomconsistsofelevensamplepoints, whereas the coarser scale at the top has only six points. In the left part, both chartsdifferonlymarginally:Bothscalesshowmonotonicallyincreasingvalues. Thus,wecanconcludethatforthefirstpartofthedata,thefinerscaledoesnot add substantially new behavior and is therefore less worth exploring. However, for the right part of the charts, things are different. The finer scale depicts a considerablelocalminimumthatisnotpresentatthecoarserscale.Identifying and emphasizing such deviating data behavior can in fact guide the user in exploring multi-scale data.
234 (cid:4) Interactive Visual Data Analysis S i S i+1 Figure5.23 One and the same time series at two different scales. GuidingtheUser The simple example of Figure 5.23 illustrates that differences between scales mightbeagoodindicatorwherefurtherinsightscanbefound.Next,wediscuss how such differences can be extracted and visualized along with the abstracted data. Extracting Differences Between Scales The extraction of differences is done for all pairs of successive data scales. For each pair, the following three steps are carried out: map sample points, compute difference measures, and aggregate differences. 1. Map sample points. First,thesamplepointsatthetwosuccessivescales must be unified. To this end, sample points that exist only at one scale are mapped to the respective other scale. This typically involves the computation of new data values, for example, by linear interpolation. The example in Figure 5.24 illustrates how every second sample point of the finer scale is mapped to a new sample point with an interpolated data value at the coarser scale. In the end, both scales have the same sample points, but different data values. 2. Compute difference measures. Computing the differences between the two scales is the second step. Various measures can be used to quantify differences. Figure 5.25 illustrates two examples, a point-wise measure and a segment-wise one. A simple point-wise measure is to compute the absolute value difference (AVD) per sample point, for example, by means of the Euclidean distance. A simple segment-wise measure is the slope sign difference (SSD). It captures whether the signs of the slopes of two segments differ. In Figure 5.25, the signs are indicated per segment as − and +. If the signs differ, the slope sign difference is 1, otherwise it is 0. While being computationally inexpensive, these simple measures capture just enough information to be useful as indicators.
Automatic Analysis Support (cid:4) 235 Original sample point Mapped sample point S i S i+1 Figure5.24 Unifying the sample points of two successive scales by mapping and interpolation. + + ++ – – + + S Si + + i+1 + + + – + + + – + + Absolute Value Difference (AVD) 0 3 0 1 0 1 0 18 0 6 0 Slope Sign Difference (SSD) 0 0 0 0 0 0 0 1 1 0 Figure5.25 Computing the absolute value difference (AVD) and the slope sign difference (SSD) between two successive data scales. 3. Aggregate differences. Because our goal is to guide users to regions worthstudyingindetail,weneedtoaggregatethepoint-wiseorsegment- wisedifferencevaluesoverlargerintervals.Figure5.26showsamaximum aggregation of AVD and an average aggregation of SSD as examples. A maximumaggregationensuresthatlargerdifferencesarenotcompensated by smaller ones within the same interval. If only the existence or absence of differences between scales is captured, the average can indicate how much additional information will appear when zooming into a certain range. In our example, both aggregations suggest drilling down primarily into the interval where the local minimum would be revealed. The outlined basic approach is freely adaptable. Different methods can be appliedformappingthesamplepointsandderivingthemissingdatavalues,for measuring the differences between scales, and for aggregating the differences. What remains to be done is to visualize the aggregated differences along with the actual data.
236 (cid:4) Interactive Visual Data Analysis Absolute Value 0 3 0 1 0 1 0 18 0 6 0 Difference (AVD) Maximum 3 1 18 6 Aggregation Slope Sign 0 0 0 0 0 0 0 1 1 0 Difference (SSD) Average 0 0 0.66 0 Aggregation Figure5.26 Aggregation of data differences with maximum aggregation for the absolute value difference (AVD) and average aggregation for the slope sign difference (SSD) function. VisualizingAggregatedScaleDifferences Toguideusersinexploringmulti- scale data, the aggregated differences are visualized along with the data. A schematic illustration of the approach is given in Figure 5.27. The main time series plot is accompanied by a stack of so-called difference-bands. Each band is further sub-divided into color-coded cells representing the aggregated differences between the adjacent data scales. There are two modes for the color-coding: global and local. For global color-coding, colors are mapped between the global minimum and maximum value of all cells. This facilitates comparisonacrossscales.Forlocalcolor-coding,colorsaremappedwithrespect to the minimum and maximum value per band. This makes it easier to see smaller differences per individual scale. In both modes, brighter cells indicate regions with no or only small differences between scales. Darker cells represent regions where the finer scale is substantially different from the coarser scale, and hence, might be interesting to inspect in more detail. Additionally, an over-plotting indicator shows, depending on the current display resolution, at which scale over-plotting will occur. It is reasonable to start the data analysis with the finest scale that is still free from over-plotting. Zoomable Time Series Plot Multi-scale Difference Over-plotting Bands Indicator Figure5.27 Visualizing aggregated differences along with the actual data.
Automatic Analysis Support (cid:4) 237 From there, the user can zoom and pan the time series plot to reach different scales and different regions as suggested by the color-coded cells. Next, we will illustrate how the described approach can support the visual analysis of a large time series from systems biology. AUseCasefromSystemsBiology Our example is based on time-series data that result from a simulation of the cell division cycle in fission yeast [Lub+12]. For our purpose, it is sufficient to know that the simulated model consists of two different proteins that control the major events of the cell division. The simulation produces 3.6 million data points that describe the dynamic change of molecule quantities over time. By repeated average aggregation, the original data are transformed into a multi-scale data abstraction with 21 levels. Differences between the scales are computed by the measures introduced before: slope sign difference (SSD) with average aggregation and absolute value difference (AVD) with maximum aggregation. Figure5.28 Time series plot of the simulation outcome and corresponding multi-scale difference bands of SSD and AVD with local color mode. Courtesy of Martin Luboschik. The outcome of the simulation is visualized in Figure 5.28. The time series shows considerable oscillations. To get more insights, we check the multi-scale difference bands of SSD (top in blue-yellow) and AVD (bottom in green- yellow). Particularly, at the finer scales that would produce over-plotting, as signaled in red color in the over-plotting indicator, we can observe something interesting. For the SSD measure, we can see for each peak in the time series a corresponding brighter notch in the otherwise rather bluish difference bands. Within each notch, there seems to be a very thin blue upward spike, which wouldsuggestthatsomeadditionalinformationcanbefound.Thisisconfirmed
238 (cid:4) Interactive Visual Data Analysis Zooming on tip of middle peak Fish-eye magnification Figure5.29 Studying the details of the middle peak-notch-spike pattern from Figure 5.28. Courtesy of Martin Luboschik. by the AVD measure, where green downward spikes are clearly visible. This prompts us to look into these parts of the data at a finer scale. We decide to study the middle peak-notch-spike pattern in more detail. In Figure 5.29, we perform two separate drill-down operations. First, we enlarge the multi-scale difference bands where the middle notch is located by applying a horizontal fish-eye magnification (a 1D variant of the fish-eye lens introduced in Section 4.6.3 of Chapter 4). We can see, yes, there are indeed some significant differences in this region as indicated by the darker, more saturated colors. Second, we switch the time series plot to the finest data scale and zoom far in on the tip of the peak. This reveals the source of the differences: There are considerable fluctuations at the tip. These are caused by the two proteins competing with each other in the simulated process. We can conclude that there is no clear tipping point as the coarser scales might have suggested. This is an interesting finding that would likely have gone unnoticed without the guidance offered by the multi-scale difference bands. In summary, data abstraction by means of sampling and aggregation is an important approach to dealing with very large data. However, representing millions of data points by a comparatively small number of samples or aggre- gated values does not adequately communicate the complete data behavior. Therefore, multi-scale data abstractions are required. They facilitate the anal- ysis of data behavior at different scales and help us generate scalable visual representations. Computing and visualizing differences between subsequent scales helps users identify those scales and ranges of the data that should be investigated in detail.
Automatic Analysis Support (cid:4) 239 Whilesamplingandaggregationarenotveryselectivewithrespecttowhich data elements are abstracted, we will next look into methods that particularly focus on data elements that are similar. 5.4 GROUPING SIMILAR DATA ELEMENTS With sampling and aggregation, data values within a certain data interval are replaced by a single representative value. Now, we consider the grouping of similar data elements. Classification and clustering are typical approaches for grouping data elements. While the goal of grouping is the same for both approaches, they achieve the goal in different ways. Classification subdivides the data space, whereas clustering partitions the set of data elements. A class describes a subspace of the data space, whereas a cluster represents a subset of similar data elements. Classes are characterized by value ranges, whereas clusters are characterized by the properties of their affiliated data elements. Representing a dataset by classes or clusters significantly reduces the data complexity and facilitates the generation of overviews. In the following, we will discuss classification and data clustering in more detail. 5.4.1 Classification Classification subdivides the data space. Each subspace defines a particular class. Data elements falling into the same subspace share the same data characteristics and are said to be in the same class. The key to classification is a suitable description of the classes. BasicMeansofSpecifyingClasses There are different strategies to define the classes. A very basic approach is to divide the domain of the data variables into intervals. For example, the age of persons in a social network could be grouped into four classes: kids (age≤12), youths (12 < age ≤ 18), adults (18 < age ≤ 60), and seniors (60 < age). This reduces the range of age values, usually between 0 and 110, to only four class values. Another example is to group the wind direction into eight major directions:N,NE,E,SE,S,SW,W,andNW.Here,thepotentiallycontinuous angle value between 0° and 360° is reduced to eight class values. A common approach to classifying multivariate data elements is to use decision trees. Decision trees partition the data space based on a series of tests. For quantitative or ordinal data variables, tests are usually comparisons against given thresholds. Categorical data can be partitioned by means of yes/no questions. The specified tests are the basic building blocks of decision trees. A test defines a node in a decision tree. For each class distinguished by a test, there is an edge attached to the test’s node. The edges connect a test to follow-up nodes in the decision tree. One test is chosen to be the root node of the decision tree. The leaves of the decision tree are the produced classes. A
240 (cid:4) Interactive Visual Data Analysis Tests Small Sales High Volume Classes Average Decreasing Sales Increasing Decreasing Sales Increasing Decreasing Sales Increasing Evolution Evolution Evolution Stable Stable Stable Small still Small and Small but Average Average Average High but High and High still decreasing stable increasing decreasing stable increasing decreasing stable increasing Figure5.30 A decision tree classifying enterprises according to their sales. data element is assigned to a class, if the data element matches all the tests along the path from the root node of the decision tree to the class leaf node. Visualizing decision trees can help users understand how the test criteria connect to a hierarchy of decisions that result in the partitioning of the data space. For this purpose, common tree visualization methods can be applied, for example, node-link diagrams as introduced in Section 3.5 of Chapter 3. A simple decision tree for a dataset with enterprise sales volumes is depicted in Figure 5.30. The first test in the root node refers to sales volumes. It distinguishesenterpriseswithsmall,average,andhighsales.Thenexthierarchy level considers the evolution of sales to further distinguish enterprises with increasing, decreasing, or stable sales. In the end, we obtain a grouping of the enterprises into nine classes: small still decreasing, small and stable, small but increasing, and so forth. Each class represents enterprises that share the same characteristics. Note that in other scenarios, decision trees are not necessarily leveled and balanced as in our example. Moreover, the structure of how tests lead to a decision for one class or the other can be significantly more complex. In fact, the specification of adequate test criteria for complex domain problems can be quite challenging. There are even scenarios where it is not possible at all to formulate the classification as a decision tree. In such cases, sophisticated algorithms and careful parameter tuning are required in order to arrive at appropriate classification results. Next, we will discuss such a sophisticated classification approach and illustrate how visual analysis can help to tune and evaluate the classification outcome. VisualClassificationSupport:AnExamplefromActivityRecognition Therecognitionofactivitiesisarelevanttaskinvariousdomains.Forexample, assistance systems observe people’s activities in order to automatically offer on-demand help [Tei+17]. To this end, people are instrumented with several sensorsthatproducelargevolumesoftime-seriesdata.Thetaskofclassification in this context is to deduce from the multivariate sensor data the activity that
Automatic Analysis Support (cid:4) 241 Ground Truth Data Activities A B C D Sensor Data Parameters Labeled Segments Activity Recognition ... ... Algorithm Figure5.31 Illustration of activity recognition based on parameter-dependent algorithms that learn from some ground truth. Adapted from [Röh+15]. a person is or was doing. A very simple example would be to label certain intervals in time as walking activities. Yet, an activity can also be to cook a meal, which is certainly not so easy to discriminate. In general, recognizing activities requires sophisticated data classifiers. A typical way how such classifiers work is to learn from human-labeled ground truth data. The ground truth is collected in observational studies where a person’s activities are video recorded in addition to being monitored by the sensors. Based on the video, a human can easily label each characteristic sequencesoftheobservationasacertainactivity,forexample,sitting,standing, orwalking.Theassumptionisthatthesensordataassociatedwiththemanually labeled activities are discriminatory enough so that a classifier can learn from the sensor data how to recognize activities robustly. Yet, as mentioned before, the algorithms involved in activity recognition typically depend on careful parameter tuning. To obtain accurate results, the algorithms need to be tested with different parameter configurations. The goal is to find configurations that produce classifications that match the ground truth best. Figure 5.31 illustrates the described situation. Next, we explain in more detail how visual analysis methods can help users understand the influence of parameters on the classification results. The better understanding makes it easier to adjust and fine-tune the parameters to achieve a better classification quality. Following the previous schematic illustration, Figure 5.32 shows a concrete visualizationoftheclassifiedsensordatainconcertwiththeunderlyingparam- eter settings and the ground truth [Röh+15]. The visual design is similar to the matrix-based visualization for feature visualization as described back in Section 5.2.3. The left part of the visualization (a) shows the parameter con- figurations. Each column corresponds to a parameter, and each row represents the values of a parameter configuration. The main part of the visualization (b) shows the activity-labeled time series as colored pixel rows from left to
242 (cid:4) Interactive Visual Data Analysis d a b c f e Figure5.32 Visualization of parameter-dependent classification outcomes for activityrecognition.(a)Parameterconfigurations;(b)Recognizedactivities;(c) Colorlegend;(d)Groundtruth;(e)and(f)Stackedhistogramswithaggregated information. Courtesy of Martin Röhlig. right. For each parameter configuration, there is a corresponding pixel row in the main part, and each activity is painted with a distinct color (c). This visualization provides a nice overview of the classification outcome for all different parameter configurations. Moreover, we can easily see which activity has been performed at which time step. Figure 5.32 also shows the ground truth as a separate colored band at the top (d). This facilitates the visual comparison of the recognized activities and the actually performed activities. Additionally, stacked histograms (e) and (f) are attached at the bottom and to the right. They show the distribution of detected activities per time step and per parameter configuration, respectively. This gives us a sense of the stability of the activity recognition, both over time (e) and over the different parameter configurations (f). For evaluating the activity recognition in more detail, it makes sense to visually analyze the discrepancy between the detected activities and the groundtruth.Tothisend,thecolorcodingandtheorderingofrowsischanged in Figure 5.33. Instead of showing distinct colors per data class, the visual representation now shows red color where the recognized activity does not correspond to the ground truth. We can now see that the number of incorrect classifications increases over time for the majority of parameter setting and that the overall quality of the activity recognition depends mostly on the parameter sensor. Whilevisualanalysiscannotimprovetheclassificationoutcome,itnonethe- less helps users to understand why the activity recognition behaves the way it does. The insights generated with the visualization can be utilized to estimate the influence of parameters and to fine-tune parameter configurations accord- ingly. Parameters with little impact can be paid less attention. For parameters
Automatic Analysis Support (cid:4) 243 Figure5.33 Highlighting the incorrectly classified time steps in red. Courtesy of Martin Röhlig. with substantial impact, the visualization can indicate parameter ranges that are good candidates for further refinement in search of better classification results. Insummary,thissectiontaughtusthatclassificationisaboutgroupingdata by dividing the data space into subspaces with specific data characteristics. Classificationcanbeaseasyastestingspecifiedcriteria,possiblyorganizedina hierarchicalschema.Yet,classificationcanalsobesocomplexthatsophisticated algorithms need to be involved. In both cases, visualization, either of the classification schema as for the decision trees or of the parameter-dependent classification outcome as for the activity recognition example, can help users understand how the data are grouped into classes. Next, we study clustering as another common approach to group similar data elements. 5.4.2 DataClustering The goal of clustering is to group similar data elements into clusters. Data elements in the same cluster are similar with respect to a specified similarity measure.Theelementsofdifferentclustersshouldbedissimilar.Clusteringcan support the visual analysis in different ways. Data elements can be arranged or colored according to their cluster affiliation. Overview representations can concentrateontheclusters,ratherthanonindividualdataelements.Clustering can also help us study the data at different levels of abstraction. All this can reduce the analysis complexity significantly. ConfiguringtheClustering Dataclusteringcanbedoneinvariousways.Therefore,itisfirstofallnecessary toconfiguretheclustering,thatis,tospecifywhattoclusterandhowtocluster.
244 (cid:4) Interactive Visual Data Analysis Letusnextbrieflydiscussthedecisionsthatneedtobemadebeforetheactual clustering can be performed. What to cluster? First, we must decide which data elements should be takenintoaccount.Redundantdataelementsanddataelementsthatrepresent outliers can distort the grouping, and thus they are usually excluded from clustering. Second,inthecaseofmultivariatedata,wemustdecidewhichdatavariables should be taken into account. Should we consider all variables or only selected ones?Wemustalsodecidewhetherthevariablesshouldbeweightedequallyor whether some of them should be given higher or lower weight. Variables that do not contribute to the main topic of interest could be ignored or assigned a lower weight. Highly relevant variables should get higher weights to give them more influence on the clustering outcome. Forexample,consideradatasetthatcontainsinformationaboutcompanies, their name and field of business, chief executive officer, sales, revenue, balance sheet total, and number of employees. If the analysis objective is to cluster highlyprofitableandbigcompanies,theattributerevenueshouldbeprioritized, the attributes sales, balance sheet total, and number of employees should be included, and the attributes name, field of business, and chief executive officer can be neglected. How to cluster? Third, we have to decide on the similarity measure to be used to determine whether data elements are similar. A widely applied measure for quantitative data is the Euclidean distance, which describes the straight-line distance between two data elements. The Manhattan distance defines the distance between two data elements as the sum of the absolute difference of their data values. Further measures for quantitative data are the cosine distance or correlation coefficients such as the Pearson correlation. For qualitative data, other distance measures must be applied, for example the Hamming distance, the Levenshtein distance, or the Jaccard index. Continuing our previous example, for clustering companies according to profits and size, the Euclidean distance would be an appropriate measure. Yet, things are different when we change our analysis objective. Let’s now assume that the business data are given on a yearly basis. If our analysis objective is to study companies with similar development over time, a correlation-based measurewouldbeappropriate.Thisway,theclusteringwouldgroupcompanies that have similar development histories. Finally, we must decide on the clustering strategy to apply. The clustering strategy defines how the data elements being similar according to the chosen similarity measure are grouped into clusters. There are four basic clustering strategies:partitioningclustering,hierarchicalclustering,grid-basedclustering, anddensity-basedclustering[HKP11].Eachclusteringstrategytakesadifferent approachandcomeswithitsownsetofparameters,aswewillseeinamoment.
Automatic Analysis Support (cid:4) 245 Taken together, excluding data elements from clustering, weighting vari- ables, using different similarity measures, applying different clustering strate- gies, and choosing different parameters, all these decisions must be carefully weighed. It is important to realize that slightly different decisions can produce totally different clustering results. Which configuration to employ depends on the data and the objective of the analysis. ComputingtheClusters In the following, we will explain how clusters of similar data elements can actually be computed. To this end, we will briefly describe the four different clustering strategies just mentioned. Partitioning clustering algorithms generate k < n groups from n data elements by partitioning the data space. The most popular method is the k-means algorithm. It describes each cluster by its centroid. Initially, the k centroids are distributed randomly in the data space. Then, the data elements are assigned to their closest centroid. The positions of the centroids are then updated to the average position of the data elements within a cluster. After updating the centroid positions, it can happen that for some data elements, their assigned cluster centroid is no longer the closest one. This requires a reassignment of the data elements to the cluster that is now the closest. These changes again imply re-calculations of the centroid positions. This iterative clustering process stops when the centroid positions have settled at sufficiently stable positions. The k-means algorithm is popular for its simplicity and general appli- cability. However, a difficulty is that the number of clusters must be specified in advance, which can be problematic for unknown data. Hierarchical clustering algorithms organize nested clusters in a hierarchy. The similarity of data elements within the clusters increases as the hierarchy is descended. The root node of the hierarchy represents the cluster that contains all data elements. The leaves of the hierarchy represent clusters that contain only a single data element. A horizontal cut through the hierarchy describes a particular level of abstraction of the clustered data. Defining an appropriate cut is the difficultyofhierarchicalclustering.Here,visualizingtheclusterhierarchy as a so-called dendrogram can help users find a good level of abstraction. Cutting the hierarchy near the root produces an overview of the data. Cutting closer to the leaves delivers more details. Hierarchical clustering can be implemented in two ways. Agglomerative algorithms recursively group the two most similar data elements in a bottom-up fashion. A popular example is Ward’s method. In contrast, divisive algorithms recursively split the data into two dissimilar subsets in a top-down fashion until each data element forms its own cluster.
246 (cid:4) Interactive Visual Data Analysis Grid-based clustering algorithms subdivide the data domain by means of a regular grid. Each data element falls into a particular cell of the grid, and for each grid cell statistical meta-data, such as minimum, maximum, or average, are computed. Based on a data query, clusters are formed by connecting those grid cells that contain a sufficient number of data elements that match the query. Obviously, the grid resolution is a critical parameter in terms of the clustering quality. STING (statistical information grid)isan example thatusesahierarchical structure ofgrid cells. Such multi-resolution grids make it possible to formulate cluster queries against very large databases. Density-based clustering algorithms distinguish between regions with higher and lower density, and define clusters based on this distinction. DBSCAN (density-based spatial clustering of applications with noise) is a prominent example of density-based clustering. DBSCAN groups data elements based on two criteria. The data elements must be close to each other (distance) and there must be sufficiently many data elements around the same spot (density). Data elements that lie in sparse regions are considered noise or outliers. Being robust against outliers is a partic- ular advantage of this method. On the other hand, DBSCAN requires careful tuning of the distance and density criteria in order to be able to detect clusters. It works less well with evenly distributed data. Figure 5.34 illustrates the four clustering strategies applied to an artificial bivariate dataset. The data (a) contain two well-separated subsets and also two major outliers and two minor outliers. It is interesting to see how the different clustering algorithms cope with these specific data characteristics. For the k-means clustering, setting k =2 seems to be appropriate, since we can see that there are two distinguishable subsets. Similarly, cutting the dendrogram(f)neartheroottoobtaintwoclustersforWard’smethodseemsto makesense.However,thetwoclustersextractedin(b)and(c)donotcorrespond tothetwoobvioussubsetsinthedata.Beingapartitioningapproach,k-means generates convex clusters. Moreover, the major outliers distort the clustering of k-means and Ward’s method. Incontrast,STING(d)andDBSCAN(e)producetwoclustersrepresenting the two obvious subsets quite well. Yet, STING also generates clusters for each of the four outliers. DBSCAN treats the outliers differently. The major outliersarerecognizedassuch.Theminoroutliers,however,areassignedtothe two clusters. In order to keep them as outliers too, the distance parameter of DBSCAN could be decreased. However, only carefully so, because the clusters could fall apart if the distance parameter is too low. In light of these examples, it is clear that there is no silver bullet strategy to clustering. Instead, it is necessary to configure the clustering appropriately to obtain suitable results. However, it is often unclear how the clustering result should look like, especially when analyzing unknown data. Similarly, how different configurations affect the result is often unclear as well. Again,
Automatic Analysis Support (cid:4) 247 (a)Input data. (b)K-means. (c)Ward’s method. (d)STING. (e)DBSCAN. (f)Dendrogram. Figure5.34 Illustration of clustering strategies. Adapted from [Gla14] with permission of Sylvia Saalfeld. interactive visual methods can help us perform the clustering appropriately, check the computed clusters, and, if necessary, to specify a new configuration that delivers more plausible results. StratomeX is a tool that supports the direct visual comparison of cluster- ing results generated with different algorithms and configurations [Lex+12]. Figure 5.35 shows the interface of StratomeX. The columns represent the same microRNA data, but differently clustered by three algorithms. Bands between the columns connect the same data elements. We can nicely see that data elements being together in a group for one clustering are distributed across several groups for another clustering. For example, the data elements of the highlighted first group of the k-means clustering (middle column) disperse into three different groups for the hierarchical clustering (left column). On the other hand, a large portion of the k-means group also forms a large portion of an affinity propagation cluster. Based on these observations, the user can inspect the data elements in the marked clusters in more detail in order to judge whether the produced clusters are in fact adequate groupings. Finally,itisworthmentioningthatinadditiontotheillustratedfundamen- tal clustering strategies, there are further ways of clustering data. For example,
248 (cid:4) Interactive Visual Data Analysis (a)Hierarchical clustering. (b)K-means clustering. (c)Affinitypropagation. Figure 5.35 Comparison of clusters generated with hierarchical clustering, k-means, and affinity propagation. Software courtesy of Alexander Lex. distribution-based clustering groups data elements according to statistical distribution models. Spectral clustering works on the spectrum of eigenvalues of the similarity matrix of the data. Clustering can also be done with neural models, such as self-organizing maps, as we will see next. HybridSOM-basedClustering Self-organizing maps (SOM) are a form of artificial neural networks. SOMs can be applied to all kinds of data analysis problems. Here, we utilize them for grouping similar data elements. SOMs are suited for finding groups even in large and unstructured data. Basic SOM Clustering A SOM basically corresponds to a regular grid of neurons, each being represented as a so-called reference vector. The number of components per reference vector, that is, the dimensionality, corresponds to the number of data values per data element. SOMs operate in two steps: training and mapping. In the training phase, the grid of neurons is trained based on so-called input vectors, which in our case is a subset of the data elements to be clustered. In the mapping phase, the remaining data elements are processed.
Automatic Analysis Support (cid:4) 249 In the training phase, the following four steps are performed to align the reference vectors with the input vectors (the training data): 1. Initialization Thereferencevectorsofallneuronsinthegridareinitialized with random values. 2. Competition For each input vector, all neurons in the grid compete for the assignment. The neuron with the most similar reference vector is the winner neuron. The corresponding reference vector is updated such that it more closely represents the input vector. 3. Cooperation The winner neuron stimulates neurons in its local neighbor- hood. The stimulus impact decreases with the training duration. 4. Adaptation The reference vectors associated with the stimulated neurons are adjusted with regard to the input vector. The degree of adaptation also decreases with the training duration. After the training phase, the grid’s reference vectors reflect a similarity- based arrangement of the input vectors. Adjacent grid cells with similar reference vectors represent groups of similar data elements. Then, in the mapping phase, the remaining data elements, that is, those data elements that were not used in the training phase, can be assigned to the closest matching neuron. With the outlined procedure, SOMs can produce useful groupings of data elements. Yet, it is not easy to determine the appropriate number of neurons (or reference vectors) required to generate results that are balanced in terms of clustering quality and computation time. The proper number of neurons is not known in advance, as it depends heavily on the given data. With too few neurons, the clustering quality will be low. With too many neurons, the quality might no longer improve, but the computation time will be higher, which might impair the interactive visual data analysis. Hybrid Clustering with SOM To keep computational costs and cluster quality balanced, it makes sense to implement a hybrid clustering procedure that combines SOM clustering with a second clustering algorithm [JTS08]. Such a hybrid clustering involves two steps. First, a coarse SOM training generates a moderate number of SOM clusters. Then, a second clustering algorithm, such as hierarchical clustering, further refines each SOM cluster. Figure 5.36 illustrates how such a hybrid clustering can be employed to sort the rows of a table lens visualization, a tabular data representation that we introduced in Section 3.2.1 of Chapter 3. The goal is to order the rows of the table such that similar data tuples are arranged closely together. To this end, a SOM delivers a rough grouping of rows in the first step. In the secondstep,hierarchicalclusteringcreatesaclusterhierarchyperSOMcluster. Figure 5.36b shows the ordered table lens after the hybrid clustering has been
250 (cid:4) Interactive Visual Data Analysis applied. While the original arrangement of rows in Figure 5.36a exhibits many color switches, the hybrid clustering leads to fewer color switches across rows. Additionally, the right-most column in the table in Figure 5.36b shows the cluster hierarchies as icicle plots. This visual representation can be utilized to adjust the visualized level of abstraction by expanding and collapse clusters. (a)Unordered rows before clustering. (b)Ordered rows after clustering. Figure5.36 Applying hybrid SOM-based clustering to sort rows in a table lens visualization. Thanks to the hybrid nature of the procedure, the clustering is computa- tionally efficient. Since we need only a small number of neurons for the initial rough SOM grouping, computation time is reduced. The same applies to the hierarchicalclusteringpart.Becausewedonotcomputeaglobalhierarchy,but refine each SOM cluster separately, the individual runtime is reduced. Over- all, combining clustering methods to a hybrid approach can help us balance clustering quality and computational costs. The clustering strategies described so far are generally applicable to multi- variate data. Yet, when we go beyond multivariate data, it is often necessary to employ advanced clustering strategies that are dedicated to the peculiarities of a specific class of data. The next section will introduce such an advanced strategy for the particular case of clustering multivariate dynamic graphs. 5.4.3 ClusteringMultivariateDynamicGraphs Clustering graphs is a complex problem [Sch07]. The goal is to group the nodes of a graph into clusters based on some notion of similarity. The clusters can then be represented by so-called meta-nodes, which basically subsume all nodesbelongingtoacluster.Visualizingthemeta-nodesfacilitatesanoverview on key characteristics of a graph. By applying the clustering repeatedly on nodes and meta-nodes, a clustering hierarchy can be generated to facilitate multi-scale data analyses, which is particularly useful for large and complex graph structures.
Automatic Analysis Support (cid:4) 251 Inthefollowing,wewillstudywaystoclustermultivariatedynamicgraphs, that is, graphs whose nodes have data attributes and also references to time. A multivariate dynamic graph DG can be denoted as a sequence of graphs DG = (G ,G ,...,G ), where G = (V ,E ) is a graph with multivariate 1 2 n i i i nodes V and edges E at time t ∈T. i i i There are two basic clustering strategies taking into account the time- varying character of multivariate dynamic graphs [Had14]. Attribute-driven methods group nodes according to the temporal evolution of their attribute values. In contrast, structure-driven methods cluster a graph based on its structure as defined by the time-depended existence of nodes and edges. Attribute-basedClustering Attribute-based clustering operates on the super-graph SG = S G , which i subsumes all graphs G ∈DG. The goal is to extract from SG clusters whose i attributes share similar temporal behavior, for example, stable, increasing, decreasing, or recurring values. This can be achieved in three steps: preprocess, configure, and cluster. 1. Preprocess First, the attribute values associated with the graph elements mustbetransformedintotimeseriesthatdescribethetemporalbehavior. To this end, the attribute values are concatenated according to the sequence as defined by DG. However, the resulting time series might contain missing values, because nodes and edges do not necessarily exist at all points in time. These missing values are unambiguously marked with a dedicated null value ω. Additionally, minor fluctuations in the time series should be filtered out in order to reduce the influence of noise on the clustering results. In the end, we obtain for each graph element and for each attribute a complete and smoothed time series with exactly one value (actual data or ω) per time point. 2. Configure The second step is to configure the clustering, which in the first place means deciding which attributes should be taken into account. Moreover, the role of the ω values must decided. Including or excluding ω leads to different clustering outcomes. If the ω values are included, theydominatetheclusteringofthosenodesthatexistonlyforafewtime points. Such rare nodes will then appear in a distinct cluster, which can be intended. On the other hand, excluding the ω values leads to clusters that contain nodes with similar temporal behavior, which is what we typically want. The configuration also involves a decision on the similarity measure and the clustering method to be used. Typically hierarchical clustering is applied. However, other methods can be used as well, in particular methods that are tailored to clustering time series [Lia05].
252 (cid:4) Interactive Visual Data Analysis (a)Initial grouping. (b)Refined clusters. Figure5.37 Two-step procedure of clustering nodes based on their attributes. First, nodes with similar attribute behavior are grouped. Second, groups are refined based on connected components. Adapted from [Had14]. 3. Cluster Finally, the super-graph SG is actually clustered using a two-step procedure. It is illustrated in Figure 5.37 with a node-link visualization whose 13 nodes incorporate miniature charts of their attribute values over time. The first step is to group nodes with similar time series as in Figure 5.37a. Our example shows three preliminary groups (in red, green, and blue) and the resulting abstracted graph at the bottom with three meta-nodes. However, the green and blue groups subsume nodes that are actually not directly connected in the super-graph SG. This is not yet the desired result. Therefore, the structure-preserving second step refines the groups with respect to their intra-group connected components. Figure 5.37b visual- izes the resulting clusters by varying edge colors. The red group has only one connected component. Yet, the green group and the blue group each consist of two connected components, indicated by brighter and darker shades of green and blue, respectively. As the final result, we obtain an abstracted graph that better captures the original connectivity and is still considerably reduced to five meta-nodes. The extracted attribute-based clusters can now be used to visualize an original multivariate dynamic graph in an abstracted fashion. To this end, the visualization displays the meta-nodes and the corresponding meta-edges and only traces of the original data. Figure 5.38 shows such an abstracted representation. The meta-nodes are represented as small boxes depicting the timeseriesthatisrepresentativeofthecorrespondingcluster.Alabelshowsthe
Automatic Analysis Support (cid:4) 253 Figure5.38 Visualizationofanattribute-basedclusteredmultivariatedynamic graph. Reprinted from [Had+13]. number of original nodes subsumed in a meta-node. The box colors encode the intra-clustersimilarityoftheunderlyingoriginaltimeseries,withdarkercolors symbolizing larger similarity. The meta-edges visualize connections between the clusters. The edge width encodes the number of original edges subsumed in a meta-edge. Overall, this abstracted visual representation can help us gain an overview of the key temporal behaviors and structural relationships of multivariate dynamic graphs. Structure-basedClusteringofDynamicGraphs Structure-based clustering aims to extract key characteristics of the structural changes that occur in a dynamic graph over time. In contrast to the previous section, we now group graphs G ∈ DG with similar structure, rather than i nodeswithsimilarattributebehavior.Inotherwords,ourclustersnowcontain anumberofsimilargraphs.Assuch,aclustercanbeunderstoodasaparticular state thatthedynamicgraphassumesatcertainperiodsintime.Consequently, thechangesinthestructureofadynamicgraphcanbeunderstoodastransitions between states. Following this thinking, the idea of structure-based clustering is to construct a state-transition graph that can help us understand how the structural aspect of a dynamic graph varies temporally. Computing the state-transition graph involves two steps: the initial setup of states and transitions, and the subsequent hierarchical grouping of similar states to meta-states and the corresponding definition of meta-transitions. The overall procedure is illustrated in Figure 5.39 and will be explained in more detail next.
254 (cid:4) Interactive Visual Data Analysis a) 1 2 3 4 5 6 7 8 b) 1 6 5 7 2 8 3 4 1, 6 5, 7 2, 8 3, 4 1, 6, 5, 7 2, 8, 3, 4 State Transition State grouping 1-8 Retained structure Changed structure Figure5.39 Structure-basedclustering.(a)Initialsetofstatesandtransitions based on the sequence of graphs G ∈DG; (b) Hierarchical grouping of states i and transitions based on similar structures. Adapted from [Had14]. 1. Initial setup. Initially, each original graph G ∈DG is represented by a i separate state. Any two states accommodating successive graphs G and i G are connected by transitions. The created states and transitions as i+1 shown in Figure 5.39a are the basis for the subsequent grouping. 2. Hierarchical grouping. For the actual grouping of states, their pairwise similarity needs to be calculated. This can be done in different ways, for example, by means of the graph edit distance, which basically counts the number of inserted, deleted, and substituted nodes and edges. States with high similarity can be subsumed to form new meta-states. Figure 5.39b illustrates this with pairs of states. The new meta-states themselves can in turn be grouped into further new meta-states. In our example,thisisdoneinalevel-wisewaytocreateastatehierarchy.Atits coarsest level is a meta-state that subsumes all original graphs G ∈DG. i Foreachlevelofthehierarchy(exceptforthecoarsestone),corresponding meta-transitions are determined. If a meta-state contains a graph G ∈ i DG and another state contains the subsequent graph G ∈DG, then i+1 a meta-transition is inserted between the two states.
Automatic Analysis Support (cid:4) 255 Figure5.40 Example of a state-transition graph characterizing an underlying dynamic graph. Courtesy of Steffen Hadlak. Overall, the outlined procedure generates a hierarchical clustering of the original dynamic graph. Each level of the clustering hierarchy corresponds to a state-transition graph that represents the dynamic graph at a different level of abstraction. To support the visual analysis of large dynamic graphs, a state-transition graph can be visualized as in Figure 5.40. The states are shown as boxes with an embedded miniature node-link diagram that communicates the rep- resentative structure of the subsumed original graphs. The border width of the boxes reflects the number of subsumed graphs. Transitions are visualized as directed links between states. Link width encodes the frequency of state changes subsumed by a transition. This visual design allows us to detect interesting temporal patterns in a dynamic graph: • Dominant graph structures are visible as states whose boxes have wider borders. • Rare graph structures manifest in states with thin borders and only few thin incident transitions. • Typical structural changes are expressed by paths along transitions that have wider links. • Recurring structural changes correspond to cycles in the state-transition graph. • Branching behavior isindicatedbystateshavingmorethanoneoutgoing transition.
256 (cid:4) Interactive Visual Data Analysis DemonstratingExample Let us next demonstrate the benefit of the introduced clustering methods for the visual analysis of multivariate dynamic graphs. Our example concerns the analysis of link quality in a wireless network operated by the OpenNet community. The network consists of 297 WiFi devices and 2,008 links between them monitored at 217,253 time points (five months at minute resolution). Each link has a quality attribute that describes the probability of a successful packet transmission along the link. The goal of OpenNet is to provide an overallsatisfyingnetworkquality.Visuallyanalyzingthedynamicallychanging network allows them to identify, locate, and quickly resolve quality issues. Tosupporttheanalysis,bothattribute-basedandstructure-basedclustering can be utilized. Attribute-based clustering groups the devices for which the averagelinkqualityoftheirincidentlinksevolvessimilarly.Theclusteredsuper- graph enables OpenNet to distinguish clusters in the network with varying, consistently high, or consistently low link quality. Analyzing the derived time series at multiple temporal scales as in Section 5.3.2 can provide more insight into the evolution of the link quality. Coarser temporal scales facilitate the exploration of global trends, whereas finer scales support the inspection of more fine-grained events such as dropouts. Dropouts describe incidents that may affect larger parts of the network. They are critical if they occur frequently. To investigate this aspect, structure- based clustering is applied and a generated state-transition graph is visualized in Figure 5.41a. Wide borders of the states emphasize recurring structures, which have a high impact on the stability of the wireless connections. As we can see, S , S , and S represent such frequent states. 1 2 3 a b c Figure5.41 Analyzing a wireless network supported by structure-based clus- tering. (a) State-transition graph; (b) Average link quality of selected state; (c) Representative graph structure of selected state. Courtesy of Steffen Hadlak.
Automatic Analysis Support (cid:4) 257 For a more in-depth analysis of states, two additional views are available, a temporal view and a structural overview. They depict further information for a selected state. The temporal view in Figure 5.41b visualizes the average quality of all links contained in the selected state. The structural overview in Figure 5.41c shows the state’s representative graph structure. When the structural overview is well connected, and the temporal overview shows a sufficiently high link quality without dropouts, then we can conclude that the state is sufficiently functional. By analyzing all states in this manner, it is possible to detect the states with dropouts. Whether these states are singular events or occur more frequently can be seen from the border width of the state’s box in the view of the state-transition graph. Thissmallexampledemonstrateshowthetheoreticalconceptsforclustering multivariate dynamic graphs can be employed in practice to address a real- world data analysis problem. The clustering not only helps us to reduce the amount of data to be visualized, but also to detect characteristic structures and interesting temporal patterns. At this point, we conclude the section on grouping data elements with the help of classification and clustering. We have learned that classification subdivides the data space in order to define meaningful subspaces, the classes. Each class describes data with particular properties. How to define the classes depends on the application domain and the tasks to be performed. Clustering, on the other hand, creates groups based on the similarity of dataelements.Theclusteringoutcomedependsnotsomuchondomain-specific or task-specific constraints, but primarily on the data and on the configuration of the clustering. The latter includes the selection of appropriate similarity measuresandsuitableclusteringstrategies,theparameterizationoftheselected methods, the weighting of data attributes, and the treatment of outliers. All these decisions lead to different clustering results. Overall, configuring classification and clustering approaches is a non-trivial task. Interactive visual representations can help us obtain suitable setups that reliably extract and convey the key characteristics of the analyzed data. Only then can classification and clustering support the generation of meaningful overviews that show the important as suggested by Keim’s visual analytics mantra [Kei+06]. 5.5 REDUCING DIMENSIONALITY Clustering and classification as discussed before are concerned with data elements. In this section, we will reduce the number of data variables. Data with very many variables are difficult to analyze for the following reasons: • Data issues: With many variables, the data space gets so huge that its dataelementsbecomesparse.Asadirectconsequence,thenumberofdata elements needed to estimate a function describing a data property grows
258 (cid:4) Interactive Visual Data Analysis exponentially. With the sparsity of the data space, the data get more uniform and global structures and features disappear. This phenomenon has been coined as the curse of dimensionality [Bel61]. • Visualization issues: With an increasing number of variables, the visual representations become more and more complex. Over-plotting can become a serious issue. It is more difficult to perceive relevant structures, outliers, or trends. • Performance issues: Visualizing many variables also places higher demands on memory and runtime. It becomes more and more difficult to ensure interactive frame rates. In order to address these issues, dimensionality reduction aims to reduce the number of data variables. The goal is to figure out which data variables carry relevant information and to focus the data analysis on them, neglecting the unimportant data variables. This can substantially simplify the analysis. The idea of dimensionality reduction is to project data elements from the high-dimensional data space onto a lower-dimensional projection space in such a way that the original information is preserved as much as possible. There are different approaches to perform the projection. Their difference lies in how they measure which parts and characteristics of the data are considered relevant and hence should be preserved in the projection space. Two different approaches can be differentiated: linear methods and non- linear methods. Linear methods define the dimensions of the projection space as linear combination of data variables. A prominent linear method is the principal component analysis (PCA). It is based on a rotation of the data space so that the dimensions of the projection space, the principal components, describe the directions of the largest data variations. In contrast, non-linear methods define the dimensions of the projection space with regard to certain proximity constraints. For example, with multi-dimensional scaling (MDS) pairwise data distances are preserved. It can be quite difficult to figure out which approach to apply and how to configure it. As before, we can employ interactive visual methods to help users understand and steer the automatic computations toward meaningful projections. Next, we discuss this in more detail for the example of using PCA to determine relevant data variables. 5.5.1 PrincipalComponentAnalysis InordertogetabetterunderstandingofPCA,letusbrieflyexplainthegeneral idea by the simple example in Figure 5.42. Figure 5.42a depicts a number of data elements in a two-dimensional data space spanned by the variables V 1 and V . The figure also shows the two principal components PC and PC . 2 1 2 We can easily see that PC is aligned with the axis of the greatest variance in 1 the original data. PC is orthogonal to PC . 2 1
Automatic Analysis Support (cid:4) 259 V V 2 2 PC 1 PC PC 2 2 V PC PC 1 1 1 V 1 (a)Original data space. (b)Principal component space. (c)Reduced space. Figure5.42 Reducing dimensionality with principal component analysis. In Figure 5.42b, the data space has been rotated according to PC and 1 PC to define the projection space, more specifically, the principal component 2 space, or short PC space. In PC space, most of the information inherent in the data elements is distributed along the horizontal axis, whereas the vertical axis carries only little information. This allows us to apply dimensionality reduction and to represent the data just by PC . 1 Figure 5.42c illustrates the result of the dimensionality reduction. The data elements are now only given with respect to PC . Although we lose 1 the information with respect to PC , the one-dimensional representation still 2 clearly shows two data groups at the beginning and the end of PC and one 1 outlier in the center. This simple example shows us how PCA can be utilized to decrease the data complexity, while still maintaining the distribution of data elements along the major data trend described by PC . 1 For actually calculating the principal components, we need tabular data modeled as an n×m matrix X, where the columns represent m data vari- ables and the rows represent n data elements. By applying a singular value decomposition, the matrix X is decomposed as follows: X =W ·Σ·CT The rows of the matrix CT are the transposed eigenvectors of the data’s covariance matrix XTX. They define the principal components as linear combinations of the original data variables. The factors involved in the linear combinations are denoted as loadings. The loadings capture how much an original data variable contributes to a principal component. ThematrixΣisadiagonalmatrix.Thediagonalentriescontainsignificance values resulting from the ranked square roots of the eigenvalues of the data’s covariance matrix XTX. A principal component’s significance tells us how much original data variance it covers. The principal components in CT are ordered with regard to their signifi- cance. The first principal component captures most of the data’s variance and thus represents the major data trend. The second principal component cap- tures most of the remaining variance and so on. The least significant principal
260 (cid:4) Interactive Visual Data Analysis components, that is, the last rows of CT represent only very small variances. These components can be neglected in order to reduce dimensionality. The rows of the matrix W contain the transformed coordinates of the data elements in the principal component space. These coordinates are denoted as scores to make them distinguishable from the original positions of the data elements. Next, we will see how principal components, loadings, significance, and scores can be utilized for data analysis purposes. 5.5.2 VisualDataAnalysiswithPrincipalComponents Visualdataanalysiswithprincipalcomponentstypicallyrequirestwomutually dependent analysis perspectives. First, we investigate how visual methods can help us understand and steer the dimensionality reduction with PCA. Second, we describe how the actual data analysis can be carried out based on the principal components. VisualSupportforPrincipalComponentAnalysis Before the PCA can be calculated, it needs to be decided what should be included in the data matrix X. Theoretically, X can cover the whole dataset. However, it can make sense to compute principal components only for a data subset depending on the task at hand. For example, variables describing time and space are often excluded to leave them as a frame of reference. Moreover, highly correlated variables might overemphasize certain data trends, and outliers could distort the analysis results. It is up to the user to exclude these variables and data elements from the PCA. The decision regarding which parts of the data should be excluded can be supported by visual methods. Table-based visualizations as introduced in Section 3.2.1 of Chapter 3 can be used for this purpose. When a table-based visualization is sorted properly, highly correlated variables and outliers to be ignored can be recognized easily. OncethePCAhasbeencarriedoutonthedefineddatasubset,weobtainthe principal components. For each principal component, we know its significance and how each variable contributes to it, that is, the loadings. Based on this information, we can reduce the PC space. Typically, only the two or three most significant principal components are retained, because they capture the major data trends and can easily be plotted in 2D or 3D visual representation. However, less significant principal components might also bear interesting and unexpected information. This raises the question of how to choose principal components in an adequate way. To answer this question, we need to take not only the significance but also the loadings into account. In fact a subset of principal components sufficiently characterizes the original data, only if:
Automatic Analysis Support (cid:4) 261 1. all loadings of these principal components are sufficiently high, that is, all variables are represented well, and 2. the loadings of the remaining principal components weighted by their significance are sufficiently small. Again, a table-based visualization can help us check whether these two conditions are satisfied [MNS06]. In Figure 5.43, we illustrate this for a demo- graphic dataset. The rows of the table show the variables of the original data, including population, literacy, or life expectancy. The columns contain the principal components ordered from left to right according to their significance. The table cells visualize loadings weighted by significance. Each table cell corresponds to a pair of a data variable and a principal component. Per cell, a bar encodes information as follows. The bar extends from the cell center to the right and is colored blue when the loading has a positive sign. The bar extends to the left and is yellow when the sign is negative. The length of the bar represents the value of the variable’s loading weighted by the principal component’s significance. As such, the bars in a column act as an indicator for the relevance of the corresponding principal component. The bars in a row show the influence of the data variables on the different principal components. Figure 5.43 Table-based visualization of loadings weighted by significance. Reprinted from [Aig+11]. In Figure 5.43, we can see many longer bars in the first three columns. This means that the first three principal components actually capture a lot of information, suggesting that all other components could be candidates for reduction. However, the first two variables, population and population density, hardly contribute to the first three principal components. Hence, condition one as stated above is violated. Moreover, there are also a couple of longer bars in columns three to six, which means that also the second condition is not met. If we retain only the first three principal components, we will likely lose information. Instead, we should consider at least four, better five or six principal components. Only then are all variables faithfully represented by the reduced PC space. However, still interesting information might be hidden in the data. In order to reveal them, it may help to look at the unweighted loadings. In Figure 5.44, the plain loadings are visualized in the same way as before. Now, we can
262 (cid:4) Interactive Visual Data Analysis Figure5.44 Visualizing unweighted loadings empasizes the contribution of individual variables to each principal component. Reprinted from [Aig+11]. see the contribution of the individual variables to each principal component. This facilitates the detection of deviating data behavior. For example, in the ninth principal component, we can identify a difference between the loadings of the two variables for life expectancy of females and males. This difference might suggest including the principal component in the analysis as well or investigating the two variables separately in more detail. In summary, the above examples demonstrated how the visualization of loadings and significance can help us make an informed decision when it comes to reducing dimensionality of data. Next, we briefly describe how the actually reduced principal component space can be analyzed. VisualDataAnalysisBasedonPrincipalComponents Visual data analysis based on principal components means showing the data elements in the PC space, more precisely, in the reduced PC space. Generally, thevisualizationisstraightforward.Thedataelementscansimplybevisualized accordingtothescores calculatedduringthePCA.Ingeneral,thevisualization techniques for multivariate data introduced in Section 3.2 in Chapter 3 can be utilized for this purpose. For example, we could use parallel coordinates with as many axes as we have principal components and plot each data element based on its score values. Alternatively, we could adapt the idea of the tabular visualizationusedbefore.Wemaintaintheprincipalcomponentsinthecolumns, but show data elements as rows. Each table cell would then represent a score value.Whilethesearealltechnicallyfeasiblevisualrepresentations,akeyissue needs to be addressed. Because the principal components spanning the PC space correspond to a combination of several original data variables, interpreting the scores and relating them back to the original data space is difficult for a human observer. Therefore, we need visual support helping users gain an understanding of the rather abstract principal components. Two visual approaches can be useful in this regard: • Annotating principal components with data variables: The principal com- ponents are labeled with the names of those variables that mainly con-
Automatic Analysis Support (cid:4) 263 tribute to them. To avoid clutter, labels can be shown on demand when the user hovers over a principal component with the cursor. • Associating scores with data values: A subset of scores in PC space can be associated with their corresponding values in data space. This can be done either by a side-by-side visualization of scores and data values or by a combined visualization. In the side-by-side case, brushing & linking allows users to connect the scores from one view with the data values of the other view. In the case of a combined visualization, a limited number of principal components and data variables are presented simultaneously. An example could be our table-based visualization. As before, the table columns represent the principal components, but the rows now show the original data variables, and the bars in the cells represent scores. In such a visual representation, small bars symbolize data values that follow the main trend of a principal component, whereas larger bars correspond to substantial deviations from the trend. To conclude, again we have seen that automatic computations, PCA in this case, can support the interactive visual data analysis. PCA is a helpful approach to determine which data variables play an important role and which can potentially be neglected. Knowing this, we can simplify the visualization and concentrate the analysis on those parts of the data where insight are likely to be made. Yet, we have also seen again that employing automatic computations requires a careful configuration and a good understanding of the consequences. Visualmethodscanhelpusersinthisregard.Infact,weneedatightinterplayof automaticcalculationandhumansense-makingbasedonvisualrepresentations. 5.6 SUMMARY In this chapter, we discussed automatic computational methods to support interactive visual data analysis. The key idea was to reduce the complexity of the data and their visual representations. A figurative overview of the covered methods is provided in Figure 5.45. First, we briefly described how the complexity of visual representations can be addressed with density-based visualization and bundling. The idea of density-based visualization is to compute and visualize the frequency of data elements. Bundling improves the visual structure in visualization images by summarizing graphical elements. For the most part of this chapter, we studied ways to reduce the size and complexity of the data. A first strategy was to focus the analysis on relevant parts of the data. The degree-of-interest (DoI) approach was introduced as a means to determine the relevancy of data elements based on well-defined interest functions. A related approach is feature-based visual analysis, where characteristic data features are automatically extracted and visualized.
264 (cid:4) Interactive Visual Data Analysis Density-based Bundling Degree of Feature-based visualization interest (DoI) visual analysis Sampling Aggregation Classification Clustering Dimensionality reduction Figure5.45 Overview of automatic computational methods to support inter- active visual data analysis by reducing the complexity of the data and their visual representations. We further discussed how abstracting and grouping data elements can facilitate the data analysis. Sampling and aggregation are two well-established data abstraction methods. Sampling methods aim to generate a reduced, but stillrepresentativesubsetoftheoriginaldata.Aggregationmethodsreplacethe data values within an interval by statistical descriptions such as the average or the mode. Classification and clustering are typical strategies for grouping data elements. With classification, the data space is partitioned into classes with specific properties, whereas with clustering, the data are organized in clusters of similar data elements. Last but not least, we described how dimensionality reduction can help us reduce the complexity of data with many variables. All of the above approaches address the “Analyse First”-step of the visual analytics mantra as stated in the beginning of this chapter [Kei+06]. Although each approach pursues a different strategy, they all have in common that they transform unstructured raw data into a form that emphasizes the data’s basic properties and key characteristics. This way, the data is prepared to be more meaningful for human exploration and insight discovery [Sac+14]. However, applying automatic computations typically requires a careful configuration. What computational methods should be applied, maybe even in combination, and how should their parameters be set? Finding suitable answerstothesequestionsisimportant,becausedifferentmethodsanddifferent parameter settings lead to different results, and not all of them might be appropriate for the task at hand. In this chapter, we have seen several examples where visual methods were employed not only to show the data, but also to help users understand how automatic computations work and how their parameters should be set. In fact, we have seen a mutual relationship between automatic computation and visual methods. On the one hand, automatic computations support the visual
Automatic Analysis Support (cid:4) 265 analysisbyextractingimportantcharacteristicsofthedata.Ontheotherhand, visual methods help us to configure the automatic computations appropriately. Both sides of the relationship are tied together by the human user who has interactive control over all involved methods [End+17]. This control paired with expressive visual representations enhances trust in and interpretability of the generated analysis results. Overall, we have now dealt with the fundamental ingredients of interactive visualdataanalysis.Inthenextchapter,wewillmoveontoadvancedconcepts for visualization, interaction, and automatic computation. FURTHER READING General Literature: [Kei+06] • [Kei+10] • [End+17] Decluttering Visual Representations: [NH06] • [BW08a] • [LHT17] Focusing on Relevant Data: [RPS01] • [DGH03] • [Abe+14] Abstracting Data: [ED06] • [EF10] • [Lub+12] Grouping Data: [HKP11] • [XW05] • [Emm+16] Reducing Dimensionality: [Jol02] • [LV07] • [Sac+17]
6 CHAPTER Advanced Concepts CONTENTS 6.1 Visualization in Multi-display Environments ............... 268 6.1.1 Environment and Requirements .................... 269 6.1.2 Supporting Collaborative Visual Data Analysis .... 270 6.1.3 Multi-display Analysis of Climate Change Impact . 276 6.2 Guiding the User ............................................ 277 6.2.1 Characterization of Guidance ....................... 278 6.2.2 Guiding the Navigation in Hierarchical Graphs .... 283 6.2.3 Guiding the Visual Analysis of Heterogeneous Data 286 6.3 Progressive Visual Data Analysis ........................... 288 6.3.1 Conceptual Considerations .......................... 290 6.3.2 Multi-threading Architecture ....................... 294 6.3.3 Scenarios ............................................ 297 6.4 Summary .................................................... 303 IN THE PREVIOUS Chapters 3 to 5, we described the three fundamental parts of interactive visual data analysis: the visualization, the interaction, and the automatic analysis support. We learned about basic visualization strategies, interaction techniques, and computational methods, which usually work in concert to facilitate data analysis activities. The knowledge from the previous chapters enables the reader to apply existing approaches to analytic problems at hand or to design new ones if necessary. The goal of this chapter is to broaden our view on interactive visual data analysis to advanced topics that are not yet mainstream, but have the potential to become so in the future. Following the structure of the previous three chapters, we will discuss three selected topics: • advanced visualization in multi-display environments, • advanced interaction through user guidance, and • advanced automatic computation via progressive procedures. Section 6.1 will discuss the visualization of data in multi-display envi- ronments. We will see that such environments provide us with interesting 267
268 (cid:4) Interactive Visual Data Analysis opportunities to enhance the visual data analysis. They allow us to show more visual content and also to show it to more people. This enables collaborative data analysis and in-depth discussions involving several experts. Section 6.2 deals with facilitating the human-computer partnership by providing guidance that helps users carry out expedient analysis actions. We will discuss a conceptual framework that characterizes guidance in the context of interactive visual data analysis, and two selected examples will illustrate how guidance can actually be implemented. Finally, in Section 6.3, we examine the challenge of time-consuming auto- matic computations, which may impede the data analysis. One answer to this problem is to progressively compute intermediate results and present them to the user timely, allowing for early evaluation and, if necessary, intervention. Withthesethreetopics,wearemovingintoaterrainofactivedataanalysis research.Thatsaid,thecontenttocomeisnaturallysubjecttofurtheracademic discussions and revisions. Still the following sections provide an outlook on exiting new perspectives on interactive visual data analysis. Let us start with visualization in multi-display environments. 6.1 VISUALIZATION IN MULTI-DISPLAY ENVIRONMENTS Most of the visual analysis solutions described in this book are designed for traditional environments where a single user is working with one or two displays. However, such environments are limited in two regards. First, only a single individual is involved in the data analysis. Critical reflections of results or creative discussions of alternative analysis strategies are hardly possible. Second, the available display space is limited. This can make the analysis of larger volumes of data difficult. A natural step to address these limitations is to bring the visual data analysis to advanced multi-display environments (MDEs). MDEs can facilitate visualdataanalysisbyenablingmoreuserstoobservemoredata.Theincreased overalldisplayspacenotonlymakesitpossibletovisualizemoredataelements, but also to show more aspects of the data simultaneously. The increased physicalsizeofMDEsallowsmultipleuserstostudythevisualizeddata,which promotes collaborative analytic work. Collaborative analysis in MDEs is particularly useful in scenarios where experts from different application domains have to discuss heterogeneous data toarriveatacommonunderstandingofcomplexphenomena.Onesuchscenario isthestudyoftheimpactofclimatechange.Climatechangeaffectsamultitude ofsectors,suchasagriculture,forestry,ecosystems,andeconomy.Expertsfrom manyfieldsneed toworktogetherand sharetheirdomain-specific observations anddataanalyses tobetterunderstandthe crucial questions relatedtoclimate change. Collaborative visual data analysis in MDEs can be a paradigm shift away from the otherwise single-user setting at regular desktop computers. Yet, this shift also comes with new challenges. For example, distributing and
Advanced Concepts (cid:4) 269 Figure 6.1 Smart meeting room at the University of Rostock. Reprinted from [ENS15]. arranging visual representations manually on multiple displays can be a time- consuming task that should not be burdened on the users. Keeping track of analysis steps and intermediate findings is also more difficult in a collaborative setting. Therefore, the visual data analysis in MDEs must be facilitated by dedicated methods that relieve users of laborious manual work and allow them to concentrate on their analytic objectives. This section outlines how advanced visual data analysis can be implemented in MDEs. 6.1.1 EnvironmentandRequirements Tostartwith,wewillbrieflyintroduceaparticularMDEandtherequirements that need to be considered to support collaborative visual data analyses in it. Environment Our environment is a smart meeting room as illustrated in Figure 6.1. A smart meeting room is an instance of smart environments, which Cook and Das define as “a small world where all kinds of smart devices are continuously working to make inhabitants’ lives more comfortable” [CD04]. The smart meeting room combines several heterogeneous displays to form a coherent display space in which content can be distributed even across device boundaries. Moreover, the smart meeting room can dynamically integrate various input, computing, and output devices into the device ensemble, which allows users to bring and use their own personal devices. These characteristics make a smart meeting room a distinguished MDE, namely a smart MDE. While the MDE provides a technical basis, it is the MDE’s smartness that enables the actual visual data analysis.
270 (cid:4) Interactive Visual Data Analysis Requirements for Collaborative Visual Analysis In order to support the collaborative visual data analysis in smart MDEs, we must understand how users work. In the first place, the users compile a corpus of information to be analyzedanddiscussed.Anyusermaycontributecontents,suchasvisualization images, slides, or documents. For simplicity, let us call these different contents justviews.Then,theusersdefinewhichviewsshouldbeshowntogetherandin what sequence. This way, a kind of presentation is created that meaningfully aligns the views with the analysis goals. While setting up the presentation is the task of the users, distributing and arranging the views on the multiple displays should be the task of the machine. This brings us to the first two requirements: • The system should afford easy contribution of views and creation of the presentation. This ensures that the users can control the content and the flow toward the analysis goal. • The system should distribute and lay out the views in the environment automatically according to the state of the presentation. This relieves the user from cumbersome manual layout tasks. While discussing and analyzing the data contained in the views, it is often necessary to go back to previously shown information, to add, adjust, or replace certain visual representations, or to compare performed analysis steps. Furthermore, newly gained insights may raise new questions, which in turn might require inspecting additional data. In this case, the users must be able to generate the needed visualizations without interrupting the ongoing presentation. From this, two further requirements can be derived: • The system should support the seamless switching between the presenta- tion of views and the generation and adjustment of views. This narrows the gap between phases of presentation and phases of more personal exploratory analytical work. • The system should keep track of the analysis history, including changes to the presentation and the findings made during the analysis session. This helps users to return to previously discussed aspects and to recap the analysis and its results. Next, we will illustrate by concrete methods how these requirements can be addressed to support the collaborative visual data analysis in smart MDEs. 6.1.2 SupportingCollaborativeVisualDataAnalysis The solutions to be presented next focus on supporting four key tasks: the creation of the presentation, the layout of views on multiple displays, the adjustment of presentation and views during the analysis, and the recording of the visual analysis process.
Advanced Concepts (cid:4) 271 Figure6.2 Graphicalinterfaceforcreatingandcontrollingmulti-displayvisual analysis presentations, including content pool (top), logical presentation struc- ture (middle), and preview (bottom). Courtesy of Christian Eichner. CreatingaMulti-displayPresentation The creation of presentations for collaborative visual data analysis can be supported by a graphical user interface as shown in Figure 6.2. The interface is available on all devices being connected to the smart MDE, so that everyone can participate in preparing the presentation, even from their personal devices. The top panel of Figure 6.2 represents the content pool and affords the collection of views. Visualization images, slides, or documents can be added to the content pool by simple drag and drop. A special thing about the collected viewsisthattheycanbelinkedtothesoftwarethatwasusedtogeneratethem. The linked software can be started at any time by clicking the corresponding view. This allows users to alter existing views or to generate new ones on the fly during the presentation. The middle panel in Figure 6.2 serves to set up the presentation based on the views in the content pool. For this purpose, views from the top panel are dragged to the vertical layers in the middle panel. From left to right, the layers represent the sequence of views to be shown in the course of the presentation. Views within the same layer are visible at the same time. Links can be added between views to define spatial and temporal constraints for the later automatic layout of the views on multiple displays. Spatial constraints (within layers) link views that are to be shown on the same display, whereas temporal constraints (between subsequent layers) link views to be replaced when advancing the presentation from one step to the next.
272 (cid:4) Interactive Visual Data Analysis The bottom panel in Figure 6.2 provides a preview of the presentation. It tells the users which views are currently being shown in the smart MDE, which views were displayed before, and which views are still to come. This panel is also used to advance the presentation to the next layer, to return to previously shown layers, or to directly go to a certain layer. In summary, the described graphical interface enables users to set up a multi-display visual analysis presentation. To avoid conflicts during the collaborativeworkandtoensureaconsistentpresentation,amoderatorshould chair the process. LayoutofViewsonMultipleDisplays Smart MDEs provide the technological basis for distributing the graphical contents on multiple displays. What is needed to support collaborative visual analysis is a mechanism that distributes and arranges the views according to the prepared presentation. This can be done with a dedicated multi-display layout algorithm. The automatic view layout has to be carried out in two steps as illustrated in Figure 6.3. In the first step, views are allocated to the available displays of the environment. The second step computes the arrangement of views per display.Becausebothstepsmustconsidervariousinfluencingfactors,including the number and the properties of the available displays and the user-defined temporal and spatial constraints, it makes sense to define the automatic layout as an optimization problem [Eic+15]. The goal of the optimization is to find view positions such that the overall layout quality is maximal. Three terms contribute to the overall layout quality: the spatial quality, the temporal quality, and the visibility quality. The spatial quality is high if the views being linked via spatial constraints are near to each other, ideally on the same display. The temporal quality is high if temporally stable layouts are produced. That is, views being linked via temporal constraints are ideally presented at the same position when advancing the presentation from one step to the next. The visibility quality is a bit more complex to define. It rates how well users can see the different views in the display environment, which can Display 1 View View Allocation Display 2 Arrangement Views Display 3 Figure6.3 Basic two-step procedure of the automatic view layout.
Advanced Concepts (cid:4) 273 be approximated by considering the directional visibility of views. A smart MDE can estimate this information based on the display configuration (size, position, and orientation of displays) and by tracking the participating users (position and viewing direction of users) [RLS11]. The outlined optimization problem has to be solved whenever a change happens either to the presentation or to the environment. In order to manage to compute the layout for a dozen of views in less than a second, a heuristic optimization approach should be applied. For example, by employing the branch-and-cut algorithm, overestimating the achievable qualities of possible layouts, and reusing previous calculations, it is possible to obtain expedient results in a short time. InteractiveAdjustmentsinMulti-displayEnvironments Oncethelayoutmechanismhasdelivereditsinitialresults,theviewsareshown in the smart MDE and the data analysis can start. The regular process will be to discuss the depicted information, agree on intermediate findings and results, andadvancethepresentationfurther.Yet,withchangingtopicsofinterest,the actual course of the analysis might divert from the originally planned one. In this case, the users must be able to adjust the presentation on the fly. To this end, two types of interactive adjustments should be supported: adjustment of the layout of views and adjustment of the content of views. Adjusting the Layout of Views Adjusting the view layout involves moving and resizing views. For example, views might need to be moved from one display to another for side-by-side comparison. Fine details spotted during the comparative analysis could make it necessary to enlarge a view. To this end, the moderator (or another authorized user) can select and adjust a certain view to improve the visibility of details. (a)Point at central view. (b)Move view downward. (c)Enlarge view. Figure6.4 AdjustingpositionandsizeofaviewusingaWiiRemotecontroller. Reprinted from [Rad+12]. An example of such a layout adjustment is illustrated in Figure 6.4. We alreadysawthesevisualrepresentationsofgraphsinSection3.5.3ofChapter3, but now they are projected onto a canvas of the smart MDE. A user points at
274 (cid:4) Interactive Visual Data Analysis Different View Contents 3. Store 4. Update 2. Generate 1. Launch Graphical Interface Visualization Software Multi-display Environment Figure6.5 Changing the content of views by launching visualization software. the central Magic Eye View using a Wii Remote controller, moves the view a bit downward, and then enlarges the view to make it stand out. On the user’s side, these interactions are easy to perform. On the system’s side, dedicated mechanisms such as device integration and interaction mapping are employed to facilitate the interaction [Rad+15]. Adjusting the Content of Views We already mentioned that views can be linked with a compatible software. This makes it possible to re-generate the content of views to better align them with varying analysis requirements. Figure 6.5 illustrates an example with the feature-based visualization already described in Section 5.2.2. First, the linked visualization software is launched from the graphical interface with a click on the view’s thumbnail. This can be done by users who have the visualization software installed on their personal device. On the personal device, the data can then be explored until a suitable new view has been found. The new view can be an alteration of the already existing version or a totally new visual representation. Once generated, new views are immediately stored and integrated into the presentation, and the layout of views is updated automatically. The great flexibility offered by this mechanism is a key advantage for the collaborative data analysis. KeepingTrackofCollaborativeDataAnalyses It is generally accepted that keeping an analysis history is beneficial for coor- dinating the insight-generation process and reflecting about it [KNS04]. Com- monly, analysis histories store information about the interactive adjustments performed by users and the findings that have been derived. In a collaborative multi-display setting, it makes sense to log additional information about who contributedandchangedviewsandwhenandwhereviewshavebeendisplayed. Some of this information can be determined automatically. For example, the
Advanced Concepts (cid:4) 275 Figure6.6 Graphical interface for analysis coordination and meta-analysis, including filtering support (top), analysis history graph (middle), and timeline with undo and redo buttons (bottom). Courtesy of Christian Eichner. view layout mechanism can keep track of when and where views were shown. Interactive adjustments can also be recorded automatically, including the actions that were taken, the users who carried them out, and the resulting visual representations. Yet, some information cannot be derived automatically. For example, findings derived from a view need to be annotated manually. The recorded and annotated information is stored in the form of a graph. The graph’s nodes represent views, more precisely, the state changes logged per view. As such, a node captures a piece of analytical progress made during the data analysis. Links between nodes form paths of analytical progress as defined by the sequence of actions taken. Toactuallygainfromtheanalysishistory,itcanbedisplayedinagraphical interface as illustrated with a small example in Figure 6.6. During an ongoing analysis session, the interface can be employed to reset the analysis to a previous state via selective undo and redo. “Selective” means it only affects operations that were triggered by a certain user, affected a specific view, or concerned a particular display. This is helpful when the data analysis stalls in a dead end or if the participating users cannot come to an agreement about findingsandintermediateanalysisresults.Undoandredoallowsthemoderator to keep the analysis going, for example, by collecting further evidence for or againstahypothesisfrompreviousviews.If,afterreturningtoapreviousstate, an alternative course of actions is pursued, a new analysis branch is created, which is also visible in Figure 6.6. A graphical depiction of the analysis history can also support a post-hoc meta-analysis to understand how individual analysis steps contributed to the generation of new insights. For example, in Figure 6.6, we can see that
276 (cid:4) Interactive Visual Data Analysis three alternative analysis routes were tried out. Small icons overlaid on the thumbnails indicate which interactions were performed, and the thumbnails’ colored borders tell us who performed them. When a thumbnail is clicked, an on-demand text box will provide information about when and where a view was displayed and which findings have been derived from it. With the help of additional filter controls, the meta-analysis can even answer questions such as which adjustments led to promising findings or which results required longer discussions. Taken together, the support for creating collaborative analysis sessions, for laying out views on multiple displays, for adjusting views and their content on the fly, and for keeping track of and utilizing an analysis history is essential for facilitating visual data analysis in smart MDEs. Next, we will illustrate how a smart MDE can be put to use for analyzing the impact of climate change. 6.1.3 Multi-displayAnalysisofClimateChangeImpact As already indicated, analyzing the impact of climate change requires the collaboration of multiple experts. In this section, we sketch a scenario with experts from meteorology, forestry, and hydrology. The course of collaborative analysis could be as follows [Eic+15]. First,themeteorologistwantstoexplainextremeprecipitationeventsbased onvisualrepresentationsthatshealreadyhasonherlaptopcomputer.Shelogs into the smart MDE and contributes her contents via the graphical interface introduced earlier. The automatic layout will make sure that all experts can see the visual representations well. Now the joint analysis can start. The explanations of the meteorologist trigger the hydrologist to join the discussion.Tobettermakehispoint,thehydrologistusesthegraphicalinterface on his computer to put a visualization about groundwater recharge into the content pool. Then, he defines a spatial constraint to link his view to a precipitation view of the climatologist. In the blink of an eye, the view layout in the smart MDE is automatically updated to show the linked views side-by- side on the same display. This allows a comparison and in-depth discussion of the views. At some point, the experts realize that they need additional views to make progress. Therefore, the hydrologist connects his view with the climateimpactsonline.com web portal. This allows him to generate the necessary views on the fly. Depending on the discussion of the new views, the experts can decide to either keep them or to perform undo operations to reset the presentation to the original views. Finally, the forestry expert takes part in the discussion. She explains how climate-related risks in Africa and South America are expected to develop in comparison to Europe. To illustrate this, she puts further views into the content pool and defines spatial and temporal constraints to connect them to the existing views. Again, the presentation is updated automatically.
Advanced Concepts (cid:4) 277 In the course of the visual analysis, the knowledge and findings of the different experts are discussed and combined to form a broader understanding of the impacts of climate change. After the analysis session, all experts can inspect the analysis history to reflect about the discussion or reproduce the analysis results. In summary, we see that bringing interactive visual data analysis to smart multi-displays environments is an exciting opportunity for collaborative sense- making. Yet, in order to fully exploit this opportunity, it is necessary to designdedicatedsupporttofacilitatetheanalysis.Here,weillustratedselected solutions based on a mix of automatic methods and interactive graphical interfaces implemented in a smart multi-display environment, which together form an advanced visualization environment. Next up in this chapter on advanced concepts is advanced interaction through user guidance. 6.2 GUIDING THE USER The meta-analysis in the previous section confirmed what we already stated severaltimes:Visualdataanalysisisnotaone-wayroadbutadynamicprocess during which several what-if scenarios are tried out. How should the data be processed with analytical calculations? Should clusters be computed, if yes, how many clusters are expected? Are certain data variables to be excluded from the processing? Which techniques should be employed to visualize the data? Should different data facets be represented in an integrated fashion or in separate dedicated views? Which parts of the data should be explored in what sequence? Only if appropriate answers to these questions can be found is it possible to make progress toward the desired analytic results. Butwhohastoanswerthequestions,theusers?Orcouldthemachinestepin andhelpusoutincertainsituations?Thisiswhatguidance isabout[Sch+13b; Cen+17; Col+18]. As a response to the challenge of ensuring analytic progress, it is the goal to guide users towards choices that present the most interesting aspects of the data with the most suitable combination of visual, interactive, and analysis methods. In Section 5.3.2 of Chapter 5, we already saw how guidance can assist userswhenexploringmulti-scaletimeseries.Wewillnextdiscusstherelatively new topic of guidance in more detail. The first part of this section will be of conceptual nature offering an in-depth characterization of guidance. The second and the third parts will be more practical. Two examples will illustrate how guidance can help users in analyzing complex data other than time series.
278 (cid:4) Interactive Visual Data Analysis 6.2.1 CharacterizationofGuidance So, what is guidance? In order to make more clear what we mean by guidance, we will start with a definition of guidance in the context of interactive visual data analysis, discuss its key aspects, and then introduce a conceptual model. DefinitionofGuidance Guidance is a broad term with much room for interpretation. The Oxford Dictionary and the Merriam-Webster Dictionary define guidance as “advice or information aimed at resolving a problem or difficulty” and “the act or process of guiding someone or something”. These definitions are interesting because they highlight guidance as a process aiming at solving a problem. This is also reflected in the following definition of guidance in the context of interactive visual data analysis [Cen+17]: “Guidance is a computer-assisted process that aims to actively resolve a knowledgegapencounteredbyusersduringaninteractivevisualanalytics session.” Cenedaetal.,2017 The three important aspects of this definition are emphasized in italics. First, guidance is a dynamic process that runs alongside the regular data analysis activities of the user. Second, there is a knowledge gap that causes the data analysis to stall. The user does not know how to proceed. The goal of guidance is to narrow the knowledge gap. Finally, the definition of guidance describes an interactive scenario where human and machine cooperate. The above definition also suggests what guidance is not. Guidance is not just an additional algorithm that computes a unique answer to the knowledge gap. Typically, this is not even possible due to ill-defined or too complex analyticproblems.Ifguidancewereabletocomputeapreciseanswer,wecould neglect the interactive visual approach to data analysis at all, compute the answer, and provide it to the user right away. But this would contradict with the idea of having the human in the loop. Thatsaid,guidancedoesnottakeoverthereasoningpart.Instead,guidance facilitates the data analysis to help users in forming decisions. Making the decisions remains the responsibility of the user. In this sense, guidance is comparable to mentors helping students. While mentors do not necessarily know the solution of the students’ problems, they can provide hints as how to approach the problems, guiding the students towards finding solutions on their own.
Advanced Concepts (cid:4) 279 Current Analysis Situation Path? Target? Figure6.7 A knowledge gap exists when target or path is unknown. AspectsofGuidance Allinall,guidanceisinfactacatalystforhuman-computercooperation.There are four main aspects that are worth further detailed consideration [Cen+17]: • Knowledge Gap: Why is guidance needed? • Input: What information can be utilized for providing guidance? • Output: How is guidance conveyed and how does it look like? • Degree: How much help does guidance provide? Knowledge Gap The knowledge gap captures what a user needs to know to make progress. While one can easily imagine many different knowledge gaps, from a conceptual point of view, there are only two distinct types of knowledge gaps: • Target unknown.Theuserdoesnotknowthedesiredresult.Forexample, the user does not know which data features to look at to falsify a hypothesis. • Path unknown. The user does not know how to reach the desired result. For example, the user knows the relevant features, but has no clue how to configure the visualization to reveal them. Figure 6.7 illustrates the two types of knowledge gaps. Certainly, if both target and path are known to the user, no guidance is needed. In general, capturing the knowledge gap is difficult. Users may or may not beawareoftheirknowledgegap.Ifusersareawareofit,theycanactivelymake it known to the system. If not, the system has to infer the knowledge gap, for example, by detecting deviations from domain conventions or long dwell times during exploration. Ultimately, guidance should narrow down the knowledge gap in a dynamic process that eventually converges to zero knowledge gap. Input The input subsumes the different sources of information based on which guidance can be generated. In the context of interactive visual data analysis systems, the following inputs can be utilized:
280 (cid:4) Interactive Visual Data Analysis Data input relates to information that is readily available or derivable from theoriginaldata,includingtherawdatathemselves,statisticalproperties of the data, extracted topology, or meta-data. Domain knowledge input refers to information that is commonly agreed upon in the application domain, such as domain models and conventions, established workflows, or expert systems. Visualization input captureswhattheuserisactuallyseeingonthedisplay. It includes both the specification of the visualization transformation as well as the actual visualization images. User knowledge input corresponds to information that users input to the system or that the system can infer from the user. Examples include annotations, preferences, or user interests. History input is based on keeping track of the course of analysis sessions by logging interaction steps, employed methods and parameters, intermedi- ate views, or visited parts of the data. Output Concerning the output of guidance, there are two aspects to be considered. In the first place, guidance must be generated, and secondly, it must be conveyed to the user. Generating Guidance Conceptually, generating guidance can be modeled as a function guidance(g,i)→o that takes the knowledge gap g and some input i from the available sourcesandthencomputesasuitableoutputo.Suitablemeanstheoutput contains pieces of information that alleviate the user’s problem. Hence, iterating the function several times should narrow the knowledge gap. Each iteration contributes a variable amount of knowledge, depending on the user’s expertise and perceptual and cognitive abilities. Differentknowledgegapscanbetackledwithdifferentguidancefunctions addressingdiverseaspectsofthedataanalysis.Forexample,iftheuseris unsure about what data should be investigated, guidance could identify interesting data subsets and suggest navigational routes toward them. If the user needs help in structuring the analysis into a series of tasks that match the analysis objective, the system could hint at what to do next. A user who is baffled by the variety of interactive, visual, and analytical methods could be supported by suggesting suitable combinations of techniques and corresponding parametrizations. Thegeneratedoutputcanaddresstheknowledgegapdirectlyorindirectly. For example, if a user has difficulties configuring a clustering algorithm, direct guidance could suggest promising parameter values. For the same
Advanced Concepts (cid:4) 281 problem, indirect guidance could accentuate interesting sub-structures in the data, whose analysis (note the indirection) could help the user better understand the influence of parameter values. Conveying Guidance The next step is to convey the guidance output in a way that actually facilitates the data analysis, yet without interfering too much with it. This can involve enhancing perception or inducing impulses in the user to trigger exploratory actions. It seems natural that guidance is primarily conveyed visually, for exam- ple, by adjusting the visualization, providing visual enhancements, or including additional graphical interface elements. We will later see two examples, where visually conveyed guidance is central. Yet, depending on the application context, guidance can also be conveyed via non-visual channels, including sounds or tactile feedback. Guidance Degree The guidance degree characterizes the extent to which guidance is required and actually provided. The degree is defined on a scale whose two extremes are labeled “no guidance/full freedom” and “fully guid- ed/nofreedom”.Apparently,thedegreeofguidanceisinverselyproportionalto the users’ freedom. In general, an effective guidance solution restricts freedom as little as possible, but as much as necessary. Ideally, the guidance degree is not fixed, but rather resonates with the course of the analysis session. In practice, the actually delivered guidance typically follows one of three characteristic scenarios: Orienting: Merelyorientingtheuserrepresentsalowdegreeofguidance.The objective of orienting is to support the user in building and maintaining a mental map. Providing visual cues hinting at potential targets and suitablepathsisacommonstrategyforimplementingorientation.Visual overview techniques also provide a kind of orientation. Directing: Directing represents a moderate degree of guidance. In contrast to orienting, directing emphasizes a certain preference for a future course of action. The system suggests to the user a set of options that lead to promising results. The suggestions may differ in terms of quality and costs. Visual preview techniques can help users make informed decisions for one or the other option. Prescribing: A rather high degree of guidance is reached when the system prescribes certain analytical steps towards a specified goal. This can be compared to a guided presentation that takes the user through the analysis process. To keep the human in the loop, it is important to visually present the intermediate steps taken by the system and to make the decisions that lead from one step to the next understandable. Of course it must be possible for the user to regain control and continue the analysis on another path or to another target.
282 (cid:4) Interactive Visual Data Analysis DATA ANALYTIC AND VISUAL USER TRANSFORMATION D T I P dK/dt K Data Transf. Image Percept. Knowl. S dS/dt E Spec. Explor. Figure6.8 Adapted variant of van Wijk’s model of visualization. Artifacts as boxes: data [D], specifications [S], visualization images [I], and user knowledge [K]. Functions as circles: analytic and visual transformation (T), perception and cognition (P), and interactive exploration (E). Adapted from [vWij06]. These three scenarios complete our characterization of the different aspects of guidance in the context of interactive visual data analysis. In the following, we will shape the idea of guidance into a conceptual model. ConceptualModelofGuidedInteractiveVisualDataAnalysis The conceptual model will help us understand how guidance interoperates with the interactive visual data analysis. To this end, we will attach guidance- related components to an existing model that describes how visually driven data analysis leads to new knowledge. The basic model we will be using is van Wijk’s model of visualization [vWij06]. A slightly adapted variant of van Wijk’s model is shown in Figure 6.8. It combines the data transformation and knowledge generation models from Sections 2.3.2 and 2.3.3 in Chapter 2 and the human action cycle from Section 4.1.2 in Chapter 4 in a fairly simple and elegant way. Boxes represent artifacts, such as data or images, while circles represent functions that process some input and generate some output. According to the depicted model, visual data analysis works as follows. Analytic and visual methods transform (T) data [D] into images [I] based on somespecifications[S].Humansperceive(P)theimagesandcognitivelyextract the visually encoded information to accumulate more and more knowledge [K]. Based on their accumulated knowledge, users can interactively explore (E) the data by adjusting the specifications. As indicated by dK/dt, knowledge change occurs as a consequence of the interpretation of the visual representations and the exploratory adjustment of the specification dS/dt. Now let us attach the guidance-related components and see how they can help us to keep the knowledge generation loop going. The central component intheextendedmodelinFigure6.9istheguidancegenerationprocess(G*).It draws from different sources of input and computes different forms of guidance as output. Before any measures of guidance can be taken, the particular knowledge gap of the user must be known, which is indicated by the link
Advanced Concepts (cid:4) 283 DATA ANALYTIC AND VISUAL USER TRANSFORMATION D T I P dK/dt K Data Transf. Image Percept. Knowl. S dS/dt E D* Spec. Explor. Domain C* O* H* Prescribing Cues Options History G* Orienting Directing Guidance Input Output GUIDANCE Figure 6.9 Conceptual model of guided interactive visual data analysis. *Added artifacts and functions: domain conventions and models [D*], his- tory and provenance [H*], visual cues [C*], options and alternatives [O*], and guidance generation (G*). Adapted from [Cen+17]. between [K] and (G*). Further sources of input connect to (G*), including the original data [D], the visualization images [I] and the underlying specification [S], the interaction history or provenance [H*], and domain conventions or models [D*]. On the output side, guidance can be delivered in various ways and to different degrees. Basic guidance can show visual cues [C*] alongside the visualization to help users orient themselves. Directing users to promising analysis paths works by determining and offering options [O*] that, if chosen during the interactive exploration, lead to improved visualizations. Finally, the guidance mechanism can take over control and circumvent progress-hindering obstacles automatically by prescribing certain specifications [S] directly. Thesketchedconceptualmodelprovidesuswithablueprintofhowguidance methods are coupled with interactive visual data analysis. Next, we will introduce two examples that illustrate how guidance can be implemented based on the introduced conceptual model. 6.2.2 GuidingtheNavigationinHierarchicalGraphs The first example is about navigational guidance, which we already briefly covered in the introduction in Section 1.2.3. Now, we focus on an approach for guiding the navigation in hierarchical graphs [GST13]. Data, Analytic and Visual Transformation, Exploration A hierarchical graph is a regular graph on top of which a hierarchy defines a nested structure
284 (cid:4) Interactive Visual Data Analysis of clusters. Cuts through the hierarchy define views of the underlying graph at different levels of abstraction. The views correspond to regular plain graphs and as such can be visualized using standard node-link diagrams. Hierarchical graphs can be navigated vertically and horizontally. Vertical navigation changes the level of abstraction and with it the degree of detail shown in the visualization. Different abstractions can be created by expanding or collapsing clusters to include or exclude their individual nodes from the node-link diagram. Horizontal navigation relates to changing which part of the node-link diagram is visible on the screen. Zooming and panning are the typical operations to support horizontal navigation. KnowledgeGap Acomprehensiveexplorationofahierarchicalgraphusually requires numerous horizontal and vertical navigation steps. However, it is not always easy for users to decide on where they should continue the data exploration, which corresponds to a target unknown knowledge gap. Even if users have an idea of the data they want to inspect, it can be difficult to define an appropriate sequence of horizontal and vertical navigational steps to get to the desired target, which is obviously a path unknown knowledge gap. Guidance To assist users during the visual analysis of hierarchical graphs, it makes sense to offer guidance at two degrees. First, orienting guidance can indicate to the user where interesting nodes are located. Second, directing guidance can recommend and provide direct access to nodes that are most worth visiting next based on the current exploration situation. The guidance generation (G*) starts with a search for recommendation candidates. The search takes place in the neighborhood of the data currently being visible to ensure that the recommendations are indeed related to what theuserisseeinginthevisualization.Infact,thesearchinvolvesthreedifferent neighborhoods. The graph neighborhood relating to distances in the graph, the attribute neighborhood concerning similarities among the attribute values associated with nodes, and the visualization neighborhood as defined by node positions in the graph layout. While the graph neighborhood and the attribute neighborhood are given in [D], the visualization neighborhood takes into account [S] and [I]. Once a set of candidates has been collected, the next step is to select a few of them to be suggested to the user. This can be done by means of a degree of interest (DoI) function, as discussed in Section 5.2.1 in the previous chapter. TheDoIfunctionincludesseveralcomponents,suchasaprioridomaininterest given in [D*], user interest from [K], and the distance to the current view as specified in [S]. It further makes sense to model interest degradation for data that have already been visited as stored in [H*]. Finally, the candidates with the highest DoI are presented as navigation recommendations to the user. The visual design should follow a defensive strategy in order to only minimally interfere with regular data exploration.
Advanced Concepts (cid:4) 285 Potential navigation target Recommended navigation target On-screen Enriched Off-screen wedge Figure6.10 Navigation recommendations for graph visualization. Only when users have difficulties in determining a good next navigation target on their own should their attention shift to the navigation recommendations. Figure 6.10 utilizes the enriched wedges introduced in Section 4.5.3 in Chapter 4 to guide users. The wedges themselves serve as visual cues [C*] that indicate direction and distance of recommended targets for horizontal navigation.ThebarsinthewedgesvisualizethecomponentsoftheDoIfunction to make clear to the user why a target is recommended. Recommendedtargetsforverticalnavigationarebydefinitionnotcontained in the currently visualized graph cut, and hence they cannot be pointed at with an enriched wedge. Therefore, visual cues for vertical navigation recommendations are attached to anchor nodes whose expansion (or collapse) would make the recommended target visible. In Figure 6.10, mildly pulsing rings around anchors suggest that an expand (outward pulsing) or a collapse (inward pulsing) operation will uncover a target of interest. Both the enriched wedges and the pulsing rings serve a second purpose. Eachofthemisassociatedwithanavigationshortcuttobeusedasoption[O*] for the data exploration. If the user decides to follow a shortcut by clicking or taping on it, the cut through the hierarchical graph and the view on the graph layout are automatically set such that the associated target becomes visible. This enables the user to get to the target without traversing the path to it manually. Meanwhile, new navigation recommendations are prepared in the background based on the new analysis situation. Should further assistance be necessary, the system can suggest new targets at once. In summary, this first example of guided visual analysis illustrated how users can be assisted in making informed navigation decisions when exploring hierarchical graphs. Next, we will look at an approach for guiding the visual analysis of large heterogeneous data.
286 (cid:4) Interactive Visual Data Analysis 6.2.3 GuidingtheVisualAnalysisofHeterogeneousData The second example investigates how guidance can help physicians exploring heterogeneous biomedical data [Str+12]. The visual analysis is conducted to inform the treatment of newly diagnosed cancer patients. Data,AnalyticandVisualTransformation,Exploration Asillustratedin Figure6.11,thedatainvolvedinthetreatmentplanningarediverse.Physicians havetoconsultpatient-relateddatafromdifferentsources,includinganamnesis information, MR, CT, and X-ray images, tissue samples, and lab results. Moreover,generalbiomedicaldatahavetobeconsidered,includingproteinand gene expression data, pathways and published articles from medical databases. The visual analysis of such heterogeneous data naturally involves diverse analytic computations and different visual representations. For example, multi- variate patient data usually need to be filtered before they can be represented inatabularorparallelcoordinatesvisualization.Geneexpressionsaretypically clustered before being represented in dendrogram heatmaps. During the data analysis, physicians carry out different exploratory tasks inastep-by-stepfashion.Forexample,theybrowsepatientdatatofindsimilar cases,studyanamnesistranscriptstounderstanddiagnoses,andinspectrelated publications, gene expressions, and pathways to learn about latest research results. In the end, the objective is to determine the treatment for a patient. Knowledge Gap In the first place, it must be decided which particular part of the heterogeneous data is to be investigated to accomplish the task at hand. Second, suitable analytic and visual transformations must be employed to make the relevant information visible. Third, individual analysis steps must be sequenced properly to gain comprehensive insight into a case. Physicians who are not visualization experts might find these questions difficult to deal with, resulting in target unknown and path unknown knowledge gaps. Guidance The guidance generation (G*) relies on a tailored domain model [D*]. It is constructed by data analysis experts in an extensive modeling phase prior to the visual data analysis. The modeling starts with the individual subsets of the heterogeneous biomedical data. They are shown as larger boxes in Figure 6.11. Each part of the data is then annotated. The annotations indicate which analytical and visual tools can process certain data, and which tasks can be performed by inspecting them. Finally, links are established between the individual parts of the data to model workflows as sequences of analysis tasks. Each workflow forms the basis for a concrete analysis session that pursues a specific goal. The result of these efforts is a tailored domain model [D*], which will be the basis for guiding the visual analysis.
Advanced Concepts (cid:4) 287 Tissue Data Samples View tissue Lab Inspect lab results T CV Results View anamnesis X-ray DV TL DB Record treatment decision Images Anamnesis Determine patients I V CV Browse patients DV GV TL DB Patient Select patients MR Images Information Discard patients I V CV PC TL GV DB Inspect images Segment tumor Diseases CT Images (ICD-10) Search disease View disease description I V CV Protein Gene TL B DB Cluster expression data Expression Expression Inspect expression data Tools Filter expression data HHMM PPCC RR WW HM PC R W IImage View Find gene VVolume View Protein DB Gene DB View gene information TTissue View TLTable Lens (NCBI) (NCBI) Explore related pathways PCParallel Coordinates Find pathway GVGlyph View TL B DB TL B DB HMHeatmap Investigate related articles NLNode-link Diagram Pathways BBrowser Publications (KEGG, BioCarta) DVDocument View Tasks DBSQL Query (PubMed) NL B DB CVOpen CV RR Toolkit B DB WWEKA Toolkit Figure6.11 Tailored domain model as the basis for user guidance. Adapted from [Str+12]. Twotypesofguidancesupportthephysicians.Orienting guidance isoffered to help physicians keep track of the data that have already been explored and those data that are still to be examined. To this end, the current state of the analysis workflow and the data that have already been analyzed are indicated through visual cues [C*]. In addition, directing guidance recommends tasks to be accomplished next. The corresponding options [O*] are based on the current analysis path through the domain model. Altogether, the guidance provides physicians with information about where they are, what they have already done, and what they can do now. Figure 6.12 illustrates the described approach. A series of larger symbols at the bottom of the figure depict the analysis path taken. Possible options for continuing the analysis path are shown as smaller symbols in the bottom-right corner. The highlighted red option hints at the recommended next step. The actual visual analysis is carried out with the visual representations shown in the central part of the figure. The view for the current analysis step faces the user, whereas other views are tilted. Visual links assist the physicians in relating the information being displayed in the different views. Aswehaveseen,guidancecanbeassimpleassuggestingnavigationalsteps, but also as comprehensive as coordinating many different aspects, including views, methods, and tasks based on domain-specific workflow models.
288 (cid:4) Interactive Visual Data Analysis Figure6.12 Depictingtheanalysispath(bottom)andprovidingrecommended next steps (bottom-right) can guide the visual analysis of heterogeneous biomedical data. Reprinted from [Str+12]. We can conclude that guidance has the potential to improve the human- computer cooperation in interactive visual data analyses. Guidance has to be unobtrusive to the user, and adaptive to the particular context, as the type of assistance a user requires varies and depends on many factors. Good guidance provided in difficult analysis situations allows users to focus more on a deeper understandingoftheinterestingphenomenainthedata,ratherthanthemeans employed for acquiring the understanding. 6.3 PROGRESSIVE VISUAL DATA ANALYSIS In general, interactive visual data analysis depends on a smoothly running loopofhumananalyticalthinkingandsystem-generatedvisualrepresentations. Guidance, as described in the previous section, helps us keep the loop alive on the human side. Let us briefly return to van Wijk’s original model, but now depicted with a slightly different focus in Figure 6.13. We can see how knowledge [K] is generated step-by-step through perception (P), as indicated by dK/dt. Similarly, interactive exploration (E) changes the transformation specification [S] incrementally, which is denoted as dS/dt. Guidance aims to keep these human-oriented processes running. But we can also see that there is one more process participating in the loop. It is the system-oriented transformation (T) of data [D] into images
Advanced Concepts (cid:4) 289 DATA ANALYTIC AND VISUAL USER TRANSFORMATION ? dK/dt D T I P K Data Transf. Image Percept. Knowl. dS/dt S E Spec. Explor. Figure6.13 Incremental processes highlighted in van Wijk’s model of visual- ization. Adapted from [vWij06]. [I] based on the specification [S]. In van Wijk’s model, in our book so far, and in the vast majority of the literature in general, this transformation is considered to be a single, non-incremental step. A corresponding dI/dt is therefore missing. However, transforming large and complex data can require considerableprocessingtime,whichcanleadtoastutteringanalysisloop.This computational challenge of interactive visual data analysis can be addressed by introducing progressive means, which will be discussed in this section. So, in addition to human aspects, the system architecture is critical for a smooth interactive visual data analysis. Heer and Shneiderman make the following statement on the engineering of efficient visualization infrastruc- tures [HS12]: “Especiallyforlargedatasets,supportingreal-timeinteractivityrequirescare- ful attention to system design and poses important research challenges rangingfromlow-latencyarchitecturestointelligentsamplingandaggregation methods.” HeerandShneiderman,2012 Bringing in line the cost involved in generating visual representations with the need to present visual feedback immediately is a major technical chal- lenge. A straightforward monolithic implementation of the classic visualization pipeline might not be sufficient in this regard. Any analytical computation, mapping transformation, or graphical operation along the pipeline that fails to deliver results within interactive response time will disrupt fluid interactive analytic work [LH14]. What is needed is an architecture that can cope with complex, time- consuming computations, while still being able to react to interactive user requests and to provide rich visual feedback. One option to address this need is to implement what is called progressive visual data analysis. That is, the
290 (cid:4) Interactive Visual Data Analysis datatransformation(T)isrealizedinaprogressivefashion,whichconceptually replaces the question mark in Figure 6.13 with the missing dI/dt. Next, we introduce the basics of progressive visual data analysis. First, we consider fundamental conceptual aspects. Then we outline a multi-threading architecture for implementing progressive solutions. Finally, we discuss key scenarios where progressive means can enhance the visual data analysis. 6.3.1 ConceptualConsiderations The key idea behind progressive visual analysis is to generate partial visu- alization results of increasing completeness and correctness [SPG14]. There are two options for breaking down time-consuming computations on large data [Sch+16]: • Subdivide computations into smaller steps. • Subdivide data into smaller chunks. Performing calculations on smaller chunks of data or in smaller computa- tional steps has three key advantages with respect to: • responsiveness of the system, • transparency of the involved calculations, and • control of the visual analysis. Taking smaller steps on smaller data chunks allows a system to be respon- sive to interaction requests of the user. Long-running tasks, which block the system, are avoided and the latency between user request and system response isreduced.Showingpartialresultsinaprogressivefashionalsofacilitatestrans- parency.Particularlyanalyticalmethodsoftencomeasblack boxes whoseinner workingsremainundisclosedtotheuser.Throughprogressiveapproaches,users can observe and thus better understand how analytical calculations converge to a final result. Responsiveness and transparency together improve control of the overall analytic process. Already before the final result is produced, users cansteertheanalyticprocess,forexample,byprioritizingregionsofinterestor stopping calculations whose partial results did not yield any fruitful insights. ModelingtheSubdivisionofComputationsandData In the following, we describe how subdivisions of computations and data can be modeled conceptually [Sch+16]. To this end, we build upon the data transformation as introduced in Section 2.3 of Chapter 2. Originally, we described the data transformation as a pipeline of monolithic operators that carry out computations on the entire data and pass on a single result to subsequent operators. Now, for progressive visual data analysis, the notion of operators and transitions between them needs to be refined to accommodate smaller computational steps on chunks of data.
Advanced Concepts (cid:4) 291 Monolithic Monolithic Data Data operator transition chunking buffering Progressive Progressive Increasing Reducing operator transition granularity granularity Figure6.14 Extended notation for operators and transitions for progressive visual data analysis. Adapted from [Sch+16]. Progressive Operators Progressive operators realize a sub-division of the computational process. To make this clear, the operator notation is extended to include small marks that represent the generated results. As illustrated in Figure 6.14, a monolithic operator generates only a single, complete result. On the other hand, a progressive operator can generate multiple partial results of increasing quality, indicated by increasingly darker shades of gray, until the complete result is produced. How many results can theoretically be produced is operator-dependent. A prerequisite for more than a single complete result is that the underlying algorithms are able to produce results incrementally. Often, classic implemen- tations of known algorithms do not have this property. For example, a sorting algorithm usually does not provide intermediate results. Therefore, it can be necessary to utilize or develop adapted variants that fulfill said requirement. How many partial results should sensibly be produced is user-dependent, because different analytic tasks require different update rates. For example, it may be more helpful to show only those partial results that differ at least by a given ∆ from the previously shown one, rather than showing each and every possible partial result. In any case, each partial result must be a valid input to the subsequent operatorthatfollowsdownthetransformationpipeline.Thepassingonofdata and results is modeled via progressive transitions. Progressive Transitions For the classic pipeline, we have not talked about transitions between operators per se, because they trivially transfer the full data. Yet, for progressive visual data analysis, transitions deserve special attention, because they model the subdivision of the data. In Figure 6.14, a progressive transition can be recognized by a series of marks along its arrow line. These marks denote the data flow to be a stream of data chunks. The distance between the marks indicates the size of the chunks. The smaller the space between consecutive marks, the smaller is the chunk size. To create data chunks in the first place, a transition for data chunking is required. There are different strategies for chunking the data: • Incremental Chunking. The data are sub-divided into disjoint subsets. Eachsubsetcontainsasampledversionoftheoriginaldata,andtheunion
292 (cid:4) Interactive Visual Data Analysis of all subsets represents the entire dataset. Ideally, incremental chunking leads to the key features of the data being visible in the progressive visualization early. The challenge is to find a suitable sampling strategy and an appropriate order for processing the chunks. • Semantic Chunking. The subdivision is based on the semantics of the data and the visualization. For example, for a geographic visualization of movement data, it makes sense to deal separately with geographic boundaries, streets, map tiles, and movement trajectories. Small and relevant semantic chunks should be processed early, whereas larger and less-relevant chunks can be queued in the data stream later. • Level-of-Detail Chunking. The data are sub-divided at different levels of granularity. The result is a hierarchy of chunks, where the root node representstheentiredatasetatahighlevelofabstraction,andtheleaves represent smaller subsets with details at finer granularity. Level-of-detail chunking can be done with respect to the data space or the view space, forexample,byhierarchicaldataabstractionormulti-resolutionmethods, respectively. The outlined chunking strategies can also be applied in combination. For example,fortheprogressivevisualizationofalargegraph,itcanmakesenseto create a graph hierarchy where each chunk represents the graph at a different level of abstraction. Per chunk in the hierarchy, nodes and edges can be organized in separate semantic chunks. These can further be sampled into subsets for incremental chunking. The conceptual counterpart of data chunking is data buffering. It accu- mulates data chunks or partial results to produce the full data or complete results. For example, visualization images can work with buffering in that they accumulate several partial results of an upstream operator before showing a meaningful or complete result. To make progressive operators compatible in terms of the granularity of data chunks, it also makes sense to introduce transitions that increase or decrease data granularity along the transformation pipeline. Such granularity changes can be necessary, for example, when one operator requires larger data chunks to create partial results, while a subsequent operator is only able to process smaller pieces of data. DesigningProgressiveVisualDataAnalysis With the introduced notations, we can now design a progressive analytical and visual transformation of data into images. Figure 6.15 shows a simple example pipeline [Sch+16]. First, the data are sequenced into chunks, each of them is analytically processed by a monolithic similarity search. The partial results of thesearcharethenmappedtoascatterplot.Inordertoproduceadensitymap, the partial scatter plots are further sub-divided into smaller chunks. Then,
Advanced Concepts (cid:4) 293 D I Data Image Similarity search Scatter plot Density map Figure6.15 Simpleexampleofaprogressivetransformationpipeline.Adapted from [Sch+16]. the density map operator generates partial results with increasing quality. Finally,thevisualizationimagebuffersallpartialresultsintoacompletevisual representation of the original data. The design process of such progressive visualization pipelines follows the generaldesignstepsforregularvisualizationsolutionsasdescribedinSection2.3 of Chapter 2. Still, there are some peculiarities that deserve special attention. Meaningful Partial Results First and foremost, a key requirement of pro- gressive visual data analysis must be kept in mind: The employed progressive means should generate meaningful partial results [Sch+16]. That is, the par- tial results should be interpretable and lead to valid partial insight. What meaningful means concretely depends on the application at hand. Oneoptiontoobjectivelyquantifymeaningfulnessaremetricsthatmeasure the quality of partial results. Quality metrics can be calculated with respect to the data or their visual representation. A very simple quality metric is the ratio of the already processed data. The more data have been processed, the higher is the quality. For another example, if a progressive computation involves some statistical error, for example due to the employed data chunking, the error could be used as a quality indicator. Also the differences between successive partial visual representations can indicate result quality. If the difference is rather low, not much new insight can be expected. On the other hand, a significant change between two visual representations could mark the beginning of a progressively unfolding visual pattern. Progress, error, and differences are valuable indicators that can support users in judging the partial results and steering the analysis. Therefore, design- ers of progressive visual data analysis solutions should incorporate suitable quality metrics and consider visualizing them along with the actual data. Progress bars should be a mandatory element in the visualization interface. DesignPerspectives Anotheraspectofthedesignistodecidewhereandhow operators and transitions should employ progressive means. These decisions have to take two perspectives into account: the input perspective and the output perspective. Input Perspective From an input perspective, the data and the involved transformation calculations are relevant. If the data do not fit into the memory, progressive data chunking needs to be considered. If, on the
294 (cid:4) Interactive Visual Data Analysis other hand, the data are highly structured, it might be rather difficult to come up with a reasonable sub-division strategy for the data. In terms of computations,iftheruntimeofanoperatorisratherlong,itmakessense to replace it with a progressive alternative. Yet, if a time-consuming operatorproducesmerelyoptionaldecorations,itmaybemoresensibleto leave the operator as is, because it is acceptable to wait for the operator to finish in the few cases this is indeed needed. Output Perspective From the output perspective, the tasks and interests of the analyst are relevant. The designer needs to know whether the analysis follows an overview-first strategy or a detail-first strategy. With an overview-first strategy, the goal is to show the complete data as quickly as possible, where a lower degree of detail is initially acceptable. Following this strategy means progressively presenting data abstractions with increasing granularity. First, a coarse overview is displayed based on which the user can start interpreting the data. Looking into regions of interest and studying specific details becomes possible as more and finer-grained information is being transmitted progressively. In contrast to that, the detail-first strategy prioritizes highly detailed representations, where it is acceptable to see only subsets of the data in the beginning. The detail-first strategy progressively visualizes chunks of theoriginaldata,ratherthandataabstractions.Oncethefirstchunkhas been transmitted, a detailed view of a specific piece of data is presented to the user. During the progression, more and more pieces will be added until the visualization shows the complete data. Note, however, that the visibility of details is gradually diminished the more data chunks are being presented. In summary, this section described the basic building blocks of progres- sive visual data analysis and the design requirements and perspectives to be considered. Yet, these conceptual aspects are only one part of progressive visualdataanalysis.Anotherimportantpracticalquestionishowtoimplement the underlying progressive software. This question will be addressed in the following. 6.3.2 Multi-threadingArchitecture Progressive visual data analysis software must be able to generate and show partial results while still being responsive to user requests. Therefore, it is practical to follow a multi-threading architecture [Pir+09]. A schematic outline of such an architecture is given in Figure 6.16. It consists of the data, the specification, and the image artifacts, which we already know from earlier figures. Additionally, there are two central computing components: the control thread and the processing threads.
Advanced Concepts (cid:4) 295 D Access Processing Update III Data Threads ImImI amagagegee HUMAN Start Invalidate Access Terminate S Modify Control Spec. Thread Figure6.16 Amulti-threadingarchitectureforprogressivevisualdataanalysis. Control Thread The control thread is in charge of receiving user input and coordinating the other components. As these tasks are usually easy to accomplish, the overall system stays responsive at all times. The typical workflowofthecontrolthreadisasfollows.Whenaninteractionrequestarrives, the control thread modifies the specification and invalidates the visualization. Additionally, any processing threads that operate with the old specification are terminated early because they would no longer generate valid, but obsolete results.Finally,newprocessingthreadsarestartedtogeneratenewvalidvisual output. Start Process Thread #1 Thread #2 Thread #3 Refine Transmit Figure6.17 Illustrationofasynchronousprocessingthreadsoperatingondata chunks stored in priority queues. Processing Threads Theprocessingthreadsimplementtheprogressiveoper- ators introduced before. As illustrated in Figure 6.17, these threads have a priority queue of data chunks to be processed. When incoming data chunks are appended to the queue, the processing thread starts working. It removes a data chunk from the queue and processes it in an internal loop until a given break condition is reached. This condition can be based on the processing time spent or on the quality of the partial result. If the partial result is of sufficient quality, it can be transmitted to the subsequent thread’s priority queue. If the result is not yet good enough, it is re-appended to the thread’s own queue for further refinement. The different shades of gray in Figure 6.17 indicate how close the queued chunks are to the final result. These shades correspond directly to the marks in the progressive operators, where white stands for unprocessed chunks, gray for intermediate results, and black for the final result.
296 (cid:4) Interactive Visual Data Analysis Human Computer Human Control Processing Request Request Partial result Partial result Partial result Interaction latency Result Complete result Feedback latency (a)Regular single-thread solution. (b)Progressive multi-thread solution. Figure6.18 Comparison of single-thread and multi-thread solutions. Assoonasmeaningfulresultsreachtheendofthepipeline,thevisualization gets updated. Here it makes sense to organize the visual output into multiple layers according to the different data chunking strategies (semantic layers, incremental layers, or level-of-detail layers). Using multiple layers enables the architecture to provide rich and scalable visual feedback, and to avoid redundant computations by reusing cached results that remain valid after a user interaction. If a processing thread’s queue is empty, the thread stops working. A processing thread can also be terminated early at any time by the control thread, either by emptying its queue or by dismissing thread and queue altogether. A key advantage of the described multi-threading architecture is the reduc- tion of latency by generating partial results. This is made clear in Figure 6.18. The classic monolithic approach is depicted in Figure 6.18a. As indicated by the colored vertical bars, the human has to wait for a while until a single complete response is generated. While the system is busy producing the com- plete response, the system is unresponsive and no feedback about the ongoing computations is delivered to the user. InFigure6.18b,wecanseehowtheprogressivemulti-threadingarchitecture reduces the latency. The interaction latency is much shorter and the system remains responsive at all times, because the control thread handles only brief coordinationtasks.Theprocessingthreadworksonthedatainthebackground andprovidesmeaningfulpartialresultsassoonastheyareready,whichreduces the feedback latency. Up to this point, we have discussed how progressive visual data analysis can be modeled, designed, and implemented. The next section will illustrate how progressive means can be put to use to support a smooth interactive visual data analysis.
Advanced Concepts (cid:4) 297 Preprocessing Mapping Rendering Operators Operators Operators Data Analytical Visual Image Values Abstractions Abstractions Data Value Analytical Visual Image Operators Operators Operators Operators Progressive Progressive Progressive data processing visualization display Figure6.19 The three typical scenarios of progressive visual data analysis: progressive data processing, progressive visualization, and progressive display. 6.3.3 Scenarios Operating progressively is a general approach and any phase of the analytical andvisualdatatransformationcanbenefitfromit.Dependingonwhichphases primarily utilize progressive means, three typical scenarios can be identified: progressive data processing, progressive visualization, and progressive display. Figure 6.19 illustrates the three scenarios and their relation to the phases of the visualization pipeline. Progressive data processing subsumes all data chunkingandcomputationaloperatorsthatworkondatavaluesandanalytical abstractions. Progressive visualization covers chunking and operators that are used to create and manipulate visual abstractions. Finally, progressive display deals with chunking and operators at the level of image data. Next, we discuss the three scenarios of progressive visual data analysis in more detail. ProgressiveDataProcessing Given the fact that progressive data processing takes place in the beginning of the analytical and visual transformation, it is primarily concerned with the input perspective, that is, the properties of the data and the calculations. Progressive data processing is typically applied to cope with two challenging situations. First, if the data are too large to be processed in a timely manner all at once, progressive data processing can provide early feedback that allows users to see in which direction the calculations are going, and if necessary, to steer the calculations toward more desirable results. Second, if the data processing involves algorithmic black boxes or the data are unknown, looking at progressively generated partial results can help users better understand both the data and the operations performed on them. Progressive value operators, preprocessing operators, and analytical opera- tors aim at an efficient and effective handling of the data by sub-dividing the involved computations. An example of a progressive value operator is adaptive
298 (cid:4) Interactive Visual Data Analysis sampling. Adaptive sampling delivers data samples successively and allows userstoadaptthesamplingstrategyonthefly.Thisway,therepresentativeness of data being analyzed can be improved based on insight gained from early partial results. Progressive preprocessing operators transform data values into analytical abstractions in a step-by-step fashion. Chapter 5 discussed several fundamen- tal methods for creating analytical abstractions. Many of them can also be employed in a progressive setting. For example, if the data to be analyzed do not fit into memory, a principal component analysis can be done progressively. In particular, the involved singular value decomposition can be computed in incremental steps [Sar+02]. For another example, if a k-means clustering is part of the analysis pipeline, but users are unfamiliar with the data and hence do not know what a suitable k is, it makes sense to employ iterative clustering [Kim+17]. It allows users to adjust the number of clusters or the positionofclusterrepresentativesontheflywhilealreadyexaminingthepartial clusters being produced. Progressive analytical operators are concerned with computations on the derived analytical abstractions. This can involve structuring, organizing, and prioritizing the abstractions. For example, data abstractions can be organized in hierarchical data structures. When traversing such data structures, it is sensible to prioritize data chunks representing larger data deviations early. As we know from Section 5.3.2 of Chapter 5, larger deviations tend to add more information to the visualization than chunks with only smaller changes. The progressive operators described so far sub-divide the computations on data. Yet, with very large data, it is hardly possible to deliver results in interactive frame rates. Therefore, a second key concern of progressive data processing is to sub-divide the data into chunks using a suitable strategy (incremental chunking, semantic chunking, or level-of-detail chunking). The following example will briefly illustrate the positive effect that chunking can have on the visual data analysis. The example concentrates on the first part of the progressive pipeline presented earlier in Figure 6.15. The dataset to be processed comes from the Fatality Analysis Reporting System of the US National Highway Traffic Safety Administration and contains information about more than 370,000 car crashes between 2001 and 2009. When analyzing the crashes with respect to certain criteria, a similarity search must be performed on the data. Yet, searching the whole dataset is computationally expensive. Therefore, the search operates on chunks of 5,000 crashes instead. This ensures reasonably quick feedback for interactive adjustments [Sch+16]. Figure6.20aillustrateshowthechunkedsearchprogressivelyfillsthedisplay withcrashesmatchingthesearchcriteria.Duringtheprocess,userscanupdate the criteria as needed. Moreover, they can define a region of interest in order to prioritize the data chunks associated with that region. Such a prioritized progression is illustrated in Figure 6.20b. The effectiveness of the visual data analysis is increased because interesting data can be inspected early.
Advanced Concepts (cid:4) 299 First First reliable Reliable Mature Final partial result partial result partial result partial result result 20 Iterations 120 Iterations 180 Iterations 220 Iterations (a)Regular progression of chunks. First User Reliable Mature Final partial result selection partial result partial result result 20 Iterations 34 Iterations 57 Iterations 88 Iterations (b)Prioritized progression of chunks. Figure6.20 Visualizationofprogressivelyprocesseddatachunksofcarcrashes from a database with more than 370,000 entries. Courtesy of Marco Angelini. This section provided just a glimpse on what is possible in terms of pro- gressive data processing. Next, we will discuss the scenario of progressive visualization. ProgressiveVisualization Progressive visualization is primarily concerned with the graphically oriented transformation steps that map data to visual abstractions and render them to visualization images. Progressive visualization is typically applied when the amount of data or visual abstractions is excessively large, or if the involved mapping and rendering operations are computationally expensive. In such cases, it is usually not feasible to process everything in a single step. Even if it was, the visualization could be too dense for the user to interpret it with ease. Progressive visualizations utilize chunking and progressive computations to buildupvisual representationsstep-by-step.Theindividual steps arecomputa- tionallyeasiertodealwithandtheuserhasachancetoobservetheconstruction process and hence can gain a better understanding of the visualization design and the data. Picking up on the output strategies overview-first and detail-first discussed earlier, we will next present two examples that illustrate progressive visualiza- tion. Progressive Force-directed Graph Layout As we know from Section 3.5 of Chapter 3, node-link diagrams are a common representation for graphs. Nodes are basically represented as dots and edges are shown as links between
300 (cid:4) Interactive Visual Data Analysis ^ 1 Iteration 5 Iterations 10 Iterations 50 Iterations 100 Iterations Figure 6.21 Progressive force-directed layout of a social network with 747 nodes and 60,050 edges. dots. These visual abstractions are easy to create. Yet, a key challenge is to determine a suitable layout of the dots. This requires sophisticated graph layout algorithms, which are usually computationally expensive. For our example, we utilize the force-directed layout approach, which is known to generate visually pleasing results and also to have a high runtime complexity due to the involved simulation of the forces. Yet, as the simulation is iterative by nature, it perfectly fits the scenario of progressive visualization. Followingtheoverview-firststrategy,theprogressionstartswitharoughlayout thatcontainstheentiregraph.Witheachiterationofthesimulation,thelayout is refined until a high-quality output is obtained. High quality means that the simulated forces are in equilibrium. Figure 6.21 shows a series of snapshots of a progressively unfolding node- link diagram of a social network with 747 nodes and 60,050 edges. One can nicely observe how the layout algorithm separates the key structures in the data between the 5th and 50th iteration. ProgressiveNetworkMapping Oursecondexamplewillillustratethedetail- first strategy applied to a climate network with 6,816 nodes and 232,940 edges. Climate researchers use such networks to study climate change on Earth. The nodes of the network span a grid across the globe, so there is no need to compute a graph layout. Yet, the number of visual abstractions is relatively high. For our example, we visualize the climate network on a 3D globe. Each node is represented as a 3D sphere and each edge is shown as a 3D curved link. Creating these 3D visual abstractions poses significant stress on the graphics hardware. Therefore, it makes sense to process the data in chunks. First, semantic chunking is applied: The nodes form one chunk and the edges form
Advanced Concepts (cid:4) 301 Nodes being added Edges being added Figure6.22 Progressive visualization of a climate network with about 6,816 nodes and 232,940 edges. another chunk. Because there are still very many nodes and edges per chunk, an additional sub-division by incremental chunking takes place. Eventually, we obtainsevenchunksofnodesand233chunksofedges,eachholdingathousand dataelements.Theprogressivevisualizationwillprocessthesechunksoneafter the other. The result is depicted in Figure 6.22. According to the detail-first strategy, the progression starts with full details, but only for a part of the data. The node chunks come first followed by the chunks of edges. The first pieces of data are presented to the user after about five seconds. Without progressive chunking, the user would see a blank screen for about 40 seconds. The above examples illustrate quite well how progressive visualizations graduallyimprovethevisualoutputbeforetheyultimatelyendinarefinedand completefinalresult.Thegradualbuild-upofvisualrepresentationscontributes to a better understanding of the visualization and the data. ProgressiveDisplay Lastbutnotleast,theprogressivedisplayofvisualrepresentationsisascenario where progressive techniques are employed to support the visual data analysis. Thisscenariobringsusclosertomethodsforprogressiveencoding,transmission, and decoding of image data, which have a long history in computer graphics research. Progressive display techniques are particularly interesting in environments with limited connection bandwidth and in environments with heterogeneous outputdevices,suchasthesmartmeetingroomdescribedinSection6.1.Based on a well-prepared stream of image data, progressive techniques can ensure that important image characteristics are transmitted first, whereas additional
302 (cid:4) Interactive Visual Data Analysis details are progressively fetched to complete the view. Moreover, individual output devices can acquire exactly the amount of image data they need for their particular size and resolution. Smaller low-resolution devices can stop the transmission early, whereas larger high-resolution displays extract the full pixel information from the data stream [Ros+11]. Figure6.23illustratessuchadevice-dependentprogressivetransmissionofa treemapvisualization.Thetreemapistobedisplayedonheterogeneousdisplays inasmartmeetingroom.Notethatthereisonlyonestaticvisualizationimage. Theprogressiveimageviewerrunningoneachdisplaywillextractandtransmit exactly the pixels that match the device resolution. Progressive display is not limited to device-dependent adaptations. It is also possible to display visualization images progressively based on user interests [Rad+15]. For example, when a discussion in a smart meeting room shifts from a global perspective to local areas, the progressive display can provide further image data allowing users to see the areas of interest at higher resolution. Figure 6.24 shows an example with a color-coded map of the world. Once theuserhasmarkedaregionofinterest,additionalimagedataareprogressively transmitted to enhance the visualization. It will then be shown either with an overview+detail approach or as a focus+context display. Both overview+detail andfocus+contexthavealreadybeenintroducedasfundamentalvisualarrange- ments in Section 3.1.2 of Chapter 3. Note again that the progressive display operates only on the visualization image. There is no need to reprocess the entire dataset in order to obtain interest-dependent views of it. Whenever user interests change, new regions of interest can be marked and additional image data is progressively transferred accordingly. Technically, a key requirement is that the visualization images be encoded in such a way that progressive image transmission and refinement are possible. ProgressiveJPEG,forexample,canpresentanoverviewoftheimageandthen refine the image successively. The JPEG2000 standard adds support for the Image Progressive Heterogeneous File Transm ission Display Devices Figure6.23 Device-dependent progressive transmission of a treemap visual- ization image. Adapted from [Ros+11].
Advanced Concepts (cid:4) 303 (a)Global view. (b)Overview+detail view. (c)Focus+context view. Figure6.24 Progressive display for a dynamically defined region of interest. Courtesy of Axel Radloff-Delosea. definition and prioritized transmission of regions of interest. The two examples of the device-dependent treemap visualization and interest-dependent map visualization are based on JPEG2000-encoded image data [RS09]. With the three scenarios of progressive data processing, progressive visual- ization, and finally progressive display, this section on progressive techniques for interactive visual data analysis is complete. Overall, allowing fluid inter- action and providing timely feedback are important aspects for visual data analysis. Studies provide first evidence of the positive impact of progressive techniques [Zgr+17]. Users can generate more insights per minute and are more agile in their interactions when working with progressive visualizations. Meaningful partial results can be employed to pre-process information, extract insights early, or decide quickly that an alternative visualization could be more useful for the task at hand. All these benefits suggest that progressive visual data analysis is an important approach when exploring large datasets. 6.4 SUMMARY Research on interactive visual data analysis is advancing on various fronts. In this chapter, we discussed a selection of three advanced topics: advanced visualization in multi-display environments, guidance as advanced support for interactive analysis, and progressive techniques for advanced processing of analytical and visual data abstractions. It is in the nature of such advanced topics that they have typically not yet been fully investigated in research. There are still open issues to be discussed and remaining challenges to be addressed. Recent works on multi-device visualization further study the seamless integration of multiple devices to form coherent visual analysis environ- ments [Kis+17; LKD19; Hor+19]. Interesting questions are how to best utilize displays of different form factor and their different interaction modalities, how to support collaborative exploration in dynamic multi-devices ensem- bles, and how to technically cope with a heterogeneous zoo of platforms and infrastructures.
304 (cid:4) Interactive Visual Data Analysis Withrespecttoguidanceforinteractivevisualdataanalysis,moreresearch is needed on designing and implementing actual guidance solutions [Cen+17]. This involves questions such as: how can knowledge gaps and the required degree of guidance be inferred, when is the right moment to provide guidance, what are suitable methods to convey guidance to the user, and how can the success or failure of guidance be evaluated? As indicated in the previous section, progressive techniques bear much potential to enhance interactive visual data analysis. Yet, research on this topic is still young and there are many questions to be investigated [Fek+19]. Already the conceptual definition and fundamental theories behind progressive techniques are subject to discussions. Moreover, the benefits and also threats of progressive techniques need to be studied in more depth from a human perspective. As a conclusion, we can state that the advanced methods discussed in this chapter illustrate quite nicely how data analysis can go beyond plain visualization, interaction, and computation. Yet, it is also clear that these advanced methods are not yet mature and that it will certainly need some time before they appear in regular data analysis solutions for day-to-day use. FURTHER READING Multi-display Visualization: [CD04] • [Chu+15] • [Rad+15] • [EST19] Guidance: [Hor99] • [Cen+17] • [Col+18] ProgressiveAnalysis: [Fis+12]•[Müh+14]•[SPG14]•[BEF17]•[Ang+18]
7 CHAPTER Summary CONTENTS 7.1 What’s been Discussed ..................................... 305 7.2 How to Continue ............................................ 307 T HIS BOOK on interactive visual data analysis comes to an end. We workedthroughsixchaptersonvisual,interactive,andanalyticalmethods for making sense of data. In this final chapter, we will briefly recapitulate the key points of the book and provide you with ideas on how to continue once you put this book down. 7.1 WHAT’S BEEN DISCUSSED In Chapter 2, we introduced fundamental aspects. In the first place, there are the three key criteria of interactive visual data analysis: expressiveness, effectiveness, and efficiency. Expressiveness tells us that visual representations should show us exactly what is in the data and that interaction should allow us to do exactly what is needed to understand the data. Effectiveness calls for the consideration of human factors. Efficiency is satisfied when the costs and the benefits of the interactive visual analysis are balanced. Further, we discussed the data, the tasks, and the context as three key influencingfactorsofinteractivevisualdataanalysis.Whilethedataproperties characterizewhat istobeanalyzed,thetaskaspectisconcernedwithwhy data are analyzed. The context, describes by whom and where the data analysis is carried out. Finally, we studied models of the processes that take place at different stages of interactive visual data analysis. We considered the design process as a cascade of individual design steps, the process of transforming the data into visual representations as a pipeline of operators, and the knowledge generation process as analysis loops that produce new findings and insight. Chapter 3 was dedicated to visualization. We started with basic methods forencodingdatavisuallyandpresentingthemtotheuserinameaningfulway. Visual variables such as position, length, or color hue were introduced as the fundamental means to convey information graphically. In fact, visual variables 305
306 (cid:4) Interactive Visual Data Analysis modify the appearance of graphical marks, which allows us to perceive visual differences and interpret the depicted information. Visual variables and marks are the basic building blocks of visualiza- tion techniques. We introduced many different techniques for different data classes. For multivariate data, we described table-based, combinded bivariate, polyline-based, glyph-based, pixel-based, and nested visualizations. They can be considered the basic tools of the trade. Then we moved on to visualization techniques for temporal data and geo-spatial data, and combinations thereof. We characterized time and geographic space as special dimensions and pre- sentedvariousdedicatedvisualizationstakingintoaccountthespecificsoftime and space. Last but not least, our interest concerned the visualization of graph data. Graphs describe not only data elements per se, but also relations between data elements. Basic graph visualization techniques show these relations as linksbetweenelementsor viacleverarrangementsof elements. Wealsolearned how multi-faceted graphs can be visualized by combining ideas from different visualization techniques. Following the chapter on visualization was Chapter 4 on interacting with visualizations. We begun the chapter with a description of various scenarios where it is useful or even necessary to let the user interact. What “interacting” actually means was abstractly modeled as a cycle of actions. In order to allow this cycle to run smoothly, several requirements and guidelines need to be considered. In terms of interaction for visual data analysis, we built upon basic oper- ations and discussed fundamental selection and accentuation techniques. To address the analysis of large data, a whole section was dedicated to navigating zoomable visualizations. Zooming is a fundamental technique to satisfy the need for an overview of the data and also for finer details of selected parts of the data. Interactive lenses were introduced as a versatile lightweight means for locally adapting visual representations of data. The potential of naturally inspiredinteractionwasillustratedfortheimportanttaskofvisualcomparison. Leaving classic desktop interaction behind, we moved on to modern inter- action technologies and learned how they can enhance visual data analysis. Touch interaction makes working with visual representations of data truly direct. Tangible views provide an extended interaction vocabulary making it possible to analyze data in new and interesting ways. Finally, with proxemic interaction it is possible to explore more and larger data on high-resolution display walls. Chapter 5 taught us that large and complex data also require analytical support through automatic computation. The primary goal was to reduce the complexity of the data and their visual representations. Density-based representationsandbundlingwereintroducedasmethodstoreducecomplexity on the visual side. Reducing the complexity of the data can be done in various ways. We considereddegree-of-interestapproachesandfeature-basedmethodsasameans
Summary (cid:4) 307 to focus on relevant data. Another option is to abstract the data via sampling and aggregation. Data abstraction can be done repeatedly to generate multi- scale data abstractions. Automatic computations can also serve to group data elements. We intro- duced two fundamental approaches: classification and clustering. Classification is a means to sub-divide the data space, and clustering groups data elements according to a certain similarity measure. How complex multivariate dynamic graphs can be clustered was discussed in a separate section. Finally, we described principal component analysis as a tool to address the challengeofanalyzingdatawithverymanyvariables.Thekeyideaherewasto project the original high-dimensional data space to a lower-dimensional space. In the end, Chapter 6 provided an outlook on advanced topics in inter- active visual data analysis research. Following the three-fold structure of the previous chapters, we discussed advanced visualization, advanced interaction, and advanced automatic computation. Presenting data in smart multi-display environments can be considered an advanced form of visualization. This makes it possible for multiple users to analyze data collaboratively. We also saw that several mechanisms must be integrated into the environment to support the data analysis. Userguidance was introduced asanadvancedmeansto supportinteraction when users have difficulties in making analytical progress. We elaborated on a characterization of guidance and a conceptual model. Our last advanced concept considered the use of progressive methods to enhance automatic computation. We made a step from otherwise monolithic operators that generate a single complete result to progressive operators that generate several partial results with increasing quality. 7.2 HOW TO CONTINUE Now that this book is over, we want to point you to ideas of how you can continue with the topic of interactive visual data analysis. We think of four key avenues for your next activities: learn, apply, create, and advance. Learn A first activity could be to go on and study interactive visual data analysisfurther.Certainly,thisbookofferedabroadoverview,butitcouldnot discuss all aspects in full detail. Therefore, each chapter ended with a list of references recommended for further reading. The lists contain excellent books that cover topics we could only touch here briefly. We also included research articles with in-depth investigations of selected topics of interest. Weparticularlyrecommendlearningmoreabouthumanfactors.Perception and cognition are key to interactive visual data analysis. Learning more about them will definitely pay off. Another direction for further studies are the fields of volume visualization and flow visualization. They are concerned with
308 (cid:4) Interactive Visual Data Analysis potentially time-varying 3D volumetric data and vector fields. Such data are particularly relevant in medicine and engineering. Apply If you are a practitioner and have not yet used interactive visual methods,yournextstepmightbetoapplysomeofthemethodsandtechniques discussed in this book to your analytic domain problems. Interactive visual data analysis can be beneficial in all application domains where data are available, which means in virtually any domain. In the financial sector, visual representations can help us understand transactionsordetectfraud.Inurbanplanning,theycanhelpusbuildmodelsof humanmobility.Climateresearchersrelyonvisualrepresentationtoinvestigate the impact of climate change and to communicate their findings to the general public. In humanities, interactive visual data analysis can be applied to reveal hidden relationships in large text corpora. Sociologists can be supported in predicting election outcomes based on visual representations of stance in social media. Even the classic spreadsheet applications already offer basic visualization capabilities. Tableau, QlikView, and Plotly are flagships among the software packages for interactive visual data analysis in general. Many other dedicated tools exist for specific application problems. For example, Gephi, Cytoscape, and Tulip are dedicated to the visual analysis of graphs. KNIME is a software that focuses on the analytics part. As you can see, there are many options to apply the knowledge gained in this book. It is up to you to introduce or strengthen interactive visual data analysis in your particular application domain. Create If you are intrigued by the examples presented in this book or if the existing tools do not meet the requirements of your application problem, go ahead and create your own interactive visual analysis techniques. Depending on your level of expertise, you can extend the software mentioned before or implement a tailored visualization solution from scratch. Readerswithprogrammingskillscanresorttoopensourcesoftwarelibraries. Forexample,D3.jsisthequasi-standardforweb-basedvisualization.Thefollow- up project Observable even allows multiple people to develop visualization in a team effort. Many other visualization toolkits and libraries exist, for example, the Visualization Toolkit (VTK) and Processing for general-purpose applications, GraphStream and sigma.js specifically for visualizing graphs, or mapbox and CesiumJS for geo-visualization. Writing software for visualizing larger data typically requires advanced programming skills. Analytic computations should be distributed on multiple computing threads and the visualization should utilize the enormous power of moderngraphicscards.WorkingonsuchalowlevelwithAPIssuchasOpenMP, OpenCL, OpenGL, WebGL, DirectX, or Vulkan might be more difficult, but it will allow you to visualize millions of data elements at interactive frame rates.
Summary (cid:4) 309 Advance Interactivevisualdataanalysisisarelativelyyoungfieldofresearch. So your next step could be to advance the field. There are still many unsolved problems to be tackled and new discoveries to be made. Visual data analysis has found its way into smart multi-display environments. And it will be used in many different environments, including large display walls, industrial settings,ormobilesmartwatches.Howvisualization,interaction,andautomatic computation can be designed to scale with these heterogeneous environments is an open question. Dealing with ever-increasing data remains a major challenge. Progressive data analysis has been discussed as a suitable means to address this challenge. But we are still lacking the concepts and models to broadly apply progression beyond the simple examples illustrated in this book. Increasing data size and increasing data complexity also lead to more complex tools. How can we support users in handling these tools? Providing user guidance is one option. However, when is the right moment to provide guidance and what is the appropriate amount of guidance with respect to the situation at hand? When we think about guidance for users, we could also consider guidance for designers and developers. How should a visualization be designed given certaindatacharacteristicsandanalysistasks?Usually,comingupwithagood answer requires a good amount of experience. Wouldn’t it be great if we had actionable guidelines to come to our aid? Yes, it would, but we are not there yet. Wecouldgoonlistingresearchquestionsforpages.Asamatteroffact,there is still much to be investigated. The major scientific conferences on interactive visual data analysis testify to this fact. VIS, EuroVis, and PacificVis publish innovative research results annually. Journals such as IEEE Transactions on Visualization and Computer Graphics, the Computer Graphics Forum, and Information Visualization are great for submitting original articles on visual analytics research. In related fields, such as human-computer interaction, cognitive psychology, data mining, or machine learing, it is also possible to publish—and to learn. This concludes our broad overview of concepts, models, methods, and techniques,basicandadvancedones,forinteractivevisualdataanalysis.Given what you have learned from reading this book, you have everything now to make an informed choice on where to venture next in this exciting field. Amongallthepossibledirectionsoflearning,applying,creating,andadvancing interactive visual data analysis, with one being as exciting as the other, you cannot choose wrongly.
Bibliography [AA06] Andrienko, N. and Andrienko, G. Exploratory Analysis of Spatial and Temporal Data – A Systematic Approach.Springer,2006.doi: 10.1007/3-540-31190-4 (cited on pages 31, 127). [Abe+14] Abello, J., Hadlak, S., Schumann, H., and Schulz, H.-J. “A Mod- ular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks”. In: IEEE Transactions on Visual- ization and Computer Graphics 20.3 (2014), pp. 337–350. doi: 10.1109/TVCG.2013.109(citedonpages215,217–219,265,346). [Aig+05] Aigner, W., Miksch, S., Thurnher, B., and Biffl, S. “Planning- Lines:NovelGlyphsforRepresentingTemporalUncertaintiesand theirEvaluation”.In:Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2005, pp. 457–463. doi: 10.1109/IV.2005.97 (cited on pages 93, 94). [Aig+11] Aigner, W., Miksch, S., Schumann, H., and Tominski, C. Visual- izationofTime-OrientedData.Springer,2011.doi:10.1007/978- 0-85729-079-3 (cited on pages 87, 127, 168, 261, 262, 346). [All83] Allen, J. F. “Maintaining Knowledge about Temporal Intervals”. In: Communications of the ACM 26.11 (1983), pp. 832–843. doi: 10.1145/182.358434 (cited on page 83). [AMA07] Archambault, D. W., Munzner, T., and Auber, D. “TopoLayout: Multilevel Graph Layout by Topological Features”. In: IEEE TransactionsonVisualizationandComputerGraphics13.2(2007), pp. 305–317. doi: 10.1109/TVCG.2007.46 (cited on page 118). [AN13] Andrews, C. and North, C. “The Impact of Physical Navigation onSpatialOrganizationforSensemaking”.In:IEEE Transactions on Visualization and Computer Graphics 19.12 (2013), pp. 2207– 2216. doi: 10.1109/TVCG.2013.205 (cited on page 204). [And+10] Andrienko, G., Andrienko, N., Demšar, U., Dransch, D., Dykes, J., Fabrikant, S. I., Jern, M., Kraak, M.-J., Schumann, H., and Tominski, C. “Space, Time and Visual Analytics”. In: Interna- tional Journal of Geographical Information Science 24.10 (2010), pp. 1577–1600. doi: 10.1080/13658816.2010.508043 (cited on page 96). 311
312 (cid:4) Bibliography [And+13] Andrienko, G., Andrienko, N., Bak, P., Keim, D., and Wrobel, S. Visual Analytics of Movement.Springer,2013.doi:10.1007/978- 3-642-37583-5 (cited on pages 127, 225). [Ang+18] Angelini, M., Santucci, G., Schumann, H., and Schulz, H.-J. “A Review and Characterization of Progressive Visual Ana- lytics”. In: Informatics 5.3 (2018), p. 31. doi: 10.3390/ informatics5030031 (cited on page 304). [AS05] Amar,R.A.andStasko,J.T.“KnowledgePreceptsforDesignand EvaluationofInformationVisualizations”.In:IEEE Transactions onVisualizationandComputerGraphics 11.4(2005),pp.432–442. doi: 10.1109/TVCG.2005.63 (cited on page 18). [AS94] Ahlberg, C. and Shneiderman, B. “Visual Information Seeking: TightCouplingofDynamicQueryFilterswithStarfieldDisplays”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1994, pp. 313–317. doi: 10.1145/191666.191775 (cited on page 149). [Asi85] Asimov, D. “The Grand Tour: A Tool for Viewing Multidimen- sionalData”.In:SIAM Journal on Scientific and Statistical Com- puting 6.1 (1985), pp. 128–143. doi: 10.1137/0906011 (cited on page 69). [AWP97] Andrews, K., Wolte, J., and Pichler, M. “Information Pyramids: A New Approach to Visualising Large Hierarchies”. In: Proceed- ings of the IEEE Visualization Conference (Vis). Late Breaking Hot Topics. IEEE Computer Society, 1997, pp. 49–52 (cited on page 117). [Bac+17] Bach, B., Dragicevic, P., Archambault, D. W., Hurter, C., and Carpendale, S. “A Descriptive Framework for Temporal Data VisualizationsBasedonGeneralizedSpace-TimeCubes”.In:Com- puter Graphics Forum 36.6 (2017), pp. 36–61. doi: 10.1111/cgf. 12804 (cited on page 127). [Bat+99] Battista, G. D., Eades, P., Tamassia, R., and Tollis, I. G. Graph Drawing: Algorithms for the Visualization of Graphs. 1st edition. Prentice Hall, 1999 (cited on page 113). [BC87] Becker, R. A. and Cleveland, W. S. “Brushing Scatterplots”. In: Technometrics 29.2 (1987), pp. 127–142. doi: 10.2307/1269768 (cited on pages 149, 157). [BCK08] Boriah, S., Chandola, V., and Kumar, V. “Similarity Measures forCategoricalData:AComparativeEvaluation”.In:Proceedings of the SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2008, pp. 243– 254. doi: 10.1137/1.9781611972788.22 (cited on page 191).
Bibliography (cid:4) 313 [Bed11] Bederson, B. B. “The Promise of Zoomable User Interfaces”. In: Behaviour & Information Technology 30.6 (2011), pp. 853–866. doi: 10.1080/0144929X.2011.586724 (cited on pages 159, 206). [BEF17] Badam,S.K.,Elmqvist,N.,andFekete,J.-D.“SteeringtheCraft: UIElementsandVisualizationsforSupportingProgressiveVisual Analytics”. In: Computer Graphics Forum 36.3 (2017), pp. 491– 502. doi: 10.1111/cgf.13205 (cited on page 304). [Beh+16] Behrisch, M., Bach, B., Riche, N. H., Schreck, T., and Fekete, J. “Matrix Reordering Methods for Table and Network Visualiza- tion”. In: Computer Graphics Forum 35.3 (2016), pp. 693–716. doi: 10.1111/cgf.12935 (cited on page 115). [Bel61] Bellman, R. E. Adaptive Control Processes: A Guided Tour. PrincetonUniversityPress,1961.doi:10.1002/nav.3800080314 (cited on page 258). [Ber67] Bertin, J. Sémiologie Graphique. Gauthier-Villars, 1967 (cited on page 54). [Ber81] Bertin, J. Graphics and Graphic Information-Processing. de Gruyter, 1981 (cited on pages 3, 131). [Ber83] Bertin, J. Semiology of Graphics (W. J. Berg, trans). University of Wisconsin Press, 1983 (cited on page 54). [BH94] Bederson,B.B.andHollan,J.D.“Pad++:AZoomingGraphical Interface for Exploring Alternate Interface Physics”. In: Proceed- ings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 1994, pp. 17–26. doi: 10.1145/ 192426.192435 (cited on page 206). [BHvW00] Bruls,M.,Huizing,K.,andvanWijk,J.J.“SquarifiedTreemaps”. In: Proceedings of the Joint Eurographics - IEEE TCVG Sympo- sium on Visualization (VisSym). Springer, 2000, pp. 33–42. doi: 10.1007/978-3-7091-6783-0_4 (cited on page 117). [Bie+93] Bier, E. A., Stone, M. C., Pier, K., Buxton, W., and DeRose, T. D. “Toolglass and Magic Lenses: the See-Through Interface”. In: Proceedings of the Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). ACM Press, 1993, pp. 73–80. doi: 10.1145/166117.166126 (cited on page 206). [BLS99] Brandstädt, A., Le, V. B., and Spinrad, J. P. Graph Classes: A Survey. SIAM, 1999. doi: 10.1137/1.9780898719796 (cited on page 112). [BMG10] Ballendat,T.,Marquardt,N.,andGreenberg,S.“ProxemicInter- action:DesigningforaProximityandOrientation-AwareEnviron- ment”.In:ProceedingsoftheInternationalConferenceonInterac- tive Tabletops and Surfaces (ITS).ACMPress,2010,pp.121–130. doi: 10.1145/1936652.1936676 (cited on page 203).
314 (cid:4) Bibliography [Bor+13] Borgo, R., Kehrer, J., Chung, D. H. S., Maguire, E., Laramee, R. S., Hauser, H., Ward, M., and Chen, M. “Glyph-based Visual- ization: Foundations, Design Guidelines, Techniques and Appli- cations”. In: Eurographics 2013 - State of the Art Reports. Euro- graphics Association, 2013, pp. 39–63. doi: 10.2312/conf/ EG2013/stars/039-063 (cited on page 73). [BR03] Baudisch, P. and Rosenholtz, R. “Halo: A Technique for Visualiz- ing Off-Screen Objects”. In: Proceedings of the SIGCHI Confer- ence Human Factors in Computing Systems (CHI). ACM Press, 2003, pp. 481–488. doi: 10.1145/642611.642695 (cited on page 165). [Bre16] Brehmer, M. M. “Why Visualization? Task Abstraction for Anal- ysis and Design”. PhD thesis. University of British Columbia, 2016 (cited on page 50). [BRL09] Bertini, E., Rigamonti, M., and Lalanne, D. “Extended Excentric Labeling”. In: Computer Graphics Forum 28.3 (2009), pp. 927– 934. doi: 10.1111/j.1467-8659.2009.01456.x (cited on pages 175, 177). [BRT95] Bergman, L. D., Rogowitz, B. E., and Treinish, L. “A Rule-Based Tool for Assisting Colormap Selection”. In: Proceedings of the IEEE Visualization Conference (Vis). IEEE Computer Society, 1995,pp.118–125.doi:10.1109/VISUAL.1995.480803(citedon pages 56, 127). [Buj+91] Buja,A.,McDonald,J.A.,Michalak,J.,andStuetzle,W.“Interac- tiveDataVisualizationUsingFocusingandLinking”.In:Proceed- ings of the IEEE Visualization Conference (Vis).IEEEComputer Society, 1991, pp. 156–163, 419. doi: 10.1109/VISUAL.1991. 175794 (cited on page 157). [But+08] Butkiewicz, T., Dou, W., Wartell, Z., Ribarsky, W., and Chang, R. “Multi-Focused Geospatial Analysis Using Probes”. In: IEEE TransactionsonVisualizationandComputerGraphics14.6(2008), pp. 1165–1172. doi: 10.1109/TVCG.2008.149 (cited on page 102). [Bux90] Buxton, W. “A Three-state Model of Graphical Input”. In: Proceedings of the IFIP International Conference on Human- Computer Interaction (INTERACT). North-Holland, 1990, pp. 449–456 (cited on page 145). [BW08a] Bachthaler, S. and Weiskopf, D. “Continuous Scatterplots”. In: IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1428–1435. doi: 10.1109/TVCG.2008.119 (cited on pages 209, 265, 341).
Bibliography (cid:4) 315 [BW08b] Byron, L. and Wattenberg, M. “Stacked Graphs – Geometry & Aesthetics”. In: IEEE Transactions on Visualization and Com- puter Graphics 14.6 (2008), pp. 1245–1252. doi: 10.1109/TVCG. 2008.166 (cited on page 89). [BW18] Belmonte,N.andWang,Y.RefoldingtheEarth:InteractiveMyri- ahedralProjectionandFabrication.PosterattheIEEEConference on Information Visualization. Berlin, Germany, 2018 (cited on page 100). [CD04] Cook, D. and Das, S. K. Smart Environments: Technology, Pro- tocols and Applications. Wiley-Interscience, 2004. doi: 10.1002/ 047168659X (cited on pages 269, 304). [Cen+17] Ceneda,D.,Gschwandtner,T.,May,T.,Miksch,S.,Schulz,H.-J., Streit, M., and Tominski, C. “Characterizing Guidance in Visual Analytics”.In:IEEETransactionsonVisualizationandComputer Graphics 23.1 (2017), pp. 111–120. doi: 10.1109/TVCG.2016. 2598468 (cited on pages 277–279, 283, 304). [Che04] Chen,H.“CompoundBrushingExplained”.In:Information Visu- alization 3.2 (2004), pp. 96–108. doi: 10.1057/palgrave.ivs. 9500068 (cited on page 158). [Chi00] Chi, E. H.-H. “A Taxonomy of Visualization Techniques Using the Data State Reference Model”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 2000, pp. 69–75. doi: 10.1109/INFVIS.2000.885092 (cited on page 45). [Chu+15] Chung, H., North, C., Joshi, S., and Chen, J. “Four Considera- tions for Supporting Visual Analysis in Display Ecologies”. In: Proceedings of the IEEE Conference on Visual Analytics Science andTechnology(VAST).IEEEComputerSociety,2015,pp.33–40. doi: 10.1109/VAST.2015.7347628 (cited on page 304). [CKB08] Cockburn, A., Karlson, A., and Bederson, B. B. “A Review of Overview+Detail, Zooming, and Focus+Context Interfaces”. In: ACM Computing Surveys 41.1 (2008), 2:1–2:31. doi: 10.1145/ 1456650.1456652 (cited on page 64). [Cle93] Cleveland, W. S. Visualizing Data. Hobart Press, 1993 (cited on pages 69, 91). [CM84] Cleveland, W. S. and McGill, R. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graph- ical Methods”. In: Journal of the American Statistical Associa- tion 79.387 (1984), pp. 531–554. doi: 10.1080/01621459.1984. 10478080 (cited on page 55).
316 (cid:4) Bibliography [Col+18] Collins, C., Andrienko, N., Schreck, T., Yang, J., Choo, J., Engelke, U., Jena, A., and Dwyer, T. “Guidance in the Human- Machine Analytics Process”. In: Visual Informatics 3.1 (2018). doi: 10.1016/j.visinf.2018.09.003 (cited on pages 277, 304). [Con+08] Conversy,S.,Barboni,E.,Navarre,D.,andPalanque,P.“Improv- ing Modularity of Interactive Software with the MDPC Archi- tecture”. In: Engineering Interactive Systems: EIS 2007 Joint Working Conferences, EHCI 2007, DSV-IS 2007, HCSE 2007, Salamanca, Spain, March 22-24, 2007. Selected Papers.Editedby Gulliksen, J., Harning, M. B., Palanque, P., van der Veer, G. C., and Wesson, J. Springer, 2008, pp. 321–338. doi: 10.1007/978- 3-540-92698-6_20 (cited on page 196). [CR98] Chi, E. H.-H. and Riedl, J. T. “An Operator Interaction Frame- work for Visualization Systems”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1998, pp. 63–70. doi: 10.1109/INFVIS.1998.729560 (cited on page 44). [CRC07] Cooper, A., Reimann, R., and Cronin, D. About Face 3: The Essentials of Interaction Design. Wiley, 2007 (cited on pages 139, 140). [CvW11] Claessen, J. H. and van Wijk, J. J. “Flexible Linked Axes for MultivariateDataVisualization”.In:IEEE Transactions on Visu- alization and Computer Graphics 17.12 (2011), pp. 2310–2316. doi: 10.1109/TVCG.2011.201 (cited on page 72). [DGH03] Doleisch,H.,Gasser,M.,andHauser,H.“InteractiveFeatureSpec- ification for Focus+Context Visualization of Complex Simulation Data”. In: Proceedings of the Joint Eurographics - IEEE TCVG Symposium on Visualization (VisSym). Eurographics Association, 2003, pp. 239–248. doi: 10.2312/VisSym/VisSym03/239-248 (cited on pages 221, 265). [DH02a] Doleisch,H.andHauser,H.“SmoothBrushingforFocus+Context Visualization of Simulation Data in 3D”. In: Journal of WSCG 10.1–3 (2002), pp. 147–154. url: http://wscg.zcu.cz/ wscg2002/Papers_2002/E71.pdf (cited on page 156). [DH02b] Dragicevic, P. and Huot, S. “SpiraClock: A Continuous and Non- Intrusive Display for Upcoming Events”. In: Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). Extended Abstracts. ACM Press, 2002, pp. 604–605. doi: 10. 1145/506443.506505 (cited on page 195). [Dix+04] Dix,A.,Finlay,J.,Abowd,G.D.,andBeale,R.Human-Computer Interaction. 3rd edition. Pearson Education, 2004 (cited on page 206).
Bibliography (cid:4) 317 [DP20] Dimara, E. and Perin, C. “What is Interaction for Data Visual- ization?” In: IEEE Transactions on Visualization and Computer Graphics 26.1 (2020), pp. 119–129. doi: 10.1109/TVCG.2019. 2934283 (cited on page 206). [dSai39] DeSaint-Exupéry,A.Wind, Sand, and Stars.translatedbyLewis Galantière. Harcourt, Inc., 1939 (cited on page 62). [Düb+14] Dübel, S., Röhlig, M., Schumann, H., and Trapp, M. “2D and 3D Presentation of Spatial Data: A Systematic Review”. In: Info- Vis Workshop: Does 3D Really Make Sense for Data Visualiza- tion? IEEE Computer Society, 2014. doi: 10.1109/3DVis.2014. 7160094 (cited on pages 103, 104). [Düb+17] Dübel, S., Röhlig, M., Tominski, C., and Schumann, H. “Visual- izing 3D Terrain, Geo-Spatial Data, and Uncertainty”. In: Infor- matics 4.1 (2017), pp. 1–18. doi: 10.3390/informatics4010006 (cited on page 126). [ED06] Ellis, G. and Dix, A. J. “The Plot, the Clutter, the Sampling and its Lens: Occlusion Measures for Automatic Clutter Reduction”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2006, pp. 266–269. doi: 10.1145/1133265. 1133318 (cited on pages 178, 265). [EF10] Elmqvist,N.andFekete,J.-D.“HierarchicalAggregationforInfor- mation Visualization: Overview, Techniques, and Design Guide- lines”. In: IEEE Transactions on Visualization and Computer Graphics 16.3 (2010), pp. 439–454. doi: 10.1109/TVCG.2009.84 (cited on pages 233, 265). [Eic+14] Eichner, C., Bittig, A., Schumann, H., and Tominski, C. “Ana- lyzing Simulations of Biochemical Systems with Feature-Based VisualAnalytics.”In: Computers & Graphics 38.1(2014), pp. 18– 26. doi: 10.1016/j.cag.2013.09.001 (cited on page 221). [Eic+15] Eichner, C., Nocke, T., Schulz, H. J., and Schumann, H. “Inter- active Presentation of Geo-Spatial Climate Data in Multi- Display Environments”. In: ISPRS International Journal of Geo- Information 4.2(2015),pp.493–514.doi:10.3390/ijgi4020493 (cited on pages 272, 276). [Eic94] Eick, S. G. “Data Visualization Sliders”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 1994, pp. 119–120. doi: 10.1145/192426. 192472 (cited on page 152). [Elm+11] Elmqvist, N., Moere, A. V., Jetter, H.-C., Cernea, D., Reiterer, H., and Jankun-Kelly, T. “Fluid Interaction for Information Visu- alization”.In:Information Visualization 10.4(2011),pp.327–340. doi: 10.1177/1473871611413180 (cited on page 143).
318 (cid:4) Bibliography [Emm+16] Emmons,S.,Kobourov,S.,Gallant,M.,andBörner,K.“Analysis of Network Clustering Algorithms and Cluster Quality Metrics at Scale”. In: PLOS ONE 11.7 (2016), pp. 1–18. doi: 10.1371/ journal.pone.0159161 (cited on page 265). [End+17] Endert, A., Ribarsky, W., Turkay, C., Wong, W., Nabney, I. T., Blanco, I. D., and Rossi, F. “The State of the Art in Integrating Machine Learning into Visual Analytics”. In: Computer Graphics Forum 36.8 (2017), pp. 458–486. doi: 10.1111/cgf.13092 (cited on page 265). [ENS15] Eichner,C.,Nyolt,M.,andSchumann,H.“ANovelInfrastructure forSupportingDisplayEcologies”.In:AdvancesinVisualComput- ing: Proceedings of the International Symposium on Visual Com- puting (ISVC). Springer, 2015, pp. 722–732. doi: 10.1007/978- 3-319-27863-6\_68 (cited on pages 269, 342). [EST19] Eichner,C.,Schumann,H.,andTominski,C.Multi-display Visual Analysis: Model, Interface, and Layout Computation. Tech. rep. arXiv:1912.08558 [cs.GR]. CoRR, 2019. url: https://arxiv. org/abs/1912.08558 (cited on page 304). [FB90] Feiner, S. K. and Beshers, C. “Worlds within Worlds: Metaphors for Exploring n-dimensional Virtual Worlds”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).ACMPress,1990,pp.76–83.doi:10.1145/97924.97933 (cited on page 79). [FB95] Furnas,G.W.andBederson,B.B.“Space-ScaleDiagrams:Under- standing Multiscale Interfaces”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1995, pp. 234–241. doi: 10.1145/223904.223934 (cited on page 206). [FD13] Frisch, M. and Dachselt, R. “Visualizing Offscreen Elements of Node-Link Diagrams”. In: Information Visualization 12.2 (2013), pp. 133–162. doi: 10.1177/1473871612473589 (cited on page 165). [Fek+19] Fekete,J.-D.,Fisher,D.,Nandi,A.,andSedlmair,M.“Progressive Data Analysis and Visualization (Dagstuhl Seminar 18411)”. In: Dagstuhl Reports 8.10 (2019), pp. 1–40. doi: 10.4230/DagRep.8. 10.1 (cited on page 304). [Fis+12] Fisher,D.,Popov,I.O.,Drucker,S.M.,andm.c.schraefel.“Trust Me, I’m Partially Right: Incremental Visualization Lets Analysts Explore Large Datasets Faster”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2012, pp. 1673–1682. doi: 10.1145/2207676.2208294 (cited on page 304).
Bibliography (cid:4) 319 [FR91] Fruchterman, T. M. J. and Reingold, E. M. “Graph Drawing by Force-DirectedPlacement”.In:Software: Practice and Experience 21.11 (1991), pp. 1129–1164. doi: 10.1002/spe.4380211102 (cited on page 113). [Fra98] Frank,A.U.“DifferentTypesof“Times”inGIS”.In:Spatial and Temporal Reasoning in Geographic Information Systems. Edited by Egenhofer, M. J. and Golledge, R. G. Oxford University Press, 1998, pp. 40–62 (cited on page 84). [FS04] Fuchs, G. and Schumann, H. “Intelligent Icon Positioning for Interactive Map-Based Information Systems”. In: Innovations Through Information Technology. Edited by Khosrow-Pour, M. Hershey, PA, USA: Idea Group Inc., 2004, pp. 261–264. doi: 10.4018/978-1-59140-261-9.ch067. url: http://www.irma- international.org/viewtitle/32349/ (cited on page 102). [Fur86] Furnas, G. W. “Generalized Fisheye Views”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 1986, pp. 16–23. doi: 10.1145/22339.22342 (cited on page 214). [Fur97] Furnas,G.W.“EffectiveViewNavigation”.In:Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). ACM Press, 1997, pp. 367–374. doi: 10.1145/258549.258800 (cited on page 206). [Gla+14] Gladisch, S., Schumann, H., Ernst, M., Füllen, G., and Tominski, C.“Semi-AutomaticEditingofGraphswithCustomizedLayouts”. In: Computer Graphics Forum 33.3 (2014), pp. 381–390. doi: 10.1111/cgf.12394 (cited on pages 182, 183). [Gla14] Glaßer, S. “Visual Analysis, Clustering, and Classification of Contrast-Enhanced Tumor Perfusion MRI Data”. PhD the- sis. Otto von Guericke University Magdeburg, 2014 (cited on page 247). [Gle+11] Gleicher,M.,Albers,D.,Walker,R.,Jusufi,I.,Hansen,C.D.,and Roberts,J.C.“VisualComparisonforInformationVisualization”. In: Information Visualization 10.4 (2011), pp. 289–309. doi: 10. 1177/1473871611416549 (cited on pages 184, 206). [Gle18] Gleicher, M. “Considerations for Visualizing Comparison”. In: IEEE Transactions on Visualization and Computer Graphics 24.1 (2018), pp. 413–423. doi: 10.1109/TVCG.2017.2744199 (cited on page 206). [GS06] Griethe, H. and Schumann, H. “The Visualization of Uncertain Data: Methods and Problems”. In: Proceedings of the Simulation and Visualization (SimVis). SCS Publishing House e.V., 2006, pp. 143–156 (cited on page 125).
320 (cid:4) Bibliography [GST13] Gladisch, S., Schumann, H., and Tominski, C. “Navigation Rec- ommendations for Exploring Hierarchical Graphs”. In: Advances in Visual Computing: Proceedings of the International Symposium on Visual Computing (ISVC). Springer, 2013, pp. 36–47. doi: 10.1007/978-3-642-41939-3_4 (cited on pages 165, 283). [Gua13] Guastello, S. J. Human Factors Engineering and Ergonomics: A Systems Approach. 2nd edition. CRC Press, 2013 (cited on page 50). [Gus+08] Gustafson, S., Baudisch, P., Gutwin, C., and Irani, P. “Wedge: Clutter-Free Visualization of Off-Screen Locations”. In: Proceed- ings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2008, pp. 787–796. doi: 10.1145/ 1357054.1357179 (cited on page 165). [GYZ14] Gross, J. L., Yellen, J., and Zhang, P., eds. Handbook of Graph Theory. CRC Press, 2014 (cited on page 112). [Had+10] Hadlak,S.,Tominski,C.,Schulz,H.-J.,andSchumann,H.“Visual- izationofAttributedHierarchicalStructuresinaSpatio-Temporal Context”. In: International Journal of Geographical Information Science 24.10 (2010), pp. 1497–1513. doi: 10.1080/13658816. 2010.510840 (cited on page 121). [Had+13] Hadlak, S., Schumann, H., Cap, C. H., and Wollenberg, T. “Sup- porting the Visual Analysis of Dynamic Networks by Clustering AssociatedTemporalAttributes”.In:IEEETransactionsonVisu- alization and Computer Graphics 19.12 (2013), pp. 2267–2276. doi: 10.1109/TVCG.2013.198 (cited on page 253). [Had14] Hadlak, S. “Graph Visualization in Space and Time”. PhD thesis. University of Rostock, 2014 (cited on pages 251, 252, 254). [Häg70] Hägerstrand, T. “What About People in Regional Science?” In: Papers of the Regional Science Association 24 (1970), pp. 7–21 (cited on page 108). [Hal+16] Hall,K.W.,Perin,C.,Kusalik,P.G.,Gutwin,C.,andCarpendale, M. S. T. “Formalizing Emphasis in Information Visualization”. In: Computer Graphics Forum 35.3 (2016), pp. 717–737. doi: 10.1111/cgf.12936 (cited on page 155). [Han09] Hanrahan, P. Systems of Thought. Keynote presentation at the Eurographics/IEEESymposiumonVisualization(EuroVis).2009 (cited on page 52). [Har96] Harris, R. L. Information Graphics: A Comprehensive Illustrated Reference. Managment Graphics, 1996 (cited on page 72).
Bibliography (cid:4) 321 [Hau06] Hauser, H. “Generalizing Focus+Context Visualization”. In: Sci- entific Visualization: The Visual Extraction of Knowledge from Data. Springer, 2006, pp. 305–327. doi: 10.1007/3-540-30790- 7_18 (cited on page 64). [HAW08] Heer, J., Agrawala, M., and Willett, W. “Generalized Selection viaInteractiveQueryRelaxation”.In:Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2008, pp. 959–968. doi: 10.1145/1357054.1357203 (cited on page 158). [HB03] Harrower, M. A. and Brewer, C. A. “ColorBrewer.org: An Online Tool for Selecting Color Schemes for Maps”. In: The Cartographic Journal 40.1 (2003), pp. 27–37. doi: 10.1179/ 000870403235002042 (cited on page 127). [HB10] Heer, J. and Bostock, M. “Crowdsourcing Graphical Percep- tion: Using Mechanical Turk to Assess Visualization Design”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press, 2010, pp. 203–212. doi: 10.1145/1753326.1753357 (cited on page 55). [HC04] Heer, J. and Card, S. K. “DOITrees Revisited: Scalable, Space- Constrained Visualization of Hierarchical Data”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2004, pp. 421–424. doi: 10.1145/989863.989941 (cited on page 215). [HE12] Healey, C. G. and Enns, J. T. “Attention and Visual Memory in VisualizationandComputerGraphics”.In:IEEETransactionson Visualization and Computer Graphics 18.7 (2012), pp. 1170–1188. doi: 10.1109/TVCG.2011.127 (cited on pages 56, 156). [Hei+12] Heinrich, J., Luo, Y., Kirkpatrick, A. E., and Weiskopf, D. “Eval- uationofaBundlingTechniqueforParallelCoordinates”.In:Pro- ceedings of the International Conference on Computer Graphics Theory and Applications and International Conference on Infor- mation Visualization Theory and Applications (VISIGRAPP). SciTePress,2012,pp.594–602.doi:10.5220/0003821205940602 (cited on page 213). [HFM07] Henry, N., Fekete, J.-D., and McGuffin, M. J. “NodeTrix: a Hybrid Visualization of Social Networks”. In: IEEE Transactions on Visualization and Computer Graphics 13.6 (2007), pp. 1302– 1309. doi: 10.1109/TVCG.2007.70582 (cited on page 118). [HHN85] Hutchins,E.L.,Hollan,J.D.,andNorman,D.A.“DirectManip- ulation Interfaces”. In: Human-Computer Interaction 1.4 (1985), pp. 311–338. doi: 10.1207/s15327051hci0104_2 (cited on pages 138, 194, 205).
322 (cid:4) Bibliography [HJ05] Hansen, C. D. and Johnson, C. R., eds. The Visualization Hand- book. Elsevier, 2005 (cited on pages 21, 22). [HKP11] Han, J., Kamber, M., and Pei, J. Data Mining: Concepts and Techniques. Morgan Kaufmann, 2011 (cited on pages 244, 265). [HM90] Haber,R.B.andMcNabb,D.A.“VisualizationIdioms:AConcep- tualModelforScientificVisualizationSystems”.In:Visualization in Scientific Computing. Edited by Nielson, G. M., Shriver, B. D., and Rosenblum, L. J. IEEE Computer Society, 1990, pp. 74–93 (cited on page 44). [Hol06] Holten, D. “Hierarchical Edge Bundles: Visualization of Adja- cency Relations in Hierarchical Data”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 741–748. doi: 10.1109/TVCG.2006.147 (cited on page 213). [Hor+19] Horak, T., Mathisen, A., Klokmose, C. N., Dachselt, R., and Elmqvist, N. “Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI). ACM Press,2019,616:1–616:13.doi:10.1145/3290605.3300846(cited on page 303). [Hor99] Horvitz, E. “Principles of Mixed-Initiative User Interfaces”. In: Proceedings of the SIGCHI Conference Human Factors in Com- puting Systems (CHI). ACM Press, 1999, pp. 159–166. doi: 10. 1145/302979.303030 (cited on page 304). [HS12] Heer, J. and Shneiderman, B. “Interactive Dynamics for Visual Analysis”. In: Communications of the ACM 55.4 (2012), pp. 45– 54. doi: 10.1145/2133806.2133821 (cited on pages 144, 289). [HSS15] Hadlak, S., Schumann, H., and Schulz, H.-J. “A Survey of Multi-faceted Graph Visualization”. In: EuroVis State of the Art Reports. Eurographics Association, 2015, pp. 1–20. doi: 10. 2312/eurovisstar.20151109 (cited on pages 112, 119, 120). [HvW09] Holten,D.andvanWijk,J.J.“Force-DirectedEdgeBundlingfor Graph Visualization”. In: Computer Graphics Forum 28.3 (2009), pp. 983–990. doi: 10.1111/j.1467-8659.2009.01450.x (cited on page 213). [II91] II, R. J. M. Introduction to Shannon Sampling and Interpolation Theory. Springer, 1991 (cited on page 232). [Ins09] Inselberg, A. Parallel Coordinates – Visual Multidimensional GeometryandItsApplications.Springer,2009.doi:10.1007/978- 0-387-68628-8 (cited on page 71).
Bibliography (cid:4) 323 [Ise+13] Isenberg, P., Isenberg, T., Hesselmann, T., Lee, B., von Zadow, U., and Tang, A. “Data Visualization on Interactive Surfaces: A ResearchAgenda”.In:IEEEComputerGraphicsandApplications 33.2 (2013), pp. 16–24. doi: 10.1109/MCG.2013.24 (cited on page 206). [Jak+13] Jakobsen, M. R., Haile, Y. S., Knudsen, S., and Hornbæk, K. “Information Visualization and Proxemics: Design Opportunities and Empirical Findings”. In: IEEE Transactions on Visualization and Computer Graphics 19.12 (2013), pp. 2386–2395. doi: 10. 1109/TVCG.2013.166 (cited on page 204). [JD13] Jansen, Y. and Dragicevic, P. “An Interaction Model for Visual- izations Beyond The Desktop”. In: IEEE Transactions on Visual- ization and Computer Graphics 19.12 (2013), pp. 2396–2405. doi: 10.1109/TVCG.2013.134 (cited on page 206). [JMG07] Jankun-Kelly, T. J., Ma, K.-L., and Gertz, M. “A Model and FrameworkforVisualizationExploration”.In:IEEETransactions onVisualizationandComputerGraphics 13.2(2007),pp.357–369. doi: 10.1109/TVCG.2007.28 (cited on page 44). [Jol02] Jolliffe,I.T.PrincipalComponentAnalysis.2ndedition.Springer, New York, USA, 2002 (cited on page 265). [JTS08] John,M.,Tominski,C.,andSchumann,H.“VisualandAnalytical Extensions for the Table Lens”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2008, pp. 680907-1–680907-12. doi: 10.1117/12.766440 (cited on pages 57, 60, 249). [Kei+06] Keim, D. A., Mansmann, F., Schneidewind, J., and Ziegler, H. “Challenges in Visual Data Analysis”. In: Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2006, pp. 9–16. doi: 10.1109/IV.2006.31 (cited on pages 3, 207, 208, 257, 264, 265). [Kei+10] Keim, D., Kohlhammer, J., Ellis, G., and Mannsmann, F., eds. Mastering the Information Age – Solving Problems with Visual Analytics. Eurographics Association, 2010 (cited on page 265). [Kim+17] Kim, H., Choo, J., Lee, C., Lee, H., Reddy, C. K., and Park, H. “PIVE: Per-Iteration Visualization Environment for Real- Time Interactions with Dimension Reduction and Clustering”. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI Press, 2017, pp. 1001–1009. url: http:// aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14381 (cited on page 298).
324 (cid:4) Bibliography [Kis+17] Kister, U., Klamka, K., Tominski, C., and Dachselt, R. “GraSp: Combining Spatially-aware Mobile Devices and a Display Wall for Graph Visualization and Interaction”. In: Computer Graphics Forum 36.3 (2017), pp. 503–514. doi: 10.1111/cgf.13206 (cited on page 303). [KK17] Kerracher, N. and Kennedy, J. “Constructing and Evaluating Visualisation Task Classifications: Process and Considerations”. In: Computer Graphics Forum 36.3 (2017), pp. 47–59. doi: 10. 1111/cgf.13167 (cited on pages 31, 50). [KK93] Keller, P. R. and Keller, M. M. Visual Cues: Practical Data Visualization. IEEE Computer Society, 1993 (cited on page 34). [KK94] Keim, D. A. and Kriegel, H.-P. “VisDB: Database Exploration Using Multidimensional Visualization”. In: IEEE Computer Graphics and Applications 14.5 (1994), pp. 40–49. doi: 10.1109/ 38.310723 (cited on page 75). [KM94] Kirsh, D. and Maglio, P. “On Distinguishing Epistemic from Pragmatic Action”. In: Cognitive Science 18.4 (1994), pp. 513– 549. doi: 10.1207/s15516709cog1804_1 (cited on page 131). [KNS04] Kreuseler, M., Nocke, T., and Schumann, H. “A History Mech- anism for Visual Data Mining”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 2004, pp. 49–56. doi: 10.1109/INFVIS.2004.2 (cited on page 274). [KP88] Krasner,G.E.andPope,S.T.“ACookbookforUsingtheModel- View-Controller User Interface Paradigm in Smalltalk-80”. In: Journal of Object-Oriented Programming 1.3 (1988), pp. 26–49 (cited on page 146). [KPW14] Kerren, A., Purchase, H. C., and Ward, M. O., eds. Multivariate Network Visualization.Springer,2014.doi:10.1007/978-3-319- 06793-3 (cited on page 127). [Kra03] Kraak, M.-J. “The Space-Time Cube Revisited from a Geovisu- alization Perspective”. In: Proceedings of the 21st International CartographicConference(ICC).Newcastle,UK:TheInternational Cartographic Association (ICA), 2003, pp. 1988–1995 (cited on page 108). [KRD14] Kister,U.,Reipschläger,P.,andDachselt,R.“Multi-TouchManip- ulation of Magic Lenses for Information Visualization”. In: Pro- ceedings of the International Conference on Interactive Table- tops and Surfaces (ITS). ACM Press, 2014, pp. 431–434. doi: 10.1145/2669485.2669528 (cited on page 178).
Bibliography (cid:4) 325 [KS99] Kreuseler,M.andSchumann,H.“InformationVisualizationUsing a New Focus+Context Technique in Combination with Dynamic Clustering of Information Space”. In: Proceedings of the ACM Workshop on New Paradigms in Information Visualization and Manipulation (NPIVM). ACM Press, 1999, pp. 1–5 (cited on page 123). [Kul06] Kulpa, Z. “A Diagrammatic Approach to Investigate Interval Relations”. In: Journal of Visual Languages & Computing 17.5 (2006), pp. 466–502. doi: 10.1016/j.jvlc.2005.10.004. url: http://dx.doi.org/10.1016/j.jvlc.2005.10.004 (cited on page 88). [LA94] Leung, Y. K. and Apperley, M. D. “A Review and Taxonomy of Distortion-Oriented Presentation Techniques”. In: ACM Transac- tions on Computer-Human Interaction 1.2 (1994), pp. 126–160. doi: 10.1145/180171.180173 (cited on page 64). [Lam08] Lam, H. “A Framework of Interaction Costs in Information Visu- alization”.In:IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1149–1156. doi: 10.1109/TVCG.2008. 109 (cited on page 136). [Lee+12] Lee, B., Isenberg, P., Riche, N. H., and Carpendale, S. “Beyond MouseandKeyboard:ExpandingDesignConsiderationsforInfor- mation Visualization Interactions”. In: IEEE Transactions on VisualizationandComputerGraphics 18.12(2012),pp.2689–2698. doi: 10.1109/TVCG.2012.204 (cited on page 206). [Leh+11] Lehmann,A.,Schumann,H.,Staadt,O.,andTominski,C.“Phys- ical Navigation to Support Graph Exploration on a Large High- Resolution Display”. In: Advances in Visual Computing: Pro- ceedings of the International Symposium on Visual Computing (ISVC). Springer, 2011, pp. 496–507. doi: 10.1007/978-3-642- 24028-7_46 (cited on pages 202–204). [Lex+12] Lex, A., Streit, M., Schulz, H.-J., Partl, C., Schmalstieg, D., Park, P. J., and Gehlenborg, N. “StratomeX: Visual Analysis of Large-Scale Heterogeneous Genomics Data for Cancer Subtype Characterization”. In: Computer Graphics Forum 31.3 (2012), pp. 1175–1184. doi: 10.1111/j.1467-8659.2012.03110.x (cited on page 246). [LF06] Leskovec, J. and Faloutsos, C. “Sampling from Large Graphs”. In: Proceedings of the ACM Conference on Knowledge discovery and data mining (SIGKDD). ACM Press, 2006, pp. 631–636. doi: 10.1145/1150402.1150479 (cited on page 232).
326 (cid:4) Bibliography [LH14] Liu, Z. and Heer, J. “The Effects of Interactive Latency on Exploratory Visual Analysis”. In: IEEE Transactions on Visual- ization and Computer Graphics 20.12 (2014), pp. 2122–2131. doi: 10.1109/TVCG.2014.2346452 (cited on pages 140, 289). [LHT17] Lhuillier, A., Hurter, C., and Telea, A. “State of the Art in Edge and Trail Bundling Techniques”. In: Computer Graphics Forum 36.3 (2017), pp. 619–645. doi: 10.1111/cgf.13213 (cited on pages 212, 213, 265). [Lia05] Liao,W.“ClusteringofTimeSeriesData—ASurvey”.In:Pattern Recognition38.11(2005),pp.1857–1874.doi:10.1016/j.patcog. 2005.01.025 (cited on page 251). [LKD19] Langner, R., Kister, U., and Dachselt, R. “Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances”. In: IEEE Trans- actions on Visualization and Computer Graphics 25.1 (2019), pp. 608–618. doi: 10.1109/TVCG.2018.2865235 (cited on page 303). [Loh19] Lohr, S. L. Sampling: Design and Analysis. 2nd edition. CRC Press, 2019 (cited on page 232). [LS07] Luboschik, M. and Schumann, H. “Explode to Explain – Illus- trative Information Visualization”. In: Proceedings of the Inter- national Conference Information Visualisation (IV). IEEE Com- puterSociety,2007.doi:10.1109/IV.2007.50(citedonpages77, 341). [LTM18] Lam, H., Tory, M., and Munzner, T. “Bridging from Goals to Tasks with Design Study Analysis Reports”. In: IEEE Trans- actions on Visualization and Computer Graphics 24.1 (2018), pp. 435–445. doi: 10.1109/TVCG.2017.2744319 (cited on pages 29, 50). [Lub+12] Luboschik,M.,Maus,C.,Schulz,H.-J.,Schumann,H.,andUhrma- cher, A. “Heterogeneity-Based Guidance for Exploring Multiscale Data in Systems Biology”. In: Proceedings of the IEEE Sympo- sium on Biological Data Visualization (BioVis). IEEE Computer Society, 2012, pp. 33–40. doi: 10.1109/BioVis.2012.6378590 (cited on pages 233, 236, 265). [Lub+15] Luboschik, M., Röhlig, M., Bittig, A. T., Andrienko, N., Schu- mann, H., and Tominski, C. “Feature-Driven Visual Analytics of Chaotic Parameter-Dependent Movement”. In: Computer Graph- ics Forum 34.3 (2015), pp. 421–430. doi: 10.1111/cgf.12654 (cited on pages 225, 229). [LV07] Lee,J.A.andVerleysen,M.Nonlinear Dimensionality Reduction. Springer, 2007. doi: 10.1007/978-0-387-39351-3 (cited on page 265).
Bibliography (cid:4) 327 [LWW90] LeBlanc, J., Ward, M. O., and Wittels, N. “Exploring n- Dimensional Databases”. In: Proceedings of the IEEE Visual- ization Conference (Vis). IEEE Computer Society, 1990, pp. 230– 237. doi: 10.1109/VISUAL.1990.146386 (cited on page 78). [Mac86] Mackinlay,J.“AutomatingtheDesignofGraphicalPresentations of Relational Information”. In: ACM Transactions on Graphics 5.2 (1986), pp. 110–141. doi: 10.1145/22949.22950 (cited on pages 50, 54). [Mac95] MacEachren, A. M. How Maps Work: Representation, Visualiza- tion, and Design. Guilford Press, 1995 (cited on pages 54, 99, 127). [Mar+18] Marriott, K., Schreiber, F., Dwyer, T., Klein, K., Riche, N. H., Itoh, T., Stuerzlinger, W., and Thomas, B. H., eds. Immersive Analytics. Springer, 2018. doi: 10.1007/978-3-030-01388-2 (cited on page 206). [MDB87] McCormick, B. H., DeFanti, T. A., and Brown, M. D. “Visualiza- tion in Scientific Computing”. In: ACM SIGGRAPH Computer Graphics 21.6 (1987), p. 3. doi: 10.1145/41997.41998 (cited on page 2). [Mey+15] Meyer, M., Sedlmair, M., Quinan, P. S., and Munzner, T. “The Nested Blocks and Guidelines Model”. In: Information Visualiza- tion 14.3 (2015), pp. 234–249. doi: 10.1177/1473871613510429 (cited on page 41). [ML17] McNabb, L. and Laramee, R. S. “Survey of Surveys (SoS) - Mapping The Landscape of Survey Papers in Information Visual- ization”. In: Computer Graphics Forum 36.3 (2017), pp. 589–617. doi: 10.1111/cgf.13212 (cited on page 126). [MNS06] Müller, W., Nocke, T., and Schumann, H. “Enhancing the Visu- alization Process with Principal Component Analysis to Support the Exploration of Trends”. In: Asia-Pacific Symposium on Infor- mation Visualisation (APVIS). Australian Computer Society, 2006, pp. 121–130. url: https://dl.acm.org/citation.cfm? id=1151922 (cited on page 261). [Mos+09] Moscovich, T., Chevalier, F., Henry, N., Pietriga, E., and Fekete, J.-D. “Topology-Aware Navigation in Large Networks”. In: Pro- ceedings of the SIGCHI Conference Human Factors in Com- puting Systems (CHI). ACM Press, 2009, pp. 2319–2328. doi: 10.1145/1518701.1519056 (cited on page 166).
328 (cid:4) Bibliography [Müh+14] Mühlbacher,T.,Piringer,H.,Gratzl,S.,Sedlmair,M.,andStreit, M.“OpeningtheBlackBox:StrategiesforIncreasedUserInvolve- ment in Existing Algorithm Implementations”. In: IEEE Trans- actions on Visualization and Computer Graphics 20.12 (2014), pp. 1643–1652. doi: 10.1109/TVCG.2014.2346578 (cited on page 304). [Mun09] Munzner, T. “A Nested Model for Visualization Design and Vali- dation”. In: IEEE Transactions on Visualization and Computer Graphics 15.6(2009),pp.921–928.doi:10.1109/TVCG.2009.111 (cited on pages 41, 42). [Mun14] Munzner,T.Visualization Analysis and Design.AKPeters/CRC Press, 2014 (cited on page 50). [MW14] Mackinlay, J. D. and Winslow, K. Designing Great Visualiza- tions. White Paper. Tableau Software Inc., 2014. url: https:// www.tableau.com/sites/default/files/media/designing- great-visualizations.pdf (cited on page 55). [MW95] Martin, A. R. and Ward, M. O. “High Dimensional Brushing for Interactive Exploration of Multivariate Data”. In: Proceedings of the IEEE Visualization Conference (Vis). IEEE Computer Society, 1995, pp. 271–278. doi: 10.1109/VISUAL.1995.485139 (cited on pages 149, 156). [Nac+16] Nachmanson, L., Nocaj, A., Bereg, S., Zhang, L., and Holroyd, A. “Node Overlap Removal by Growing a Tree”. In: Proceedings of the International Symposium on Graph Drawing (GD). Springer, 2016, pp. 33–43. doi: 10.1007/978-3-319-50106-2_3 (cited on page 114). [NH06] Novotny, M. and Hauser, H. “Outlier-Preserving Focus+Context Visualization in Parallel Coordinates”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 893–900. doi: 10.1109/TVCG.2006.170 (cited on pages 210, 211, 265, 342). [Nob+19] Nobre, C., Streit, M., Meyer, M., and Lex, A. “The State of the ArtinVisualizingMultivariateNetworks”.In:ComputerGraphics Forum 38.3 (2019), pp. 807–832. doi: 10.1111/cgf.13728 (cited on page 127). [Nor13] Norman, D. A. The Design of Everyday Things. Revised and expanded edition. Basic Books, 2013 (cited on pages 135, 136). [Nor88] Norman, D. A. The Psychology of Everyday Things. Basic Books, 1988 (cited on pages 135, 136).
Bibliography (cid:4) 329 [NSS05] Nocke, T., Schlechtweg, S., and Schumann, H. “Icon-Based Visu- alization Using Mosaic Metaphors”. In: Proceedings of the Inter- national Conference Information Visualisation (IV). IEEE Com- puter Society, 2005, pp. 103–109. doi: 10.1109/IV.2005.58 (cited on pages 75, 341). [PB07] Preim, B. and Bartz, D. Visualization in Medicine: Theory, Algo- rithms, and Applications. Morgan Kaufmann, 2007 (cited on page 22). [Pin+12] Pindat, C., Pietriga, E., Chapuis, O., and Puech, C. “JellyLens: Content-aware Adaptive Lenses”. In: Proceedings of the ACM Symposium on User Interface Software and Technology (UIST). ACM Press, 2012, pp. 261–270. doi: 10.1145/2380116.2380150 (cited on page 177). [Pir+09] Piringer, H., Tominski, C., Muigg, P., and Berger, W. “A Multi- Threading Architecture to Support Interactive Visual Explo- ration”. In: IEEE Transactions on Visualization and Computer Graphics 15.6 (2009), pp. 1113–1120. doi: 10.1109/TVCG.2009. 110 (cited on page 294). [Pos+03] Post, F. H., Vrolijk, B., Hauser, H., Laramee, R. S., and Doleisch, H. “The State of the Art in Flow Visualisation: Feature Extrac- tion and Tracking”. In: Computer Graphics Forum 22.4 (2003), pp. 775–792. doi: 10.1111/j.1467-8659.2003.00723.x (cited on pages 220, 221). [PW06] Plumlee, M. and Ware, C. “Zooming versus Multiple Win- dow Interfaces: Cognitive Costs of Visual Comparisons”. In: ACM Transactions on Computer-Human Interaction 13.2 (2006), pp. 179–209. doi: 10 . 1145 / 1165734 . 1165736 (cited on page 185). [Qia+12] Qiang, Y., Delafontaine, M., Versichele, M., Maeyer, P. D., and de Weghe, N. V. “Interactive Analysis of Time Intervals in a Two-dimensional Space”. In: Information Visualization 11.4 (2012), pp. 255–272. doi: 10.1177/1473871612436775 (cited on page 89). [Rad+12] Radloff, A., Lehmann, A., Staadt, O. G., and Schumann, H. “Smart Interaction Management: An Interaction Approach for Smart Meeting Rooms”. In: Proceedings of the Eighth Interna- tional Conference on Intelligent Environments (IE). IEEE Com- puter Society, 2012, pp. 228–235. doi: 10.1109/IE.2012.34 (cited on pages 273, 342).
330 (cid:4) Bibliography [Rad+15] Radloff, A., Tominski, C., Nocke, T., and Schumann, H. “Sup- porting Presentation and Discussion of Visualization Results in Smart Meeting Rooms”. In: The Visual Computer 31.9 (2015), pp. 1271–1286. doi: 10.1007/s00371-014-1010-x (cited on pages 274, 302, 304). [Rag+16] Ragan, E. D., Endert, A., Sanyal, J., and Chen, J. “Character- izing Provenance in Visualization and Data Analysis: An Orga- nizational Framework of Provenance Types and Purposes”. In: IEEE Transactions on Visualization and Computer Graphics 22.1 (2016), pp. 31–40. doi: 10.1109/TVCG.2015.2467551 (cited on page 125). [RC94] Rao, R. and Card, S. K. “The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus + Context Visualization for Tabular Information”. In: Proceedings of the SIGCHIConferenceHumanFactorsinComputingSystems(CHI). ACM Press, 1994, pp. 318–322. doi: 10.1145/191666.191776 (cited on page 68). [RLP10] Riche,N.H.,Lee,B.,andPlaisant,C.“UnderstandingInteractive Legends: a Comparative Evaluation with Standard Widgets”. In: Computer Graphics Forum 29.3 (2010), pp. 1193–1202. doi: 10.1111/j.1467-8659.2009.01678.x (cited on page 151). [RLS11] Radloff, A., Luboschik, M., and Schumann, H. “Smart Views in Smart Environments”. In: Proceedings of the Smart Graphics. Springer, 2011, pp. 1–12. doi: 10.1007/978-3-642-22571-0_1 (cited on pages 12, 273, 341). [Rob07] Roberts,J.C.“StateoftheArt:Coordinated&MultipleViewsin Exploratory Visualization”. In: Proceedings of the International Conference on Coordinated and Multiple Views in Exploratory Visualization (CMV). IEEE Computer Society, 2007, pp. 98–102. doi: 10.1109/CMV.2007.20 (cited on page 65). [Röh+15] Röhlig, M., Luboschik, M., Krüger, F., Kirste, T., Schumann, H., Bögl, M., Bilal, A., and Miksch, S. “Supporting Activity Recognition by Visual Analytics”. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE Computer Society, 2015, pp. 41–48. doi: 10.1109/VAST. 2015.7347629 (cited on page 241). [Ros+04] Rosario,G.E.,Rundensteiner,E.A.,Brown,D.C.,Ward,M.O., andHuang,S.“MappingNominalValuestoNumbersforEffective Visualization”. In: Information Visualization 3.2 (2004), pp. 80– 95. doi: 10.1057/palgrave.ivs.9500072 (cited on page 57).
Bibliography (cid:4) 331 [Ros+11] Rosenbaum,R.,Giménez,A.,Schumann,H.,andHamann,B.“A Flexible Low-complexity Device Adaptation Approach for Data Presentation”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2011, 78680F-1–78680F- 12. doi: 10.1117/12.871975 (cited on page 302). [RPS01] Reinders, F., Post, F. H., and Spoelder, H. J. “Visualization of Time-Dependent Data with Feature Tracking and Event Detec- tion.” In: The Visual Computer 17.1 (2001), pp. 55–71. doi: 10.1007/PL00013399 (cited on pages 220, 265). [RS09] Rosenbaum,R.andSchumann,H.“ProgressiveRefinement:More Than a Means to Overcome Limited Bandwidth”. In: Proceedings of the Conference on Visualization and Data Analysis (VDA). SPIE/IS&T, 2009, pp. 72430-1–72430-13. doi: 10.1117/12. 810501 (cited on page 303). [RS17] Röhlig, M. and Schumann, H. “Visibility Widgets for Unveiling OccludedDatain3DTerrainVisualization”.In:JournalofVisual Languages & Computing 42 (2017), pp. 86–98. doi: 10.1016/j. jvlc.2017.08.008 (cited on page 105). [Ruz+12] Ruzinoor,C.M.,Shariff,A.R.M.,Pradhan,B.,RodziAhmad,M., and Rahim, M. S. M. “A Review on 3D Terrain Visualization of GIS Data: Techniques and Software”. In: Geo-spatial Information Science 15.2 (2012), pp. 105–115. doi: 10.1080/10095020.2012. 714101 (cited on page 101). [Sac+14] Sacha, D., Stoffel, A., Kwon, B. C., Ellis, G., and Keim, D. A. “Knowledge Generation Model for Visual Analytics”. In: IEEE Transactions on Visualization and Computer Graphics 20.12 (2014), pp. 1604–1613. doi: 10.1109/TVCG.2014.2346481 (cited on pages 47, 264). [Sac+17] Sacha,D.,Zhang,L.,Sedlmair,M.,Lee,J.,Peltonen,J.,Weiskopf, D., North, S. C., and Keim, D. A. “Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis”. In: IEEE Transactions on Visualization and Computer Graphics 23.1 (2017), pp. 241–250. doi: 10.1109/TVCG.2016.2598495 (cited on page 265). [Sai+05] Saito, T., Miyamura, H. N., Yamamoto, M., Saito, H., Hoshiya, Y.,andKaseda,T.“Two-TonePseudoColoring:CompactVisual- ization for One-Dimensional Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society,2005,pp.173–180.doi:10.1109/INFVIS.2005.1532144 (cited on page 59).
332 (cid:4) Bibliography [Sar+02] Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. “Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems”. In: Proceedings of the 5th International Conference on Computer and Information Technology (ICCIT). East West University, Dhaka, Bangladesh, 2002, pp. 399–404 (cited on page 298). [SB94] Sarkar, M. and Brown, M. H. “Graphical Fisheye Views”. In: Communications of the ACM 37.12 (1994), pp. 73–83. doi: 10. 1145/198366.198384 (cited on page 178). [Sch+13a] Schulz, H.-J., Nocke, T., Heitzler, M., and Schumann, H. “A Design Space of Visualization Tasks”. In: IEEE Transactions on VisualizationandComputerGraphics 19.12(2013),pp.2366–2375. doi: 10.1109/TVCG.2013.120 (cited on pages 29, 34, 50). [Sch+13b] Schulz, H.-J., Streit, M., May, T., and Tominski, C. Towards a Characterization of Guidance in Visualization. Poster at IEEE ConferenceonInformationVisualization(InfoVis).Atlanta,USA, 2013 (cited on page 277). [Sch+16] Schulz, H.-J., Angelini, M., Santucci, G., and Schumann, H. “An Enhanced Visualization Process Model for Incremental Visual- ization”. In: IEEE Transactions on Visualization and Computer Graphics 22.7 (2016), pp. 1830–1842. doi: 10.1109/TVCG.2015. 2462356 (cited on pages 290–293, 298). [Sch+17] Schulz, H.-J., Nocke, T., Heitzler, M., and Schumann, H. “A Systematic View on Data Descriptors for the Visual Analysis of TabularData”.In:InformationVisualization16.3(2017),pp.232– 256. doi: 10.1177/1473871616667767 (cited on page 26). [Sch07] Schaeffer,S.E.“GraphClustering”.In:Computer Science Review 1.1 (2007), pp. 27–64. doi: 10.1016/j.cosrev.2007.05.001 (cited on page 250). [SH10] Shaer, O. and Hornecker, E. “Tangible User Interfaces: Past, Present and Future Directions”. In: Foundations and Trends in Human-Computer Interaction 3.1–2 (2010), pp. 4–137. doi: 10.1561/1100000026 (cited on page 197). [She68] Shepard, D. “A Two-dimensional Interpolation Function for Irregularly-spaced Data”. In: Proceedings of the 23rd ACM National Conference. ACM Press, 1968, pp. 517–524. doi: 10. 1145/800186.810616 (cited on page 25). [Shn94] Shneiderman, B. “Dynamic Queries for Visual Information Seek- ing”. In: IEEE Software 11.6 (1994), pp. 70–77. doi: 10.1109/ 52.329404 (cited on pages 140, 149).
Bibliography (cid:4) 333 [Shn96] Shneiderman, B. “The Eyes Have It: A Task by Data Type Taxonomy for Information Visualizations”. In: Proceedings of the IEEE Symposium on Visual Languages (VL). IEEE Computer Society,1996,pp.336–343.doi:10.1109/VL.1996.545307(cited on pages 34, 159). [SHS11] Schulz, H.-J., Hadlak, S., and Schumann, H. “The Design Space of Implicit Hierarchy Visualization: A Survey”. In: IEEE Trans- actions on Visualization and Computer Graphics 17.4 (2011), pp. 393–411. doi: 10.1109/TVCG.2010.79 (cited on page 117). [SM00] Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642- 57193-0 (cited on pages 13, 75, 80, 341). [SM07] Shen, Z. and Ma, K.-L. “Path Visualization for Adjacency Matri- ces”. In: Proceedings of the Joint Eurographics - IEEE TCVG Symposium on Visualization (VisSym). Eurographics Association, 2007, pp. 83–90. doi: 10.2312/VisSym/EuroVis07/083-090 (cited on page 115). [SMD12] Spindler, M., Martsch, M., and Dachselt, R. “Going Beyond the Surface: Studying Multi-layer Interaction Above the Tabletop”. In: Proceedings of the SIGCHI Conference Human Factors in Computing Systems (CHI).ACMPress,2012,pp.1277–1286.doi: 10.1145/2207676.2208583 (cited on page 201). [SP09] Shneiderman, B. and Plaisant, C. Designing the User Interface: Strategies for Effective Human-Computer Interaction. 5th edition. Addison-Wesley, 2009 (cited on pages 138, 143, 206). [SP13] Sedig, K. and Parsons, P. “Interaction Design for Complex Cog- nitive Activities with Visual Representations: A Pattern-Based Approach”. In: AIS Transactions on Human-Computer Interac- tion 5.2 (2013), pp. 84–133 (cited on pages 133, 134). [SP16] Sedig, K. and Parsons, P. Design of Visualizations for Human- Information Interaction: A Pattern-Based Framework. Vol. 4. Synthesis Lectures on Visualization. Morgan and Claypool Pub- lishers,2016. doi: 10.2200/S00685ED1V01Y201512VIS005(cited on page 206). [Spe07] Spence, R. Information Visualization: Design for Interaction. 2nd edition. Prentice Hall, 2007 (cited on pages 13, 127, 131, 140, 162). [SPG14] Stolper, C. D., Perer, A., and Gotz, D. “Progressive Visual Ana- lytics: User-Driven Visual Exploration of In-Progress Analytics”. In: IEEE Transactions on Visualization and Computer Graphics 20.12 (2014), pp. 1653–1662. doi: 10.1109/TVCG.2014.2346574 (cited on pages 290, 304).
334 (cid:4) Bibliography [Spi+10] Spindler,M.,Tominski,C.,Schumann,H.,andDachselt,R.“Tan- gible Views for Information Visualization”. In: Proceedings of the International Conference on Interactive Tabletops and Surfaces (ITS). ACM Press, 2010, pp. 157–166. doi: 10.1145/1936652. 1936684 (cited on pages 198, 199). [Spi+14] Spindler, M., Schuessler, M., Martsch, M., and Dachselt, R. “Pinch-Drag-Flick vs. Spatial Input: Rethinking Zoom & Pan on Mobile Displays”. In: Proceedings of the SIGCHI Confer- ence Human Factors in Computing Systems (CHI). ACM Press, 2014, pp. 1113–1122. doi: 10.1145/2556288.2557028 (cited on page 201). [SS06] Schulz, H.-J. and Schumann, H. “Visualizing Graphs – A Gen- eralized View”. In: Proceedings of the International Conference Information Visualisation (IV). IEEE Computer Society, 2006, pp. 166–173. doi: 10.1109/IV.2006.130 (cited on page 113). [Ste98] Steiner,A.“AGeneralisationApproachtoTemporalDataModels and their Implementations”. PhD thesis. Swiss Federal Institute of Technology, Zürich, Switzerland, 1998 (cited on pages 85, 86). [Str+12] Streit,M.,Schulz,H.-J.,Lex,A.,Schmalstieg,D.,andSchumann, H.“Model-DrivenDesignfortheVisualAnalysisofHeterogeneous Data”. In: IEEE Transactions on Visualization and Computer Graphics 18.6 (2012), pp. 998–1010. doi: 10.1109/TVCG.2011. 108 (cited on pages 286–288, 342). [Tam13] Tamassia, R., ed. Handbook of Graph Drawing and Visualization. CRC Press, 2013 (cited on page 113). [TAS04] Tominski, C., Abello, J., and Schumann, H. “Axes-Based Visu- alizations with Radial Layouts”. In: Proceedings of the ACM Symposium on Applied Computing (SAC). ACM Press, 2004, pp. 1242–1247. doi: 10.1145/967900.968153 (cited on pages 87, 171). [TAS09] Tominski, C., Abello, J., and Schumann, H. “CGV – An Inter- active Graph Visualization System”. In: Computers & Graphics 33.6 (2009), pp. 660–678. doi: 10.1016/j.cag.2009.06.002 (cited on pages 123, 166). [Tat+12] Tatu, A., Maass, F., Färber, I., Bertini, E., Schreck, T., Seidl, T., and Keim, D. A. “Subspace Search and Visualization to Make Sense of Alternative Clusterings in High-dimensional Data”. In: Proceedings of the IEEE Conference on Visual Analytics Science and Technology (VAST). IEEE Computer Society, 2012, pp. 63– 72. doi: 10.1109/VAST.2012.6400488 (cited on page 191). [TC05] Thomas, J. J. and Cook, K. A. Illuminating the Path: The Research and Development Agenda for Visual Analytics. IEEE Computer Society, 2005 (cited on page 29).
Bibliography (cid:4) 335 [Tei+17] Teipel, S., Heine, C., Hein, A., Krüger, F., Kutschke, A., Kernebeck, S., Halek, M., Bader, S., and Kirste, T. “Multidimen- sional Assessment of Challenging Behaviors in Advanced Stages ofDementiainNursingHomes—TheinsideDEMFramework”.In: Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Mon- itoring 8 (2017), pp. 36–44. doi: 10.1016/j.dadm.2017.03.006 (cited on page 240). [Tel14] Telea, A. C. Data Visualization: Principles and Practice. 2nd edi- tion. A K Peters/CRC Press, 2014 (cited on pages 21, 22). [TFJ12] Tominski, C., Forsell, C., and Johansson, J. “Interaction Sup- port for Visual Comparison Inspired by Natural Behavior”. In: IEEETransactionsonVisualizationandComputerGraphics18.12 (2012), pp. 2719–2728. doi: 10.1109/TVCG.2012.237 (cited on pages 186, 188–190). [TFS08a] Thiede, C., Fuchs, G., and Schumann, H. “Smart Lenses”. In: Proceedings of the Smart Graphics (SG). Springer, 2008, pp. 178– 189. doi: 10.1007/978-3-540-85412-8_16 (cited on page 206). [TFS08b] Tominski, C., Fuchs, G., and Schumann, H. “Task-Driven Color Coding”.In:ProceedingsoftheInternationalConferenceInforma- tion Visualisation (IV). IEEE Computer Society, 2008, pp. 373– 380. doi: 10.1109/IV.2008.24 (cited on page 58). [TLH10] Talbot, J., Lin, S., and Hanrahan, P. “An Extension of Wilkin- son’s Algorithm for Positioning Tick Labels on Axes”. In: IEEE TransactionsonVisualizationandComputerGraphics16.6(2010), pp. 1036–1043. doi: 10.1109/TVCG.2010.130 (cited on page 57). [Tob70] Tobler, W. R. “A Computer Movie Simulating Urban Growth in the Detroit Region”. In: Economic Geography 46.6 (1970), pp. 234–240. doi: 10.2307/143141 (cited on pages 25, 98). [Tom+06] Tominski,C.,Abello,J.,vanHam,F.,andSchumann,H.“Fisheye Tree Views and Lenses for Graph Visualization”. In: Proceedings of the International Conference Information Visualisation (IV). IEEEComputerSociety,2006,pp.17–24.doi:10.1109/IV.2006. 54 (cited on page 179). [Tom+12] Tominski, C., Schumann, H., Andrienko, G., and Andrienko, N. “Stacking-Based Visualization of Trajectory Attribute Data”. In: IEEETransactionsonVisualizationandComputerGraphics18.12 (2012), pp. 2565–2574. doi: 10.1109/TVCG.2012.265 (cited on pages 107, 181). [Tom+17] Tominski, C., Gladisch, S., Kister, U., Dachselt, R., and Schu- mann, H. “Interactive Lenses for Visualization: An Extended Survey”. In: Computer Graphics Forum 36.6 (2017), pp. 173–200. doi: 10.1111/cgf.12871 (cited on pages 173, 174, 206).
336 (cid:4) Bibliography [Tom15] Tominski, C. Interaction for Visualization. Synthesis Lectures on Visualization 3. Morgan & Claypool, 2015. doi: 10.2200/ S00651ED1V01Y201506VIS003 (cited on pages 203, 206, 341). [Tom16] Tominski, C. “CompaRing: Reducing Costs of Visual Compar- ison”. In: Short Paper Proceedings of the Eurographics Confer- ence on Visualization (EuroVis). Eurographics Association, 2016, pp. 137–141. doi: 10.2312/eurovisshort.20161175 (cited on page 192). [TS08] Tominski, C. and Schumann, H. “Enhanced Interactive Spiral Display”. In: Proceedings of the Annual SIGRAD Conference, Special Theme: Interactivity. Linköping University Electronic Press, 2008, pp. 53–56. url: https://www.ep.liu.se/ecp_ article/index.en.aspx?issue=034;article=013 (cited on page 89). [TS12] Tominski, C. and Schulz, H.-J. “The Great Wall of Space-Time”. In:Proceedings of the Workshop on Vision, Modeling & Visualiza- tion (VMV). Eurographics Association, 2012, pp. 199–206. doi: 10.2312/PE/VMV/VMV12/199-206 (cited on page 109). [TSS05] Tominski, C., Schulze-Wollgast, P., and Schumann, H. “3D Infor- mation Visualization for Time Dependent Data on Maps”. In: Proceedings of the International Conference Information Visual- isation (IV). IEEE Computer Society, 2005, pp. 175–181. doi: 10.1109/IV.2005.3 (cited on page 108). [Tuf83] Tufte, E. R. The Visual Display of Quantitative Information. Graphics Press, 1983 (cited on pages 18, 86). [Tuk77] Tukey, J. W. Exploratory Data Analysis. Addison-Wesley, 1977 (cited on page 58). [vHP09] Van Ham, F. and Perer, A. “Search, Show Context, Expand on Demand: Supporting Large Graph Exploration with Degree-of- Interest ”. In: IEEE Transactions on Visualization and Computer Graphics 15.6(2009),pp.953–960.doi:10.1109/TVCG.2009.108 (cited on page 215). [vHSD09] Van Ham, F., Schulz, H.-J., and Dimicco, J. M. “Honeycomb: Visual Analysis of Large Scale Social Networks”. In: Proceedings of the 12th IFIP Conference on Human-Computer Interaction (INTERACT). Springer, 2009, pp. 429–442. doi: 10.1007/978- 3-642-03658-3_47 (cited on page 115). [Vic99] Vicente, K. J. Cognitive Work Analysis: Toward Safe, Productive, and Healthy Computer-Based Work. CRC Press, 1999 (cited on page 50).
Bibliography (cid:4) 337 [vLan+11] Von Landesberger, T., Kuijper, A., Schreck, T., Kohlhammer, J., van Wijk, J. J., Fekete, J.-D., and Fellner, D. W. “Visual Analysis of Large Graphs: State-of-the-Art and Future Research Challenges”.In:Computer Graphics Forum 30.6(2011),pp.1719– 1749. doi: 10.1111/j.1467-8659.2011.01898.x (cited on page 127). [vLan+14] VonLandesberger,T.,Bremm,S.,Schreck,T.,andFellner,D.W. “Feature-Based Automatic Identification of Interesting Data Seg- ments in Group Movement Data”. In: Information Visualization 13.3 (2014), pp. 190–212. doi: 10.1177/1473871613477851.2 (cited on page 225). [vLan18] Von Landesberger, T. “Insights by Visual Comparison: The State and Challenges”. In: IEEE Computer Graphics and Applications 38.3 (2018), pp. 140–148. doi: 10.1109/MCG.2018.032421661 (cited on page 206). [VW93] Visvalingam, M. and Whyatt, J. D. “Line Generalisation by Repeated Elimination of Points”. In: The Cartographic Jour- nal 30.1 (1993), pp. 46–51. doi: 10.1179/000870493786962263 (cited on page 100). [vWal+96] Van Walsum, T., Post, F. H., Silver, D., and Post, F. J. “Feature Extraction and Iconic Visualization”. In: IEEE Transactions on Visualization and Computer Graphics 2.2 (1996), pp. 111–119. doi: 10.1109/2945.506223 (cited on page 222). [vWij06] Van Wijk, J. J. “Views on Visualization”. In: IEEE Transactions onVisualizationandComputerGraphics 12.4(2006),pp.421–433. doi: 10.1109/TVCG.2006.80 (cited on pages 47, 50, 282, 289). [vWij08] Van Wijk, J. J. “Unfolding the Earth: Myriahedral Projections”. In: The Cartographic Journal 45.1 (2008), pp. 32–42. doi: 10. 1179/000870408X276594 (cited on page 100). [vWN04] Van Wijk, J. J. and Nuij, W. A. A. “A Model for Smooth View- ing and Navigation of Large 2D Information Spaces”. In: IEEE TransactionsonVisualizationandComputerGraphics10.4(2004), pp. 447–458. doi: 10.1109/TVCG.2004.1 (cited on page 167). [vWvS99] VanWijk,J.J.andvanSelow,E.R.“ClusterandCalendarBased Visualization of Time Series Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1999, pp. 4–9. doi: 10.1109/INFVIS.1999.801851 (cited on pages 92, 93, 341). [War02] Ward,M.O.“ATaxonomyofGlyphPlacementStrategiesforMul- tidimensional Data Visualization”. In: Information Visualization 1.2 (2002), pp. 194–210. doi: 10.1057/palgrave.ivs.9500025 (cited on page 75).
338 (cid:4) Bibliography [War12] Ware, C. Information Visualization: Perception for Design. 3rd edition. Morgan Kaufmann, 2012 (cited on pages 50, 127). [WGK15] Ward, M. O., Grinstein, G., and Keim, D. Interactive Data Visu- alization: Foundations, Techniques, and Applications. 2nd edition. A K Peters/CRC Press, 2015 (cited on pages 13, 127). [WH04] Wolfe, J. M. and Horowitz, T. S. “What Attributes Guide the Deployment of Visual Attention and How do They do it?” In: Nature Reviews Neuroscience 05.6 (2004), pp. 495–501. doi: 10. 1038/nrn1411 (cited on page 155). [Wil11] Wills, G. Visualizing Time: Designing Graphical Representations for Statistical Data. Springer, 2011. doi: 10.1007/978-0-387- 77907-2 (cited on page 127). [Wil96] Wills,G.J.“Selection:524,288WaystoSay“ThisisInteresting””. In:ProceedingsoftheIEEESymposiumInformationVisualization (InfoVis). IEEE Computer Society, 1996, pp. 54–60. doi: 10. 1109/INFVIS.1996.559216 (cited on page 150). [WWK00] WangBaldonado,M.Q.,Woodruff,A.,andKuchinsky,A.“Guide- lines for Using Multiple Views in Information Visualization”. In: Proceedings of the Conference on Advanced Visual Interfaces (AVI). ACM Press, 2000, pp. 110–119. doi: 10.1145/345513. 345271 (cited on page 65). [XW05] Xu, R. and Wunsch, D. C. “Survey of Clustering Algorithms”. In: IEEE Transactions on Neural Networks 16.3 (2005), pp. 645–678. doi: 10.1109/TNN.2005.845141 (cited on page 265). [Yi+07] Yi, J. S., ah Kang, Y., Stasko, J. T., and Jacko, J. A. “Toward a Deeper Understanding of the Role of Interaction in Informa- tion Visualization”. In: IEEE Transactions on Visualization and Computer Graphics 13.6 (2007), pp. 1224–1231. doi: 10.1109/ TVCG.2007.70515 (cited on page 132). [Yu+12] Yu, L., Efstathiou, K., Isenberg, P., and Isenberg, T. “Efficient Structure-Aware Selection Techniques for 3D Point Cloud Visual- izations with 2DOF Input”. In: IEEE Transactions on Visualiza- tion and Computer Graphics 18.12 (2012), pp. 2245–2254. doi: 10.1109/TVCG.2012.217 (cited on page 158). [Zgr+17] Zgraggen,E.,Galakatos,A.,Crotty,A.,Fekete,J.-D.,andKraska, T. “How Progressive Visualizations Affect Exploratory Analysis”. In: IEEE Transactions on Visualization and Computer Graphics 23.8 (2017), pp. 1977–1987. doi: 10.1109/TVCG.2016.2607714 (cited on page 303). [ZH16] Zhou, L. and Hansen, C. D. “A Survey of Colormaps in Visual- ization”. In: IEEE Transactions on Visualization and Computer Graphics 22.8 (2016), pp. 2051–2069. doi: 10.1109/TVCG.2015. 2489649 (cited on page 127).
Index 2D, 63, 103 density-based representations 3D, 63, 103, 107 data density, 209 visual density, 210 action cycle, 135 dimensional stacking, 78 action patterns, 133 dimensionality reduction, 257 activity recognition, 240 dimming, 154 aggregation, 232 direct manipulation, 138 analysis history, 133, 274 dynamic filtering, 8, 149 analytic questions, 31 dynamic graphs, 250 animation, 66, 86, 147, 167 dynamic querying, 149 attenuation, 153 attribute space, 22, 27 effectiveness, 17, 55 efficiency, 18 box-whisker, 58 elementary question, 31 bring & go, 166 emphasis, 153 brushing & linking, 70, 157 exploration, 29, 47 bundling, 212 expressiveness, 17 cartographic generalization, 100 factors, 19 cartography, 99 context, 35 classification, 239 data, 19 clustering, 92, 243, 250 tasks, 28 color coding, 6, 57 feature-based analysis, 220 computational analysis, 208 features confirmation, 30 definition, 220 context, 35 extraction, 221 cyclic visualization, 89 visualization, 222 filtering, 154 data abstraction, 231 first law of geography, 98 data chunking, 291 fish-eye distortion, 64, 178 data classes, 27 fluid interaction, 143, 289 data domain, 19 focus+context, 64, 302 data element, 21 data scale, 19 geo-spatial data, 98 data scope, 24 geographic space, 96 data space, 22 globe, 300 data table, 21 glyphs, 73, 108 decision trees, 239 goals, 29 degree of interest, 11, 214, 284 graph data, 5, 28, 111 339
340 (cid:4) Index graph layout, 113, 299 multi-scale data, 233 graph lenses, 178 multi-scale slider, 169 graph visualization, 6, 113, 250, 283 multi-threading, 294 guidance, 10, 233, 277 multiple views, 9, 65, 123, 227 definition, 278 multivariate data, 23, 27, 67 degree, 281 guidelines, 143 navigation, 159, 283 nested model, 41 highlighting, 154 networks, 111 human factors, 35 node-link diagram, 6, 113, 299 human in the loop, 131 occlusion, 105 icicle plot, 249 off-screen visualization, 165 incremental visualization, see overview+detail, 64, 162, 227, 302 progressive visual analysis interaction parallel coordinates, 71 continuous, 146, 149, 151, 169 partial results, 293 discrete, 146 PCA, 258 interaction costs, 136 presentation, 30 interaction intents, 132 principal component analysis, 258 interactive lenses, 173 progressive visual analysis, 290 interpolation, 25 quality criteria, 17 knowledge gap, 279 reference space, 22, 27 knowledge generation, 48 region of interest, 298 lasso, 149 representation effect, 52 latency, 289, 296 rubberband, 149 magic lenses, see interactive lenses sampling, 232 mantra scalar data, 21 visual analytics, 3, 208 scatter plot, 61 visual information seeking, 159 scatter plot matrix, 69 map projection, 99 selection, 148 maps, 99 self-organizing maps, 248 marks, 54, 62 singular value decomposition, 259 matrix, 115, 227, 241 sliders, 151 matrix ordering, 116 small multiples, 86 MDS, 258 smart environments, 269 meta-data, 26 smooth brushing, 156 mosaic plot, 78 space-time cube, 108, 121 movement data, 106, 224 spatial data, 27 multi-dimensional data, 23 spatio-temporal data, 27, 106 multi-dimensional scaling, 258 spiral visualization, 89 multi-display environments, 268 stream graph, 89 multi-faceted graphs, 28, 112, 118 synoptic question, 31
Index (cid:4) 341 table lens, 68, 249 verification, 48 tasks, 28 visibility widgets, 105 temporal data, 27, 85 visual comparison, 58 temporal relations, 83 visual cues, 11, 165 tensor data, 21 visual encoding, 54 terrain rendering, 100 visual variables, 54 time, 82 visualization trajectory visualization, 107 axes-based, 72, 87, 171 treemap visualization, 117, 302 glyph-based, 73, 101 trees, 111 nested, 77, 91 tuple, 21 pixel-based, 75 two-tone coloring, 59, 68 table-based, 67, 261 visualization definition, 2 uncertainty, 93, 125 visualization design, 41 undo, 133, 275 visualization pipeline, 44 univariate data, 23 visualization reference model, 44 usability, 143 zoomable interfaces, 159 value range, 21 animated transitions, 167 variable, 22 zooming & panning, 163 vector data, 21
Figure Credits CopyrightedFigures The following figures have been taken from previously published works. The copyright of these figures remains with the respective publishers. Figure 1.5 Reprinted by permission from Springer Nature Customer Service Centre GmbH: Radloff, A. et al. “Smart Views in Smart Environments”. In: Proceedings of the Smart Graphics. Springer, 2011, pp. 1–12. doi: 10.1007/978-3-642-22571-0_1, © 2011. Figure 3.24a and Figure 3.28 (central part) Reprinted by permission from Springer Nature Customer Service Centre GmbH: Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642-57193-0, © 2000. Figure 3.24b © 2005 IEEE. Reprinted, with permission, from Schumann, H. and Müller, W. Visualisierung: Grundlagen und Allgemeine Methoden. Springer, 2000. doi: 10.1007/978-3-642-57193-0. Figure 3.26 (main part) © 2007 IEEE. Reprinted, with permission, from Luboschik, M. and Schumann, H. “Explode to Explain – Illustrative Information Visualization”. In: Proceedings of the International Confer- ence Information Visualisation (IV). IEEE Computer Society, 2007. doi: 10.1109/IV.2007.50. Figure 3.38 © 1999 IEEE. Reprinted, with permission, from van Wijk, J. J. and van Selow, E. R. “Cluster and Calendar Based Visualization of Time Series Data”. In: Proceedings of the IEEE Symposium Information Visualization (InfoVis). IEEE Computer Society, 1999, pp. 4–9. doi: 10.1109/INFVIS.1999.801851. Figure 4.49 © 2015 Morgan & Claypool. Reprinted with kind permis- sion from Tominski, C. Interaction for Visualization. Synthesis Lec- tures on Visualization 3. Morgan & Claypool, 2015. doi: 10.2200/ S00651ED1V01Y201506VIS003. Figure 5.1 © 2008 IEEE. Reprinted, with permission, from Bachthaler, S. and Weiskopf, D. “Continuous Scatterplots”. In: IEEE Transactions on Visualization and Computer Graphics 14.6 (2008), pp. 1428–1435. doi: 10.1109/TVCG.2008.119. 343
344 (cid:4) Figure Credits Figure 5.3 © 2006 IEEE. Reprinted, with permission, from Novotny, M. and Hauser, H. “Outlier-Preserving Focus+Context Visualization in Parallel Coordinates”. In: IEEE Transactions on Visualization and Computer Graphics 12.5 (2006), pp. 893–900. doi: 10.1109/TVCG.2006.170. Figures 5.9 and 5.10 © 2014 IEEE. Reprinted, with permission, from Abello, J. et al. “A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks”. In: IEEE Transactions on Visualization and Computer Graphics 20.3 (2014), pp. 337–350. doi: 10.1109/TVCG.2013.109. Figures 5.43 and 5.44 Reprinted by permission from Springer Nature Cus- tomer Service Centre GmbH: Aigner, W. et al. Visualization of Time- Oriented Data. Springer, 2011. doi: 10.1007/978-0-85729-079-3, © 2011. Figure 6.1 Reprinted by permission from Springer Nature Customer Service Centre GmbH: Eichner, C. et al. “A Novel Infrastructure for Supporting Display Ecologies”. In: Advances in Visual Computing: Proceedings of the International Symposium on Visual Computing (ISVC). Springer, 2015, pp. 722–732. doi: 10.1007/978-3-319-27863-6\_68, © 2015. Figure 6.4 © 2012 IEEE. Reprinted, with permission, from Radloff, A. et al. “Smart Interaction Management: An Interaction Approach for Smart Meeting Rooms”. In: Proceedings of the Eighth International Conference onIntelligentEnvironments(IE).IEEEComputerSociety,2012,pp.228– 235. doi: 10.1109/IE.2012.34. Figure 6.12 © 2012 IEEE. Reprinted, with permission, from Streit, M. et al. “Model-Driven Design for the Visual Analysis of Heterogeneous Data”. In: IEEE Transactions on Visualization and Computer Graphics 18.6 (2012), pp. 998–1010. doi: 10.1109/TVCG.2011.108. FiguresLicensedUnderCreativeCommonsLicense The figures listed in the following are licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit creativecommons.org/licenses/by/4.0/. Figures 3.12, 3.51, 5.15, 5.16, 5.19 to 5.21, 5.32 and 5.33 by Martin Röhlig. Figure 3.25 by Thomas Nocke. Figures 3.46 and 3.50 by Steve Dübel. Figure 5.4 by Helwig Hauser. Figures 5.8, 5.40 and 5.41 by Steffen Hadlak.
Figure Credits (cid:4) 345 Figure 5.12a and Figures 5.13, 5.14, 6.2 and 6.6 by Christian Eichner. Figures 5.22, 5.28 and 5.29 by Martin Luboschik. Figure 6.20 by Marco Angelini. Figure 6.24 by Axel Radloff-Delosea. AvailabilityofOriginalFigures All original figures from this book are released under the Creative Commons Attribution 4.0 International License (CC BY 4.0). The figures are available on https://ivda-book.de.
--- End of content from file: (AK Peters Visualization) Christian Tominski_ Heidrun Schumann - Interactive Visual Data Analysis-CRC Press (2020).pdf ---


--- Start of content from file: 2021 Chan, Stanley ~ Introduction to Probability for Data Science [Michigan Publishing] _.pdf ---
Introduction to Probability for Data Science Stanley H. Chan Purdue University
Copyright (cid:13)c2021 Stanley H. Chan This book is published by Michigan Publishing under an agreement with the author. It is made available free of charge in electronic form to any student or instructor interested in the subject matter. Published in the United States of America by Michigan Publishing Manufactured in the United States of America ISBN 978-1-60785-746-4 (hardcover) ISBN 978-1-60785-747-1 (electronic) ii
To Vivian, Joanna, and Cynthia Chan And ye shall know the truth, and the truth shall make you free. John 8:32 iii
iv
Preface Thisbookisanintroductorytextbookinundergraduateprobability.Ithasamission:tospell out the motivation, intuition, and implication of the probabilistic tools we use in science and engineering. From over half a decade of teaching the course, I have distilled what I believetobethecoreofprobabilisticmethods.Iputthebookinthecontextofdatascience to emphasize the inseparability between data (computing) and probability (theory) in our time. Probability is one of the most interesting subjects in electrical engineering and com- puter science. It bridges our favorite engineering principles to the practical reality, a world thatisfullofuncertainty.However,becauseprobabilityissuchamaturesubject,theunder- graduate textbooks alone might fill several rows of shelves in a library. When the literature issorich,thechallengebecomeshowonecanpiercethroughtotheinsightwhiledivinginto thedetails.Forexample,manyofyouhaveusedanormalrandomvariablebefore,buthave you ever wondered where the “bell shape” comes from? Every probability class will teach you about flipping a coin, but how can “flipping a coin” ever be useful in machine learning today? Data scientists use the Poisson random variables to model the internet traffic, but where does the gorgeous Poisson equation come from? This book is designed to fill these gaps with knowledge that is essential to all data science students. Thisleadstothethreegoalsofthebook.(i)Motivation:Intheoceanofmathematical definitions,theorems,andequations,whyshouldwespendourtimeonthisparticulartopic but not another? (ii) Intuition: When going through the derivations, is there a geometric interpretation or physics beyond those equations? (iii) Implication: After we have learned a topic, what new problems can we solve? The book’s intended audience is undergraduate juniors/seniors and first-year gradu- ate students majoring in electrical engineering and computer science. The prerequisites are standard undergraduate linear algebra and calculus, except for the section about charac- teristic functions, where Fourier transforms are needed. An undergraduate course in signals and systems would suffice, even taken concurrently while studying this book. Thelengthofthebookissuitableforatwo-semestercourse.Instructorsareencouraged tousethesetofchaptersthatbestfitstheirclasses.Forexample,abasicprobabilitycourse canuseChapters1-5asitsbackbone.Chapter6onsamplestatisticsissuitableforstudents whowishtogaintheoreticalinsightsintoprobabilisticconvergence.Chapter7onregression and Chapter 8 on estimation best suit students who want to pursue machine learning and signalprocessing.Chapter9discussesconfidenceintervalsandhypothesistesting,whichare criticaltomoderndataanalysis.Chapter10introducesrandomprocesses.Myapproachfor random processes is more tailored to information processing and communication systems, which are usually more relevant to electrical engineering students. Additional teaching resources can be found on the book’s website, where you can v
findlecturevideosandhomeworkvideos.Throughoutthebookyouwillsee many“practice exercises”,whichareeasyproblemswithworked-outsolutions.Theycanbeskippedwithout loss to the flow of the book. Acknowledgements: If I could thank only one person, it must be Professor Fawwaz Ulaby of the University of Michigan. Professor Ulaby has been the source of support in all aspects, from the book’s layout to technical content, proofreading, and marketing. The book would not have been published without the help of Professor Ulaby. I am deeply movedbyProfessorUlaby’svisionthateducationshouldbemadeaccessibletoallstudents. Withtextbookpricesrocketingup,theEECSfreetextbookinitiativelaunchedbyProfessor Ulaby is the most direct response to the publishers, teachers, parents, and students. Thank you, Fawwaz, for your unbounded support — technically, mentally, and financially. Thank you also for recommending Richard Carnes. The meticulous details Richard offered have significantly improved the fluency of the book. Thank you, Richard. I thank my colleagues at Purdue who had shared many thoughts with me when I taughtthecourse(inalphabeticalorder):ProfessorsMarkBell,MaryComer,SaulGelfand, Amy Reibman, and Chih-Chun Wang. My teaching assistant I-Fan Lin was instrumental in the early development of this book. To the graduate students of my lab (Yiheng Chi, Nick Chimitt, Kent Gauen, Abhiram Gnanasambandam, Guanzhe Hong, Chengxi Li, Zhiyuan Mao, Xiangyu Qu, and Yash Sanghvi): Thank you! It would have been impossible to finish the book without your participation. A few students I taught volunteered to help edit the book: Benjamin Gottfried, Harrison Hsueh, Dawoon Jung, Antonio Kincaid, Deepak Ravikumar, Krister Ulvog, Peace Umoru, Zhijing Yao. I would like to thank my Ph.D. advisor Professor Truong Nguyen for encouraging me to write the book. Finally, I would like to thank my wife Vivian and my daughters, Joanna and Cynthia, for their love, patience, and support. Stanley H. Chan, West Lafayette, Indiana May, 2021 Companion website: https://probability4datascience.com/ vi
Contents 1 Mathematical Background 1 1.1 Infinite Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Geometric Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.2 Binomial Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2 Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.2.1 Taylor approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.2.2 Exponential series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2.3 Logarithmic approximation . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3.1 Odd and even functions . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.3.2 Fundamental Theorem of Calculus . . . . . . . . . . . . . . . . . . . . 17 1.4 Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 1.4.1 Why do we need linear algebra in data science? . . . . . . . . . . . . . 20 1.4.2 Everything you need to know about linear algebra . . . . . . . . . . . 21 1.4.3 Inner products and norms . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.4.4 Matrix calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.5 Basic Combinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.5.1 Birthday paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.5.2 Permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.5.3 Combination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2 Probability 43 2.1 Set Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.1.1 Why study set theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 2.1.2 Basic concepts of a set . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.1.3 Subsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.1.4 Empty set and universal set . . . . . . . . . . . . . . . . . . . . . . . . 48 2.1.5 Union . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.1.6 Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 2.1.7 Complement and difference . . . . . . . . . . . . . . . . . . . . . . . . 52 2.1.8 Disjoint and partition . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 2.1.9 Set operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 2.1.10 Closing remarks about set theory . . . . . . . . . . . . . . . . . . . . . 57 vii
CONTENTS 2.2 Probability Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 2.2.1 Sample space Ω . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 2.2.2 Event space F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 2.2.3 Probability law P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 2.2.4 Measure zero sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 2.2.5 Summary of the probability space . . . . . . . . . . . . . . . . . . . . 74 2.3 Axioms of Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 2.3.1 Why these three probability axioms? . . . . . . . . . . . . . . . . . . . 75 2.3.2 Axioms through the lens of measure . . . . . . . . . . . . . . . . . . . 76 2.3.3 Corollaries derived from the axioms . . . . . . . . . . . . . . . . . . . 77 2.4 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 2.4.1 Definition of conditional probability . . . . . . . . . . . . . . . . . . . 81 2.4.2 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 2.4.3 Bayes’ theorem and the law of total probability . . . . . . . . . . . . . 89 2.4.4 The Three Prisoners problem . . . . . . . . . . . . . . . . . . . . . . . 92 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 2.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 2.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 3 Discrete Random Variables 103 3.1 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 3.1.1 A motivating example . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 3.1.2 Definition of a random variable . . . . . . . . . . . . . . . . . . . . . . 105 3.1.3 Probability measure on random variables . . . . . . . . . . . . . . . . 107 3.2 Probability Mass Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 3.2.1 Definition of probability mass function . . . . . . . . . . . . . . . . . . 110 3.2.2 PMF and probability measure. . . . . . . . . . . . . . . . . . . . . . . 110 3.2.3 Normalization property . . . . . . . . . . . . . . . . . . . . . . . . . . 112 3.2.4 PMF versus histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 3.2.5 Estimating histograms from real data . . . . . . . . . . . . . . . . . . 117 3.3 Cumulative Distribution Functions (Discrete) . . . . . . . . . . . . . . . . . . 121 3.3.1 Definition of the cumulative distribution function . . . . . . . . . . . . 121 3.3.2 Properties of the CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 3.3.3 Converting between PMF and CDF . . . . . . . . . . . . . . . . . . . 124 3.4 Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 3.4.1 Definition of expectation. . . . . . . . . . . . . . . . . . . . . . . . . . 125 3.4.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 130 3.4.3 Properties of expectation . . . . . . . . . . . . . . . . . . . . . . . . . 130 3.4.4 Moments and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 3.5 Common Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . 136 3.5.1 Bernoulli random variable . . . . . . . . . . . . . . . . . . . . . . . . . 137 3.5.2 Binomial random variable . . . . . . . . . . . . . . . . . . . . . . . . . 143 3.5.3 Geometric random variable . . . . . . . . . . . . . . . . . . . . . . . . 149 3.5.4 Poisson random variable . . . . . . . . . . . . . . . . . . . . . . . . . . 152 3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 3.7 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 3.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 viii
CONTENTS 4 Continuous Random Variables 171 4.1 Probability Density Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 4.1.1 Some intuitions about probability density functions. . . . . . . . . . . 172 4.1.2 More in-depth discussion about PDFs . . . . . . . . . . . . . . . . . . 174 4.1.3 Connecting with the PMF . . . . . . . . . . . . . . . . . . . . . . . . . 178 4.2 Expectation, Moment, and Variance . . . . . . . . . . . . . . . . . . . . . . . 180 4.2.1 Definition and properties . . . . . . . . . . . . . . . . . . . . . . . . . 180 4.2.2 Existence of expectation . . . . . . . . . . . . . . . . . . . . . . . . . . 183 4.2.3 Moment and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 4.3 Cumulative Distribution Function . . . . . . . . . . . . . . . . . . . . . . . . 185 4.3.1 CDF for continuous random variables . . . . . . . . . . . . . . . . . . 186 4.3.2 Properties of CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 4.3.3 Retrieving PDF from CDF . . . . . . . . . . . . . . . . . . . . . . . . 193 4.3.4 CDF: Unifying discrete and continuous random variables . . . . . . . 194 4.4 Median, Mode, and Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.4.1 Median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 4.4.2 Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.4.3 Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 4.5 Uniform and Exponential Random Variables. . . . . . . . . . . . . . . . . . . 201 4.5.1 Uniform random variables . . . . . . . . . . . . . . . . . . . . . . . . . 202 4.5.2 Exponential random variables . . . . . . . . . . . . . . . . . . . . . . . 205 4.5.3 Origin of exponential random variables. . . . . . . . . . . . . . . . . . 207 4.5.4 Applications of exponential random variables . . . . . . . . . . . . . . 209 4.6 Gaussian Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 4.6.1 Definition of a Gaussian random variable . . . . . . . . . . . . . . . . 211 4.6.2 Standard Gaussian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 4.6.3 Skewness and kurtosis . . . . . . . . . . . . . . . . . . . . . . . . . . . 216 4.6.4 Origin of Gaussian random variables . . . . . . . . . . . . . . . . . . 220 4.7 Functions of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 223 4.7.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 4.7.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 4.8 Generating Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 4.8.1 General principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 4.8.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 4.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 4.10 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235 4.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 5 Joint Distributions 241 5.1 Joint PMF and Joint PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 5.1.1 Probability measure in 2D . . . . . . . . . . . . . . . . . . . . . . . . . 244 5.1.2 Discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . 245 5.1.3 Continuous random variables . . . . . . . . . . . . . . . . . . . . . . . 247 5.1.4 Normalization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 5.1.5 Marginal PMF and marginal PDF . . . . . . . . . . . . . . . . . . . . 250 5.1.6 Independent random variables . . . . . . . . . . . . . . . . . . . . . . 252 5.1.7 Joint CDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 5.2 Joint Expectation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 ix
CONTENTS 5.2.1 Definition and interpretation . . . . . . . . . . . . . . . . . . . . . . . 257 5.2.2 Covariance and correlation coefficient . . . . . . . . . . . . . . . . . . 262 5.2.3 Independence and correlation . . . . . . . . . . . . . . . . . . . . . . . 264 5.2.4 Computing correlation from data . . . . . . . . . . . . . . . . . . . . . 265 5.3 Conditional PMF and PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 5.3.1 Conditional PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 5.3.2 Conditional PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 5.4 Conditional Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 5.4.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275 5.4.2 The law of total expectation . . . . . . . . . . . . . . . . . . . . . . . 276 5.5 Sum of Two Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 280 5.5.1 Intuition through convolution . . . . . . . . . . . . . . . . . . . . . . . 280 5.5.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 5.5.3 Sum of common distributions . . . . . . . . . . . . . . . . . . . . . . . 282 5.6 Random Vectors and Covariance Matrices . . . . . . . . . . . . . . . . . . . . 286 5.6.1 PDF of random vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 286 5.6.2 Expectation of random vectors . . . . . . . . . . . . . . . . . . . . . . 288 5.6.3 Covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289 5.6.4 Multidimensional Gaussian . . . . . . . . . . . . . . . . . . . . . . . . 290 5.7 Transformation of Multidimensional Gaussians . . . . . . . . . . . . . . . . . 293 5.7.1 Linear transformation of mean and covariance. . . . . . . . . . . . . . 293 5.7.2 Eigenvalues and eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 295 5.7.3 Covariance matrices are always positive semi-definite . . . . . . . . . . 297 5.7.4 Gaussian whitening . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 5.8 Principal-Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 303 5.8.1 The main idea: Eigendecomposition . . . . . . . . . . . . . . . . . . . 303 5.8.2 The eigenface problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 5.8.3 What cannot be analyzed by PCA? . . . . . . . . . . . . . . . . . . . 311 5.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 5.10 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 5.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315 6 Sample Statistics 319 6.1 Moment-Generating and Characteristic Functions . . . . . . . . . . . . . . . . 324 6.1.1 Moment-generating function. . . . . . . . . . . . . . . . . . . . . . . . 324 6.1.2 Sum of independent variables via MGF . . . . . . . . . . . . . . . . . 327 6.1.3 Characteristic functions . . . . . . . . . . . . . . . . . . . . . . . . . . 329 6.2 Probability Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 6.2.1 Union bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 6.2.2 The Cauchy-Schwarz inequality . . . . . . . . . . . . . . . . . . . . . . 335 6.2.3 Jensen’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 6.2.4 Markov’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339 6.2.5 Chebyshev’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 6.2.6 Chernoff’s bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 6.2.7 Comparing Chernoff and Chebyshev . . . . . . . . . . . . . . . . . . . 344 6.2.8 Hoeffding’s inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 6.3 Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 6.3.1 Sample average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 x
CONTENTS 6.3.2 Weak law of large numbers (WLLN) . . . . . . . . . . . . . . . . . . . 354 6.3.3 Convergence in probability . . . . . . . . . . . . . . . . . . . . . . . . 356 6.3.4 Can we prove WLLN using Chernoff’s bound? . . . . . . . . . . . . . 359 6.3.5 Does the weak law of large numbers always hold? . . . . . . . . . . . . 360 6.3.6 Strong law of large numbers . . . . . . . . . . . . . . . . . . . . . . . . 361 6.3.7 Almost sure convergence . . . . . . . . . . . . . . . . . . . . . . . . . . 362 6.3.8 Proof of the strong law of large numbers . . . . . . . . . . . . . . . . . 364 6.4 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 6.4.1 Convergence in distribution . . . . . . . . . . . . . . . . . . . . . . . . 368 6.4.2 Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 372 6.4.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 6.4.4 Limitation of the Central Limit Theorem . . . . . . . . . . . . . . . . 379 6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381 6.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 6.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 7 Regression 389 7.1 Principles of Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394 7.1.1 Intuition: How to fit a straight line? . . . . . . . . . . . . . . . . . . . 395 7.1.2 Solving the linear regression problem . . . . . . . . . . . . . . . . . . . 397 7.1.3 Extension: Beyond a straight line . . . . . . . . . . . . . . . . . . . . . 401 7.1.4 Overdetermined and underdetermined systems . . . . . . . . . . . . . 409 7.1.5 Robust linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . 412 7.2 Overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418 7.2.1 Overview of overfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . 419 7.2.2 Analysis of the linear case . . . . . . . . . . . . . . . . . . . . . . . . . 420 7.2.3 Interpreting the linear analysis results . . . . . . . . . . . . . . . . . . 425 7.3 Bias and Variance Trade-Off. . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 7.3.1 Decomposing the testing error . . . . . . . . . . . . . . . . . . . . . . 430 7.3.2 Analysis of the bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 7.3.3 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436 7.3.4 Bias and variance on the learning curve . . . . . . . . . . . . . . . . . 438 7.4 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 7.4.1 Ridge regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 7.4.2 LASSO regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 7.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457 7.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458 7.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459 8 Estimation 465 8.1 Maximum-Likelihood Estimation . . . . . . . . . . . . . . . . . . . . . . . . . 468 8.1.1 Likelihood function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468 8.1.2 Maximum-likelihood estimate . . . . . . . . . . . . . . . . . . . . . . . 472 8.1.3 Application 1: Social network analysis . . . . . . . . . . . . . . . . . . 478 8.1.4 Application 2: Reconstructing images . . . . . . . . . . . . . . . . . . 481 8.1.5 More examples of ML estimation . . . . . . . . . . . . . . . . . . . . . 484 8.1.6 Regression versus ML estimation . . . . . . . . . . . . . . . . . . . . . 488 8.2 Properties of ML Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 xi
CONTENTS 8.2.1 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 8.2.2 Unbiased estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 8.2.3 Consistent estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . 494 8.2.4 Invariance principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500 8.3 Maximum A Posteriori Estimation . . . . . . . . . . . . . . . . . . . . . . . . 503 8.3.1 The trio of likelihood, prior, and posterior . . . . . . . . . . . . . . . . 503 8.3.2 Understanding the priors . . . . . . . . . . . . . . . . . . . . . . . . . 504 8.3.3 MAP formulation and solution . . . . . . . . . . . . . . . . . . . . . . 506 8.3.4 Analyzing the MAP solution . . . . . . . . . . . . . . . . . . . . . . . 508 8.3.5 Analysis of the posterior distribution . . . . . . . . . . . . . . . . . . . 512 8.3.6 Conjugate prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513 8.3.7 Linking MAP with regression . . . . . . . . . . . . . . . . . . . . . . . 517 8.4 Minimum Mean-Square Estimation . . . . . . . . . . . . . . . . . . . . . . . . 521 8.4.1 Positioning the minimum mean-square estimation . . . . . . . . . . . 521 8.4.2 Mean squared error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 8.4.3 MMSE estimate = conditional expectation . . . . . . . . . . . . . . . 524 8.4.4 MMSE estimator for multidimensional Gaussian . . . . . . . . . . . . 530 8.4.5 Linking MMSE and neural networks . . . . . . . . . . . . . . . . . . . 533 8.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534 8.6 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535 8.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536 9 Confidence and Hypothesis 543 9.1 Confidence Interval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545 9.1.1 The randomness of an estimator . . . . . . . . . . . . . . . . . . . . . 545 9.1.2 Understanding confidence intervals . . . . . . . . . . . . . . . . . . . . 547 9.1.3 Constructing a confidence interval . . . . . . . . . . . . . . . . . . . . 550 9.1.4 Properties of the confidence interval . . . . . . . . . . . . . . . . . . . 553 9.1.5 Student’s t-distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 556 9.1.6 Comparing Student’s t-distribution and Gaussian . . . . . . . . . . . . 560 9.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 9.2.1 A brute force approach . . . . . . . . . . . . . . . . . . . . . . . . . . 562 9.2.2 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564 9.3 Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 9.3.1 What is a hypothesis? . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 9.3.2 Critical-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 569 9.3.3 p-value test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573 9.3.4 Z-test and T-test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576 9.4 Neyman-Pearson Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579 9.4.1 Null and alternative distributions . . . . . . . . . . . . . . . . . . . . . 579 9.4.2 Type 1 and type 2 errors . . . . . . . . . . . . . . . . . . . . . . . . . 581 9.4.3 Neyman-Pearson decision . . . . . . . . . . . . . . . . . . . . . . . . . 584 9.5 ROC and Precision-Recall Curve . . . . . . . . . . . . . . . . . . . . . . . . . 591 9.5.1 Receiver Operating Characteristic (ROC) . . . . . . . . . . . . . . . . 591 9.5.2 Comparing ROC curves . . . . . . . . . . . . . . . . . . . . . . . . . . 594 9.5.3 The ROC curve in practice . . . . . . . . . . . . . . . . . . . . . . . . 600 9.5.4 The Precision-Recall (PR) curve . . . . . . . . . . . . . . . . . . . . . 603 9.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607 xii
CONTENTS 9.7 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608 9.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 10 Random Processes 613 10.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 614 10.1.1 Everything you need to know about a random process . . . . . . . . . 614 10.1.2 Statistical and temporal perspectives . . . . . . . . . . . . . . . . . . . 616 10.2 Mean and Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . . . 620 10.2.1 Mean function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620 10.2.2 Autocorrelation function . . . . . . . . . . . . . . . . . . . . . . . . . . 624 10.2.3 Independent processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 631 10.3 Wide-Sense Stationary Processes . . . . . . . . . . . . . . . . . . . . . . . . . 632 10.3.1 Definition of a WSS process . . . . . . . . . . . . . . . . . . . . . . . . 633 10.3.2 Properties of R (τ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634 X 10.3.3 Physical interpretation of R (τ) . . . . . . . . . . . . . . . . . . . . . 635 X 10.4 Power Spectral Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 10.4.1 Basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 10.4.2 Origin of the power spectral density . . . . . . . . . . . . . . . . . . . 643 10.5 WSS Process through LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . 646 10.5.1 Review of linear time-invariant systems . . . . . . . . . . . . . . . . . 646 10.5.2 Mean and autocorrelation through LTI Systems. . . . . . . . . . . . . 647 10.5.3 Power spectral density through LTI systems . . . . . . . . . . . . . . . 649 10.5.4 Cross-correlation through LTI Systems . . . . . . . . . . . . . . . . . . 652 10.6 Optimal Linear Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656 10.6.1 Discrete-time random processes . . . . . . . . . . . . . . . . . . . . . . 656 10.6.2 Problem formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657 10.6.3 Yule-Walker equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 659 10.6.4 Linear prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 10.6.5 Wiener filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 665 10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672 10.8 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673 10.8.1 The Mean-Square Ergodic Theorem . . . . . . . . . . . . . . . . . . . 677 10.9 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678 10.10Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679 A Appendix 683 xiii
CONTENTS xiv
Chapter 1 Mathematical Background “Datascience”hasdifferentmeaningstodifferentpeople.Ifyouaskabiologist,datascience could mean analyzing DNA sequences. If you ask a banker, data science could mean pre- dicting the stock market. If you ask a software engineer, data science could mean programs anddatastructures;ifyouaskamachinelearningscientist,datasciencecouldmeanmodels and algorithms. However, one thing that is common in all these disciplines is the concept of uncertainty. We choose to learn from data because we believe that the latent information is embedded in the data — unprocessed, contains noise, and could have missing entries. If there is no randomness, all data scientists can close their business because there is simply no problem to solve. However, the moment we see randomness, our business comes back. Therefore, data science is the subject of making decisions in uncertainty. Themathematicsofanalyzinguncertaintyisprobability.Itisthe tooltohelpusmodel, analyze,andpredictrandomevents.Probabilitycanbestudiedinasmanywaysasyoucan thinkof.Youcantakearigorouscourseinprobabilitytheory,ora“probabilityfordummies” on the internet, or a typical undergraduate probability course offered by your school. This bookisdifferentfromallthese.Ourgoalistotellyouhowthingswork inthecontextofdata science. For example, why do we need those three axioms of probabilities and not others? Where does the “bell shape” Gaussian random variable come from? How many samples do we need to construct a reliable histogram? These questions are at the core of data science, and they deserve close attention rather than sweeping them under the rug. Tohelpyougetusedtothepaceandstyleofthisbook,inthischapter,wereviewsome of the very familiar topics in undergraduate algebra and calculus. These topics are meant to warm up your mathematics background so that you can follow the subsequent chapters. Specifically, in this chapter, we cover several topics. First, in Section 1.1 we discuss infinite series,somethingthatwillbeusedfrequentlywhenweevaluatetheexpectationandvariance of random variables in Chapter 3. In Section 1.2 we review the Taylor approximation, which will be helpful when we discuss continuous random variables. Section 1.3 discusses integration and reviews several tricks we can use to make integration easy. Section 1.4 deals with linear algebra, aka matrices and vectors, which are fundamental to modern data analysis. Finally, Section 1.5 discusses permutation and combination, two basic techniques to count events. 1
CHAPTER 1. MATHEMATICAL BACKGROUND 1.1 Infinite Series Imagine that you have a fair coin. If you get a tail, you flip it again. You do this repeatedly until you finally get a head. What is the probability that you need to flip the coin three times to get one head? This is a warm-up exercise. Since the coin is fair, the probability of obtaining a head is 1. The probability of getting a tail followed by a head is 1 × 1 = 1. Similarly, the 2 2 2 4 probabilityofgettingtwotailsandthenaheadis 1×1×1 = 1.Ifyoufollowthislogic,you 2 2 2 8 canwritedowntheprobabilitiesforallothercases.Foryourconvenience,wehavedrawnthe first few in Figure 1.1. As you have probably noticed, the probabilities follow the pattern {1,1,1,...}. 2 4 8 Figure 1.1: Supposeyouflipacoinuntilyouseeahead.ThisrequiresyoutohaveN−1tailsfollowed byahead.Theprobabilityofthissequenceofeventsare 1, 1, 1,...,whichformsaninfinitesequence. 2 4 8 We can also summarize these probabilities using a familiar plot called the histogram as shown in Figure 1.2. The histogram for this problem has a special pattern, that every value is one order higher than the preceding one, and the sequence is infinitely long. 0.5 0.4 0.3 0.2 0.1 0 1 2 3 4 5 6 7 8 9 10 Figure1.2:Thehistogramofflippingacoinuntilweseeahead.Thex-axisisthenumberofcoinflips, and the y-axis is the probability. Let us ask something harder: On average, if you want to be 90% sure that you will get a head, what is the minimum number of attempts you need to try? Five attempts? Ten attempts? Indeed, if you try ten attempts, you will very likely accomplish your goal. However, this would seem to be overkill. If you try five attempts, then it becomes unclear whether you will be 90% sure. 2
1.1. INFINITE SERIES This problem can be answered by analyzing the sequence of probabilities. If we make two attempts, then the probability of getting a head is the sum of the probabilities for one attempt and that of two attempts: 1 P[success after 1 attempt]= =0.5 2 1 1 P[success after 2 attempts]= + =0.75 2 4 Therefore, if you make 3 attempts or 4 attempts, you get the following probabilities: 1 1 1 P[success after 3 attempts]= + + =0.875 2 4 8 1 1 1 1 P[success after 4 attempts]= + + + =0.9375. 2 4 8 16 So if we try four attempts, we will have a 93.75% probability of getting a head. Thus, four attempts is the answer. The MATLAB / Python codes we used to generate Figure 1.2 are shown below. % MATLAB code to generate a geometric sequence p = 1/2; n = 1:10; X = p.^n; bar(n,X,’FaceColor’,[0.8, 0.2,0.2]); # Python code to generate a geometric sequence import numpy as np import matplotlib.pyplot as plt p = 1/2 n = np.arange(0,10) X = np.power(p,n) plt.bar(n,X) Thiswarm-upexercisehasperhapsraisedsomeofyourinterestinthesubject.However, we will not tell you everything now. We will come back to the probability in Chapter 3 when we discuss geometric random variables. In the present section, we want to make sure you have the basic mathematical tools to calculate quantities, such as a sum of fractional numbers. For example, what if we want to calculate P[success after 107 attempts]? Is there a systematic way of performing the calculation? Remark. You should be aware that the 93.75% only says that the probability of achieving thegoalishigh.Ifyouhaveabadday,youmaystillneedmorethanfourattempts.Therefore, when we stated the question, we asked for 90% “on average”. Sometimes you may need more attempts and sometimes fewer attempts, but on average, you have a 93.75% chance of succeeding. 1.1.1 Geometric Series A geometric series is the sum of a finite or an infinite sequence of numbers with a constant ratio between successive terms. As we have seen in the previous example, a geometric series 3
CHAPTER 1. MATHEMATICAL BACKGROUND appears naturally in the context of discrete events. In Chapter 3 of this book, we will use geometric series when calculating the expectation and moments of a random variable. Definition 1.1. Let 0<r <1, a finite geometric sequence of power n is a sequence of numbers (cid:26) (cid:27) 1,r,r2,...,rn . An infinite geometric sequence is a sequence of numbers (cid:26) (cid:27) 1,r,r2,r3,... . Theorem 1.1. The sum of a finite geometric series of power n is (cid:88)n 1−rn+1 rk =1+r+r2+···+rn = . (1.1) 1−r k=0 Proof. We multiply both sides by 1−r. The left hand side becomes (cid:32) n (cid:33) (cid:88) rk (1−r)=(cid:0) 1+r+r2+···+rn(cid:1) (1−r) k=0 =(cid:0) 1+r+r2+···+rn(cid:1) −(cid:0) r+r2+r3+···+rn+1(cid:1) ( =a) 1−rn+1, where (a) holds because terms are canceled due to subtractions. (cid:3) A corollary of Equation (1.1) is the sum of an infinite geometric sequence. Corollary 1.1. Let 0<r <1. The sum of an infinite geometric series is ∞ (cid:88) 1 rk =1+r+r2+···= . (1.2) 1−r k=0 Proof. We take the limit in Equation (1.1). This yields (cid:88)∞ (cid:88)n 1−rn+1 1 rk = lim rk = lim = . n→∞ n→∞ 1−r 1−r k=0 k=0 (cid:3) Remark. Note that the condition 0 < r < 1 is important. If r > 1, then the limit lim rn+1 in Equation (1.2) will diverge. The constant r cannot equal to 1, for oth- n→∞ erwise the fraction (1−rn+1)/(1−r) is undefined. We are not interested in the case when r =0, because the sum is trivially 1: (cid:80)∞ 0k =1+01+02+···=1. k=0 4
1.1. INFINITE SERIES ∞ Practice Exercise 1.1. Compute the infinite series (cid:80) 1 . 2k k=2 Solution. ∞ (cid:88) 1 1 1 = + +···+ 2k 4 8 k=2 (cid:18) (cid:19) 1 1 1 = 1+ + +··· 4 2 4 1 1 1 = · = . 4 1− 1 2 2 Remark. You should not be confused about a geometric series and a harmonic series. A harmonic series concerns with the sum of {1,1,1,1,...}. It turns out that1 2 3 4 ∞ (cid:88) 1 1 1 1 =1+ + + +···=∞. n 2 3 4 n=1 On the other hand, a squared harmonic series {1, 1 , 1 , 1 ,...} converges: 22 32 42 (cid:88)∞ 1 1 1 1 π2 =1+ + + +···= . n2 22 32 42 6 n=1 The latter result is known as the Basel problem. We can extend the main theorem by considering more complicated series, for example the following one. Corollary 1.2. Let 0<r <1. It holds that ∞ (cid:88) 1 krk−1 =1+2r+3r2+···= . (1.3) (1−r)2 k=1 Proof. Take the derivative on both sides of Equation (1.2). The left hand side becomes ∞ d (cid:88) rk = d (cid:0) 1+r+r2+···(cid:1) dr dr k=0 ∞ (cid:88) =1+2r+3r2+···= krk−1 k=1 (cid:18) (cid:19) d 1 1 The right hand side becomes = . dr 1−r (1−r)2 (cid:3) 1ThisresultcanbefoundinTomApostol,Mathematical Analysis,2ndEdition,Theorem8.11. 5
CHAPTER 1. MATHEMATICAL BACKGROUND Practice Exercise 1.2. Compute the infinite sum (cid:80)∞ k· 1 . k=1 3k Solution. We can use the derivative result: ∞ (cid:88) 1 1 1 1 k· =1· +2· +3· +··· 3k 3 9 27 k=1 (cid:18) (cid:19) 1 1 1 1 1 1 1 3 = · 1+2· +3· +··· = · = · = . 3 3 9 3 (1− 1)2 3 4 4 3 9 1.1.2 Binomial Series A geometric series is useful when handling situations such as N −1 failures followed by a success. However, we can easily twist the problem by asking: What is the probability of getting one head out of 3 independent coin tosses? In this case, the probability can be determined by enumerating all possible cases: P[1 head in 3 coins]=P[H,T,T]+P[T,H,T]+P[T,T,H] (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 1 1 1 1 1 1 = × × + × × + × × 2 2 2 2 2 2 2 2 2 3 = . 8 Figure 1.3 illustrates the situation. Figure 1.3: When flipping three coins independently, the probability of getting exactly one head can come from three different possibilities. What lessons have we learned in this example? Notice that you need to enumerate all possible combinations of one head and two tails to solve this problem. The number is 3 in our example. In general, the number of combinations can be systematically studied using combinatorics, which we will discuss later in the chapter. However, the number of combinations motivates us to discuss another background technique known as the binomial series. The binomial series is instrumental in algebra when handling polynomials such as (a+b)2 or (1+x)3. It provides a valuable formula when computing these powers. Theorem1.2 (Binomialtheorem). Foranyrealnumbersaandb,thebinomialseries of power n is n (cid:18) (cid:19) (cid:88) n (a+b)n = an−kbk, (1.4) k k=0 where (cid:0)n(cid:1) = n! . k k!(n−k)! 6
1.1. INFINITE SERIES The binomial theorem is valid for any real numbers a and b. The quantity (cid:0)n(cid:1) reads k as “n choose k”. Its definition is (cid:18) (cid:19) n n! def = , k k!(n−k)! where n! = n(n−1)(n−2)···3·2·1. We shall discuss the physical meaning of (cid:0)n(cid:1) in k Section 1.5. But we can quickly plug in the “n choose k” into the coin flipping example by letting n=3 and k =1: (cid:18) (cid:19) 3 3! Number of combinations for 1 head and 2 tails= = =3. 1 1!2! So you can see why we want you to spend your precious time learning about the binomial theorem. In MATLAB and Python, (cid:0)n(cid:1) can be computed using the commands as follows. k % MATLAB code to compute (N choose K) and K! n = 10; k = 2; nchoosek(n,k) factorial(k) # Python code to compute (N choose K) and K! from scipy.special import comb, factorial n = 10 k = 2 comb(n, k) factorial(k) The binomial theorem makes the most sense when we also learn about the Pascal’s identity. Theorem 1.3 (Pascal’s identity). Let n and k be positive integers such that k ≤ n. Then, (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) n n n+1 + = . (1.5) k k−1 k Proof. We start by recalling the definition of (cid:0)n(cid:1) . This gives us k (cid:18) (cid:19) (cid:18) (cid:19) n n n! n! + = + k k−1 k!(n−k)! (k−1)!(n−(k−1))! (cid:18) (cid:19) 1 1 =n! + , k!(n−k)! (k−1)!(n−k+1)! where we factor out n! to obtain the second equation. Next, we observe that 1 (n−k+1) n−k+1 × = , k!(n−k)! (n−k+1) k!(n−k+1)! 1 k k × = . (k−1)!(n−k+1)! k k!(n−k+1)! 7
CHAPTER 1. MATHEMATICAL BACKGROUND Substituting into the previous equation we obtain (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) n n n−k+1 k + =n! + k k−1 k!(n−k+1)! k!(n−k+1)! (cid:18) (cid:19) n+1 =n! k!(n−k+1)! (cid:18) (cid:19) (n+1)! n+1 = = . k!(n+1−k)! k (cid:3) The Pascal triangle is a visualization of the coefficients of (a+b)n as shown in Fig- ure 1.4.Forexample,whenn=5,weknowthat(cid:0)5(cid:1) =10.However,byPascal’sidentity,we 3 know that (cid:0)5(cid:1) =(cid:0)4(cid:1) +(cid:0)4(cid:1) . So the number 10 is actually obtained by summing the numbers 3 2 3 4 and 6 of the previous row. Figure 1.4: Pascal triangle for n = 0,...,5. Note that a number in one row is obtained by summing two numbers directly above it. Practice Exercise 1.3. Find (1+x)3. Solution. Using the binomial theorem, we can show that n (cid:18) (cid:19) (cid:88) 3 (1+x)3 = 13−kxk =1+3x+3x2+x3. k k=0 Practice Exercise 1.4. Let 0<p<1. Find n (cid:18) (cid:19) (cid:88) n pn−k(1−p)k. k k=0 Solution. By using the binomial theorem, we have n (cid:18) (cid:19) (cid:88) n pn−k(1−p)k =(p+(1−p))n =1. k k=0 This result will be helpful when evaluating binomial random variables in Chapter 3. 8
1.1. INFINITE SERIES We now prove the binomial theorem. Please feel free to skip the proof if this is your first time reading the book. Proof of the binomial theorem. We prove by induction. When n=1, 1 (cid:88) (a+b)1 =a+b= a1−kbk. k=0 Therefore, the base case is verified. Assume up to case n. We need to verify case n+1. n (cid:18) (cid:19) (cid:88) n (a+b)n+1 =(a+b)(a+b)n =(a+b) an−kbk k k=0 n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n = an−k+1bk+ an−kbk+1. k k k=0 k=0 WewanttoapplythePascal’sidentitytocombinethetwoterms.Inordertodoso,wenote that the second term in this sum can be rewritten as n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n an−kbk+1 = an+1−k−1bk+1 k k k=0 k=0 n+1(cid:18) (cid:19) (cid:88) n = an+1−(cid:96)b(cid:96), where (cid:96)=k+1 (cid:96)−1 (cid:96)=1 n (cid:18) (cid:19) (cid:88) n = an+1−(cid:96)b(cid:96)+bn+1. (cid:96)−1 (cid:96)=1 The first term in the sum can be written as n (cid:18) (cid:19) n (cid:18) (cid:19) (cid:88) n (cid:88) n an−k+1bk = an+1−(cid:96)b(cid:96)+an+1, where (cid:96)=k. k (cid:96) k=0 (cid:96)=1 Therefore, the two terms can be combined using Pascal’s identity to yield n (cid:20)(cid:18) (cid:19) (cid:18) (cid:19)(cid:21) (cid:88) n n (a+b)n+1 = + an+1−(cid:96)b(cid:96)+an+1+bn+1 (cid:96) (cid:96)−1 (cid:96)=1 n (cid:18) (cid:19) n+1(cid:18) (cid:19) (cid:88) n+1 (cid:88) n+1 = an+1−(cid:96)b(cid:96)+an+1+bn+1 = an+1−(cid:96)b(cid:96). (cid:96) (cid:96) (cid:96)=1 (cid:96)=0 Hence, the (n+1)th case is also verified. By the principle of mathematical induction, we have completed the proof. (cid:3) The end of the proof. Please join us again. 9
CHAPTER 1. MATHEMATICAL BACKGROUND 1.2 Approximation Consider a function f(x)=log(1+x), for x>0 as shown in Figure 1.5. This is a nonlinear function, and we all know that nonlinear functions are not fun to deal with. For example, (cid:82)b if you want to integrate the function xlog(1+x) dx, then the logarithm will force you a to do integration by parts. However, in many practical problems, you may not need the full range of x > 0. Suppose that you are only interested in values x (cid:28) 1. Then the logarithm can be approximated, and thus the integral can also be approximated. 2 0.2 1.5 0.15 1 0.1 0.5 0.05 0 0 0 1 2 3 4 5 0 0.05 0.1 0.15 0.2 Figure 1.5: The function f(x)=log(1+x) and the approximation f(cid:98)(x)=x. To see how this is even possible, we show in Figure 1.5 the nonlinear function f(x)= log(1+x)andanapproximationf(cid:98)(x)=x.Theapproximationiscarefullychosensuchthat for x (cid:28) 1, the approximation f(cid:98)(x) is close to the true function f(x). Therefore, we can argue that for x(cid:28)1, log(1+x)≈x, (1.6) thereby simplifying the calculation. For example, if you want to integrate xlog(1+x) for 0 < x < 0.1, then the integral can be approximated by (cid:82)0.1 xlog(1+x) dx ≈ (cid:82)0.1 x2 dx = 0 0 x3 = 3.33×10−4. (The actual integral is 3.21×10−4.) In this section we will learn about 3 the basic approximation techniques. We will use them when we discuss limit theorems in Chapter 6, as well as various distributions, such as from binomial to Poisson. 1.2.1 Taylor approximation Given a function f : R → R, it is often useful to analyze its behavior by approximating f using its local information. Taylor approximation (or Taylor series) is one of the tools for such a task. We will use the Taylor approximation on many occasions. Definition 1.2 (TaylorApproximation). Letf :R→Rbeacontinuousfunctionwith infinite derivatives. Let a ∈ R be a fixed constant. The Taylor approximation of f at 10
1.2. APPROXIMATION x=a is f(cid:48)(cid:48)(a) f(x)=f(a)+f(cid:48)(a)(x−a)+ (x−a)2+··· 2! (cid:88)∞ f(n)(a) = (x−a)n, (1.7) n! n=0 where f(n) denotes the nth-order derivative of f. Taylor approximation is a geometry-based approximation. It approximates the function according to the offset, slope, curvature, and so on. According to Definition 1.2, the Taylor series has an infinite number of terms. If we use a finite number of terms, we obtain the nth-order Taylor approximation: First-Order: f(x)=f(a)+f(cid:48)(a)(x−a)+O((x−a)2) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) offset slope f(cid:48)(cid:48)(a) Second-Order: f(x)=f(a)+f(cid:48)(a)(x−a)+ (x−a)2+O((x−a)3). 2! (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) offset slope curvature Here, the big-O notation O(εk) means any term that has an order at least power k. For small ε, i.e., ε(cid:28)1, a high-order term O(εk)≈0 for large k. Example 1.1. Let f(x)=sinx. Then the Taylor approximation at x=0 is f(cid:48)(cid:48)(0) f(cid:48)(cid:48)(cid:48)(0) f(x)≈f(0)+f(cid:48)(0)(x−0)+ (x−0)2+ (x−0)3 2! 3! sin(0) cos(0) =sin(0)+(cos0)(x−0)− (x−0)2− (x−0)3 2! 3! x3 x3 =0+x−0− =x− . 6 6 We can expand further to higher orders, which yields x3 x5 x7 f(x)=x− + − +··· 3! 5! 7! We show the first few approximations in Figure 1.6. OneshouldberemindedthatTaylorapproximationapproximatesafunctionf(x) at a particular point x = a. Therefore, the approximation of f near x = 0 and the approximationoff nearx=π/2aredifferent.Forexample,theTaylorapproximation at x=π/2 for f(x)=sinx is π π (cid:16) π(cid:17) sinπ (cid:16) π(cid:17)2 cosπ (cid:16) π(cid:17)3 f(x)=sin +cos x− − 2 x− − 2 x− 2 2 2 2! 2 3! 2 1(cid:16) π(cid:17)2 1(cid:16) π(cid:17)2 =1+0− x− −0=1− x− . 4 2 4 2 11
CHAPTER 1. MATHEMATICAL BACKGROUND 4 4 sin x sin x 3rd order 3rd order 2 5th order 2 5th order 7th order 7th order 0 0 -2 -2 -4 -4 -10 -5 0 5 10 -10 -5 0 5 10 x x (a) Approximate at x=0 (b) Approximate at x=π/2 Figure 1.6: Taylor approximation of the function f(x)=sinx. 1.2.2 Exponential series An immediate application of the Taylor approximation is to derive the exponential series. Theorem 1.4. Let x be any real number. Then, x2 x3 (cid:88)∞ xk ex =1+x+ + +···= . (1.8) 2 3! k! k=0 Proof. Let f(x)=ex for any x. Then, the Taylor approximation around x=0 is f(cid:48)(cid:48)(0) f(x)=f(0)+f(cid:48)(0)(x−0)+ (x−0)2+··· 2! e0 =e0+e0(x−0)+ (x−0)2+··· 2! x2 (cid:88)∞ xk =1+x+ +···= . 2 k! k=0 (cid:3) (cid:88)∞ λke−λ Practice Exercise 1.5. Evaluate . k! k=0 Solution. (cid:88)∞ λke−λ (cid:88)∞ λk =e−λ =e−λeλ =1. k! k! k=0 k=0 This result will be useful for Poisson random variables in Chapter 3. 12
1.2. APPROXIMATION √ If we substitute x=jθ where j = −1, then we can show that (jθ)2 ejθ =1+jθ+ +··· (cid:124)(cid:123)(cid:122)(cid:125) 2! =cosθ+jsinθ (cid:18) θ2 θ4 (cid:19) (cid:18) θ3 (cid:19) = 1− + +··· +j θ− +··· 2! 4! 3! (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) real imaginary Matching the real and the imaginary terms, we can show that θ2 θ4 cosθ =1− + +··· 2! 4! θ3 θ5 sinθ =θ− + +··· 3! 5! This gives the infinite series representations of the two trigonometric functions. 1.2.3 Logarithmic approximation Taylor approximation also allows us to find approximations to logarithmic functions. We start by presenting a lemma. Lemma 1.1. Let 0<x<1 be a constant. Then, x2 log(1+x)=x− +O(x3). (1.9) 2 Proof. Let f(x)=log(1+x). Then, the derivatives of f are 1 1 f(cid:48)(x)= , and f(cid:48)(cid:48)(x)=− . (1+x) (1+x)2 Taylor approximation at x=0 gives f(cid:48)(cid:48)(0) f(x)=f(0)+f(cid:48)(0)(x−0)+ (x−0)2+O(x3) 2 (cid:18) 1 (cid:19) (cid:18) 1 (cid:19) x2 =log1+ x− +O(x3) (1+0) (1+0)2 2 x2 =x− +O(x3). 2 (cid:3) The difference between this result and the result we showed in the beginning of this section is the order of polynomials we used to approximate the logarithm: • First-order: log(1+x)=x • Second-order: log(1+x)=x−x2/2. What order of approximation is good? It depends on where you want the approximation to be good, and how far you want the approximation to go. The difference between first-order and second-order approximations is shown in Figure 1.7. 13
CHAPTER 1. MATHEMATICAL BACKGROUND 2 2 1.5 1.5 1 1 0.5 0.5 0 0 0 1 2 3 4 5 0 1 2 3 4 5 First-order approximation Second-order approximation Figure 1.7: The function f(x)=log(1+x), the first-order approximation f(cid:98)(x)=x, and the second- order approximation f(cid:98)(x)=x−x2/2. Example 1.2. When we prove the Central Limit Theorem in Chapter 6, we need to use the following result. (cid:18) s2 (cid:19)N lim 1+ =es2/2. N→∞ 2N The proof of this equation can be done using the Taylor approximation. Consider (cid:16) (cid:17) Nlog 1+ s2 . By the logarithmic lemma, we can obtain the second-order approxi- N mation: (cid:18) s2 (cid:19) s2 s4 log 1+ = − . 2N 2N 4N2 Therefore, multiplying both sides by N yields (cid:18) s2 (cid:19) s2 s4 Nlog 1+ = − . 2N 2 4N Putting the limit N →∞ we can show that (cid:26) (cid:18) s2 (cid:19)(cid:27) s2 lim Nlog 1+ = . N→∞ 2N 2 Taking exponential on both sides yields (cid:26) (cid:18) s2 (cid:19)(cid:27) (cid:26) s2(cid:27) exp lim Nlog 1+ =exp . N→∞ 2N 2 Moving the limit outside the exponential yields the result. Figure 1.8 provides a pic- torial illustration. 14
1.3. INTEGRATION 1.8 1.6 1.4 1.2 1 0 0.2 0.4 0.6 0.8 1 Figure 1.8: We plot a sequence of function f (x)=(cid:16) 1+ s2 (cid:17)N and its limit f(x)=es2/2. N 2N 1.3 Integration When you learned calculus, your teacher probably told you that there are two ways to compute an integral: • Substitution: (cid:90) 1(cid:90) f(ax)dx= f(u)du. a • By parts: (cid:90) (cid:90) udv =uv− v du. Besides these two, we want to teach you two more. The first technique is even and odd functions when integrating a function symmetrically about the y-axis. If a function is even, you just need to integrate half of the function. If a function is odd, you will get a zero. The second technique is to leverage the fact that a probability density function integrates to 1. We will discuss the first technique here and defer the second technique to Chapter 4. Besides the two integration techniques, we will review the fundamental theorem of calculus. We will need it when we study cumulative distribution functions in Chapter 4. 1.3.1 Odd and even functions Definition 1.3. A function f :R→R is even if for any x∈R, f(x)=f(−x), (1.10) and f is odd if f(x)=−f(−x). (1.11) 15
CHAPTER 1. MATHEMATICAL BACKGROUND Essentially, an even function flips over about the y-axis, whereas an odd function flips over both the x- and y-axes. Example 1.3. The function f(x)=x2−0.4x4 is even, because f(−x)=(−x)2−0.4(−x)4 =x2−0.4x4 =f(x). See Figure 1.9(a) for illustration. When integrating the function, we have (cid:90) 1 (cid:90) 1 (cid:90) 1 (cid:20) x3 0.4 (cid:21)x=1 38 f(x)dx=2 f(x)dx=2 x2−0.44 dx=2 − x5 = . 3 5 75 −1 0 0 x=0 Example 1.4. The function f(x)=xexp(−x2/2) is odd, because (cid:26) (−x)2(cid:27) (cid:26) x2(cid:27) f(−x)=(−x)exp − =−xexp − =−f(x). 2 2 See Figure 1.9(b) for illustration. When integrating the function, we can let u=−x. Then, the integral becomes (cid:90) 1 (cid:90) 0 (cid:90) 1 f(x)dx= f(x)dx+ f(x)dx −1 −1 0 (cid:90) 1 (cid:90) 1 = f(−u)du+ f(x)dx 0 0 (cid:90) 1 (cid:90) 1 =− f(u)du+ f(x)dx=0. 0 0 1 1 0.5 0.5 0 0 -0.5 -0.5 -1 -1 -1.5 -1 -0.5 0 0.5 1 1.5 -1.5 -1 -0.5 0 0.5 1 1.5 x x (a) Even function (b) Odd function Figure 1.9: An even function is symmetric about the y-axis, and so the integration (cid:82)a f(x) dx = −a 2(cid:82)af(x)dx. An odd function is anti-symmetric about the y-axis. Thus, (cid:82)a f(x)dx=0. 0 −a 16
1.3. INTEGRATION 1.3.2 Fundamental Theorem of Calculus OurfollowingresultistheFundamental Theorem of Calculus.Itisahandytoolthatlinks integration and differentiation. Theorem 1.5 (Fundamental Theorem of Calculus). Let f :[a,b]→R be a continu- ous function defined on a closed interval [a,b]. Then, for any x∈(a,b), d (cid:90) x f(x)= f(t)dt, (1.12) dx a Beforeweprovetheresult,letusunderstandthetheoremifyouhaveforgottenitsmeaning. Example 1.5. Consider a function f(t) = t2. If we integrate the function from 0 to x, we will obtain another function (cid:90) x (cid:90) x x3 F(x)d =ef f(t)dt= t2 dt= . 3 0 0 On the other hand, we can differentiate F(x) to obtain f(x): d d x3 f(x)= F(x)= =x2. dx dx 3 The fundamental theorem of calculus basically puts the two together: d (cid:90) x f(x)= f(t)dt. dx 0 That’s it. Nothing more and nothing less. How can the fundamental theorem of calculus ever be useful when studying probabil- ity? Very soon you will learn two concepts: probability density function and cumulative distribution function. These two functions are related to each other by the fundamental theorem of calculus. To give you a concrete example, we write down the probability density function of an exponential random variable. (Please do not panic about the exponential random variable. Just think of it as a “rapidly decaying” function.) f(x)=e−x, x≥0. It turns out that the cumulative distribution function is (cid:90) x (cid:90) x F(x)= f(t)dt= e−t dt=1−e−x. 0 0 You can also check that f(x) = d F(x). The fundamental theorem of calculus says that if dx youtellmeF(x)=(cid:82)x e−t dt(forwhateverreason),Iwillbeabletotellyouthatf(x)=e−x 0 merely by visually inspecting the integrand without doing the differentiation. Figure 1.10illustratesthepairoffunctionsf(x)=e−x andF(x)=1−e−x.Onething youshouldnoticeisthattheheight ofF(x)istheareaunderthecurveoff(t)from−∞tox. Forexample,inFigure 1.10weshowtheareaunderthecurvefrom0to2.Correspondingly in F(x), the height is F(2). 17
CHAPTER 1. MATHEMATICAL BACKGROUND 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 0 1 2 3 4 5 0 1 2 3 4 5 f(x) F(x) Figure 1.10: The pair of functions f(x)=e−x and F(x)=1−e−x The following proof of the Fundamental Theorem of Calculus can be skipped if it is your first time reading the book. Proof. Our proof is based on Stewart (6th Edition), Section 5.3. Define the integral as a function F: (cid:90) x F(x)= f(t)dt. a The derivative of F with respect to x is d F(x+h)−F(x) F(x)= lim dx h→0 h (cid:32) (cid:33) 1 (cid:90) x+h (cid:90) x = lim f(t)dt− f(t)dt h→0h a a 1 (cid:90) x+h = lim f(t)dt h→0h x (a) 1 (cid:90) x+h(cid:26) (cid:27) ≤ lim max f(τ) dt h→0h x x≤τ≤x+h (cid:26) (cid:27) = lim max f(τ) . h→0 x≤τ≤x+h Here, the inequality in (a) holds because f(t)≤ max f(τ) x≤τ≤x+h for all x≤t≤x+h. The maximum exists because f is continuous in a closed interval. 18
1.3. INTEGRATION Using the parallel argument, we can show that d F(x+h)−F(x) F(x)= lim dx h→0 h (cid:32) (cid:33) 1 (cid:90) x+h (cid:90) x = lim f(t)dt− f(t)dt h→0h a a 1 (cid:90) x+h = lim f(t)dt h→0h x 1 (cid:90) x+h(cid:26) (cid:27) ≥ lim min f(τ) dt h→0h x x≤τ≤x+h (cid:26) (cid:27) = lim min f(τ) . h→0 x≤τ≤x+h Combining the two results, we have that (cid:26) (cid:27) (cid:26) (cid:27) d lim min f(τ) ≤ F(x)≤ lim max f(τ) . h→0 x≤τ≤x+h dx h→0 x≤τ≤x+h However, since the two limits are both converging to f(x) as h → 0, we conclude that d F(x)=f(x). dx (cid:3) Remark.AnalternativeproofistouseMeanValueTheoremintermsofRiemann-Stieltjes integrals (see, e.g., Tom Apostol, Mathematical Analysis, 2nd edition, Theorem 7.34). To handlemoregeneralfunctionssuchasdeltafunctions,onecanusetechniquesinLebesgue’s integration. However, this is beyond the scope of this book. This is the end of the proof. Please join us again. In many practical problems, the fundamental theorem of calculus needs to be used in conjunction with the chain rule. Corollary 1.3. Letf :[a,b]→Rbeacontinuousfunctiondefinedonaclosedinterval [a,b]. Let g : R → [a,b] be a continuously differentiable function. Then, for any x ∈ (a,b), d (cid:90) g(x) f(t)dt=g(cid:48)(x)·f(g(x)). (1.13) dx a Proof. We can prove this with the chain rule: Let y =g(x). Then we have d (cid:90) g(x) dy d (cid:90) y f(t)dt= · f(t)dt=g(cid:48)(x)f(y), dx dx dy a a which completes the proof. (cid:3) 19
CHAPTER 1. MATHEMATICAL BACKGROUND Practice Exercise 1.6. Evaluate the integral d (cid:90) x−µ 1 (cid:26) t2 (cid:27) √ exp − dt. dx 2πσ2 2σ2 0 Solution. Let y =x−µ. Then by using the fundamental theorem of calculus, we can show that d (cid:90) x−µ 1 (cid:26) t2 (cid:27) dy d (cid:90) y 1 (cid:26) t2 (cid:27) √ exp − dt= · √ exp − dt dx 2πσ2 2σ2 dx dy 2πσ2 2σ2 0 0 d(x−µ) 1 (cid:26) y2 (cid:27) = · √ exp − dx 2πσ2 2σ2 1 (cid:26) (x−µ)2(cid:27) = √ exp − . 2πσ2 2σ2 This result will be useful when we do linear transformations of a Gaussian random variable in Chapter 4. 1.4 Linear Algebra Thetwomostimportantsubjectsfordatascienceareprobability,whichisthesubjectofthe book you are reading, and linear algebra, which concerns matrices and vectors. We cannot cover linear algebra in detail because this would require another book. However, we need to highlight some ideas that are important for doing data analysis. 1.4.1 Why do we need linear algebra in data science? Consider a dataset of the crime rate of several cities as shown below, downloaded from https://web.stanford.edu/~hastie/StatLearnSparsity/data.html. The table shows that the crime rate depends on several factors such as funding for the police department, the percentage of high school graduates, etc. city crime rate funding hs no-hs college college4 1 478 40 74 11 31 20 2 494 32 72 11 43 18 3 643 57 71 18 16 16 4 341 31 71 11 25 19 . . . . . . . . . . . . . . . . . . . . . 50 940 66 67 26 18 16 20
1.4. LINEAR ALGEBRA Whatquestionscanweaskaboutthistable?Wecanask:Whatisthemostinfluential cause of the crime rate? What are the leading contributions to the crime rate? To answer these questions, we need to describe these numbers. One way to do it is to put the numbers in matrices and vectors. For example,       478 40 74 494 32 72 y crime =  . .  , x fund =  . .  , x hs =  . .  ,...  .   .   .  940 66 67 Withthisvectorexpressionofthedata,theanalysisquestionscanroughlybetranslated to finding β’s in the following equation: y =β x +β x +···+β x . crime fund fund hs hs college4 college4 This equation offers a lot of useful insights. First, it is a linear model of y . We call crime it a linear model because the observable y is written as a linear combination of the crime variables x ,x , etc. The linear model assumes that the variables are scaled and added fund hs togeneratetheobservedphenomena.Thisassumptionisnotalwaysrealistic,butitisoften a fair assumption that greatly simplifies the problem. For example, if we can show that all β’s are zero except β , then we can conclude that the crime rate is solely dependent on fund the police funding. If two variables are correlated, e.g., high school graduate and college graduate, we would expect the β’s to change simultaneously. The linear model can further be simplified to a matrix-vector equation:  |   | | |  β  fund     y cr| |ime    =    x fu || nd x || hs ··· x col || lege4        β . . .hs     | | | | β college4 Here, the lines “|” emphasize that the vectors are column vectors. If we denote the matrix in the middle as A and the vector as β, then the equation is equivalent to y =Aβ. So we can find β by appropriately inverting the matrix A. If two columns of A are dependent, we will not be able to resolve the corresponding β’s uniquely. Asyoucanseefromtheabovedataanalysisproblem,matricesandvectorsofferaway to describe the data. We will discuss the calculations in Chapter 7. However, to understand howtointerprettheresultsfromthematrix-vectorequations,weneedtoreviewsomebasic ideas about matrices and vectors. 1.4.2 Everything you need to know about linear algebra Throughout this book, you will see different sets of notations. For linear algebra, we also haveasetofnotations.Wedenotex∈Rd ad-dimensionalvectortakingrealnumbersasits entries.AnM-by-N matrixisdenotedasX ∈RM×N.Thetransposeofamatrixisdenoted as XT. A matrix X can be viewed according to its columns and its rows:  — x1 —   | | | — x2 — X =x 1 x 2 ··· x N, and X =  . .  . | | |  .  — xM — 21
CHAPTER 1. MATHEMATICAL BACKGROUND Here,x denotesthejthcolumnofX,andxi denotestheithrowofX.The(i,j)thelement j of X is denoted as x or [X] . The identity matrix is denoted as I. The ith column of I ij ij is denoted as e =[0,...,1,...,0]T, and is called the ith standard basis vector. An all-zero i vector is denoted as 0=[0,...,0]T. Whatisthemostimportantthingtoknowaboutlinearalgebra?Fromadataanalysis pointofview,Figure1.11givesustheanswer.Thepictureisstraightforward,butitcaptures alltheessence.Inalmostallthedataanalysisproblems,ultimately,therearethreethingswe careabout:(i)Theobservablevectory,(ii)thevariablevectorsx ,and(iii)thecoefficients n β . The set of variable vectors {x }N spans a vector space in which all vectors are living. n n n=1 Some of these variable vectors are correlated, and some are not. However, for the sake of this discussion, let us assume they are independent of each other. Then for any observable vector y, we can always project y in the directions determined by {x }N . The projection n n=1 of y onto x is the coefficient β . A larger value of β means that the variable x has more n n n n contributions. Figure 1.11: Representing an observable vector y by a linear combination of variable vectors x , x 1 2 and x . The combination weights are β ,β ,β . 3 1 2 3 Why is this picture so important? Because most of the data analysis problems can be expressed, or approximately expressed, by the picture: N (cid:88) y = β x . n n n=1 If you recall the crime rate example, this equation is precisely the linear model we used to describe the crime rate. This equation can also describe many other problems. Example 1.6.Polynomial fitting.Consideradatasetofpairsofnumbers(t ,y )for m m m = 1,...,M, as shown in Figure 1.12. After a visual inspection of the dataset, we propose to use a line to fit the data. A line is specified by the equation y =at +b, m=1,...,M, m m where a ∈ R is the slope and b ∈ R is the y-intercept. The goal of this problem is to find one line (which is fully characterized by (a,b)) such that it has the best fit to all the data pairs (t ,y ) for m=1,...,M. This problem can be described in matrices m m 22
1.4. LINEAR ALGEBRA and vectors by noting that       y t 1 1 1 . . .  . = a  . + b .,  .   .  . (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) y M β1 t M β2 1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) y x1 x2 or more compactly, y =β x +β x . 1 1 2 2 Here,x =[t ,...,t ]T containsallthevariablevalues,andx =[1,...,1]T contains 1 1 M 2 a constant offset. 5 t y m m 0.1622 2.1227 4 0.7943 3.3354 . . . . . . 3 0.7379 3.4054 0.2691 2.5672 2 data best fit 0.4228 2.3796 candidate 0.6020 3.2942 1 0 0.2 0.4 0.6 0.8 1 Figure 1.12: Example of fitting a set of data points. The problem can be described by y = β x +β x . 1 1 2 2 Example 1.7. Image compression. The JPEG compression for images is based on the concept of discrete cosine transform (DCT). The DCT consists of a set of basis vectors,or{x }N usingournotation.Inthemoststandardsetting,eachbasisvector n n=1 x consistsof8×8pixels,andthereareN =64ofthesex ’s.Givenanimage,wecan n n partition the image into M small blocks of 8×8 pixels. Let us call one of these blocks y. Then, DCT represents the observation y as a linear combination of the DCT basis vectors: N (cid:88) y = β x . n n n=1 The coefficients {β }N are called the DCT coefficients. They provide a representa- n n=1 tion of y, because once we know {β }N , we can completely describe y because the n n=1 basis vectors {x }N are known and fixed. The situation is depicted in Figure 1.13. n n=1 HowcanwecompressimagesusingDCT?Inthe1970s,scientistsfoundthatmost images have strong leading DCT coefficients but weak tail DCT coefficients. In other words, among the N = 64 β ’s, only the first few are important. If we truncate the n number of DCT coefficients, we can effectively compress the number of bits required to represent the image. 23
CHAPTER 1. MATHEMATICAL BACKGROUND Figure1.13:JPEGimagecompressionisbasedontheconceptofdiscretecosinetransform,which can be formulated as a matrix-vector problem. We hope by now you are convinced of the importance of matrices and vectors in the context of data science. They are not “yet another” subject but an essential tool you must know how to use. So, what are the technical materials you must master? Here we go. 1.4.3 Inner products and norms We assume that you know the basic operations such as matrix-vector multiplication, taking the transpose, etc. If you have forgotten these, please consult any undergraduate linear algebra textbook such as Gilbert Strang’s Linear Algebra and its Applications. We will highlight a few of the most important operations for our purposes. Definition 1.4 (Inner product). Let x = [x ,...,x ]T, and y = [y ,...,y ]T. The 1 N 1 N inner product xTy is N (cid:88) xTy = x y . (1.14) i i i=1 Practice Exercise 1.7. Let x=[1, 0, −1]T, and y =[3, 2, 0]T. Find xTy. Solution. The inner product is xTy =(1)(3)+(0)(2)+(−1)(0)=3. Inner products are important because they tell us how two vectors are correlated. Figure1.14depictsthegeometricmeaningofaninnerproduct.Iftwovectorsarecorrelated (i.e., nearly parallel), then the inner product will give us a large value. Conversely, if the two vectors are close to perpendicular, then the inner product will be small. Therefore, the inner product provides a measure of the closeness/similarity between two vectors. Figure 1.14: Geometric interpretation of inner product: We project one vector onto the other vector. The projected distance is the inner product. 24
1.4. LINEAR ALGEBRA Creating vectors and computing the inner products are straightforward in MATLAB. We simply need to define the column vectors x and y by using the command [] with ; to denotethenextrow.Theinnerproductisdoneusingthetransposeoperationx’andvector multiplication *. % MATLAB code to perform an inner product x = [1 0 -1]; y = [3 2 0]; z = x’*y; In Python, constructing a vector is done using the command np.array. Inside this command,oneneedstoenterthearray.Foracolumnvector,wewrite[[1],[2],[3]],with anouter[],andthreeinner[]foreachentry.Ifthevectorisarowvector,theonecanomit the inner []’s by just calling np.array([1, 2, 3]). Given two column vectors x and y, the inner product is computed via np.dot(x.T,y), where np.dot is the command for inner product, and x.T returns the transpose of x. One can also call np.transpose(x), which is the same as x.T. # Python code to perform an inner product import numpy as np x = np.array([[1],[0],[-1]]) y = np.array([[3],[2],[0]]) z = np.dot(np.transpose(x),y) print(z) In data analytics, the inner product of two vectors can be useful. Consider the vectors in Table 1.1. Just from looking at the numbers, you probably will not see anything wrong. However, let’s compute the inner products. It turns out that xTx = −0.0031, whereas 1 2 xTx =2.0020. There is almost no correlation between x and x , but there is a substan- 1 3 1 2 tial correlation between x and x . What happened? The vectors x and x are random 1 3 1 2 vectors constructed independently and uncorrelated to each other. The last vector x was 3 constructed by x =2x −π/1000. Since x is completely constructed from x , they have 3 1 3 1 to be correlated. x x x 1 2 3 0.0006 −0.0011 −0.0020 −0.0014 −0.0024 −0.0059 −0.0034 0.0073 −0.0099 . . . . . . . . . 0.0001 −0.0066 −0.0030 0.0074 0.0046 0.0116 0.0007 −0.0061 −0.0017 Table 1.1: Three example vectors. One caveat for this example is that the naive inner product xTx is scale-dependent. i j For example, the vectors x = x and x = 1000x have the same amount of correlation, 3 1 3 1 25
CHAPTER 1. MATHEMATICAL BACKGROUND butthesimpleinnerproductwillgivealargervalueforthelattercase.Tosolvethisproblem we first define the norm of the vectors: Definition 1.5 (Norm). Let x=[x ,...,x ]T be a vector. The (cid:96) -norm of x is 1 N p (cid:32) N (cid:33)1/p (cid:88) (cid:107)x(cid:107) = xp , (1.15) p i i=1 for any p≥1. The norm essentially tells us the length of the vector. This is most obvious if we consider the (cid:96) -norm: 2 (cid:32) N (cid:33)1/2 (cid:88) (cid:107)x(cid:107) = x2 . 2 i i=1 By taking the square on both sides, one can show that (cid:107)x(cid:107)2 = xTx. This is called the 2 squared (cid:96) -norm, and is the sum of the squares. 2 On MATLAB, computing the norm is done using the command norm. Here, we can indicatethetypesofnorms,e.g.,norm(x,1)returnsthe(cid:96) -normwhereasnorm(x,2)returns 1 the (cid:96) -norm (which is also the default). 2 % MATLAB code to compute the norm x = [1 0 -1]; x_norm = norm(x); On Python, the norm command is listed in the np.linalg. To call the (cid:96) -norm, we use 1 np.linalg.norm(x,1), and by default the (cid:96) -norm is np.linalg.norm(x). 2 # Python code to compute the norm import numpy as np x = np.array([[1],[0],[-1]]) x_norm = np.linalg.norm(x) Using the norm, one can define an angle called the cosine angle between two vectors. Definition 1.6. The cosine angle between two vectors x and y is xTy cosθ = . (1.16) (cid:107)x(cid:107) (cid:107)y(cid:107) 2 2 The difference between the cosine angle and the basic inner product is the normaliza- tion in the denominator, which is the product (cid:107)x(cid:107) (cid:107)y(cid:107) . This normalization factor scales 2 2 the vector x to x/(cid:107)x(cid:107) and y to y/(cid:107)y(cid:107) . The scaling makes the length of the new vector 2 2 equal to unity, but it does not change the vector’s orientation. Therefore, the cosine angle is not affected by a very long vector or a very short vector. Only the angle matters. See Figure 1.15. 26
1.4. LINEAR ALGEBRA Figure 1.15: The cosine angle is the inner product divided by the norms of the vectors. Going back to the previous example, after normalization we can show that the cosine angle between x and x is cosθ = −0.0031, whereas the cosine angle between x and 1 2 1,2 1 x is cosθ =0.8958. There is still a strong correlation between x and x , but now using 3 1,3 1 3 the cosine angle the value is between −1 and +1. Remark1:Thereareothernormsonecanuse.The(cid:96) -normisusefulforsparsemodels 1 where we want to have the fewest possible non-zeros. The (cid:96) -norm of x is 1 N (cid:88) (cid:107)x(cid:107) = |x |, 1 i i=1 which is the sum of absolute values. The (cid:96) -norm picks the maximum of {x ,...,x }: ∞ 1 N (cid:32) N (cid:33)1/p (cid:88) (cid:107)x(cid:107) = lim xp ∞ i p→∞ i=1 =max{x ,...,x }, 1 N because as p→∞, only the largest element will be amplified. Remark 2: The standard (cid:96) -norm is a circle: Just consider x = [x ,x ]T. The norm 2 1 2 (cid:112) is (cid:107)x(cid:107) = x2+x2. We can convert the circle to ellipses by considering a weighted norm. 2 1 2 Definition 1.7 (Weighted (cid:96) -norm square). Let x = [x ,...,x ]T and let W = 2 1 N diag(w ,...,w ) be a non-negative diagonal matrix. The weighted (cid:96) -norm square of 1 N 2 x is (cid:107)x(cid:107)2 =xTWx W    w ... 0 x 1 1 N =(cid:2) x 1 ... x N(cid:3)  . . . ... . . .    . . .  =(cid:88) w ix2 i. (1.17) 0 ... w x i=1 N N The geometry of the weighted (cid:96) -norm is determined by the matrix W. For example, 2 if W = I (the identity operator), then (cid:107)x(cid:107)2 = (cid:107)x(cid:107)2, which defines a circle. If W is any W 2 “non-negative” matrix2, then (cid:107)x(cid:107)2 defines an ellipse. W 2Thetechnicaltermforthesematricesispositive semi-definite matrices. 27
CHAPTER 1. MATHEMATICAL BACKGROUND In MATLAB, the weighted inner product is just a sequence of two matrix-vector mul- tiplications. This can be done using the command x’*W*x as shown below. % MATLAB code to compute the weighted norm W = [1 2 3; 4 5 6; 7 8 9]; x = [2; -1; 1]; z = x’*W*x In Python, constructing the matrix W and the column vector x is done using np.array. Thematrix-vectormultiplicationisdoneusingtwonp.dotcommands:onefornp.dot(W,x) and the other one for np.dot(x.T, np.dot(W,x)). # Python code to compute the weighted norm import numpy as np W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) x = np.array([[2],[-1],[1]]) z = np.dot(x.T, np.dot(W,x)) print(z) 1.4.4 Matrix calculus The last linear algebra topic we need to review is matrix calculus. As its name indicates, matrix calculus is about the differentiation of matrices and vectors. Why do we need differ- entiation for matrices and vectors? Because we want to find the minimum or maximum of a scalar function with a vector input. Let us go back to the crime rate problem we discussed earlier. Given the data, we want to find the model coefficients β ,...,β such that the variables can best explain the 1 N observation.Inotherwords,wewanttominimizethedeviationbetweenyandtheprediction offered by our model: (cid:13) (cid:13)2 (cid:13) (cid:88)N (cid:13) minimize (cid:13)y− β x (cid:13) . (cid:13) n n(cid:13) β1,...,βN (cid:13) n=1 (cid:13) This equation is self-explanatory. The norm (cid:107)♣−♥(cid:107)2 measures the deviation. If y can be perfectly explained by {x }N , then the norm can eventually go to zero by finding a n n=1 good set of {β ,...,β }. The symbol minimize means to minimize the function by finding 1 N β1,...,βN {β ,...,β }. Note that the norm is taking a vector as the input and generating a scalar as 1 N the output. It can be expressed as (cid:13) (cid:13)2 ε(β)d =ef(cid:13) (cid:13)y−(cid:88)N β x (cid:13) (cid:13) , (cid:13) n n(cid:13) (cid:13) (cid:13) n=1 to emphasize this relationship. Here we define β = [β ,...,β ]T as the collection of all 1 N coefficients. Given this setup, how would you determine β such that the deviation is minimized? Our calculus teachers told us that we could take the function’s derivative and set it to zero 28
1.4. LINEAR ALGEBRA for scalar problems. It is the same story for vectors. What we do is to take the derivative of the error and set it equal to zero: d ε(β)=0. dβ Now the question arises, how do we take the derivatives of ε(β) when it takes a vector as input?Ifwecananswerthisquestion,wewillfindthebestβ.Theanswerisstraightforward. Since the function has one output and many inputs, take the derivative for each element independently. This is called the scalar differentiation of vectors. Definition1.8(Scalardifferentiationofvectors). Letf :RN →Rbeadifferentiable scalar function, and let y =f(x) for some input x∈RN. Then,   dy/dx 1 dy . = . . dx  .  dy/dx N Asyoucanseefromthisdefinition,thereisnothingconceptuallychallenginghere.Theonly difficultyisthatthingscangettediousbecausetherewillbemanyterms.However,thegood news is that mathematicians have already compiled a list of identities for common matrix differentiation. So instead of deriving every equation from scratch, we can enjoy the fruit of their hard work by referring to those formulae. The best place to find these equations is the Matrix Cookbook by Petersen and Pedersen.3 Here, we will mention two of the most useful results. Example 1.8. Let y =xTAx for any matrix A∈RN×N. Find dy. dx Solution. d (cid:0) xTAx(cid:1) =Ax+ATx. dx Now, if A is symmetric, i.e., A=AT, then d (cid:0) xTAx(cid:1) =2Ax. dx Example 1.9. Let ε=(cid:107)Ax−y(cid:107)2, where A∈RN×N is symmetric. Find dε. 2 dx Solution. First, we note that ε=(cid:107)Ax−y(cid:107)2 2 =xTATAx−2yTAx+yTy. 3https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf 29
CHAPTER 1. MATHEMATICAL BACKGROUND Taking the derivative with respect to x yields dε =2ATAx−2ATy dx =2AT(Ax−y). Going back to the crime rate problem, we can now show that dε 0= (cid:107)y−Xβ(cid:107)2 =2XT(Xβ−y). dβ Therefore, the solution is β(cid:98) =(XTX)−1Xy. Asyoucansee,ifwedonothaveaccesstothematrixcalculus,wewillnotbeabletosolvethe minimizationproblem.(Therearealternativepathsthatdonotrequirematrixcalculus,but theyrequireanunderstandingoflinearsubspacesandpropertiesoftheprojectionoperators. So in some sense, matrix calculus is the easiest way to solve the problem.) When we discuss the linear regression methods in Chapter 7, we will cover the interpretation of the inverses and related topics. In MATLAB and Python, matrix inversion is done using the command inv in MAT- LAB and np.linalg.inv in Python. Below is an example in Python. # Python code to compute a matrix inverse import numpy as np X = np.array([[1, 3], [-2, 7], [0, 1]]) XtX = np.dot(X.T, X) XtXinv = np.linalg.inv(XtX) print(XtXinv) Sometimes,insteadofcomputingthematrixinversewearemoreinterestedinsolvinga linearequationXβ =y(thesolutionofwhichisβ(cid:98) =(XTX)−1Xy).InbothMATLABand Python,therearebuilt-incommandstodothis.InMATLAB,thecommandis\(backslash). % MATLAB code to solve X beta = y X = [1 3; -2 7; 0 1]; y = [2; 1; 0]; beta = X\y; In Python, the built-in command is np.linalg.lstsq. # Python code to solve X beta = y import numpy as np X = np.array([[1, 3], [-2, 7], [0, 1]]) y = np.array([[2],[1],[0]]) beta = np.linalg.lstsq(X, y, rcond=None)[0] print(beta) 30
1.5. BASIC COMBINATORICS Closing remark: In this section, we have given a brief introduction to a few of the most relevant concepts in linear algebra. We will introduce further concepts in linear algebra in later chapters, such as eigenvalues, principal component analysis, linear transformations, and regularization, as they become useful for our discussion. 1.5 Basic Combinatorics The last topic we review in this chapter is combinatorics. Combinatorics concerns the numberofconfigurationsthatcanbeobtainedfromcertaindiscreteexperiments.Itisuseful because it provides a systematic way of enumerating cases. Combinatorics often becomes very challenging as the complexity of the event grows. However, you may rest assured that inthisbook,wewillnottacklethemoredifficultproblemsofcombinatorics;wewillconfine our discussion to two of the most basic principles: permutation and combination. 1.5.1 Birthday paradox Tomotivatethediscussionofcombinatorics,letusstartwiththefollowingproblem.Suppose there are 50 people in a room. What is the probability that at least one pair of people have the same birthday (month and day)? (We exclude Feb. 29 in this problem.) Thefirstthingyoumightbethinkingisthatsincethereare365days,weneedatleast 366 people to ensure that one pair has the same birthday. Therefore, the chance that 2 of 50 people have the same birthday is low. This seems reasonable, but let’s do a simulated experiment. In Figure 1.16 we plot the probability as a function of the number of people. For a room containing 50 people, the probability is 97%. To get a 50% probability, we just need 23 people! How is this possible? 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 10 20 30 40 50 60 70 80 90 100 Number of people ytilibaborP Figure 1.16: The probability for two people in a group to have the same birthday as a function of the number of people in the group. Ifyouthinkaboutthisproblemmoredeeply,youwillprobablyrealizethattosolvethe problem, we must carefully enumerate all the possible configurations. How can we do this? 31
CHAPTER 1. MATHEMATICAL BACKGROUND Well, suppose you walk into the room and sequentially pick two people. The probability that they have different birthdays is 365 364 P[The first 2 people have different birthdays]= × . 365 365 When you ask the first person to tell you their birthday, he or she can occupy any of the 365 slots. This gives us 365. The second person has one slot short because the first person 365 has taken it, and so the probability that he or she has a different birthday from the first personis 364.Notethatthiscalculationisindependentofhowmanypeopleyouhaveinthe 365 room because you are picking them sequentially. If you now choose a third person, the probability that they have different birthdays is 365 364 363 P[The first 3 people have different birthdays]= × × . 365 365 365 This process can be visualized in Figure 1.17. Figure 1.17: The probability for two people to have the same birthday as a function of the number of peopleinthegroup.Whenthereisonlyoneperson,thispersoncanlandonanyofthe365days.When therearetwopeople,thefirstpersonhasalreadytakenoneday(outof365days),sothesecondperson can only choose 364 days. When there are three people, the first two people have occupied two days, so there are only 363 days left. If we generalize this process, we see that the number of configurations is 365×364×···×(365−k+1), where k is the number of people in the room. So imagine that you keep going down the list to the 50th person. The probability that none of these 50 people will have the same birthday is P[The first 50 people have different birthdays] 365 364 363 316 = × × ×···× ≈0.03. 365 365 365 365 Thatmeansthattheprobabilityfor50peopletohavedifferentbirthdays,theprobabilityis as little as 3%. If you take the complement, you can show that with 97% probability, there is at least one pair of people having the same birthday. The general equation for this problem is now easy to see: 365×364×···×(365−k+1) P[The first k people have different birthdays]= 365×365×···×365 365! 1 = × . (365−k)! 365k 32
1.5. BASIC COMBINATORICS The first term in our equation, 365! , is called the permutation of picking k days from (365−k)! 365 options. We shall discuss this operation shortly. Why is the probability so high with only 50 people while it seems that we need 366 people to ensure two identical birthdays? The difference is the notion of probabilistic and deterministic. The 366-people argument is deterministic. If you have 366 people, you are certain that two people will have the same birthday. This has no conflict with the proba- bilistic argument because the probabilistic argument says that with 50 people, we have a 97% chance of getting two identical birthdays. With a 97% success rate, you still have a 3% chance of failing. It is unlikely to happen, but it can still happen. The more people you put into the room, the stronger guarantee you will have. However, even if you have 364 peopleandtheprobabilityisalmost100%,thereisstillnoguarantee.Sothereisnoconflict between the two arguments since they are answering two different questions. Now, let’s discuss the two combinatorics questions. 1.5.2 Permutation Permutation concerns the following question: Consider a set of n distinct balls. Suppose we want to pick k balls from the set without replacement. How many ordered configurations can we obtain? Note that in the above question, the word “ordered” is crucial. For example, the set A={a,b,c} can lead to 6 different ordered configurations (a,b,c), (a,c,b), (b,a,c), (b,c,a), (c,a,b), (c,b,a). As a simple illustration of how to compute the permutation, we can consider a set of 5 colored balls as shown in Figure 1.18. Figure1.18:Permutation.Thenumberofchoicesisreducedineverystage.Therefore,thetotalnumber is n×(n−1)×···×(n−k+1) if there are k stages. If you start with the base, which contains five balls, you will have five choices. At one level up, since one ball has already been taken, you have only four choices. You continue the process until you reached the number of balls you want to collect. The number of configurations you have generated is the permutation. Here is the formula: 33
CHAPTER 1. MATHEMATICAL BACKGROUND Theorem 1.6. The number of permutations of choosing k out of n is n! (n−k)! where n!=n(n−1)(n−2)···3·2·1. Proof. Let’s list all possible ways: Which ball to pick Number of choices Why? The 1st ball n No has been picked, so we have n choices The 2nd ball n−1 The first ball has been picked The 3rd ball n−2 The first two balls have been picked . . . . . . . . . The kth ball n−k+1 The first k−1 balls have been picked Total: n(n−1)···(n−k+1) The total number of ordered configurations is n(n−1)···(n−k+1). This simplifies to n(n−1)(n−2)···(n−k+1) (n−k)(n−k−1)···3·2·1 =n(n−1)(n−2)···(n−k+1)· (n−k)(n−k−1)···3·2·1 n! = . (n−k)! (cid:3) Practice Exercise 1.8. Consider a set of 4 balls {1,2,3,4}. We want to pick two balls at random without replacement. The ordering matters. How many permutations can we obtain? Solution. The possible configurations are (1,2), (2,1), (1,3), (3,1), (1,4), (4,1), (2,3), (3,2), (2,4), (4,2), (3,4), (4,3). So totally there are 12 configurations. We can also verify this number by noting that there are 4 balls altogether and so the number of choices for picking the first ball is 4 and the number of choices for picking the second ball is (4−1)=3. Thus, the total is 4·3=12. Referring to the formula, this result coincides with the theorem, which states that the number of permutations is 4! = 4·3·2·1 =12. (4−2)! 2·1 34
1.5. BASIC COMBINATORICS 1.5.3 Combination Another operation in combinatorics is combination. Combination concerns the following question: Consider a set of n distinct balls. Suppose we want to pick k balls from the set without replacement. How many unordered configurations can we obtain? Unlike permutation, combination treats a subset of balls with whatever ordering as one single configuration. For example, the subset (a,b,c) is considered the same as (a,c,b) or (b,c,a), etc. Let’s go back to the 5-ball exercise. Suppose you have picked orange, green, and light blue. This is the same combination as if you have picked {green, orange, and light blue}, or {green, light blue, and orange}. Figure 1.19 lists all the six possible configurations for these three balls. So what is combination? Combination needs to take these repeated cases into account. Figure 1.19: Combination. In this problem, we are interested in picking 3 colored balls out of 5. This willgiveus5×4×3=60permutations.However,sincewearenotinterestedintheordering,someof thepermutationsarerepeated.Forexample,thereare6combosof(green,lightblue,orange),whichis computed from 3×2×1. Dividing 60 permutations by these 6 choices of the orderings will give us 10 distinct combinations of the colors. Theorem 1.7. The number of combinations of choosing k out of n is n! k!(n−k)! where n!=n(n−1)(n−2)···3·2·1. Proof.Westartwiththepermutationresult,whichgivesus n! permutations.Notethat (n−k)! every permutation has exactly k balls. However, while these k balls can be arranged in any order, in combination, we treat them as one single configuration. Therefore, the task is to count the number of possible orderings for these k balls. To this end, we note that for a set of k balls, there are in total k! possible ways of ordering them. The number k! comes from the following table. 35
CHAPTER 1. MATHEMATICAL BACKGROUND Which ball to pick Number of choices The 1st ball k The 2nd ball k−1 . . . . . . The kth ball 1 Total: k(k−1)···3·2·1 Therefore, the total number of orderings for a set of k balls is k!. Since permutation gives us n! and every permutation has k! repetitions due to ordering, we divide the (n−k)! number by k!. Thus the number of combinations is n! . k!(n−k)! (cid:3) Practice Exercise 1.9. Consider a set of 4 balls {1,2,3,4}. We want to pick two balls at random without replacement. The ordering does not matter. How many com- binations can we obtain? Solution.Thepermutationresultgivesus12permutations.However,amongallthese 12 permutations, there are only 6 distinct pairs of numbers. We can confirm this by noting that since we picked 2 balls, there are exactly 2 possible orderings for these 2 balls. Therefore, we have 12 = 6 number of combinations. Using the formula of the 2 theorem, we check that the number of combinations is 4! 4·3·2·1 = =6. 2!(4−2)! (2·1)(2·1) Example 1.10. (Ross, 8th edition, Section 1.6) Consider the equation x +x +···+x =N, 1 2 K where{x }arepositiveintegers.Howmanycombinationsofsolutionsofthisequation k are there? Solution. We can determine the number of combinations by considering the figure below.TheintegerN canbemodeledasN ballsinanurn.ThenumberofvariablesK is equivalent to the number ofcolors of these balls.Since all variables are positive,the problem can be translated to partitioning the N balls into K buckets. This, in turn, is the same as inserting K−1 dividers among N −1 holes. Therefore, the number of combinations is (cid:18) (cid:19) N −1 (N −1)! = . K−1 (K−1)!(N −K)! 36
1.6. SUMMARY For example, if N =16 and K =4, then the number of solutions is (cid:18) (cid:19) 16−1 15! = =455. 4−1 3!12! Figure 1.20: One possible solution for N =16 and K =4. In general, the problem is equivalent to inserting K−1 dividers among N −1 balls. Closing remark. Permutations and combinations are two ways to enumerate all the pos- sible cases. While the conclusions are probabilistic, as the birthday paradox shows, permu- tation and combination are deterministic. We do not need to worry about the distribution of the samples, and we are not taking averages of anything. Thus, modern data analysis seldomuses theconceptsofpermutation and combination.Accordingly,combinatoricsdoes not play a large role in this book. Does it mean that combinatorics is not useful? Not quite, because it still provides us with powerful tools for theoretical analysis. For example, in binomial random variables, we need the concept of combination to calculate the repeated cases. The Poisson random vari- ablecanberegardedasalimitingcaseofthebinomialrandomvariable,andsocombination isalsoused.Therefore,whilewedonotusetheconceptsofpermutationperse,weusethem to define random variables. 1.6 Summary In this chapter, we have reviewed several background mathematical concepts that will be- come useful later in the book. You will find that these concepts are important for under- standing the rest of this book. When studying these materials, we recommend not just remembering the “recipes” of the steps but focusing on the motivations and intuitions behind the techniques. We would like to highlight the significance of the birthday paradox. Many of us come from an engineering background in which we were told to ensure reliability and guarantee success. We want to ensure that the product we deliver to our customers can survive even in the worst-case scenario. We tend to apply deterministic arguments such as requiring 366 peopletoensurecompletecoverageofthe365days.Inmoderndataanalysis,theworst-case scenario may not always be relevant because of the complexity of the problem and the cost ofsuchawarranty.Theprobabilisticargument,ortheaverageargument,ismorereasonable and cost-effective, as you can see from our analysis of the birthday problem. The heart of the problem is the trade-off between how much confidence you need versus how much effort you need to expend. Suppose an event is unlikely to happen, but if it happens, it will be 37
CHAPTER 1. MATHEMATICAL BACKGROUND a disaster. In that case, you might prefer to be very conservative to ensure that such a disaster event has a low chance of happening. Industries related to risk management such as insurance and investment banking are all operating under this principle. 1.7 Reference Introductory materials 1-1 Erwin Kreyszig, Advanced Engineering Mathematics, Wiley, 10th Edition, 2011. 1-2 HenryStarkandJohnW.Woods,ProbabilityandRandomProcesseswithApplications to Signal Processing, Prentice Hall, 3rd Edition, 2002. Appendix. 1-3 Michael J. Evans and Jeffrey S. Rosenthal, Probability and Statistics: The Science of Uncertainty, W. H. Freeman, 2nd Edition, 2009. Appendix. 1-4 James Stewart, Single Variable Calculus, Early Transcendentals, Thomson Brooks/- Cole, 6th Edition, 2008. Chapter 5. Combinatorics 1-5 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Section 1.6. 1-6 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Section 2.6. 1-7 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 3. Analysis Insomesectionsofthischapter,weuseresultsfromcalculusandinfiniteseries.Manyformal proofs can be found in the standard undergraduate real analysis textbooks. 1-8 Tom M. Apostol, Mathematical Analysis, Pearson, 1974. 1-9 Walter Rudin, Principles of Mathematical Analysis, McGraw Hill, 1976. 38
1.8. PROBLEMS 1.8 Problems Exercise 1. (Video Solution) (a) Show that (cid:88)n 1−rn+1 rk = . 1−r k=0 for any 0<r <1. Evaluate (cid:80)∞ rk. k=0 (b) Using the result of (a), evaluate 1+2r+3r2+··· . (c) Evaluate the sums (cid:88)∞ (cid:18) 1(cid:19)k+1 (cid:88)∞ (cid:18) 1(cid:19)k−1 k , and k . 3 4 k=0 k=2 Exercise 2. (Video Solution) Recall that (cid:88)∞ λk =eλ. k! k=0 Evaluate (cid:88)∞ λke−λ (cid:88)∞ λke−λ k , and k2 . k! k! k=0 k=0 Exercise 3. (Video Solution) Evaluate the integrals (a) (cid:90) b 1 (cid:18) a+b(cid:19)2 x− dx. b−a 2 a (b) (cid:90) ∞ λxe−λx dx. 0 (c) (cid:90) ∞ λx e−λ|x| dx. 2 −∞ 39
CHAPTER 1. MATHEMATICAL BACKGROUND Exercise 4. (a) ComputetheresultofthefollowingmatrixvectormultiplicationusingNumpy.Submit your result and codes.     1 2 3 1 4 5 6×2. 7 8 9 3 (b) Plot a sine function on the interval [−π,π] with 1000 data points. (c) Generate 10,000 uniformly distributed random numbers on interval [0, 1). Use matplotlib.pyplot.hist to generate a histogram of all the random numbers. Exercise 5. Calculate (cid:88)∞ (cid:18) 2(cid:19)k+1 k . 3 k=0 Exercise 6. Let (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) x 1 4 1 x= , µ= , Σ= . y 0 1 1 (a) Find Σ−1, the inverse of Σ. (b) Find |Σ|, the determinant of Σ. (c) Simplify the two-dimensional function (cid:26) (cid:27) 1 1 f(x)= exp − (x−µ)TΣ−1(x−µ) . 2π|Σ|1/2 2 (d) Use matplotlib.pyplot.contour, plot the function f(x) for the range [−3,3] × [−3,3]. Exercise 7. Out of seven electrical engineering (EE) students and five mechanical engineering (ME) students, a committee consisting of three EEs and two MEs is to be formed. In how many ways can this be done if (a) any of the EEs and any of the MEs can be included? (b) one particular EE must be on the committee? (c) two particular MEs cannot be on the committee? 40
1.8. PROBLEMS Exercise 8. Five blue balls, three red balls, and three white balls are placed in an urn. Three balls are drawn at random without regard to the order in which they are drawn. Using the counting approach to probability, find the probability that (a) one blue ball, one red ball, and one white ball are drawn. (b) all three balls drawn are red. (c) exactly two of the balls drawn are blue. Exercise 9. A collection of 26 English letters, a-z, is mixed in a jar. Two letters are drawn at random, one after the other. (a) Whatistheprobabilityofdrawingavowel(a,e,i,o,u)andaconsonantineitherorder? (b) Write a MATLAB / Python program to verify your answer in part (a). Randomly draw two letters without replacement and check whether one is a vowel and the other is a consonant. Compute the probability by repeating the experiment 10000 times. Exercise 10. There are 50 students in a classroom. (a) What is the probability that there is at least one pair of students having the same birthday? Show your steps. (b) Write a MATLAB / Python program to simulate the event and verify your answer in (a). Hint: You probably need to repeat the simulation many times to obtain a probability. Submit your code and result. You may assume that a year only has 365 days. You may also assume that all days have an equal likelihood of being taken. 41
CHAPTER 1. MATHEMATICAL BACKGROUND 42
Chapter 2 Probability Data and probability are inseparable. Data is the computational side of the story, whereas probability is the theoretical side of the story. Any data science practice must be built on thefoundationofprobability,andprobabilityneedstoaddresspracticalproblems.However, what exactly is “probability”? Mathematicians have been debating this for centuries. The frequentists argue that probability is the relative frequency of an outcome. For example, flipping a fair coin has a 1/2 probability of getting a head because if you flip the coin infinitely many times, you will have half of the time getting a head. The Bayesians argue that probability is a subjective belief. For example, the probability of getting an A in a classissubjectivebecausenoonewouldwanttotakeaclassinfinitelymanytimestoobtain the relative frequency. Both the frequentists and Bayesians have valid points. However, the differentiation is often non-essential because the context of your problem will force you to align with one or the other. For example, when you have a shortage of data, then the subjectivity of the Bayesians allows you to use prior knowledge, whereas the frequentists tell us how to compute the confidence interval of an estimate. No matter whether you prefer the frequentist’s view or the Bayesian’s view, there is somethingmorefundamentalthankstoAndrey Kolmogorov(1903-1987).Thedevelopment ofthis fundamentaldefinition willtake someeffortonour part,but ifwe distillthe essence, we can summarize it as follows: Probability is a measure of the size of a set. This sentence is not a formal definition; instead, it summarizes what we believe to be the essence of probability. We need to clarify some puzzles later in this chapter, but if you can understandwhatthissentencemeans,youarehalfwaydonewiththisbook.Tospelloutthe details, we will describe an elementary problem that everyone knows how to solve. As we discuss this problem, we will highlight a few key concepts that will give you some intuitive insights into our definition of probability, after which we will explain the sequence of topics to be covered in this chapter. Prelude: Probability of throwing a die Supposethatyouhaveafairdie.Ithas6faces:{1,2,3,4,5,6}.Whatistheprobabilitythat you get a number that is “less than 5” and is “an even number”? This is a straightforward 43
CHAPTER 2. PROBABILITY problem.Youprobablyhavealreadyfoundtheanswer,whichis 2 because“lessthan5”and 6 “an even number” means {2,4}. However, let’s go through the thinking process slowly by explicitly writing down the steps. First of all, how do we know that the denominator in 2 is 6? Well, because there are 6 six faces. These six faces form a set called the sample space. A sample space is the set containing all possible outcomes, which in our case is Ω={1,2,3,4,5,6}. The denominator 6 is the size of the sample space. How do we know that the numerator is 2? Again, implicitly in our minds, we have constructed two events: E = “less than 5” = {1,2,3,4}, and E = “an even number” 1 2 = {2,4,6}. Then we take the intersection between these two events to conclude the event E ={2,4}. The numerical value “2” is the size of this event E. So, when we say that “the probability is 2,” we are saying that the size of the event 6 E relative to the sample space Ω is the ratio 2. This process involves measuring the size 6 of E and Ω. In this particular example, the measure we use is a “counter” that counts the number of elements. This example shows us all the necessary components of probability: (i) There is a samplespace,whichisthesetthatcontainsallthepossibleoutcomes.(ii)Thereisanevent, which is a subset inside the sample space. (iii) Two events E and E can be combined to 1 2 construct another event E that is still a subset inside the sample space. (iv) Probability is a number assigned by certain rules such that it describes the relative size of the event E compared with the sample space Ω. So, when we say that probability is a measure of the size of a set, we create a mapping that takes in a set and outputs the size of that set. Organization of this chapter Asyoucanseefromthisexample,sinceprobabilityisameasureofthesizeofaset,weneed to understand the operations of sets to understand probability. Accordingly, in Section 2.1 wefirstdefinesetsanddiscusstheiroperations.Afterlearningthesebasicconcepts,wemove ontodefinethesamplespaceandeventspaceinSection2.2.There,wediscusssamplespaces that are not necessarily countable and how probabilities are assigned to events. Of course, assigning a probability value to an event cannot be arbitrary; otherwise, the probabilities may be inconsistent. Consequently, in Section 2.3 we introduce the probability axioms and formalize the notion of measure. Section 2.4 consists of a trio of topics that concern the relationshipbetweeneventsusingconditioning.WediscussconditionalprobabilityinSection 2.4.1, independence in Section 2.4.2, and Bayes’ theorem in Section 2.4.3. 2.1 Set Theory 2.1.1 Why study set theory? Inmathematics,weareofteninterestedindescribingacollectionofnumbers,forexample,a positiveinterval[a,b]onthereallineortheorderedpairsofnumbersthatdefineacircleon a graph with two axes. These collections of numbers can be abstractly defined as sets. In a nutshell,asetissimplyacollectionofthings.Thesethingscanbenumbers,buttheycanalso bealphabets,objects,oranything.Settheoryisamathematicaltoolthatdefinesoperations on sets. It provides the basic arithmetic for us to combine, separate, and decompose sets. 44
2.1. SET THEORY Whydowestartthechapterbydescribingsettheory?Becauseprobabilityisameasure of the size of a set. Yes, probability is not just a number telling us the relative frequency of events; it is an operator that takes a set and tells us how large the set is. Using the example we showed in the prelude, the event “even number” of a die is a set containing numbers{2,4,6}.Whenweapplyprobabilitytothisset,weobtainthenumber 3,asshown 6 in Figure 2.1. Thus sets are the foundation of the study of probability. Figure 2.1: Probability is a measure of the size of a set. Whenever we talk about probability, it has to be the probability of a set. 2.1.2 Basic concepts of a set Definition 2.1 (Set). A set is a collection of elements. We denote A={ξ ,ξ ,...,ξ } (2.1) 1 2 n as a set, where ξ is the ith element in the set. i Inthisdefinition,Aiscalledaset.Itisnothingbutacollectionofelementsξ ,...,ξ .What 1 n are these ξ ’s? They can be anything. Let’s see a few examples below. i Example 2.1(a). A={apple,orange,pear} is a finite set. Example 2.1(b). A={1,2,3,4,5,6} is a finite set. Example 2.1(c). A={2,4,6,8,...} is a countable but infinite set. Example 2.1(d). A={x|0<x<1} is a uncountable set. To say that an element ξ is drawn from A, we write ξ ∈A. For example, the number 1 is an element in the set {1,2,3}. We write 1∈{1,2,3}. There are a few common sets that we will encounter. For example, Example 2.2(a). R is the set of all real numbers including ±∞. Example 2.2(b). R2 is the set of ordered pairs of real numbers. Example 2.2(c). [a,b]={x|a≤x≤b} is a closed interval on R. Example 2.2(d). (a,b)={x|a<x<b} is an open interval on R. Example 2.2(e). (a,b]={x|a<x≤b} is a semi-closed interval on R. 45
CHAPTER 2. PROBABILITY Figure 2.2: From left to right: a closed interval, a semi-closed (or semi-open) interval, and an open interval. Setsarenotlimitedtonumbers.Asetcanbeusedtodescribeacollectionof functions. Example2.3.A={f :R→R|f(x)=ax+b, a,b∈R}.Thisisthesetofallstraight lines in 2D. The notation f : R → R means that the function f takes an argument from R and sends it to another real number in R. The definition f(x) = ax+b says that f is taking the specific form of ax+b. Since the constants a and b can be any real number, the equation f(x)=ax+b enumerates all possible straight lines in 2D. See Figure 2.3(a). Example 2.4. A = {f : R → [−1,1] | f(t) = cos(ω t+θ), θ ∈ [0,2π]}. This is 0 the set of all cosine functions of a fixed carrier frequency ω . The phase θ, however, 0 is changing. Therefore, the equation f(t) = cos(ω t+θ) says that the set A is the 0 collection of all possible cosines with different phases. See Figure 2.3(b). 1 0.5 0 -0.5 -1 -2 -1 0 1 2 t )t(f 2 1 0 -1 -2 -1 -0.5 0 0.5 1 t )t(f Figure 2.3: (a) The set of straight lines A={f :R→R|f(x)=ax+b, a,b∈R}. (b) The set of phase-shifted cosines A={f :R→[−1,1]|f(t)=cos(ω t+θ), θ∈[0,2π]}. 0 A set can also be used to describe a collection of sets. Let A and B be two sets. Then C ={A,B} is a set of sets. Example 2.5. Let A={1,2} and B ={apple,orange}. Then C ={A,B}={{1,2},{apple,orange}} is a collection of sets. Note that here we are not saying C is the union of two sets. We are only saying that C is a collection of two sets. See the next example. 46
2.1. SET THEORY Example 2.6. Let A={1,2} and B ={3}, then C ={A,B} means that C ={{1,2},{3}}. Therefore C contains only two elements. One is the set {1,2} and the other is the set {3}. Note that {{1,2},{3}}=(cid:54) {1,2,3}. The former is a set of two sets. The latter is a set of three elements. 2.1.3 Subsets Given a set, we often want to specify a portion of the set, which is called a subset. Definition 2.2 (Subset). B is a subset of A if for any ξ ∈ B, ξ is also in A. We write B ⊆A (2.2) to denote that B is a subset of A. B iscalledaproper subsetofAifB isasubsetofAandB (cid:54)=A.Wedenoteapropersubset as B ⊂A. Two sets A and B are equal if and only if A⊆B and B ⊆A. Example 2.7. • If A={1,2,3,4,5,6}, then B ={1,3,5} is a proper subset of A. • If A={1,2}, then B ={1,2} is an improper subset of A. • If A={t|t≥0}, then B ={t|t>0} is a proper subset of A. Practice Exercise 2.1. Let A={1,2,3}. List all the subsets of A. Solution. The subsets of A are: A={∅,{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}}. Practice Exercise 2.2. Prove that two sets A and B are equal if and only if A⊆B and B ⊆A. Solution. Suppose A ⊆ B and B ⊆ A. Assume by contradiction that A (cid:54)= B. Then necessarily there must exist an x such that x ∈ A but x (cid:54)∈ B (or vice versa). But A ⊆ B means that x ∈ A will necessarily be in B. So it is impossible to have x (cid:54)∈ B. Conversely, suppose that A=B. Then any x∈A will necessarily be in B. Therefore, we have A⊆B. Similarly, if A=B then any x∈B will be in A, and so B ⊆A. 47
CHAPTER 2. PROBABILITY 2.1.4 Empty set and universal set Definition 2.3 (Empty Set). A set is empty if it contains no element. We denote an empty set as A=∅. (2.3) A set containing an element 0 is not an empty set. It is a set of one element, {0}. The number of elements of the empty set is 0. The empty set is a subset of any set, i.e., ∅ ⊆ A for any A. We use ⊆ because A could also be an empty set. Example 2.8(a). The set A = {x| sinx > 1} is empty because no x ∈ R can make sinx>1. Example 2.8(b). The set A = {x|x > 5 and x < 1} is empty because the two conditions x>5 and x<1 are contradictory. Definition 2.4 (Universal Set). The universal set is the set containing all elements under consideration. We denote a universal set as A=Ω. (2.4) The universal set Ω contains itself, i.e., Ω ⊆ Ω. The universal set is a relative concept. Usually, we first define a universal set Ω before referring to subsets of Ω. For example, we can define Ω = R and refer to intervals in R. We can also define Ω = [0,1] and refer to subintervals inside [0,1]. 2.1.5 Union We now discuss basic set operations. By operations, we mean functions of two or more sets whose output value is a set. We use these operations to combine and separate sets. Let us first consdier the union of two sets. See Figure 2.4 for a graphical depiction. Definition 2.5 (Finite Union). The union of two sets A and B contains all elements in A or in B. That is, A∪B ={ξ |ξ ∈Aorξ ∈B}. (2.5) As the definition suggests, the union of two sets connects the sets using the logical operator ”or”. Therefore, the union of two sets is always larger than or equal to the individual sets. Example 2.9(a). If A = {1,2}, B = {1,5}, then A∪B = {1,2,5}. The overlapping element 1 is absorbed. Also, note that A∪B (cid:54)= {{1,2},{1,5}}. The latter is a set of sets. Example 2.9(b). If A=(3,4], B =(3.5,∞), then A∪B =(3,∞). Example 2.9(c). If A = {f : R → R|f(x) = ax} and B = {f : R → R|f(x) = b}, then A∪B = a set of sloped lines with a slope a plus a set of constant lines with 48
2.1. SET THEORY height b. Note that A∪B (cid:54)={f :R→R|f(x)=ax+b} because the latter is a set of sloped lines with arbitrary y-intercept. Example 2.9(d). If A={1,2} and B =∅, then A∪B ={1,2}. Example. If A={1,2} and B =Ω, then A∪B =Ω. Figure 2.4: The union of two sets contains elements that are either in A or B or both. Thepreviousexamplecanbegeneralizedinthefollowingexercise.Whatitsaysisthat ifAisasubsetofanothersetB,thentheunionofAandB isjustB.Intuitively,thisshould be straightforward because whatever you have in A is already in B, so the union will just be B. Below is a formal proof that illustrates how to state the arguments clearly. You may like to draw a picture to convince yourself that the proof is correct. Practice Exercise 2.3: Prove that if A⊆B, then A∪B =B. Solution: We will show that A∪B ⊆B and B ⊆A∪B. Let ξ ∈A∪B. Then ξ must be inside either A or B (or both). In any case, since we know that A ⊆ B, it holds that if ξ ∈ A then ξ must also be in B. Therefore, for any ξ ∈ A∪B we have ξ ∈ B. This shows A∪B ⊆ B. Conversely, if ξ ∈ B, then ξ must be inside A∪B because A∪B is a larger set than B. So if ξ ∈B then ξ ∈A∪B and hence B ⊆A∪B. Since A∪B is a subset of B or equal to B, and B is a subset of A∪B or equal to A∪B, it follows that A∪B =B. What should we do if we want to take the union of an infinite number of sets? First, we need to define the concept of an infinite union. Definition 2.6 (Infinite Union). For an infinite sequence of sets A ,A ,..., the in- 1 2 finite union is defined as ∞ (cid:91) A ={ξ | ξ ∈A for at least one n that is finite.}. (2.6) n n n=1 An infinite union is a natural extension of a finite union. It is not difficult to see that ξ ∈A or ξ ∈B ⇐⇒ ξ is in at least one of AandB. 49
CHAPTER 2. PROBABILITY Similarly, an infinite union means that ξ ∈A or ξ ∈A or ξ ∈A ... ⇐⇒ ξ is in at least one of A , A , A , .... 1 2 3 1 2 3 The finite n requirement says that we only evaluate the sets for a finite number of n’s. This n can be arbitrarily large, but it is finite. Why are we able to do this? Because the concept of an infinite union is to determine A , which is the limit of a sequence. Like any sequence ∞ ofrealnumbers,thelimitofasequenceofsetshastobedefinedbyevaluatingtheinstances of all possible finite cases. Consider a sequence of sets A = (cid:2) −1,1− 1(cid:3) , for n = 1,2,.... For example, A = n n 1 [−1,0], A =(cid:2) −1,1(cid:3) , A =(cid:2) −1,2(cid:3) , A =(cid:2) −1,3(cid:3) , etc. 2 2 3 3 4 4 Figure 2.5: The infinite union of (cid:83)∞ (cid:2) −1,1− 1(cid:3) . No matter how large n gets, the point 1 is never n=1 n included. So the infinite union is [−1,1) Totaketheinfiniteunion,weknowthattheset[−1,1)isalwaysincluded,becausethe right-hand limit 1− 1 approaches 1 as n approaches ∞. So the only question concerns the n number 1. Should 1 be included? According to the definition above, we ask: Is 1 an element of at least one of the sets A , A , ..., A ? Clearly it is not: 1 (cid:54)∈ A , 1 (cid:54)∈ A , .... In fact, 1 2 n 1 2 1(cid:54)∈A for any finite n. Therefore 1 is not an element of the infinite union, and we conclude n that ∞ ∞ (cid:20) (cid:21) (cid:91) (cid:91) 1 A = −1,1− =[−1,1). n n n=1 n=1 Practice Exercise 2.4. Find the infinite union of the sequences where (a) A = n (cid:2) −1,1− 1(cid:1) , (b) A =(cid:0) −1,1− 1(cid:3) . n n n Solution. (a) (cid:83)∞ A =[−1,1). (b) (cid:83)∞ A =(−1,1). n=1 n n=1 n 2.1.6 Intersection Theunionoftwosetsisbasedonthelogicaloperatoror.Ifweusethelogicaloperatorand, then the result is the intersection of two sets. Definition 2.7 (Finite Intersection). The intersection of two sets A and B contains all elements in A and in B. That is, A∩B ={ξ |ξ ∈A and ξ ∈B}. (2.7) Figure 2.6 portrays intersection graphically. Intersection finds the common elements of the two sets. It is not difficult to show that A∩B ⊆A and A∩B ⊆B. 50
2.1. SET THEORY Figure 2.6: The intersection of two sets contains elements in both A and B. Example 2.10(a). If A={1,2,3,4}, B ={1,5,6}, then A∩B ={1}. Example 2.10(b). If A={1,2}, B ={5,6}, then A∩B =∅. Example 2.10(c). If A=(3,4], B =[3.5,∞), then A∩B =[3.5,4]. Example 2.10(d). If A=(3,4], B =∅, then A∩B =∅. Example 2.10(e). If A=(3,4], B =Ω, then A∩B =(3,4]. Example 2.11.IfA={f :R→R|f(x)=ax}andB ={f :R→R|f(x)=b},then A∩B =theintersectionofasetofslopedlineswithaslopeaandasetofconstantlines with height b. The only line that can satisfy both sets is the line f(x)=0. Therefore, A∩B ={f|f(x)=0}. Example 2.12. If A = {{1},{2}} and B = {{2,3},{4}}, then A∩B = ∅. This is becauseAisasetcontainingtwosets,andB isasetcontainingtwosets.Thetwosets {2} and {2,3} are not the same. Thus, A and B have no elements in common, and so A∩B =∅. Similarly to the infinite union, we can define the concept of infinite intersection. Definition 2.8 (Infinite Intersection). For an infinite sequence of sets A ,A ,..., 1 2 the infinite intersection is defined as ∞ (cid:92) A ={ξ | ξ ∈A for every finite n.} (2.8) n n n=1 To understand this definition, we note that ξ ∈A and ξ ∈B ⇐⇒ ξ is in every one of AandB. As a result, it follows that ξ ∈A and ξ ∈A and ξ ∈A ... ⇐⇒ ξ is in every one of A , A , A , .... 1 2 3 1 2 3 51
CHAPTER 2. PROBABILITY Since the infinite intersection requires that ξ is in every one of A , A , ..., A , if there is a 1 2 n set A that does not contain ξ, the infinite intersection is an empty set. i Consider the problem of finding the infinite intersection of (cid:84)∞ A , where n=1 n (cid:20) (cid:19) 1 A = 0,1+ . n n We note that the sequence of sets is [0,2], [0,1.5], [0,1.33], .... As n → ∞, we note that the limit is either [0,1) or [0,1]. Should the right-hand limit 1 be included in the infinite intersection? According to the definition above, we know that 1 ∈ A , 1 ∈ A , ..., 1 ∈ A 1 2 n for any finite n. Therefore, 1 is included and so ∞ ∞ (cid:20) (cid:19) (cid:92) (cid:92) 1 A = 0,1+ =[0,1]. n n n=1 n=1 Figure 2.7: The infinite intersection of (cid:84)∞ (cid:2) 0,1+ 1(cid:1) . No matter how large n gets, the point 1 is n=1 n never included. So the infinite intersection is [0,1] Practice Exercise 2.5. Find the infinite intersection of the sequences where (a) A =(cid:2) 0,1+ 1(cid:3) , (b) A =(cid:0) 0,1+ 1(cid:1) , (c) A =(cid:2) 0,1− 1(cid:1) , (d) A =(cid:2) 0,1− 1(cid:3) . n n n n n n n n Solution. (a) (cid:84)∞ A =[0,1]. n=1 n (b) (cid:84)∞ A =(−1,1]. n=1 n (c) (cid:84)∞ A =[0,0)=∅. n=1 n (d) (cid:84)∞ A =[0,0]={0}. n=1 n 2.1.7 Complement and difference Besides union and intersection, there is a third basic operation on sets known as the com- plement. Definition 2.9 (Complement). The complement of a set A is the set containing all elements that are in Ω but not in A. That is, Ac ={ξ |ξ ∈Ωandξ (cid:54)∈A}. (2.9) Figure 2.8 graphically portrays the idea of a complement. The complement is a set that contains everything in the universal set that is not in A. Thus the complement of a set is always relative to a specified universal set. 52
2.1. SET THEORY Figure 2.8: [Left] The complement of a set A contains all elements that are not in A. [Right] The difference A\B contains elements that are in A but not in B. Example 2.13(a). Let A={1,2,3} and Ω={1,2,3,4,5,6}. Then Ac ={4,5,6}. Example 2.13(b). Let A = {even integers} and Ω = {integers}. Then Ac = {odd integers}. Example 2.13(c). Let A={integers} and Ω=R. Then Ac ={any real number that is not an integer}. Example 2.13(d). Let A=[0,5) and Ω=R. Then Ac =(−∞,0)∪[5,∞). Example 2.13(e). Let A=R and Ω=R. Then Ac =∅. The concept of the complement will help us understand the concept of difference. Definition 2.10 (Difference). The difference A\B is the set containing all elements in A but not in B. A\B ={ξ |ξ ∈Aandξ (cid:54)∈B}. (2.10) Figure2.8portraystheconceptofdifferencegraphically.NotethatA\B (cid:54)=B\A.Theformer removes the elements in B whereas the latter removes the elements in A. Example 2.14(a). Let A = {1,3,5,6} and B = {2,3,4}. Then A\B = {1,5,6} and B\A={2,4}. Example 2.14(b). Let A = [0,1], B = [2,3], then A\B = [0,1], and B\A = [2,3]. This example shows that if the two sets do not overlap, there is nothing to subtract. Example2.14(c).LetA=[0,1],B =R,thenA\B =∅,andB\A=(−∞,0)∪(1,∞). This example shows that if one of the sets is the universal set, then the difference will either return the empty set or the complement. 53
CHAPTER 2. PROBABILITY Figure 2.9: [Left] A and B are overlapping. [Right] A and B are disjoint. Practice Exercise 2.6. Show that for any two sets A and B, the differences A\B and B\A never overlap, i.e., (A\B)∩(B\A)=∅. Solution. Suppose, by contradiction, that the intersection is not empty so that there exists an ξ ∈ (A\B)∩(B\A). Then, by the definition of intersection, ξ is an element of (A\B) and (B\A). But if ξ is an element of (A\B), it cannot be an element of B. This implies that ξ cannot be an element of (B\A) since it is a subset of B. This is a contradiction because we just assumed that the ξ can live in both (A\B) and (B\A). Difference can be defined in terms of intersection and complement: Theorem 2.1. Let A and B be two sets. Then A\B =A∩Bc (2.11) Proof. Let x ∈ A\B. Then x ∈ A and x (cid:54)∈ B. Since x (cid:54)∈ B, we have x ∈ Bc. Therefore, x ∈ A and x ∈ Bc. By the definition of intersection, we have x ∈ A∩Bc. This shows that A\B ⊆ A∩Bc. Conversely, let x ∈ A∩Bc. Then, x ∈ A and x ∈ Bc, which implies that x ∈ A and x (cid:54)∈ B. By the definition of A\B, we have that x ∈ A\B. This shows that A∩Bc ⊆A\B. (cid:3) 2.1.8 Disjoint and partition It is important to be able to quantify situations in which two sets are not overlapping. In this situation, we say that the sets are disjoint. Definition 2.11 (Disjoint). Two sets A and B are disjoint if A∩B =∅. (2.12) For a collection of sets {A ,A ,...,A }, we say that the collection is disjoint if, for 1 2 n any pair i(cid:54)=j, A ∩A =∅. (2.13) i j A pictorial interpretation can be found in Figure 2.9. 54
2.1. SET THEORY Example 2.15(a). Let A={x>1} and B ={x<0}. Then A and B are disjoint. Example 2.15(b). Let A={1,2,3} and B =∅. Then A and B are disjoint. Example 2.15(c). Let A=(0,1) and B =[1,2). Then A and B are disjoint. With the definition of disjoint, we can now define the powerful concept of partition. Definition 2.12 (Partition). A collection of sets {A ,...,A } is a partition of the 1 n universal set Ω if it satisfies the following conditions: • (non-overlap) {A ,...,A } is disjoint: 1 n A ∩A =∅. (2.14) i j • (decompose) Union of {A ,...,A } gives the universal set: 1 n n (cid:91) A =Ω. (2.15) i i=1 In plain language, a partition is a collection of non-overlapping subsets whose union is the universal set. Partition is important because it is a decomposition of Ω into a smaller subset, and since these subsets do not overlap, they can be analyzed separately. Partition is a handy tool for studying probability because it allows us to decouple complex events by treating them as isolated sub-events. Figure 2.10: A partition of Ω contains disjoint subsets of which the union gives us Ω. Example 2.16. Let Ω={1,2,3,4,5,6}. The following sets form a partition: A ={1,2,3}, A ={4,5}, A ={6} 1 2 3 Example 2.17. Let Ω={1,2,3,4,5,6}. The collection A ={1,2,3}, A ={4,5}, A ={5,6} 1 2 3 does not form a partition, because A ∩A ={5}. 2 3 55
CHAPTER 2. PROBABILITY If {A ,A ,...,A } forms a partition of the universal set Ω, then for any B ⊆ Ω, we 1 2 n can decompose B into n disjoint subsets: B∩A , B∩A , ...B∩A . Two properties hold: 1 2 n • B∩A and B∩A are disjoint if i(cid:54)=j. i j • The union of B∩A , B∩A , ...B∩A is B. 1 2 n Practice Exercise 2.7. Prove the above two statements. Solution. To prove the first statement, we can pick ξ ∈ (B ∩A ). This means that i ξ ∈ B and ξ ∈ A . Since ξ ∈ A , it cannot be in A because A and A are disjoint. i i j i j Therefore ξ cannot live in B∩A . This completes the proof, because we just showed j that any ξ ∈B∩A cannot simultaneously live in B∩A . i j To prove the second statement, we pick ξ ∈ (cid:83)n (B ∩A ). Since ξ lives in the i=1 i union,ithastoliveinatleastoneofthe(B∩A )forsomei.Nowsupposeξ ∈B∩A . i i Thismeansthatξ isinbothB andA ,soitmustliveinB.Therefore,(cid:83)n (B∩A )⊆ i i=1 i B.Now,supposewepickξ ∈B.ThensinceitisanelementinB,itmustbeanelement in all of the (B∩A )’s for any i. Therefore, ξ ∈(cid:83)n (B∩A ), and so we showed that i i=1 i B ⊆(cid:83)n (B∩A ).Combiningthetwodirections,weconcludethat(cid:83)n (B∩A )=B. i=1 i i=1 i Example 2.18. Let Ω = {1,2,3,4,5,6} and let a partition of Ω be A = {1,2,3}, 1 A = {4,5}, A = {6}. Let B = {1,3,4}. Then, by the result we just proved, B can 2 3 be decomposed into three subsets: B∩A ={1,3}, B∩A ={4}, B∩A =∅. 1 2 3 Thus we can see that B∩A , B∩A and B∩A are disjoint. Furthermore, the union 1 2 3 of these three sets gives B. 2.1.9 Set operations When handling multiple sets, it would be useful to have some basic set operations. There are four basic theorems concerning set operations that you need to know for our purposes in this book: Theorem 2.2 (Commutative). (Order does not matter) A∩B =B∩A, and A∪B =B∪A. (2.16) Theorem 2.3 (Associative). (How to do multiple union and intersection) A∪(B∪C)=(A∪B)∪C, A∩(B∩C)=(A∩B)∩C. (2.17) 56
2.1. SET THEORY Theorem 2.4 (Distributive). (How to mix union and intersection) A∩(B∪C)=(A∩B)∪(A∩C), A∪(B∩C)=(A∪B)∩(A∪C). (2.18) Theorem2.5(DeMorgan’sLaw). (Howtocomplementoverintersectionandunion) (A∩B)c =Ac∪Bc, (A∪B)c =Ac∩Bc. (2.19) Example 2.19. Consider [1,4]∩([0,2]∪[3,5]). By the distributive property we can simplify the set as [1,4]∩([0,2]∪[3,5])=([1,4]∩[0,2])∪([1,4]∩[3,5]) =[1,2]∪[3,4]. Example 2.20. Consider ([0,1]∪[2,3])c. By De Morgan’s Law we can rewrite the set as ([0,2]∪[1,3])c =[0,2]c∩[1,3]c. 2.1.10 Closing remarks about set theory It should be apparent why set theory is useful: it shows us how to combine, split, and remove sets. In Figure 2.11 we depict the intersection of two sets A={even number} and B ={less than or equal to 3}. Set theory tells us how to define the intersection so that the probability can be applied to the resulting set. Figure2.11:WhentherearetwoeventsAandB,theprobabilityofA∩B isdeterminedbyfirsttaking the intersection of the two sets and then evaluating its probability. Universal sets and empty sets are useful too. Universal sets cover all the possible outcomes of an experiment, so we should expect P[Ω] = 1. Empty sets contain nothing, and so we should expect P[∅]=0. These two properties are essential to define a probability because no probability can be greater than 1, and no probability can be less than 0. 57
CHAPTER 2. PROBABILITY 2.2 Probability Space We now formally define probability. Our discussion will be based on the slogan probability is a measure of the size of a set. Three elements constitute a probability space: • Sample Space Ω: The set of all possible outcomes from an experiment. • Event SpaceF:Thecollectionofallpossibleevents.AneventE isasubsetinΩthat defines an outcome or a combination of outcomes. • Probability Law P: A mapping from an event E to a number P[E] which, ideally, measures the size of the event. Therefore, whenever you talk about “probability,” you need to specify the triplet (Ω,F,P) to define the probability space. The necessity of the three elements is illustrated in Figure 2.12. The sample space is the interface with the physical world. It is the collection of all possible states that can result from an experiment. Some outcomes are more likely to happen, and some are less likely, but this does not matter because the sample space contains every possible outcome. The probability law is the interface with the data analysis. It is this law that defines the likelihood of each of the outcomes. However, since the probability law measures the size of a set, the probability law itself must be a function, a function whose argument is a set and whose value is a number. An outcome in the sample space is not a set. Instead, a subset in the sample space is a set. Therefore, the probability should input a subset and map it to a number. The collection of all possible subsets is the event space. Figure 2.12: Given an experiment, we define the collection of all outcomes as the sample space. A subset in the sample space is called an event. The probability law is a mapping that maps an event to a number that denotes the size of the event. A perceptive reader like you may be wondering why we want to complicate things to thisdegreewhencalculatingprobabilityistrivial,e.g.,throwingadiegivesusaprobability 1 per face. In a simple world where problems are that easy, you can surely ignore all these 6 complications and proceed to the answer 1. However, modern data analysis is not so easy. 6 If we are given an image of size 64×64 pixels, how do we tell whether this image is of a cat or a dog? We need to construct a probability model that tells us the likelihood of having a 58
2.2. PROBABILITY SPACE particular 64×64 image. What should be included in this probability model? We need to know all the possible cases (the sample space), all the possible events (the event space), andtheprobabilityofeachoftheevents(theprobabilitylaw).Ifweknowallthese,thenour decision will be theoretically optimal. Of course, for high-dimensional data like images, we need approximations to such a probability model. However, we first need to understand the theoretical foundation of the probability space to know what approximations would make sense. 2.2.1 Sample space Ω We start by defining the sample space Ω. Given an experiment, the sample space Ω is the set containing all possible outcomes of the experiment. Definition 2.13. A sample space Ω is the set of all possible outcomes from an ex- periment. We denote ξ as an element in Ω. A sample space can contain discrete outcomes or continuous outcomes, as shown in the examples below and Figure 2.13. Example 2.21: (Discrete Outcomes) • Coin flip: Ω={H, T}. • Throw a die: Ω={1,2,3,4,5,6}. • Paper / scissor / stone: Ω={paper,scissor,stone}. • Draw an even integer: Ω={2,4,6,8,...}. Example 2.22: (Continuous Outcomes) • Waiting time for a bus in West Lafayette: Ω={t | 0≤t≤30minutes}. • Phase angle of a voltage: Ω={θ | 0≤θ ≤2π}. • Frequency of a pitch: Ω={f | 0≤f ≤f }. max Figure 2.13 also shows a functional example of the sample space. In this case, the sample space contains functions. For example, • Set of all straight lines in 2D: Ω={f |f(x)=ax+b, a,b∈R}. • Set of all cosine functions with a phase offset: Ω={f |f(t)=cos(2πω t+Θ), 0≤Θ≤2π}. 0 As we see from the above examples, the sample space is nothing but a universal set. The elements inside the sample space are the outcomes of the experiment. If you change 59
CHAPTER 2. PROBABILITY Figure 2.13: The sample space can take various forms: it can contain discrete numbers, or continuous intervals, or even functions. the experiment, the possible outcomes will be different so that the sample space will be different. For example, flipping a coin has different possible outcomes from throwing a die. What if we want to describe a composite experiment where we flip a coin and throw a die? Here is the sample space: Example 2.23: If the experiment contains flipping a coin and throwing a die, then the sample space is (cid:26) (H,1),(H,2),(H,3),(H,4),(H,5),(H,6), (cid:27) (T,1),(T,2),(T,3),(T,4),(T,5),(T,6) . In this sample space, each element is a pair of outcomes. Practice Exercise 2.8.Thereare8processorsonacomputer.Acomputerjobsched- uler chooses one processor randomly. What is the sample space? If the computer job scheduler can choose two processors at once, what is the sample space then? Solution. The sample space of the first case is Ω = {1,2,3,4,5,6,7,8}. The sample space of the second case is Ω={(1,2),(1,3),(1,4),...,(7,8)}. Practice Exercise 2.9. A cell phone tower has a circular average coverage area of radius of 10 km. We observe the source locations of calls received by the tower. What is the sample space of all possible source locations? Solution.Assumethatthecenterofthetowerislocatedat(x ,y ).Thesamplespace 0 0 is the set (cid:112) Ω={(x,y)| (x−x )2+(y−y )2 ≤10}. 0 0 60
2.2. PROBABILITY SPACE Noteverysetcanbeasamplespace.Asamplespacemustbeexhaustiveandexclusive. The term “exhaustive” means that the sample space has to cover all possible outcomes. If there is one possible outcome that is left out, then the set is no longer a sample space. The term “exclusive” means that the sample space contains unique elements so that there is no repetition of elements. Example 2.24. (Counterexamples) The following two examples are NOT sample spaces. • Throw a die: Ω={1,2,3} is not a sample space because it is not exhaustive. • Throwadie:Ω={1,1,2,3,4,5,6}isnotasamplespacebecauseitisnotexclu- sive. Therefore, a valid sample space must contain all possible outcomes, and each element must be unique. We summarize the concept of a sample space as follows. What is a sample space Ω? • A sample space Ω is the collection of all possible outcomes. • The outcomes can be numbers, alphabets, vectors, or functions. The outcomes can also be images, videos, EEG signals, audio speeches, etc. • Ω must be exhaustive and exclusive. 2.2.2 Event space F Thesamplespacecontainsallthepossibleoutcomes.However,inmanypracticalsituations, we are not interested in each of the individual outcomes; we are interested in the com- binations of the outcomes. For example, when throwing a die, we may ask “What is the probabilityofrollinganoddnumber?”or“Whatistheprobabilityofrollinganumberthat is less than 3?” Clearly, “odd number” is not an outcome of the experiment because the possible outcomes are {1,2,3,4,5,6}. We call “odd number” an event. An event must be a subset in the sample space. Definition2.14. An eventE isasubsetinthesamplespaceΩ.Thesetofallpossible events is denoted as F. While this definition is extremely simple, we need to keep in mind a few facts about events. First, an outcome ξ is an element in Ω but an event E is a subset contained in Ω, i.e., E ⊆ Ω. Thus, an event can contain one outcome but it can also contain many outcomes. The following example shows a few cases of events: 61
CHAPTER 2. PROBABILITY Example 2.25. Throw a die. Let Ω={1,2,3,4,5,6}. The following are two possible events, as illustrated in Figure 2.14. • E ={even numbers}={2,4,6}. 1 • E ={less than 3}={1,2}. 2 Figure 2.14: Twoexamplesofevents:Thefirsteventcontainsnumbers{2,4,6},andthesecond event contains numbers {1,2}. Practice Exercise 2.10. The “ping” command is used to measure round-trip times for Internet packets. What is the sample space of all possible round-trip times? What is the event that a round-trip time is between 10 ms and 20 ms? Solution. The sample space is Ω=[0,∞). The event is E =[10,20]. Practice Exercise 2.11. A cell phone tower has a circular average coverage area of radius 10 km. We observe the source locations of calls received by the tower. What is theeventwhenthesourcelocationofacallisbetween2kmand5kmfromthetower? Solution. Assume that the center of the tower is located at (x ,y ). The event is 0 0 (cid:112) E ={(x,y)|2≤ (x−x )2+(y−y )2 ≤5}. 0 0 The second point we should remember is the cardinality of Ω and that of F. A sample space containing n elements has a cardinality n. However, the event space constructed from Ω will contain 2n events. To see why this is so, let’s consider the following example. Example 2.26. Consider an experiment with 3 outcomes Ω={♣,♥,(cid:122)}. We can list out all the possible events: ∅, {♣}, {♥}, {(cid:122)}, {♣,♥}, {♣,(cid:122)}, {♥,♣}, {♣,♥,(cid:122)}. So in total there are 23 = 8 possible events. Figure 2.15 depicts the situation. What is the difference between ♣ and {♣}? The former is an element, whereas the latter is a set. Thus, {♣} is an event but ♣ is not an event. Why is ∅ an event? Because we can ask “What is the probability that we get an odd number and an even number?” The probabilityisobviouslyzero,butthereasonitiszeroisthattheeventisanemptyset. 62
2.2. PROBABILITY SPACE Figure 2.15: The event space contains all the possible subsets inside the sample space. In general, if there are n elements in the sample space, then the number of events is 2n. To see why this is true, we can assign to each element a binary value: either 0 or 1. For example, in Table 2.1 we consider throwing a die. For each of the six faces, we assign a binary code. This will give us a binary string for each event. For example, the event {1,5} is encoded as the binary string 100010 because only 1 and 5 are activated. We can count the total number of unique strings, which is the number of strings that can be constructed from n bits. It is easily seen that this number is 2n. Event 1 2 3 4 5 6 Binary Code ∅ × × × × × × 000000 {1,5} (cid:13) × × × (cid:13) × 100010 {3,4,5} × × (cid:13) (cid:13) (cid:13) × 001110 . . . . . . . . . . . . {2,3,4,5,6} × (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 011111 {1,2,3,4,5,6} (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 111111 Table 2.1: Aneventspacecontains2n events,wherenisthenumberofelementsinthesamplespace. Toseethis,weencodeeachoutcomewithabinarycode.Theresultingbinarystringthenformsaunique index of the event. Counting the total number of events gives us the cardinality of the event space. The box below summarizes what you need to know about event spaces. What is an event space F? • An event space F is the set of all possible subsets. It is a set of sets. • We need F because the probability law P is mapping a set to a number. P does not take an outcome from Ω but a subset inside Ω. 63
CHAPTER 2. PROBABILITY Event spaces: Some advanced topics The following discussions can be skipped if it is your first time reading the book. What else do we need to take care of in order to ensure that an event is well defined? A few set operations seem to be necessary. For example, if E = {1} and E = {2} are 1 2 events, it is necessary that E = E ∪E = {1,2} is an event too. Another example: if 1 2 E ={5,6} and E ={1,5} are events, then it is necessary that E =E ∩E ={5} is also 1 2 1 2 an event. The third example: if E = {3,4,5,6} is an event, then E = Ec = {1,2} should 1 1 be an event. As you can see, there is nothing sophisticated in these examples. They are just some basic set operations. We want to ensure that the event space is closed under these set operations. That is, we do not want to be surprised by finding that a set constructed from two events is not an event. However, since all set operations can be constructed from union, intersection and complement, ensuring that the event space is closed under these three operations effectively ensures that it is closed to all set operations. The formal way to guarantee these is the notion of a field. This term may seem to be abstract, but it is indeed quite useful: Definition 2.15. For an event space F to be valid, F must be a field F. It is a field if it satisfies the following conditions • ∅∈F and Ω∈F. • (Closed under complement) If F ∈F, then also Fc ∈F. • (Closed under union and intersection) If F ∈F and F ∈F, then F ∩F ∈F 1 2 1 2 and F ∪F ∈F. 1 2 Forafiniteset,i.e.,asetthatcontainsnelements,thecollectionofallpossiblesubsets is indeed a field. This is not difficult to see if you consider rolling a die. For example, if E ={3,4,5,6}isinsideF,thenEc ={1,2}isalsoinsideF.ThisisbecauseF consistsof2n subsets each being encoded by a unique binary string. So if E =001111, then Ec =110000, which is also in F. Similar reasoning applies to intersection and union. At this point, you may ask: • Why bother constructing a field? The answer is that probability is a measure of the size of a set, so we must input a set to a probability measure P to get a number. The set being input to P must be a subset inside the sample space; otherwise, it will be undefined. If we regard P as a mapping, we need to specify the collection of all its inputs, which is the set of all subsets, i.e., the event space. So if we do not define the field, there is no way to define the measure P. • What if the event space is not a field? If the event space is not a field, then we can easilyconstructpathologicalcaseswherewecannotassignaprobability.Forexample, if the event space is not a field, then it would be possible that the complement of E ={3,4,5,6} (which is Ec ={1,2}) is not an event. This just does not make sense. The concept of a field is sufficient for finite sample spaces. However, there are two other types of sample spaces where the concept of a field is inadequate. The first type of 64
2.2. PROBABILITY SPACE sets consists of the countably infinite sets, and the second type consists of the sets defined on the real line. There are other types of sets, but these two have important practical applications. Therefore, we need to have a basic understanding of these two types. Sigma-field The difficultyof a countablyinfinite set isthat there areinfinitely many subsetsin the field of a countably infinite set. Having a finite union and a finite intersection is insufficient to ensure the closedness of all intersections and unions. In particular, having F ∪F ∈F does 1 2 not automatically give us (cid:83)∞ F ∈ F because the latter is an infinite union. Therefore, n=1 n for countably infinite sets, their requirements to be a field are more restrictive as we need to ensure infinite intersection and union. The resulting field is called the σ-field. Definition 2.16. A sigma-field (σ-field) F is a field such that • F is a field, and • if F ,F ,... ∈ F, then the union (cid:83)∞ F and the intersection (cid:84)∞ F are both 1 2 i=1 i i=1 i in F. When do we need a σ-field? When the sample space is countable and has infinitely many elements. For example, if the sample space contains all integers, then the collection of all possible subsets is a σ-field. For another, if E ={2}, E ={4}, E ={6}, ..., then 1 2 3 (cid:83)∞ E ={2,4,6,8, ...}={positive even numbers}. Clearly, we want (cid:83)∞ E to live in n=1 n n=1 n the sample space. Borel sigma-field While a sigma-field allows us to consider countable sets of events, it is still insufficient for considering events defined on the real line, e.g., time, as these events are not countable. So how do we define an event on the real line? It turns out that we need a different way to define the smallest unit. For finite sets and countable sets, the smallest units are the elements themselves because we can count them. For the real line, we cannot count the elements because any non-empty interval is uncountably infinite. The smallest unit we use to construct a field for the real line is a semi-closed interval def (−∞,b] = {x| −∞<x≤b}. The Borel σ-field is defined as the sigma-field generated by the semi-closed inter- vals. Definition2.17. The Borelσ-fieldBisaσ-fieldgeneratedfromsemi-closedintervals: def (−∞,b] = {x| −∞<x≤b}. The difference between the Borel σ-field B and a regular σ-field is how we measure the subsets. In a σ-field, we count the elements in the subsets, whereas, in a Borel σ-field, we use the semi-closed intervals to measure the subsets. 65
CHAPTER 2. PROBABILITY Being a field, the Borel σ-field is closed under complement, union, and intersection. In particular, subsets of the following forms are also in the Borel σ-field B: (a,b), [a,b], (a,b], [a,b), [a,∞), (a,∞), (−∞,b], {b}. For example, (a,∞) can be constructed from (−∞,a]c, and (a,b] can be constructed by taking the intersection of (−∞,b] and (a,∞). Example 2.27: Waiting for a bus. Let Ω= {0≤t≤30}. The Borel σ-field contains all semi-closed intervals (a,b], where 0≤a≤b≤30. Here are two possible events: • F ={less than 10 minutes}={0≤t<10}={0}∪({0<t≤10}∩{10}c). 1 • F ={more than 20 minutes}={20<t≤30}. 2 Further discussion of the Borel σ-field can be found in Leon-Garcia (3rd Edition,) Chapter 2.9. This is the end of the discussion. Please join us again. 2.2.3 Probability law P The third component of a probability space is the probability law P. Its job is to assign a number to an event. Definition 2.18. A probability law is a function P : F → [0,1] of an event E to a real number in [0, 1]. The probability law is thus a function, and therefore we must specify the input and the output. The input to P is an event E, which is a subset in Ω and an element in F. The output of P is a number between 0 and 1, which we call the probability. The definition above does not specify how an event is being mapped to a number. However, since probability is a measure of the size of a set, a meaningful P should be consistentforalleventsinF.Thisrequiressomerules,knownastheaxioms of probability, when we define the P. Any probability law P must satisfy these axioms; otherwise, we will see contradictions. We will discuss the axioms in the next section. For now, let us look at two examples to make sure we understand the functional nature of P. Example 2.28. Consider flipping a coin. The event space is F = {∅,{H},{T},Ω}. We can define the probability law as 1 1 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1, 2 2 as shown in Figure 2.16. This P is clearly consistent for all the events in F. Is it possible to construct an invalid P? Certainly. Consider the following proba- 66
2.2. PROBABILITY SPACE bility law: 1 1 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1. 3 3 This law is invalid because the individual events are P[{H}] = 1 and P[{T}] = 1 3 3 but the union is P[Ω] = 1. To fix this problem, one possible solution is to define the probability law as 1 2 P[∅]=0, P[{H}]= , P[{T}]= , P[Ω]=1. 3 3 Then, the probabilities for all the events are well defined and consistent. Figure 2.16: A probability law is a mapping from an event to a number. A probability law cannot be arbitrarily assigned; it must satisfy the axioms of probability. Example 2.29. Consider a sample space containing three elements Ω = {♣,♥,(cid:122)}. (cid:26) (cid:27) The event space is then F = ∅,{♣},{♥},{(cid:122)},{♣,♥},{♥,(cid:122)},{♣,(cid:122)},{♣,♥,(cid:122)} . One possible P we could define would be 1 P[∅]=0, P[{♣}]=P[{♥}]=P[{(cid:122)}]= , 3 2 P[{♣,♥}]=P[{♣,(cid:122)}]=P[{♥,(cid:122)}]= , P[{♣,♥,(cid:122)}]=1. 3 What is a probability law P? • A probability law P is a function. • It takes a subset (an element in F) and maps it to a number between 0 and 1. • P is a measure of the size of a set. • For P to be valid, it must satisfy the axioms of probability. 67
CHAPTER 2. PROBABILITY Figure 2.17: Probabilityisameasureofthesizeofaset.Theprobabilitycanbeacounterthatcounts thenumberofelements,arulerthatmeasuresthelengthofaninterval,oranintegrationthatmeasures the area of a region. A probability law P is a measure Consider the word “measure” in our slogan: probability is a measure of the size of a set. Depending on the nature of the set, the measure can be a counter, ruler, scale, or even a stopwatch. So far, all the examples we have seen are based on sets with a finite number of elements.Forthesesets,thenaturalchoiceoftheprobabilitymeasureisacounter.However, if the sets are intervals on the real line or regions in a plane, we need a different probability law to measure their size. Let’s look at the examples shown in Figure 2.17. Example 2.30 (Finite Set). Consider throwing a die, so that Ω={1,2,3,4,5,6}. Then the probability measure is a counter that reports the number of elements. If the die is fair, i.e., all the 6 faces have equal probability of happening, then an event E ={1,3} will have a probability P[E]= 2. 6 Example2.31(Intervals).SupposethatthesamplespaceisaunitintervalΩ=[0,1]. Let E be an event such that E = [a,b] where a,b are numbers in [0,1]. Then the probability measure is a ruler that measures the length of the intervals. If all the numbers on the real line have equal probability of appearing, then P[E]=b−a. Example 2.32 (Regions). Suppose that the sample space is the square Ω=[−1,1]× [−1,1]. Let E be a circle such that E ={(x,y)|x2+y2 <r2}, where r <1. Then the probability measure is an area measure that returns us the area of E. If we assume that all coordinates in Ω are equally probable, then P[E]=πr2, for r <1. Becauseprobabilityisameasureofthesizeofaset,twosetscanbecomparedaccording to their probability measures. For example, if Ω = {♣,♥,(cid:122)}, and if E = {♣} and E = 1 2 {♣,♥}, then one possible P is to assign P[E ]=P[{♣}]= 1 and P[E ]=P[{♣,♥}]=2/3. 1 3 2 68
2.2. PROBABILITY SPACE In this particular case, we see that E ⊆E and thus 1 2 P[E ]≤P[E ]. 1 2 Let’s now consider the term “size.” Notice that the concept of the size of a set is not limited to the number of elements. A better way to think about size is to imagine that it is theweightoftheset.Thismightmayseemfancifulatfirst,butitisquitenatural.Consider the following example. Example 2.33. (Discrete events with different weights) Suppose we have a sample space Ω={♣,♥,(cid:122)}. Let us assign a different probability to each outcome: 2 1 3 P[{♣}]= , P[{♥}]= , P[{(cid:122)}]= . 6 6 6 As illustrated in Figure 2.18, since each outcome has a different weight, when de- termining the probability of a set of outcomes we can add these weights (instead of counting the number of outcomes). For example, when reporting P[{♣}] we find its weightP[{♣}]= 2,whereaswhenreportingP[{♥,(cid:122)}]wefindthesumoftheirweights 6 P[{♥,(cid:122)}] = 1 + 3 = 4. Therefore, the notion of size does not refer to the number of 6 6 6 elements but to the total weight of these elements. Figure 2.18: This example shows the “weights” of three elements in a set. The weights are numbers between 0 and 1 such that the sum is 1. When applying a probability measure to this set, we sum the weights for the elements in the events being considered. For example, P[♥,(cid:122)] = yellow + green, and P[♣]= purple. Example 2.34.(Continuouseventswithdifferentweights)Supposethatthesample space is an interval, say Ω = [−1,1]. On this interval we define a weighting function f(x) where f(x ) specifies the weight for x . Because Ω is an interval, events defined 0 0 on this Ω must also be intervals. For example, we can consider two events E =[a,b] 1 and E =[c,d]. The probabilities of these events are P[E ]=(cid:82)b f(x)dx and P[E ]= 2 1 a 2 (cid:82)d f(x)dx, as shown in Figure 2.19. c Viewing probability as a measure is not just a game for mathematicians; rather, it has fundamental significance for several reasons. First, it eliminates any dependency on probability as relative frequency from the frequentist point of view. Relative frequency is a 69
CHAPTER 2. PROBABILITY Figure 2.19: If the sample space is an interval on the real line, then the probability of an event is the area under the curve of the weighting function. narrowlydefinedconceptthatislargelylimitedtodiscreteevents,e.g.,flippingacoin.While we can assign weights to coin-toss events to deal with those biased coins, the extension to continuous events becomes problematic. By thinking of probability as a measure, we can generalize the notion to apply to intervals, areas, volumes, and so on. Second, viewing probability as a measure forces us to disentangle an event from mea- sures. An event is a subset in the sample space. It has nothing to do with the measure (e.g., a ruler) you use to measure the event. The measure, on the other hand, specifies the weighting function you apply to measure the event when computing the probability. For example, let Ω = [−1,1] be an interval, and let E = [a,b] be an event. We can define two weighting functions f(x) and g(x). Correspondingly, we will have two different probability measures F and G such that (cid:90) (cid:90) b F([a,b])= dF= f(x)dx, E a (cid:90) (cid:90) b G([a,b])= dG= g(x)dx. (2.20) E a Tomakesenseofthesenotations,consideronlyP[[a,b]]andnotF([a,b])andG([a,b]).Asyou can see, the event for both measures is E =[a,b] but the measures are different. Therefore, the values of the probability are different. Example 2.35. (Two probability laws are different if their weighting functions are different.) Consider two different weighting functions for throwing a die. The first one assigns probability as the following: 1 2 3 P[{1}]= , P[{2}]= , P[{3}]= , 12 12 12 4 1 1 P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 whereas the second function assigns the probability like this: 2 2 2 P[{1}]= , P[{2}]= , P[{3}]= , 12 12 12 2 2 2 P[{4}]= , P[{5}]= , P[{6}]= . 12 12 12 70
2.2. PROBABILITY SPACE Let an event E ={1,2}. Let F be the measure using the first set of probabilities, and let G be the measure of the second set of probabilities. Then, 1 2 3 F(E)=F({1,2})= + = , 12 12 12 2 2 4 G(E)=G({1,2})= + = . 12 12 12 Therefore, although the events are the same, the two different measures will give us two different probability values. Remark. The notation (cid:82) dF in Equation (2.20) is known as the Lebesgue integral. You E should be aware of this notation, but the theory of Lebesgue measure is beyond the scope of this book. 2.2.4 Measure zero sets Understanding the measure perspective on probability allows us to understand another important concept of probability, namely measure zero sets. To introduce this concept, we pose the question: What is the probability of obtaining a single point, say {0.5}, when the sample space is Ω=[0,1]? The answer to this question is rooted in the compatibility between the measure and the sample space. In other words, the measure has to be meaningful for the events in the sample space. Using Ω=[0,1], since Ω is an interval, an appropriate measure would be the length of this interval. You may add different weighting functions to define your measure, but ultimately, the measure must be an integral. If you use a “counter” as a measure, then the counter and the interval are not compatible because you cannot count on the real line. Now, suppose that we define a measure for Ω=[0,1] using a weighting function f(x). This measure is determined by an integration. Then, for E ={0.5}, the measure is (cid:90) 0.5 P[E]=P[{0.5}]= f(x)dx=0. 0.5 In fact, for any weighting function the integral will be zero because the length of the set E is zero.1 An event that gives us zero probability is known as an event with measure 0. Figure 2.20 shows an example. Figure 2.20: The probability of obtaining a single point in a continuous interval is zero. 1We assume that f is continuous throughout [0,1]. If f is discontinuous at x = 0.5, some additional considerationswillapply. 71
CHAPTER 2. PROBABILITY What are measure zero sets? • A set E (non-empty) is called a measure zero set when P[E]=0. • For example, {0} is a measure zero set when we use a continuous measure F. • But {0} can have a positive measure when we use a discrete measure G. Example 2.36(a). Consider a fair die with Ω={1,2,3,4,5,6}. Then the set {1} has a probability of 1. The sample space does not have a measure zero event because the 6 measure we use is a counter. Example 2.36(b).ConsideranintervalwithΩ=[1,6].Thentheset{1}hasmeasure 0 because it is an isolated point with respect to the sample space. Example 2.36(c). For any intervals, P[[a,b]] = P[(a,b)] because the two end points have measure zero: P[{a}]=P[{b}]=0. Formal definitions of measure zero sets The following discussion of the formal definitions of measure zero sets is optional for the first reading of this book. We can formally define measure zero sets as follows: Definition 2.19. Let Ω be the sample space. A set A ∈ Ω is said to have measure zero if for any given (cid:15)>0, • There exists a countable number of subsets A such that A⊆∪∞ A , and n n=1 n • (cid:80)∞ P[A ]<(cid:15). n=1 n You may need to read this definition carefully. Suppose we have an event A. We construct a set of neighbors A ,...,A such that A is included in the union ∪∞ A . If the sum of 1 ∞ n=1 n the all P[A ] is still less than (cid:15), then the set A will have a measure zero. n To understand the difference between a measure for a continuous set and a countable set,considerFigure2.21.Ontheleftsideof Figure2.21weshowanintervalΩinwhichthere isanisolatedpointx .ThemeasureforthisΩisthelengthoftheinterval(relativetowhat- 0 ever weighting function you use). We define a small neighborhood A =(x − (cid:15), x + (cid:15)) 0 0 2 0 2 surrounding x . The length of this interval is not more than (cid:15). We then shrink (cid:15). How- 0 ever, regardless of how small (cid:15) is, since x is an isolated point, it is always included in the 0 neighborhood. Therefore, the definition is satisfied, and so {x } has measure zero. 0 Example 2.37. Let Ω=[0,1]. The set {0.5}⊂Ω has measure zero, i.e., P[{0.5}]=0. To see this, we draw a small interval around 0.5, say [0.5−(cid:15)/3,0.5+(cid:15)/3]. Inside this interval, there is really nothing to measure besides the point 0.5. Thus we have found 72
2.2. PROBABILITY SPACE an interval such that it contains 0.5, and the probability is P[[0.5−(cid:15)/3,0.5+(cid:15)/3]] = 2(cid:15)/3<(cid:15). Therefore, by definition, the set {0.5} has measure 0. Thesituationisverydifferentfortheright-handsideof Figure2.21.Here,themeasure is not the length but a counter. So if we create a neighborhood surrounding the isolated point x , we can always make a count. As a result, if you shrink (cid:15) to become a very small 0 number (in this case less than 1), then P[{x }] < (cid:15) will no longer be true. Therefore, the 4 0 set {x } has a non-zero measure when we use the counter as the measure. 0 Figure 2.21: [Left]Foracontinuoussamplespace,asinglepointevent{x }canalwaysbesurrounded 0 by a neighborhood A whose size P[A ] < (cid:15). [Right] If you change the sample space to discrete 0 0 elements, then a single point event {x } can still be surrounded by a neighborhood A . However, the 0 0 size P[A ]=1/4 is a fixed number and will not work for any (cid:15). 0 When we make probabilistic claims without considering the measure zero sets, we say that an event happens almost surely. Definition 2.20. An event A∈R is said to hold almost surely (a.s.) if P[A]=1 (2.21) except for all measure zero sets in R. Therefore,ifasetAcontainsmeasurezerosubsets,wecansimplyignorethembecausethey do not affect the probability of events. In this book, we will omit “a.s.” if the context is clear. Example 2.38(a).LetΩ=[0,1].ThenP[(0,1)]=1almostsurelybecausethepoints 0 and 1 have measure zero in Ω. Example 2.38(b). Let Ω = {x | x2 ≤ 1} and let A = {x | x2 < 1}. Then P[A] = 1 almost surely because the circumference has measure zero in Ω. Practice Exercise 2.12. Let Ω={f :R→[−1,1]|f(t)=cos(ω t+θ)}, where ω is 0 0 a fixed constant and θ is random. Construct a measure zero event and an almost sure event. Solution. Let E ={f :R→[−1,1]|f(t)=cos(ω t+kπ/2)} 0 foranyintegerk.Thatis,E containsallthefunctionswithaphaseofπ/2,2π/2,3π/2, etc. Then E will have measure zero because it is a countable set of isolated functions. 73
CHAPTER 2. PROBABILITY The event Ec will have probability P[Ec] = 1 almost surely because E has measure zero. This is the end of the discussion. Please join us again. 2.2.5 Summary of the probability space After the preceding long journey through theory, let us summarize. First, it is extremely important to understand our slogan: probability is a measure of the size of a set. This slogan is precise, but it needs clarification. When we say probability isameasure,wearethinkingofitasbeingtheprobabilitylawP.Ofcourse,inpractice,we alwaysthinkofprobabilityasthenumberreturnedbythemeasure.However,thedifference is not crucial. Also, “size” not only means the number of elements in the set, but it also means the relative weight of the set in the sample space. For example, if we use a weight function to weigh the set elements, then size would refer to the overall weight of the set. When we put all these pieces together, we can understand why a probability space must consist of the three components (Ω,F,P), (2.22) whereΩisthesamplespacethatdefinesallpossibleoutcomes,F istheeventspacegenerated from Ω, and P is the probability law that maps an event to a number in [0,1]. Can we drop one or more of the three components? We cannot! If we do not specify the sample space Ω, then there is no way to define the events. If we do not have a complete event space F, then some events will become undefined, and further, if the probability law is applied only to outcomes, we will not be able to define the probability for events. Finally, if we do not specify the probability law, then we do not have a way to assign probabilities. 2.3 Axioms of Probability We now turn to a deeper examination of the properties. Our motivation is simple. While the definition of probability law has achieved its goal of assigning a probability to an event, there must be restrictions on how the assignment can be made. For example, if we set P[{H}] = 1/3, then P[{T}] must be 2/3; otherwise, the sum of having a head and a tail will be greater than 1. The necessary restrictions on assigning a probability to an event are collectively known as the axioms of probability. Definition 2.21. A probability law is a function P : F → [0,1] that maps an event A to a real number in [0, 1]. The function must satisfy the axioms of probability: I. Non-negativity: P[A]≥0, for any A⊆Ω. II. Normalization: P[Ω]=1. 74
2.3. AXIOMS OF PROBABILITY III. Additivity: For any disjoint sets {A ,A ,...}, it must be true that 1 2 (cid:34)∞ (cid:35) ∞ (cid:91) (cid:88) P A = P[A ]. (2.23) i i i=1 i=1 An axiom is a proposition that serves as a premise or starting point in a logical system. Axioms are not definitions, nor are they theorems. They are believed to be true or true within a certain context. In our case, the axioms are true within the context of Bayesian probability. The Kolmogorov probability relies on another set of axioms. We will not dive into the details of these historical issues; in this book, we will confine our discussion to the three axioms given above. 2.3.1 Why these three probability axioms? Why do we need three axioms? Why not just two axioms? Why these three particular axioms? The reasons are summarized in the box below. Why these three axioms? • Axiom I (Non-negativity) ensures that probability is never negative. • Axiom II (Normalization) ensures that probability is never greater than 1. • Axiom III (Additivity) allows us to add probabilities when two events do not overlap. Axiom I is called the non-negativity axiom. It ensures that a probability value cannot be negative. Non-negativity is a must for probability. It is meaningless to say that the probability of getting an event is a negative number. AxiomIIiscalledthenormalizationaxiom.Itensuresthattheprobabilityofobserving all possible outcomes is 1. This gives the upper limit of the probability. The upper limit does not have to be 1. It could be 10 or 100. As long as we are consistent about this upper limit, we are good. However, for historical reasons and convenience, we choose 1 to be the upper limit. Axiom III is called the additivity axiom and is the most critical one among the three. The additivity axiom defines how set operations can be translated into probability oper- ations. In a nutshell, it says that if we have a set of disjoint events, the probabilities can be added. From the measure perspective, Axiom III makes sense because if P measures the size of an event, then two disjoint events should have their probabilities added. If two dis- joint events do not allow their probabilities to be added, then there is no way to measure a combined event. Similarly, if the probabilities can somehow be added even for overlap- ping events, there will be inconsistencies because there is no systematic way to handle the overlapping regions. The countable additivity stated in Axiom III can be applied to both a finite number or an infinite number of sets. The finite case states that for any two disjoint sets A and B, we have P[A∪B]=P[A]+P[B]. (2.24) 75
CHAPTER 2. PROBABILITY In other words, if A and B are disjoint, then the probability of observing either A or B is the sum of the two individual probabilities. Figure 2.22 illustrates this idea. Example 2.39. Let’s see why Axiom III is critical. Consider throwing a fair die with Ω={1,2,3,4,5,6}. The probability of getting {4,6} is 1 1 2 P[{4,6}]=P[{4}∪{6}]=P[{4}]+P[{6}]= + = . 6 6 6 Inthisequation,thesecondequalityholdsbecausetheevents{4}and{6}aredisjoint. If we do not have Axiom III, then we cannot add probabilities. Figure 2.22: Axiom III says P[A∪B]=P[A]+P[B] if A∩B =∅. 2.3.2 Axioms through the lens of measure Axioms are “rules” we must abide by when we construct a measure. Therefore, any valid measure must be compatible with the axioms, regardless of whether we have a weighting function or not. In the following two examples, we will see how the weighting functions are used in the axioms. Example 2.40. Consider a sample space with Ω = {♣,♥,(cid:122)}. The probability for each outcome is 2 1 3 P[{♣}]= , P[{♥}]= , P[{(cid:122)}]= . 6 6 6 Suppose we construct two disjoint events E = {♣,♥} and E = {(cid:122)}. Then Axiom 1 2 III says (cid:18) (cid:19) 2 1 3 P[E ∪E ]=P[E ]+P[E ]= + + =1. 1 2 1 2 6 6 6 Note that in this calculation, the measure P is still a measure P. If we endow it with a nonuniform weight function, then P applies the corresponding weights to the corresponding outcomes. This process is compatible with the axioms. See Figure 2.23 for a pictorial illustration. 76
2.3. AXIOMS OF PROBABILITY Example 2.41. Suppose the sample space is an interval Ω = [0,1]. The two events are E = [a,b] and E = [c,d]. Assume that the measure P uses a weighting function 1 2 f(x). Then, by Axiom III, we know that P[E ∪E ]=P[E ]+P[E ] 1 2 1 2 =P[[a,b]]+P[[c,d]] (by Axiom 3) (cid:90) b (cid:90) d = f(x)dx+ f(x)dx, (apply the measure). a c As you can see, there is no conflict between the axioms and the measure. Figure 2.24 illustrates this example. Figure2.23:Applyingweightingfunctionstothemeasures:Supposewehavethreeelementsintheset. To compute the probability P[{♥,(cid:122)}∪{♣}], we can write it as the sum of P[{♥,(cid:122)}] and P[{♣}]. Figure 2.24: The axioms are compatible with the measure, even if we use a weighting function. 2.3.3 Corollaries derived from the axioms TheunionofAandB isequivalenttothelogicaloperator“OR”.Oncethelogicaloperation “OR” is defined, all other logical operations can be defined. The following corollaries are examples. Corollary 2.1. Let A∈F be an event. Then, (a) P[Ac]=1−P[A]. (b) P[A]≤1. (c) P[∅]=0. 77
CHAPTER 2. PROBABILITY Proof. (a) Since Ω=A∪Ac, by finite additivity we have P[Ω]=P[A∪Ac]=P[A]+P[Ac]. By the normalization axiom, we have P[Ω]=1. Therefore, P[Ac]=1−P[A]. (b) We prove by contradiction. Assume P[A]>1. Consider the complement Ac where A∪Ac =Ω.SinceP[Ac]=1−P[A],wemusthaveP[Ac]<0becausebyhypothesisP[A]>1. But P[Ac]<0 violates the non-negativity axiom. So we must have P[A]≤1. (c) Since Ω=Ω∪∅, by the first corollary we have P[∅]=1−P[Ω]=0. (cid:3) Corollary 2.2 (Unions of Two Non-Disjoint Sets). For any A and B in F, P[A∪B]=P[A]+P[B]−P[A∩B]. (2.25) This statement is different from Axiom III because A and B are not necessarily disjoint. Figure 2.25: For any A and B, P[A∪B]=P[A]+P[B]−P[A∩B]. Proof. First, observe that A∪B can be partitioned into three disjoint subsets as A∪B = (A\B)∪(A∩B)∪(B\A). Since A\B =A∩Bc and B\A=B∩Ac, by finite additivity we have that P[A∪B]=P[A\B]+P[A∩B]+P[B\A]=P[A∩Bc]+P[A∩B]+P[B∩Ac] ( =a)P[A∩Bc]+P[A∩B]+P[B∩Ac]+P[A∩B]−P[A∩B] ( =b)P[A∩(Bc∪B)]+P[(Ac∪A)∩B]−P[A∩B] =P[A∩Ω]+P[Ω∩B]−P[A∩B]=P[A]+P[B]−P[A∩B], where in (a) we added and subtracted a term P[A∩B], and in (b) we used finite additivity so that P[A∩Bc]+P[A∩B]=P[(A∩Bc)∪(A∩B)]=P[A∩(Bc∪B)]. (cid:3) Example 2.42. The corollary is easy to understand if we consider the following ex- ample. Let Ω={1,2,3,4,5,6} be the sample space of a fair die. Let A={1,2,3} and B ={3,4,5}. Then 5 P[A∪B]=P[{1,2,3,4,5}]= . 6 78
2.3. AXIOMS OF PROBABILITY We can also use the corollary to obtain the same result: P[A∪B]=P[A]+P[B]−P[A∩B] =P[{1,2,3}]+P[{3,4,5}]−P[{3}] 3 3 1 5 = + − = . 6 6 6 6 Corollary 2.3 (Inequalities). Let A and B be two events in F. Then, (a) P[A∪B]≤P[A]+P[B]. (Union Bound) (b) If A⊆B, then P[A]≤P[B]. Proof.(a)SinceP[A∪B]=P[A]+P[B]−P[A∩B]andbynon-negativityaxiomP[A∩B]≥0, we must have P[A∪B]≤P[A]+P[B]. (b) If A⊆B, then there exists a set B\A such that B =A∪(B\A).Therefore,byfiniteadditivitywehaveP[B]=P[A]+P[B\A]≥P[A].Since P[B\A]≥0, it follows that P[A]+P[B\A]≥P[A]. Thus we have P[B]≥P[A]. (cid:3) Unionboundisafrequentlyusedtoolforanalyzingprobabilitieswhentheintersection A∩B is difficult to evaluate. Part (b) is useful when considering two events of different “sizes.” For example, in the bus-waiting example, if we let A={t≤5}, and B ={t≤10}, then P[A] ≤ P[B] because we have to wait for the first 5 minutes to go into the remaining 5 minutes. Practice Exercise 2.13. Let the events A and B have P[A] = x, P[B] = y and P[A∪B]=z. Find the following probabilities: P[A∩B], P[Ac∪Bc], and P[A∩Bc]. Solution. (a) Note that z =P[A∪B]=P[A]+P[B]−P[A∩B]. Thus, P[A∩B]=x+y−z. (b) We can take the complement to obtain the result: P[Ac∪Bc]=1−P[(Ac∪Bc)c]=1−P[A∩B]=1−x−y+z. (c) P[A∩Bc]=P[A]−P[A∩B]=x−(x+y−z)=z−y. Practice Exercise 2.14. Consider a sample space Ω={f :R→R|f(x)=ax,for all a∈R,x∈R}. There are two events: A = {f|f(x) = ax, a ≥ 0}, and B = {f|f(x) = ax, a ≤ 0}. So, basically, A is the set of all straight lines with positive slope, and B is the set of straight lines with negative slope. Show that the union bound is tight. 79
CHAPTER 2. PROBABILITY Solution. First of all, we note that P[A∪B]=P[A]+P[B]−P[A∩B]. The intersection is P[A∩B]=P[{f|f(x)=0}]. Since this is a point set in the real line, it has measure zero. Thus, P[A∩B]=0 and hence P[A∪B]=P[A]+P[B]. So the union bound is tight. Closing remark. The development of today’s probability theory is generally credited to Andrey Kolmogorov’s 1933 book Foundations of the Theory of Probability. We close this section by citing one of the tables of the book. The table summarizes the correspondence between set theory and random events. Theory of sets Random events A and B are disjoint, i.e., A∩B =∅ Events A and B are incompatible A ∩A ···∩A =∅ Events A ,...,A are incompatible 1 2 N 1 N A ∩A ···∩A =X EventX isdefinedasthesimultaneousoccur- 1 2 N rence of events A ,...,A 1 N A ∪A ···∪A =X EventX isdefinedastheoccurrenceofatleast 1 2 N one of the events A ,...,A 1 N Ac The opposite event Ac consisting of the non- occurrence of event A A=∅ Event A is impossible A=Ω Event A must occur A ,...,A form a partition of Ω Theexperimentconsistsofdeterminingwhich 1 N of the events A ,...,A occurs 1 N B ⊂A From the occurrence of event B follows the inevitable occurrence of A Table 2.2: Kolmogorov’s summary of set theory results and random events. 2.4 Conditional Probability In many practical data science problems, we are interested in the relationship between two or more events. For example, an event A may cause B to happen, and B may cause C to happen. A legitimate question in probability is then: If A has happened, what is the probability that B also happens? Of course, if A and B are correlated events, then knowing oneeventcantellussomethingabouttheotherevent.Ifthetwoeventshavenorelationship, knowing one event will not tell us anything about the other. In this section, we study the concept of conditional probability. There are three sub- topics in this section. We summarize the key points below. 80
2.4. CONDITIONAL PROBABILITY The three main messages of this section are: • Section 2.4.1: Conditional probability. Conditional probability of A given B is P[A|B]= P[A∩B]. P[B] • Section 2.4.2: Independence. Two events are independent if the occurrence of one does not influence the occurrence of the other: P[A|B]=P[A]. • Section 2.4.3: Bayes’ theorem and the law of total probability. Bayes’ theorem allowsustoswitchtheorderoftheconditioning:P[A|B]vs.P[B|A],whereasthe law of total probability allows us to decompose an event into smaller events. 2.4.1 Definition of conditional probability We start by defining conditional probability. Definition 2.22. Consider two events A and B. Assume P[B](cid:54)=0. The conditional probability of A given B is P[A∩B] P[A|B]d =ef . (2.26) P[B] According to this definition, the conditional probability of A given B is the ratio of P[A∩B] to P[B]. It is the probability that A happens when we know that B has already happened.SinceB hasalreadyhappened,theeventthatAhasalsohappenedisrepresented byA∩B.However,sinceweareonlyinterestedintherelativeprobabilityofAwithrespect to B, we need to normalize using B. This can be seen by comparing P[A|B] and P[A∩B]: P[A∩B] P[A∩B] P[A|B]= and P[A∩B]= . (2.27) P[B] P[Ω] ThedifferenceisillustratedinFigure 2.26:TheintersectionP[A∩B]calculatestheoverlap- ping area of the two events. We make no assumptions about the cause-effect relationship. Figure 2.26: Illustration of conditional probability and its comparison with P[A∩B]. What justifies this ratio? Suppose that B has already happened. Then, anything out- side B will immediately become irrelevant as far as the relationship between A and B is concerned. So when we ask: “What is the probability that A happens given that B has happened?”, we are effectively asking for the probability that A∩B happens under the 81
CHAPTER 2. PROBABILITY condition that B has happened. Note that we need to consider A∩B because we know that B has already happened. If we take A only, then there exists a region A\B which does not contain anything about B. However, since we know that B has happened, A\B is impossible. In other words, among the elements of A, only those that appear in A∩B are meaningful. Example 2.43. Let A={Purdue gets Big Ten championship}, B ={Purdue wins 15 games consecutively}. In this example, P[A]=Prob. that Purdue gets the championship, P[B]=Prob. that Purdue wins 15 games consecutively, P[A∩B]=Prob. that Purdue gets the championship and wins 15 games, P[A|B]=Prob. that Purdue gets the championship given that Purdue won 15 games. If Purdue has won 15 games consecutively, then it is unlikely that Purdue will get thechampionshipbecausethesamplespaceofallpossiblecompetitionresultsislarge. However, if we have already won 15 games consecutively, then the denominator of the probability becomes much smaller. In this case, the conditional probability is high. Example 2.44. Consider throwing a die. Let A={getting a 3} and B ={getting an odd number}. Find P[A|B] and P[B|A]. Solution. The following probabilities are easy to calculate: 1 3 P[A]=P[{3}]= , and P[B]=P[{1,3,5}]= . 6 6 Also, the intersection is 1 P[A∩B]=P[{3}]= . 6 Given these values, the conditional probability of A given B can be calculated as P[A∩B] 1 1 P[A|B]= = 6 = . P[B] 3 3 6 In other words, if we know that we have an odd number, then the probability of obtaining a 3 has to be computed over {1,3,5}, which give us a probability 1. If we 3 82
2.4. CONDITIONAL PROBABILITY do not know that we have an odd number, then the probability of obtaining a 3 has to be computed from the sample space {1,2,3,4,5,6}, which will give us 1. 6 The other conditional probability is P[A∩B] P[B|A]= =1. P[A] Therefore, if we know that we have rolled a 3, then the probability for this number being an odd number is 1. Example 2.45. Consider the situation shown in Figure 2.27. There are 12 points with equal probabilities of happening. Find the probabilities P[A|B] and P[B|A]. Solution. In this example, we can first calculate the individual probabilities: 5 6 2 P[A]= , and P[B]= , and P[A∩B]= . 12 12 12 Then the conditional probabilities are P[A∩B] 2 1 P[A|B]= = 12 = , P[B] 6 3 12 P[A∩B] 2 2 P[B|A]= = 12 = . P[A] 5 5 12 Figure 2.27: Visualization of Example 2.45: [Left] All the sets. [Middle] P(A|B) is the ratio between dots inside the light yellow region over those in yellow, which is 2. [Right] P[A|B] is the ratio between 6 dots inside the light pink region over those in pink, which is 2. 5 Example 2.46. Consider a tetrahedral (4-sided) die. Let X be the first roll and Y be the second roll. Let B be the event that min(X,Y) = 2 and M be the event that max(X,Y)=3. Find P[M|B]. Solution. As shown in Figure 2.28, the event B is highlighted in green. (Why?) Similarly,theeventM ishighlightedinblue.(Again,why?)Therefore,theprobability 83
CHAPTER 2. PROBABILITY is P[M ∩B] 2 2 P[M|B]= = 16 = . P[B] 5 5 16 Figure 2.28: Visualization of Example 2.46. [Left] Event B. [Middle] Event M. [Right] P(M|B) is the ratioofthenumberofbluesquaresinsidethegreenregiontothetotalnumberofgreensquares,which is 2. 5 Remark.NoticethatifP[B]≤P[Ω],thenP[A|B]isalwayslargerthanorequaltoP[A∩B], i.e., P[A|B]≥P[A∩B]. Conditional probabilities are legitimate probabilities Conditional probabilities are legitimate probabilities. That is, given B, the probability P[A|B] satisfies Axioms I, II, III. Theorem 2.6. Let P[B] > 0. The conditional probability P[A|B] satisfies Axioms I, II, and III. Proof. Let’s check the axioms: • Axiom I: We want to show P[A∩B] P[A|B]= ≥0. P[B] Since P[B]>0 and Axiom I requires P[A∩B]≥0, we therefore have P[A|B]≥0. • Axiom II: P[Ω∩B] P[Ω|B]= P[B] P[B] = =1. P[B] 84
2.4. CONDITIONAL PROBABILITY • Axiom III: Consider two disjoint sets A and C. Then, P[(A∪C)∩B] P[A∪C|B]= P[B] P[(A∩B)∪(C∩B)] = P[B] (a) P[A∩B] P[C∩B] = + P[B] P[B] =P[A|B]+P[C|B], where (a) holds because if A and C are disjoint then (A∩B)∩(C∩B)=∅. (cid:3) To summarize this subsection, we highlight the essence of conditional probability. What are conditional probabilities? • Conditional probability of A given B is the ratio P[A∩B]. P[B] • It is again a measure. It measures the relative size of A inside B. • Because it is a measure, it must satisfy the three axioms. 2.4.2 Independence Conditional probability deals with situations where two events A and B are related. What if the two events are unrelated? In probability, we have a technical term for this situation: statistical independence. Definition 2.23. Two events A and B are statistically independent if P[A∩B]=P[A]P[B]. (2.28) Why define independence in this way? Recall that P[A|B]= P[A∩B]. If A and B are P[B] independent, then P[A∩B]=P[A]P[B] and so P[A∩B] P[A]P[B] P[A|B]= = =P[A]. (2.29) P[B] P[B] This suggests an interpretation of independence: If the occurrence of B provides no addi- tional information about the occurrence of A, then A and B are independent. Therefore, we can define independence via conditional probability: Definition 2.24. Let A and B be two events such that P[A]>0 and P[B]>0. Then 85
CHAPTER 2. PROBABILITY A and B are independent if P[A|B]=P[A] or P[B|A]=P[B]. (2.30) The two statements are equivalent as long as P[A] > 0 and P[B] > 0. This is because P[A|B] = P[A∩B]/P[B]. If P[A|B] = P[A] then P[A∩B] = P[A]P[B], which implies that P[B|A]=P[A∩B]/P[A]=P[B]. ApictorialillustrationofindependenceisgiveninFigure2.29.Thekeymessageisthat if two events A and B are independent, then P[A|B] = P[A]. The conditional probability P[A|B] is the ratio of P[A∩B] over P[B], which is the intersection over B (the blue set). The probability P[A] is the yellow set over the sample space Ω. Figure 2.29: Independence means that the conditional probability P[A|B] is the same as P[A]. This implies that the ratio of P[A∩B] over P[B], and the ratio of P[A∩Ω] over P[Ω] are the same. Disjoint versus independent Disjoint(cid:60)Independent. (2.31) The statement says that disjoint and independent are two completely different concepts. If A and B are disjoint, then A ∩ B = ∅. This only implies that P[A ∩ B] = 0. However, it says nothing about whether P[A∩B] can be factorized into P[A]P[B]. If A and B are independent, then we have P[A∩B] = P[A]P[B]. But this does not imply that P[A∩B]=0.TheonlyconditionunderwhichDisjoint⇔IndependenceiswhenP[A]=0or P[B]=0.Figure 2.30depictsthesituation.Whentwosetsareindependent,theconditional probability (which is a ratio) remains unchanged compared to unconditioned probability. When two sets are disjoint, they simply do not overlap. Practice Exercise 2.15. Throw a die twice. Are A and B independent, where A={1st die is 3} and B ={2nd die is 4}. Solution. We can show that P[A∩B]=P[(3,4)]= 1 , P[A]= 1, and P[B]= 1. 36 6 6 So P[A∩B]=P[A]P[B]. Thus, A and B are independent. 86
2.4. CONDITIONAL PROBABILITY Figure 2.30: Independent means that the conditional probability, which is a ratio, is the same as the unconditioned probability. Disjoint means that the two sets do not overlap. Figure 2.31: The two events A and B are independent because P[A]= 1 and P[A|B]= 1. 6 6 A pictorial illustration of this example is shown in Figure 2.31. The two events are independent because A is one row in the 2D space, which yields a probability of 1. The 6 conditional probability P[A|B] is the coordinate (3,4) over the event B, which is a column. It happens that P[A|B]= 1. Thus, the two events are independent. 6 Practice Exercise 2.16. Throw a die twice. Are A and B independent? A={1st die is 3} and B ={sum is 7}. Solution. Note that P[A∩B]=P[(3,4)]= 1 , P[A]= 1, 36 6 P[B]=P[(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)]= 1. 6 So P[A∩B]=P[A]P[B]. Thus, A and B are independent. ApictorialillustrationofthisexampleisshowninFigure2.32.Noticethatwhetherthe twoeventsintersectisnothowwedetermineindependence(thatonlydeterminesdisjointor 87
CHAPTER 2. PROBABILITY not).Thekeyiswhethertheconditionalprobability(whichistheratio)remainsunchanged compared to the unconditioned probability. Figure 2.32: The two events A and B are independent because P[A]= 1 and P[A∩B]= 1. 6 6 If we let B ={sum is 8}, then the situation is different. The intersection A∩B has a probability 1 relative to B, and therefore P[A|B] = 1. Hence, the two events A and B are 5 5 dependent. If you like a more intuitive argument, you can imagine that B has happened, i.e., the sum is 8. Then the probability for the first die to be 1 is 0 because there is no way toconstruct8whenthefirstdieis1.Asaresult,wehaveeliminatedonechoiceforthefirst die, leaving only five options. Therefore, since B has influenced the probability of A, they are dependent. Practice Exercise 2.17. Throw a die twice. Let A={max is 2} and B ={min is 2}. Are A and B independent? Solution. Let us first list out A and B: A={(1,2),(2,1),(2,2)}, B ={(2,2),(2,3),(2,4),(2,5),(2,6),(3,2),(4,2),(5,2),(6,2)}. Therefore, the probabilities are 3 9 1 P[A]= , P[B]= , and P[A∩B]=P[(2,2)]= . 36 36 36 Clearly, P[A∩B](cid:54)=P[A]P[B] and so A and B are dependent. What is independence? • Two events are independent when the ratio P[A∩B]/P[B] remains unchanged compared to P[A]. • Independence (cid:54)= disjoint. 88
2.4. CONDITIONAL PROBABILITY 2.4.3 Bayes’ theorem and the law of total probability Theorem 2.7 (Bayes’ theorem). For any two events A and B such that P[A] > 0 and P[B]>0, P[B|A]P[A] P[A|B]= . P[B] Proof. By the definition of conditional probabilities, we have P[A∩B] P[B∩A] P[A|B]= and P[B|A]= . P[B] P[A] Rearranging the terms yields P[A|B]P[B]=P[B|A]P[A], which gives the desired result by dividing both sides by P[B]. (cid:3) Bayes’theoremprovidestwoviewsoftheintersectionP[A∩B]usingtwodifferentcon- ditionalprobabilities.WecallP[B|A]theconditional probabilityandP[A|B]theposterior probability. The order of A and B is arbitrary. We can also call P[A|B] the conditional probabilityandP[B|A]theposteriorprobability.Thecontextoftheproblemwillmakethis clear. Bayes’ theorem provides a way to switch P[A|B] and P[B|A]. The next theorem helps us decompose an event into smaller events. Theorem 2.8 (Law of Total Probability). Let {A ,...,A } be a partition of Ω, i.e., 1 n A ,...,A are disjoint and Ω=A ∪···∪A . Then, for any B ⊆Ω, 1 n 1 n n (cid:88) P[B]= P[B|A ]P[A ]. (2.32) i i i=1 Proof. We start from the right-hand side. n n (cid:34) n (cid:35) (cid:88) P[B|A ]P[A ]( =a)(cid:88) P[B∩A ]( =b)P (cid:91) (B∩A ) i i i i i=1 i=1 i=1 (cid:34) (cid:32) n (cid:33)(cid:35) ( =c)P B∩ (cid:91) A ( =d)P[B∩Ω]=P[B], i i=1 where (a) follows from the definition of conditional probability, (b) is due to Axiom III, (c) holdsbecauseofthedistributivepropertyofsets,and(d)resultsfromthepartitionproperty of {A ,A ,...,A }. 1 2 n (cid:3) Interpretation.Thelawoftotalprobabilitycanbeunderstoodasfollows.Ifthesample space Ω consists of disjoint subsets A ,...,A , we can compute the probability P[B] by 1 n 89
CHAPTER 2. PROBABILITY summingoveritsportionP[B∩A ],...,P[B∩A ].However,eachintersectioncanbewritten 1 n as P[B∩A ]=P[B|A ]P[A ]. (2.33) i i i In other words, we write P[B∩A ] as the conditional probability P[B|A ] times the prior i i probability P[A ]. When we sum all these intersections, we obtain the overall probability. i See Figure 2.33 for a graphical portrayal. Figure 2.33: The law of total probability decomposes the probability P[B] into multiple conditional probabilities P[B|A ]. The probability of obtaining each P[B|A ] is P[A ]. i i i Corollary 2.4. Let {A ,A ,...,A } be a partition of Ω, i.e., A ,...,A are disjoint 1 2 n 1 n and Ω=A ∪A ∪···∪A . Then, for any B ⊆Ω, 1 2 n P[B|A ]P[A ] P[A |B]= j j . (2.34) j (cid:80)n P[B|A ]P[A ] i=1 i i Proof. The result follows directly from Bayes’ theorem: P[B|A ]P[A ] P[B|A ]P[A ] P[A |B]= j j = j j . j P[B] (cid:80)n P[B|A ]P[A ] i=1 i i (cid:3) Example 2.47. Suppose there are three types of players in a tennis tournament: A, B, and C. Fifty percent of the contestants in the tournament are A players, 25% are B players, and 25% are C players. Your chance of beating the contestants depends on the class of the player, as follows: 0.3 against an A player 0.4 against a B player 0.5 against a C player If you play a match in this tournament, what is the probability of your winning the match?Supposingthatyouhavewonamatch,whatistheprobabilitythatyouplayed against an A player? Solution.Wefirstlistalltheknownprobabilities.Weknowfromthepercentage 90
2.4. CONDITIONAL PROBABILITY of players that P[A]=0.5, P[B]=0.25, P[C]=0.25. Now, let W be the event that you win the match. Then the conditional probabilities are defined as follows: P[W|A]=0.3, P[W|B]=0.4, P[W|C]=0.5. Therefore, by the law of total probability, we can show that the probability of winning the match is P[W]=P[W |A]P[A]+P[W |B]P[B]+P[W |C]P[C] =(0.3)(0.5)+(0.4)(0.25)+(0.5)(0.25)=0.375. Given that you have won the match, the probability of A given W is P[W|A]P[A] (0.3)(0.5) P[A|W]= = =0.4. P[W] 0.375 Example 2.48. Consider the communication channel shown below. The probability of sending a 1 is p and the probability of sending a 0 is 1−p. Given that 1 is sent, the probability of receiving 1 is 1−η. Given that 0 is sent, the probability of receiving 0 is 1−ε. Find the probability that a 1 has been correctly received. Solution. Define the events S =“0 is sent”, and R =“0 is received”. 0 0 S =“1 is sent”, and R =“1 is received”. 1 1 Then,theprobabilitythat1isreceivedisP[R ].However,P[R ](cid:54)=1−η because1−η 1 1 91
CHAPTER 2. PROBABILITY is the conditional probability that 1 is received given that 1 is sent. It is possible that we receive 1 as a result of an error when 0 is sent. Therefore, we need to consider the probability that both S and S occur. Using the law of total probability we have 0 1 P[R ]=P[R |S ]P[S ]+P[R |S ]P[S ] 1 1 1 1 1 0 0 =(1−η)p+ε(1−p). Now, suppose that we have received 1. What is the probability that 1 was origi- nally sent? This is asking for the posterior probability P[S |R ], which can be found 1 1 using Bayes’ theorem P[R |S ]P[S ] (1−η)p P[S |R ]= 1 1 1 = . 1 1 P[R ] (1−η)p+ε(1−p) 1 When do we need to use Bayes’ theorem and the law of total probability? • Bayes’ theorem switches the role of the conditioning, from P[A|B] to P[B|A]. Example: P[win the game|play with A] and P[play with A|win the game]. • The law of total probability decomposes an event into smaller events. Example: P[win]=P[win|A]P[A]+P[win|B]P[B]. 2.4.4 The Three Prisoners problem Now that you are familiar with the concepts of conditional probabilities, we would like to challenge you with the following problem, known as the Three Prisoners problem. If you understand how this problem can be resolved, you have mastered conditional probability. Once upon a time, there were three prisoners A, B, and C. One day, the king decided to pardon two of them and sentence the last one, as in this figure: Figure2.34:TheThreePrisonersproblem:Thekingsaysthathewillpardontwoprisonersandsentence one. One of the prisoners, prisoner A, heard the news and wanted to ask a friendly guard abouthissituation.Theguardwashonest.HewasallowedtotellprisonerAthatprisonerB would be pardoned or that prisoner C would be pardoned, but he could not tell A whether he would be pardoned. Prisoner A thought about the problem, and he began to hesitate to ask the guard. Based on his present state of knowledge, his probability of being pardoned 92
2.4. CONDITIONAL PROBABILITY is 2. However, if he asks the guard, this probability will be reduced to 1 because the guard 3 2 would tell him that one of the two other prisoners would be pardoned, and would tell him which one it would be. Prisoner A reasons that his chance of being pardoned would then drop because there are now only two prisoners left who may be pardoned, as illustrated in Figure 2.35: Figure 2.35: The Three Prisoners problem: If you do not ask the guard, your chance of being released is 2/3. If you ask the guard, the guard will tell you which one of the other prisoners will be released. Your chance of being released apparently drops to 1/2. Should prisoner A ask the guard? What has gone wrong with his reasoning? This problem is tricky in the sense that the verbal argument of prisoner A seems flawless. If he asked the guard, indeed, the game would be reduced to two people. However, this does not seem correct, because regardless of what the guard says, the probability for A to be pardoned should remain unchanged. Let’s see how we can solve this puzzle. Let X , X , X be the events of sentencing prisoners A, B, C, respectively. Let G A B C B be the event that the guard says that the prisoner B is released. Without doing anything, we know that 1 1 1 P[X ]= , P[X ]= , P[X ]= . A 3 B 3 C 3 Conditioned on these events, we can compute the following conditional probabilities that the guard says B is pardoned: 1 P[G |X ]= , P[G |X ]=0, P[G |X ]=1. B A 2 B B B C Why are these conditional probabilities? P[G |X ]=0 quite straightforward. If the king B B decides to sentence B, the guard has no way of saying that B will be pardoned. Therefore, P[G | X ] must be zero. P[G | X ] = 1 is also not difficult. If the king decides to B B B C sentence C, then the guard has no way to tell you that B will be pardoned because the guard cannot say anything about prisoner A. Finally, P[G | X ] = 1 can be understood B A 2 as follows: If the king decides to sentence A, the guard can either tell you B or C. In other words, the guard flips a coin. Withtheseconditionalprobabilitiesready,wecandeterminetheprobability.Thisisthe conditional probability P[X |G ]. That is, supposing that the guard says B is pardoned, A B whatistheprobabilitythatAwillbesentenced?ThisistheactualscenariothatAisfacing. Solving for this conditional probability is not difficult. By Bayes’ theorem we know that P[G |X ]P[X ] P[X |G ]= B A A , A B P[G ] B 93
CHAPTER 2. PROBABILITY and P[G ]=P[G |X ]P[X ]+P[G |X ]P[X ]+P[G |X ]P[X ] according to the law of B B A A B B B B C C total probability. Substituting the numbers into these equations, we have that P[G ]=P[G |X ]P[X ]+P[G |X ]P[X ]+P[G |X ]P[X ] B B A A B B B B C C 1 1 1 1 1 = × +0× +1× = , 2 3 3 3 2 P[G |X ]P[X ] 1 × 1 1 P[X |G ]= B A A = 2 3 = . A B P[G ] 1 3 B 2 Therefore,giventhattheguardsaysB ispardoned,theprobabilitythatAwillbesentenced remains 1. In fact, what you can show in this example is that P[X | G ] = 1 = P[X ]. 3 A B 3 A Therefore,thepresenceorabsenceoftheguarddoesnotaltertheprobability.Thisisbecause what the guard says is independent of whether the prisoners will be pardoned. The lesson we learn from this problem is not to rely on verbal arguments. We need to write down the conditional probabilities and spell out the steps. Figure 2.36: The Three Prisoners problem is resolved by noting that P[X |G ] = P[X ]. Therefore, A B A the events X and G are independent. A B How to resolve the Three Prisoners problem? • The key is that G , G , G do not form a partition. See Figure 2.36. A B C • G (cid:54)=X . When G happens, the remaining set is not X ∪X . B B B A C • The ratio P[X ∩G ]/P[G ] equals P[X ]. This is independence. A B B A 94
2.5. SUMMARY 2.5 Summary By now, we hope that you have become familiar with our slogan probability is a measure of the size of a set. Let us summarize: • Probability = a probability law P. You can also view it as the value returned by P. • Measure=aruler,ascale,astopwatch,oranothermeasuringdevice.Itisatoolthat tells you how large or small a set is. The measure has to be compatible with the set. If a set is finite, then the measure can be a counter. If a set is a continuous interval, then the measure can be the length of the interval. • Size = the relative weight of the set for the sample space. Measuring the size is done by using a weighting function. Think of a fair coin versus a biased coin. The former has a uniform weight, whereas the latter has a nonuniform weight. • Set = an event. An event is a subset in the sample space. A probability law P always maps a set to a number. This is different from a typical function that maps a number to another number. If you understand what this slogan means, you will understand why probability can be appliedtodiscreteevents,continuousevents,eventsinn-Dspaces,etc.Youwillalsounder- stand the notion of measure zero and the notion of almost sure. These concepts lie at the foundation of modern data science, in particular, theoretical machine learning. The second half of this chapter discusses the concept of conditional probability. Con- ditional probability is a metaconcept that can be applied to any measure you use. The motivation of conditional probability is to restrict the probability to a subevent happening inthesamplespace.IfBhashappened,theprobabilityforAtoalsohappenisP[A∩B]/P[B]. If two events are not influencing each other, then we say that A and B are independent. Accordingto Bayes’theorem, wecanalso switchthe order ofA given B and B given A,ac- cordingtoBayes’theorem.Finally,thelawoftotalprobabilitygivesusawaytodecompose events into subevents. We end this chapter by mentioning a few terms related to conditional probabilities that will become useful later. Let us use the tennis tournament as an example: • P[W |A] = conditional probability = Given that you played with player A, what is the probability that you will win? • P[A] = prior probability = Without even entering the game, what is the chance that you will face player A? • P[A|W] = posterior probability = After you have won the game, what is the proba- bility that you have actually played with A? In many practical engineering problems, the question of interest is often the last one. That is,supposingthatyouhaveobservedsomething,whatisthemostlikelycauseofthatevent? Forexample,supposingwehaveobservedthisparticulardataset,whatisthebestGaussian model that would fit the dataset? Questions like these require some analysis of conditional probability, prior probability, and posterior probability. 95
CHAPTER 2. PROBABILITY 2.6 References Introduction to Probability 2-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 1. 2-2 Mark D. Ward and Ellen Gundlach, Introduction to Probability, W.H. Freeman and Company, 2016. Chapter 1 – Chapter 6. 2-3 Roy D. Yates and David J. Goodman, Probability and Stochastic Processes, 3rd Edi- tion, Wiley 2013, Chapter 1. 2-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 2. 2-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 2 and Chapter 3. 2-6 Ani Adhikari and Jim Pitman, Probability for Data Science, http://prob140.org/ textbook/content/README.html. Chapters 1 and 2. 2-7 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 2.1 – 2.7. 2-8 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 2. 2-9 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 1. Measure-Theoretic Probability 2-10 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 2.8 and 2.9. 2-11 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Appendix D. 2-12 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 2-13 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) 2-14 Patrick Billingsley, Probability and Measure, Wiley, 3rd Edition, 1995. Real Analysis 2-15 Tom M. Apostol, Mathematical Analysis, Pearson, 1974. 2-16 Walter Rudin, Principles of Mathematical Analysis, McGraw Hill, 1976. 96
2.7. PROBLEMS 2.7 Problems Exercise 1. A space S and three of its subsets are given by S = {1,3,5,7,9,11}, A = {1,3,5}, B = {7,9,11}, and C ={1,3,9,11}. Find A∩B∩C, Ac∩B, A−C, and (A−B)∪B. Exercise 2. Let A = (−∞,r] and B = (−∞,s] where r ≤ s. Find an expression for C = (r,s] in terms of A and B. Show that B =A∪C, and A∩C =∅. Exercise 3. (Video Solution) Simplify the following sets. (a) [1,4]∩([0,2]∪[3,5]) (b) ([0,1]∪[2,3])c (c) (cid:84)∞ (−1/n,+1/n) i=1 (d) (cid:83)∞ [5,8−(2n)−1] i=1 Exercise 4. We will sometimes deal with the relationship between two sets. We say that A implies B when A is a subset of B (why?). Show the following results. (a) Show that if A implies B, and B implies C, then A implies C. (b) Show that if A implies B, then Bc implies Ac. Exercise 5. Show that if A∪B =A and A∩B =A, then A=B. Exercise 6. A space S is defined as S = {1,3,5,7,9,22}, and three subsets as A = {1,3,5}, B = {7,9,11},C ={1,3,9,11}.Assumethateachelementhasprobability1/6.Findthefollowing probabilities: (a) P[A] (b) P[B] (c) P[C] (d) P[A∪B] (e) P[A∪C] (f) P[(A\C)∪B] 97
CHAPTER 2. PROBABILITY Exercise 7. (Video Solution) A collection of 26 letters, a-z, is mixed in a jar. Two letters are drawn at random, one after the other. What is the probability of drawing a vowel (a,e,i,o,u) and a consonant in either order? What is the sample space? Exercise 8. Consider an experiment consisting of rolling a die twice. The outcome of this experiment is an ordered pair whose first element is the first value rolled and whose second element is the second value rolled. (a) Find the sample space. (b) Find the set A representing the event that the value on the first roll is greater than or equal to the value on the second roll. (c) Find the set B corresponding to the event that the first roll is a six. (d) Let C correspond to the event that the first valued rolled and the second value rolled differ by two. Find A∩C. Note that A, B, and C should be subsets of the sample space specified in Part (a). Exercise 9. A pair of dice are rolled. (a) Find the sample space Ω (b) Find the probabilities of the events: (i) the sum is even, (ii) the first roll is equal to the second, (iii) the first roll is larger than the second. Exercise 10. Let A, B and C be events in an event space. Find expressions for the following: (a) Exactly one of the three events occurs. (b) Exactly two of the events occurs. (c) Two or more of the events occur. (d) None of the events occur. Exercise 11. Asystemiscomposedoffivecomponents,eachofwhichiseitherworkingorfailed.Consider anexperimentthatconsistsofobservingthestatusofeachcomponent,andlettheoutcomes of the experiment be given by all vectors (x ,x ,x ,x ,x ), where x is 1 if component i is 1 2 3 4 5 i working and 0 if component i is not working. (a) How many outcomes are in the sample space of this experiment? (b) Suppose that the system will work if components 1 and 2 are both working, or if components 3 and 4 are both working, or if components 1, 3, and 5 are all working. Let W be the event that the system will work. Specify all of the outcomes in W. 98
2.7. PROBLEMS (c) Let A be the event that components 4 and 5 have both failed. How many outcomes are in the event A? (d) Write out all outcomes in the event A∩W. Exercise 12. (Video Solution) A number x is selected at random in the interval [−1,2]. Let the events A = {x|x < 0}, B = {x||x−0.5| < 0.5}, C = {x|x > 0.75}. Find (a) P[A|B], (b) P[B|C], (c) P[A|Cc], (d) P[B|Cc]. Exercise 13. (Video Solution) Let the events A and B have P[A] = x, P[B] = y and P[A∪B] = z. Find the following probabilities: (a) P[A∩B], (b) P[Ac∩Bc], (c) P[Ac∪Bc], (d) P[A∩Bc], (e) P[Ac∪B]. Exercise 14. (a) ByusingthefactthatP[A∪B]≤P[A]+P[B],showthatP[A∪B∪C]≤P[A]+P[B]+ P[C]. (b) By using the fact that P[(cid:83)n A ]≤(cid:80)n P[A ], show that k=1 k k=1 k (cid:34) n (cid:35) n (cid:92) (cid:88) P A ≥1− P[Ac]. k k k=1 k=1 Exercise 15. Usethedistributivepropertyofsetoperationstoprovethefollowinggeneralizeddistributive law: (cid:32) n (cid:33) n (cid:92) (cid:92) A∪ B = (A∪B ). i i i=1 i=1 Hint: Use mathematical induction. That is, show that the above is true for n=2 and that it is also true for n=k+1 when it is true for n=k. Exercise 16. The following result is known as the Bonferroni’s Inequality. (a) Prove that for any two events A and B, we have P(A∩B)≥P(A)+P(B)−1. (b) Generalize the above to the case of n events A ,A ,...,A , by showing that 1 2 n P(A ∩A ∩···∩A )≥P(A )+P(A )+···+P(A )−(n−1). 1 2 n 1 2 n Hint: You may use the generalized Union Bound P((cid:83)n A )≤(cid:80)n P(A ). i=1 i i=1 i Exercise 17. (Video Solution) Let A, B, C be events with probabilities P[A]=0.5, P[B]=0.2, P[C]=0.4. Find 99
CHAPTER 2. PROBABILITY (a) P[A∪B] if A and B are independent. (b) P[A∪B] if A and B are disjoint. (c) P[A∪B∪C] if A, B and C are independent. (d) P[A∪B∪C] if A, B and C are pairwise disjoint; can this happen? Exercise 18. (Video Solution) Ablockofinformationistransmittedrepeatedoveranoisychanneluntilanerror-freeblock is received. Let M ≥ 1 be the number of blocks required for a transmission. Define the following sets. (i) A = {M is even} (ii) B = {M is a multiple of 3} (iii) C = {M is less than or equal to 6} Assume that the probability of requiring one additional block is half of the probability without the additional block. That is: (cid:18) 1(cid:19)k P[M =k]= , k =1,2,.... 2 Determine the following probabilities. (a) P[A], P[B], P[C], P[Cc] (b) P[A∩B], P[A\B], P[A∩B∩C] (c) P[A|B], P[B|A] (d) P[A|B∩C], P[A∩B|C] Exercise 19. (Video Solution) A binary communication system transmits a signal X that is either a +2-voltage signal or a −2-voltage signal. A malicious channel reduces the magnitude of the received signal by thenumberofheadsitcountsintwotossesofacoin.LetY betheresultingsignal.Possible values of Y are listed below. 2 Heads 1 Head No Head X =−2 Y =0 Y =−1 Y =−2 X =+2 Y =0 Y =+1 Y =+2 Assume that the probability of having X =+2 and X =−2 is equal. (a) Find the sample space of Y, and hence the probability of each value of Y. (b) What are the probabilities P[X =+2|Y =1] and P[Y =1|X =−2]? Exercise 20. (Video Solution) A block of 100 bits is transmitted over a binary communication channel with a probability of bit error p=10−2. 100
2.7. PROBLEMS (a) If the block has 1 or fewer errors, then the receiver accepts the block. Find the prob- ability that the block is accepted. (b) If the block has more than 1 error, then the block is retransmitted. What is the probability that 4 blocks are transmitted? Exercise 21. (Video Solution) A machine makes errors in a certain operation with probability p. There are two types of errors. The fraction of errors that are type A is α and the fraction that are type B is 1−α. (a) What is the probability of k errors in n operations? (b) What is the probability of k type A errors in n operations? 1 (c) What is the probability of k type B errors in n operations? 2 (d) Whatisthejointprobabilityofk typeAerrorsandk typeBerrorsinnoperations? 1 2 Hint:Thereare(cid:0)n(cid:1)(cid:0)n−k1(cid:1) possibilitiesofhavingk typeAerrorsandk typeBerrors k1 k2 1 2 in n operations. (Why?) Exercise 22. (Video Solution) A computer manufacturer uses chips from three sources. Chips from sources A, B and C are defective with probabilities 0.005, 0.001 and 0.01, respectively. The proportions of chips from A, B and C are 0.5, 0.1 and 0.4 respectively. If a randomly selected chip is found to be defective, find (a) the probability that the chips are from A. (b) the probability that the chips are from B. (c) the probability that the chips are from C. Exercise 23. (Video Solution) In a lot of 100 items, 50 items are defective. Suppose that m items are selected for testing. Wesaythatthemanufacturingprocessismalfunctioningiftheprobabilitythatoneormore itemsaretestedtobedefective.Callthisfailureprobabilityp.Whatshouldbetheminimum m such that p≥0.99? Exercise 24. (Video Solution) Oneoftwocoinsisselectedatrandomandtossedthreetimes.Thefirstcoincomesupheads with probability p =1/3 and the second coin with probability p =2/3. 1 2 (a) What is the probability that the number of heads is k =3? (b) Repeat (a) for k =0,1,2. (c) Find the probability that coin 1 was tossed given that k heads were observed, for k =0,1,2,3. (d) In part (c), which coin is more probably when 2 heads have been observed? 101
CHAPTER 2. PROBABILITY Exercise 25. (Video Solution) Considerthefollowingcommunicationchannel.Asourcetransmitsastringofbinarysymbols through a noisy communication channel. Each symbol is 0 or 1 with probability p and 1−p, respectively, and is received incorrectly with probability ε and ε . Errors in different 0 1 symbols transmissions are independent. Denote S as the source and R as the receiver. (a) What is the probability that a symbol is correctly received? Hint: Find P[R=1∩S =1] and P[R=0∩S =0]. (b) Find the probability of receiving 1011 conditioned on that 1011 was sent, i.e., P[R=1011|S =1011]. (c) To improve reliability, each symbol is transmitted three times, and the received string is decoded by the majority rule. In other words, a 0 (or 1) is transmitted as 000 (or 111, respectively), and it is decoded at the receiver as a 0 (or 1) if and only if the received three-symbol string contains at least two 0s (or 1s, respectively). What is the probability that the symbol is correctly decoded, given that we send a 0? (d) Suppose that the scheme of part (c) is used. What is the probability that a 0 was sent if the string 101 was received? (e) Suppose the scheme of part (c) is used and given that a 0 was sent. For what value of ε is there an improvement in the probability of correct decoding? Assume that 0 ε (cid:54)=0. 0 102
Chapter 3 Discrete Random Variables When working on a data analysis problem, one of the biggest challenges is the disparity between the theoretical tools we learn in school and the actual data our boss hands to us. By actual data, we mean a collection of numbers, perhaps organized or perhaps not. When we are given the dataset, the first thing we do would certainly not be to define the Borel σ-field and then define the measure. Instead, we would normally compute the mean, the standard deviation, and perhaps some scores about the skewness. ThesituationisbestexplainedbythelandscapeshowninFigure3.1.Ontheonehand, we have well-defined probability tools, but on the other hand, we have a set of practical “battle skills” for processing data. Often we view them as two separate entities. As long as we can pull the statistics from the dataset, why bother about the theory? Alternatively, we have a set of theories, but we will never verify them using the actual datasets. How can we bridge the two? What are the missing steps in the probability theory we have learned so far? The goal of this chapter (and the next) is to fill this gap. Figure 3.1: The landscape of probability and data. Often we view probability and data analysis as two different entities. However, probability and data analysis are inseparable. The goal of this chapter is to link the two. Three concepts to bridge the gap between theory and practice Thestartingpointofourdiscussionisaprobabilityspace(Ω,F,P).Itisanabstractconcept, butwehopewehaveconvincedyouinChapter2ofitssignificance.However,theprobability space is certainly not “user friendly” because no one would write a Python program to 103
CHAPTER 3. DISCRETE RANDOM VARIABLES implement those theories. How do we make the abstract probability space more convenient so that we can model practical scenarios? The first step is to recognize that the sample space and the event space are all based on statements, for example, “getting a head when flipping a coin” or “winning the game.” These statements are not numbers, but we (engineers) love numbers. Therefore, we should ask a very basic question: How do we convert a statement to a number? The answer is the concept of random variables. Key Concept 1: What are random variables? Random variables are mappings from events to numbers. Now,supposethatwehaveconstructedarandomvariablethattranslatesstatementsto numbers. The next task is to endow the random variable with probabilities. More precisely, weneedtoassignprobabilitiestotherandomvariablesothatwecanperformcomputations. This is done using the concept called probability mass function (PMF). Key Concept 2: What are probability mass functions (PMFs)? Probability mass functions are the ideal histograms of random variables. The best way to think about a PMF is a histogram, something we are familiar with. A histogram has two axes: The x-axis denotes the set of states and the y-axis denotes the probability. For each of the states that the random variable possesses, the histogram tells us the probability of getting a particular state. The PMF is the ideal histogram of a randomvariable.Itprovidesacompletecharacterizationoftherandomvariable.Ifyouhave a random variable, you must specify its PMF. Vice versa, if you tell us the PMF, you have specified a random variable. We ask the third question about pulling information from the probability mass func- tion, such as the mean and standard deviation. How do we obtain these numbers from the PMF? We are also interested in operations on the mean and standard deviations. For ex- ample, if a professor offers ten bonus points to the entire class, how will it affect the mean and standard deviation? If a store provides 20% off on all its products, what will happen to its mean retail price and standard deviation? However, the biggest question is perhaps the difference between the mean we obtain from a PMF and the mean we obtain from a his- togram. Understanding this difference will immediately help us build a bridge from theory to practice. Key Concept 3: What is expectation? Expectation = Mean = Average computed from a PMF. Organization of this chapter The plan for this chapter is as follows. We will start with the basic concepts of random variables in Section 3.1. We will formally define the random variables and discuss their relationship with the abstract probability space. Once this linkage is built, we can put 104
3.1. RANDOM VARIABLES the abstract probability space aside and focus on the random variables. In Section 3.2 we will define the probability mass function (PMF) of a random variable, which tells us the probability of obtaining a state of the random variable. PMF is closely related to the histogramofadataset.Wewillexplaintheconnection.InSection3.3wetakeasmalldetour to consider the cumulative distribution functions (CDF). Then, we discuss the mean and standarddeviationinSection3.4.Section3.5detailsafewcommonlyusedrandomvariables, including Bernoulli, binomial, geometric, and Poisson variables. 3.1 Random Variables 3.1.1 A motivating example Consider an experiment with 4 outcomes Ω = {♣,♦,♥,♠}. We want to construct the probabilityspace(Ω,F,P).ThesamplespaceΩisalreadydefined.TheeventspaceF isthe set of all possible subsets in Ω, which, in our case, is a set of 24 subsets. For the probability law P, let us assume that the probability of obtaining each outcome is 1 2 2 1 P[{♣}]= , P[{♦}]= , P[{♥}]= , P[{♠}]= . 6 6 6 6 Therefore, we have constructed a probability space (Ω,F,P) where everything is perfectly defined. So, in principle, they can live together happily forever. Alazydatascientistcomes,andthereisa(small)problem.Thedatascientistdoesnot want to write the symbols ♣,♦,♥,♠. There is nothing wrong with his motivation because all of us want efficiency. How can we help him? Well, the easiest solution is to encode each symbol with a number, for example, ♣←1, ♦←2, ♥←3, ♠←4, where the arrow means that we assign a number to the symbol. But we can express this more formally by defining a function X :Ω→R with X(♣)=1, X(♦)=2, X(♥)=3, X(♠)=4. Thereisnothingnewhere:wehavemerelyconvertedthesymbolstonumbers,withthehelp of a function X. However, with X defined, the probabilities can be written as 1 2 2 1 P[X =1]= , P[X =2]= , P[X =3]= , P[X =4]= . 6 6 6 6 This is much more convenient, and so the data scientist is happy. 3.1.2 Definition of a random variable The story above is exactly the motivation for random variables. Let us define a random variable formally. Definition3.1. ArandomvariableX isafunctionX :Ω→Rthatmapsanoutcome ξ ∈Ω to a number X(ξ) on the real line. 105
CHAPTER 3. DISCRETE RANDOM VARIABLES This definition may be puzzling at first glance. Why should we overcomplicate things by defining a function and calling it a variable? If you recall the story above, we can map the notations of the story to the notations of the definition as follows. Symbol Meaning Ω sample space = the set containing ♣,♦,♥,♠ ξ an element in the sample space, which is one of ♣,♦,♥,♠ X afunctionthatmaps♣tothenumber1,♦tothenumber2,etc X(ξ) a number on the real line, e.g., X(♣)=1 This explains our informal definition of random variables: Key Concept 1: What are random variables? Random variables are mappings from events to numbers. TherandomvariableX isafunction.Theinputtothefunctionisanoutcomeofthesample space, whereas the output is a number on the real line. This type of function is somewhat different from an ordinary function that often translates a number to another number. Nevertheless, X is a function. Figure 3.2: Arandomvariableisamappingfromtheoutcomesinthesamplespacetonumbersonthe realline.WecanthinkofarandomvariableX asatranslatorthattranslatesastatementtoanumber. Why do we call this function X a variable? X is a variable because X has multiple states. As we illustrate in Figure 3.2, the mapping X translates every outcome ξ to a number. There are multiple numbers, which are the states of X. Each state has a certain probability for X to land on. Because X is not deterministic, we call it a random variable. Example 3.1. Suppose we flip a fair coin so that Ω={head,tail}. We can define the random variable X :Ω→R as X(head)=1, and X(tail)=0. 106
3.1. RANDOM VARIABLES Therefore,whenwewriteP[X =1]weactuallymeanP[{head}].Isthereanydifference betweenP[{Head}]andP[X =1]?No,becausetheyaredescribingtwoidenticalevents. Notethattheassignmentofthevalueistotallyuptoyou.Youcansay“head”isequal to the value 102. This is allowed and legitimate, but it isn’t very convenient. Example 3.2. Flip a coin 2 times. The sample space Ω is Ω={(head,head),(head,tail),(tail,head),(tail,tail)}. Suppose that X is a random variable that maps an outcome to a number representing the sum of “head,” i.e., X(·)=number of heads. Then,forthe4ξ’sinthesamplespacethereareonly3distinctnumbers.Moreprecisely, if we let ξ = (head,head), ξ = (head,tail), ξ = (tail,head), ξ = (tail,tail), then, 1 2 3 4 we have X(ξ )=2, X(ξ )=1, X(ξ )=1, X(ξ )=0. 1 2 3 4 A pictorial illustration of this random variable is shown in Figure 3.3. This example showsthatthemappingdefinedbytherandomvariableisnotnecessarilyaone-to-one mapping because multiple outcomes can be mapped to the same number. Figure 3.3: Arandomvariablethatmapsapairofcoinstoanumber,wherethenumberrepresentsthe number of heads. 3.1.3 Probability measure on random variables By now, we hope that you understand Key Concept 1: A random variable is a mapping from a statement to a number. However, we are now facing another difficulty. We knew how to measure the size of an event using the probability law P because P(·) takes an event E ∈F andsendsittoanumberbetween[0,1].AfterthetranslationX,wecannotsendthe outputX(ξ)toP(·)becauseP(·)“eats”asetE ∈F andnotanumberX(ξ)∈R.Therefore, when we write P[X =1], how do we measure the size of the event X =1? 107
CHAPTER 3. DISCRETE RANDOM VARIABLES This question appears difficult but is actually quite easy to answer. Since the prob- ability law P(·) is always applied to an event, we need to define an event for the random variable X. If we write the sets clearly, we note that “X =a” is equivalent to the set (cid:26) (cid:12) (cid:27) (cid:12) E = ξ ∈Ω(cid:12)X(ξ)=a . (cid:12) This is the set that contains all possible ξ’s such that X(ξ) = a. Therefore, when we say “find the probability of X = a,” we are effectively asking the size of the set E = {ξ ∈ Ω|X(ξ)=a}. How then do we measure the size of E? Since E is a subset in the sample space, E is measurable by P. All we need to do is to determine what E is for a given a. This, in turn, requires us to find the pre-image X−1(a), which is defined as (cid:26) (cid:12) (cid:27) X−1(a)d =ef ξ ∈Ω(cid:12) (cid:12)X(ξ)=a . (cid:12) Wait a minute, is this set just equal to E? Yes, the event E we are seeking is exactly the pre-image X−1(a). As such, the probability measure of E is P[X =a]=P[X−1(a)]. Figure 3.4 illustrates a situation where two outcomes ξ and ξ are mapped to the same 1 2 value a on the real line. The corresponding event is the set X−1(a)={ξ ,ξ }. 1 2 Figure 3.4: When computing the probability of P[{ξ ∈ Ω|X(ξ) = a}], we effectively take the inverse mapping X−1(a) and compute the probability of the event P[{ξ∈X−1(a)}]=P[{ξ ,ξ }]. 1 2 Example 3.3. Suppose we throw a die. The sample space is Ω={1,2,3,4,5,6}. There is a natural mapping X that maps X(1)=1, X(2)=2 and so on. Thus, 108
3.1. RANDOM VARIABLES P[X ≤3]( =a)P[X =1]+P[X =2]+P[X =3] ( =b)P[X−1(1)]+P[X−1(2)]+P[X−1(3)] ( =c)P[{1}]+P[{2}]+P[{3}]= 3 . 6 In this derivation, step (a) is based on Axiom III, where the three events are disjoint. Step (b) is the pre-image due to the random variable X. Step (c) is the list of ac- tual events in the event space. Note that there is no hand-waving argument in this derivation. Every step is justified by the concepts and theorems we have learned so far. Example 3.4. Throw a die twice. The sample space is then Ω={(1,1),(1,2),...,(6,6)}. These elements can be translated to 36 outcomes: ξ =(1,1),ξ =(1,2),...,ξ =(6,6). 1 2 36 Let X =sum of two numbers. Then, if we want to find the probability of getting X =7, we can trace back and ask: Amongthe36outcomes,whichofthose ξ ’swillgive usX(ξ)=7? Or,whatistheset i X−1(7)? To this end, we can write P[X =7]=P[{(1,6),(2,5),(3,4),(4,3),(5,2),(6,1)}] =P[(1,6)]+P[(2,5)]+P[(3,4)] +P[(4,3)]+P[(5,2)]+P[(6,1)] 1 1 1 1 1 1 1 = + + + + + = . 36 36 36 36 36 36 6 Again,inthisexample,youcanseethatallthestepsarefullyjustifiedbytheconcepts we have learned so far. Closing remark. In practice, when the problem is clearly defined, we can skip the inverse mappingX−1(a).However,thisdoesnotmeanthattheprobabilitytriplet(Ω,F,P)isgone; it is still present. The triplet is now just the background of the problem. The set of all possible values returned by X is denoted as X(Ω). Since X is not necessarily a bijection, the size of X(Ω) is not necessarily the same as the size of Ω. The elements in X(Ω) are often denoted as a or x. We call a or x one of the states of X. Be careful not to confuse x and X. The variable X is the random variable; it is a function. The variable x is a state assigned by X. A random variable X has multiple states. When we write P[X =x], we describe the probability of a random variable X taking a particular state x. It is exactly the same as P[{ξ ∈Ω|X(ξ)=x}]. 109
CHAPTER 3. DISCRETE RANDOM VARIABLES 3.2 Probability Mass Function Random variables are mappings that translate events to numbers. After the translation, we have a set of numbers denoting the states of the random variables. Each state has a different probability of occurring. The probabilities are summarized by a function known as the probability mass function (PMF). 3.2.1 Definition of probability mass function Definition 3.2. The probability mass function (PMF) of a random variable X is a function which specifies the probability of obtaining a number X(ξ)=x. We denote a PMF as p (x)=P[X =x]. (3.1) X The set of all possible states of X is denoted as X(Ω). Donotget confusedbythesamplespaceΩand the setofstates X(Ω).Thesample spaceΩ contains all the possible outcomes of the experiments, whereas X(Ω) is the translation by the mapping X. The event X =a is the set X−1(a)⊆Ω. Therefore, when we say P[X =x] we really mean P[X−1(x)]. The probability mass function is a histogram summarizing the probability of each of the states X takes. Since it is a histogram, a PMF can be easily drawn as a bar chart. Example 3.5. Flip a coin twice. The sample space is Ω = {HH, HT, TH, TT}. We can assign a random variable X = number of heads. Therefore, X(“HH”)=2,X(“TH”)=1,X(“HT”)=1,X(“TT”)=0. So the random variable X takes three states: 0, 1, 2. The PMF is therefore 1 p (0)=P[X =0]=P[{“TT”}]= , X 4 1 p (1)=P[X =1]=P[{“TH”,“HT”}]= , X 2 1 p (2)=P[X =2]=P[{“HH”}]= . X 4 3.2.2 PMF and probability measure In Chapter 2, we learned that probability is a measure of the size of a set. We introduced a weighting function that weights each of the elements in the set. The PMF is the weighing functionfordiscreterandomvariables.TworandomvariablesaredifferentwhentheirPMFs are different because they are constructing two different measures. 110
3.2. PROBABILITY MASS FUNCTION To illustrate the idea, suppose there are two dice. They each have probability masses as follows. 1 2 3 4 1 1 P[{1}]= , P[{2}]= , P[{3}]= , P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 12 12 12 2 2 2 2 2 2 P[{1}]= , P[{2}]= , P[{3}]= , P[{4}]= , P[{5}]= , P[{6}]= , 12 12 12 12 12 12 Letusdefinetworandomvariables,X andY,forthetwodice.Then,thePMFsp andp X Y can be defined as 1 2 3 4 1 1 p (1)= , p (2)= , p (3)= , p (4)= , p (5)= , p (6)= , X 12 X 12 X 12 X 12 X 12 X 12 2 2 2 2 2 2 p (1)= , p (2)= , p (3)= , p (4)= , p (5)= , p (6)= . Y 12 Y 12 Y 12 Y 12 Y 12 Y 12 Thesetwoprobabilitymassfunctionscorrespondtotwodifferentprobabilitymeasures,let’s say F and G. Define the event E = {between 2 and 3}. Then, F(E) and G(E) will lead to two different results: 1 2 3 F(E)=P[2≤X ≤3]=p (2)+p (3)= + = , X X 12 12 12 2 2 4 G(E)=P[2≤Y ≤3]=p (2)+p (3)= + = . Y Y 12 12 12 Note that even though for some particular events two final results could be the same (e.g., 2≤X ≤4 and 2≤Y ≤4), the underlying measures are completely different. Figure 3.5 shows another example of two different measures F and G on the same sample space Ω = {♣,♦,♥,♠}. Since the PMFs of the two measures are different, even when given the same event E, the resulting probabilities will be different. Figure 3.5: If we want to measure the size of a set E, using two different PMFs is equivalent to using two different measures. Therefore, the probabilities will be different. Does p =p imply X =Y? If two random variables X and Y have the same PMF, X Y doesitmeanthattherandomvariablesarethesame?Theanswerisno.Considerarandom variable with a symmetric PMF, e.g., 1 1 1 p (−1)= , p (0)= , p (1)= . (3.2) X 4 X 2 X 4 Suppose Y =−X. Then, p (−1)= 1, p (0)= 1, and p (1)= 1, which is the same as p . Y 4 Y 2 Y 4 X However, X and Y are two different random variables. If the sample space is {♣,♦,♥}, we can define the mappings X(·) and Y(·) as X(♣)=−1, X(♦)=0, X(♥)=+1, Y(♣)=+1, Y(♦)=0, Y(♥)=−1. 111
CHAPTER 3. DISCRETE RANDOM VARIABLES Therefore,whenwesayp (−1)= 1,theunderlyingeventis♣.Butwhenwesayp (−1)= 1, X 4 Y 4 theunderlyingeventis♥.Thetworandomvariablesaredifferent,althoughtheirPMFshave exactly the same shape. 3.2.3 Normalization property Herewemustmentiononeimportantpropertyofaprobabilitymassfunction.Thisproperty is known as the normalization property, which is a useful tool for a sanity check. Theorem 3.1. A PMF should satisfy the condition that (cid:88) p (x)=1. (3.3) X x∈X(Ω) Proof.TheprooffollowsdirectlyfromAxiomII,whichstatesthatP[Ω]=1.Sincexcovers all numerical values X can take, and since each x is distinct, by Axiom III we have (cid:88) (cid:88) P[X =x]= P[{ξ ∈Ω|X(ξ)=x}] x∈X(Ω) x∈X(Ω)   (cid:91) =P  {ξ ∈Ω|X(ξ)=x}=P[Ω]=1. ξ∈Ω (cid:3) Practice Exercise 3.1. Let p (k)=c(cid:0)1(cid:1)k , where k =1,2,.... Find c. X 2 (cid:80) Solution. Since p (k)=1, we must have k∈X(Ω) X (cid:88)∞ (cid:18) 1(cid:19)k =1. 2 k=1 Evaluating the geometric series on the right-hand side, we can show that (cid:88)∞ (cid:18) 1(cid:19)k c (cid:88)∞ (cid:18) 1(cid:19)k c = 2 2 2 k=1 k=0 c 1 = · 2 1− 1 2 =c =⇒ c=1. Practice Exercise 3.2. Let p (k)=c·sin(cid:0)πk(cid:1) , where k =1,2,.... Find c. X 2 Solution. The reader may might be tempted to sum p (k) over all the possible k’s: X ∞ (cid:88) (cid:16)π (cid:17) ? sin k =1+0−1+0+···=0. 2 k=1 112
3.2. PROBABILITY MASS FUNCTION However, a more careful inspection reveals that p (k) is actually negative when k = X 3,7,11,.... This cannot happen because a probability mass function must be non- negative. Therefore, the problem is not defined, and so there is no solution. 0.5 1 0.5 0.25 0 -0.5 0.125 0.0625 -1 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 (a) (b) Figure 3.6: (a) The PMF of p (k) = c(cid:0)1(cid:1)k, for k = 1,2,.... (b) The PMF of p (k) = sin(cid:0)πk(cid:1) , X 2 X 2 where k=1,2,.... Note that this is not a valid PMF because probability cannot have negative values. 3.2.4 PMF versus histogram PMFs are closely related to histograms. A histogram is a plot that shows the frequency of a state. As we see in Figure 3.6, the x-axis is a collection of states, whereas the y-axis is the frequency. So a PMF is indeed a histogram. Viewing a PMF as a histogram can help us understand a random variable. For better or worse, treating a random variable as a histogram could help you differentiate a random variable from a variable. An ordinary variable only has one state, but a random variable has multiple states. At any particular instance, we do not know which state will show up before our observation. However, we do know the probability. For example, in the coin-flip example, while we do not know whether we will get “HH,” we know that the chance of getting “HH” is 1/4. Of course, having a probability of 1/4 does not mean that we will get “HH”onceeveryfourtrials.Itonlymeansthatifwerunaninfinitenumberofexperiments, then 1/4 of the experiments will give us “HH.” The linkage between PMF and histogram can be quite practical. For example, while wedonotknowthetrueunderlyingdistributionofthe26lettersoftheEnglishalphabet,we can collect a large number of words and plot the histogram. The example below illustrates how we can empirically define a random variable from the data. Example. There are 26 English letters, but the frequencies of the letters in writing are different. If we define a random variable X as a letter we randomly draw from an English text,wecanthinkofX asanobjectwith26differentstates.Themappingassociatedwiththe randomvariableisstraightforward:X(“a”)=1,X(“b”)=2,etc.Theprobabilityoflanding onaparticularstateapproximatelyfollowsahistogramshowninFigure 3.7.Thehistogram provides meaningful values of the probabilities, e.g., p (1) = 0.0847, p (2) = 0.0149, etc. X X The true probability of the states may not be exactly these values. However, when we have enough samples, we generally expect the histogram to approach the theoretical PMF. The MATLAB and Python codes used to generate this histogram are shown below. % MATLAB code to generate the histogram load(‘ch3_data_English’); bar(f/100,‘FaceColor’,[0.9,0.6,0.0]); 113
CHAPTER 3. DISCRETE RANDOM VARIABLES 0.12 0.1 0.08 0.06 0.04 0.02 0 a b c d e f g h i j k l mn o p q r s t u v w x y z Figure 3.7: The frequency of the 26 English letters. Data source: Wikipedia. xticklabels({‘a’,‘b’,‘c’,‘d’,‘e’,‘f’,‘g’,‘h’,‘i’,‘j’,‘k’,‘l’,... ‘m’,‘n’,‘o’,‘p’,‘q’,‘r’,‘s’,‘t’,‘u’,‘v’,‘w’,‘x’,‘y’,‘z’}); xticks(1:26); yticks(0:0.02:0.2); axis([1 26 0 0.13]); # Python code generate the histogram import numpy as np import matplotlib.pyplot as plt f = np.loadtxt(‘./ch3_data_english.txt’) n = np.arange(26) plt.bar(n, f/100) ntag = [‘a’,‘b’,‘c’,‘d’,‘e’,‘f’,‘g’,‘h’,‘i’,‘j’,‘k’,‘l’,‘m’,... ‘n’,‘o’,‘p’,‘q’,‘r’,‘s’,‘t’,‘u’,‘v’,‘w’,‘x’,‘y’,‘z’] plt.xticks(n, ntag) PMF = ideal histograms Ifarandomvariableismoreorlessahistogram,whyisthePMFsuchanimportantconcept? The answer to this question has two parts. The first part is that the histogram generated from a dataset is always an empirical histogram, so-called because the dataset comes from observation or experience rather than theory. Thus the histograms may vary slightly every time we collect a dataset. As we increase the number of data points in a dataset, the histogram will eventually converge to an ideal histogram, or a distribution. For example, counting the number of headsin100coinflipswillfluctuatemoreinpercentagetermsthancountingtheheadsin10 million coin flips. The latter will almost certainly have a histogram that is closer to a 50–50 distribution. Therefore, the “histogram” generated by a random variable can be considered the ultimate histogram or the limiting histogram of the experiment. To help you visualize the difference between a PMF and a histogram, we show in Figure 3.8 an experiment in which a die is thrown N times. Assuming that the die is fair, the PMF is simply p (k) = 1/6 for k = 1,...,6, which is a uniform distribution across X the 6 states. Now, we can throw the die many times. As N increases, we observe that the 114
3.2. PROBABILITY MASS FUNCTION N = 100 N = 1000 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 1 2 3 4 5 6 1 2 3 4 5 6 (a) N =100 (b) N =1000 N = 10000 N = 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 1 2 3 4 5 6 1 2 3 4 5 6 (c) N =10000 (d) PMF Figure3.8:HistogramandPMF,whenthrowingafairdieN times.AsN increases,thehistogramsare becoming more similar to the PMF. histogram becomes more like the PMF. You can imagine that when N goes to infinity, the histogram will eventually become the PMF. Therefore, when given a dataset, one way to think ofit is totreat the dataas random realizations drawnfrom a certainPMF. The more data points you have, the closer the histogram will become to the PMF. The MATLAB and Python codes used to generate Figure 3.8 are shown below. The twocommandsweuseherearerandi(inMATLAB),whichgeneratesrandomintegernum- bers, and hist, which computes the heights and bin centers of a histogram. In Python, the corresponding commands are np.random.randint and plt.hist. Note that because of the different indexing schemes in MATLAB and Python, we offset the maximum index in np.random.randintto7insteadof6.Also,weshiftthex-axessothatthebarsarecentered at the integers. % MATLAB code to generate the histogram x = [1 2 3 4 5 6]; q = randi(6,100,1); figure; [num,val] = hist(q,x-0.5); bar(num/100,‘FaceColor’,[0.8, 0.8,0.8]); axis([0 7 0 0.24]); # Python code generate the histogram import numpy as np import matplotlib.pyplot as plt q = np.random.randint(7,size=100) 115
CHAPTER 3. DISCRETE RANDOM VARIABLES plt.hist(q+0.5,bins=6) ThisgenerativeperspectiveisillustratedinFigure3.9.Weassumethattheunderlying latent random variable has some PMF that can be described by a few parameters, e.g., the meanandvariance.Giventhedatapoints,ifwecaninfertheseparameters,wemightretrieve theentirePMF(uptotheuncertaintylevelintrinsictothedataset).Werefertothisinverse process as statistical inference. Figure 3.9: Whenanalyzingadataset,onecantreatthedatapointsaresamplesdrawnaccordingtoa latentrandomvariablewithcertainaPMF.Thedatasetweobserveisoftenfinite,andsothehistogram we obtain is empirical. A major task in data analysis is statistical inference, which tries to retrieve the model information from the available measurements. Returning to the question of why we need to understand the PMFs, the second part of the answer is the difference between synthesis and analysis. In synthesis, we start with a known random variable and generate samples according to the PMF underlying the ran- dom variable. For example, on a computer, we often start with a Gaussian random variable and generate random numbers according to the histogram specified by the Gaussian ran- dom variable. Synthesis is useful because we can predict what will happen. We can, for example, create millions of training samples to train a deep neural network. We can also evaluatealgorithmsusedtoestimatestatisticalquantitiessuchasmean,variance,moments, etc., because the synthesis approach provides us with ground truth. In supervised learning scenarios, synthesis is vital to ensuring sufficient training data. The other direction of synthesis is analysis. The goal is to start with a dataset and deduce the statistical properties of the dataset. For example, suppose we want to know whether the underlying model is indeed a Gaussian model. If we know that it is a Gaussian (or if we choose to use a Gaussian), we want to know the parameters that define this Gaussian. The analysis direction addresses this model selection and parameter estimation problem. Moving forward, once we know the model and the parameters, we can make a prediction or do recovery, both of which are ubiquitous in machine learning. We summarize our discussions below, which is Key Concept 2 of this chapter. Key Concept 2: What are probability mass functions (PMFs)? PMFs are the ideal histograms of random variables. 116
3.2. PROBABILITY MASS FUNCTION 3.2.5 Estimating histograms from real data The following discussions about histogram estimation can be skipped if it is your first time reading the book. If you have a dataset, how would you plot the histogram? Certainly, if you have access to MATLAB or Python, you can call standard functions such as hist (in MATLAB) or np.histogram (in Python). However, when plotting a histogram, you need to specify the numberofbins(orequivalentlythewidthofbins).Ifyouuselargerbins,thenyouwillhave fewer bins with many elements in each bin. Conversely, if the bin width is too small, you may not have enough samples to fill the histogram. Figure 3.10 illustrates two histograms in which the bins are respectively too large and too small. 1000 50 K = 5 K = 200 800 40 600 30 400 20 200 10 0 0 0 2 4 6 8 10 0 2 4 6 8 10 (a) 5 bins (b) 200 bins Figure 3.10: The width of the histogram has substantial influence on the information that can be extracted from the histogram. The MATLAB and Pythoncodes used to generate Figure 3.10 are shown below.Note that here we are using an exponential random variable (to be discussed in Chapter 4). In MATLAB,callinganexponentialrandomvariableisdoneusingexprnd,whereasinPython the command is np.random.exponential. For this experiment, we can specify the number ofbinsk,whichcanbesettok =200ork =5.TosuppressthePythonoutputofthearray, we can add a semicolon ;. A final note is that lambda is a reserved variable in Python. Use something else. % MATLAB code used to generate the plots lambda = 1; k = 1000; X = exprnd(1/lambda,[k,1]); [num,val] = hist(X,200); bar(val,num,‘FaceColor’,[1, 0.5,0.5]); # Python code used to generate the plots import numpy as np import matplotlib.pyplot as plt lambd = 1 117
CHAPTER 3. DISCRETE RANDOM VARIABLES k = 1000 X = np.random.exponential(1/lambd, size=k) plt.hist(X,bins=200); In statistics, there are various rules to determine the bin width of a histogram. We mention a few of them here. Let K be the number of bins and N the number of samples. √ • Square-root: K = N • Sturges’ formula: K =log N +1. √ 2 • Rice Rule: K =23N √ • Scott’s normal reference rule: K = maxX−minX, where h = 3.5 √Var[X] is the bin h 3N width. For the example data shown in Figure 3.10, the histograms obtained using the above rules aregiveninFigure 3.11.Asyoucansee,differentruleshavedifferentsuggestedbinwidths. Some are more conservative, e.g., using fewer bins, whereas some are less conservative. In any case, the suggested bin widths do seem to provide better histograms than the original ones in Figure 3.10. However, no bin width is the best for all purposes. 500 500 Square-root, K = 32 Sturges Rule, K = 11 400 400 300 300 200 200 100 100 0 0 0 1 2 3 4 5 0 1 2 3 4 5 500 500 Rice Rule, K = 20 Scott Rule, K = 22 400 400 300 300 200 200 100 100 0 0 0 1 2 3 4 5 0 1 2 3 4 5 Figure 3.11: Histograms of a dataset using different bin width rules. Beyond these predefined rules, there are also algorithmic tools to determine the bin width. One such tool is known as cross-validation. Cross-validation means defining some kind of cross-validation score that measures the statistical risk associated with the his- togram.Ahistogramhavingalowerscorehasalowerrisk,andthusitisabetterhistogram. 118
3.2. PROBABILITY MASS FUNCTION Note that the word “better” is relative to the optimality criteria associated with the cross- validationscore.Ifyoudonotagreewithourcross-validationscore,ouroptimalbinwidthis not necessarily the one you want. In this case, you need to specify your optimality criteria. Theoretically, deriving a meaningful cross-validation score is beyond the scope of this book. However, it is still possible to understand the principle. Let h be the bin width of the histogram,K thenumberofbins,andN thenumberofsamples.Givenadataset,wefollow this procedure: • Step 1: Choose a bin width h. • Step2:Constructahistogramfromthedata,usingthebinwidthh.Thehistogramwill havetheempiricalPMFvaluesp ,p ,...,p ,whicharetheheightsofthehistograms (cid:98)1 (cid:98)2 (cid:98)K normalized so that the sum is 1. • Step 3: Compute the cross-validation score (see Wasserman, All of Statistics, Section 20.2): J(h)= 2 − N +1 (cid:0) p2+p2+···+p2 (cid:1) (3.4) (N −1)h (N −1)h (cid:98)1 (cid:98)2 (cid:98)K • Repeat Steps 1, 2, 3, until we find an h that minimizes J(h). Note that when we use a different h, the PMF values p ,p ,...,p will change, and the (cid:98)1 (cid:98)2 (cid:98)K number of bins K will also change. Therefore, when changing h, we are changing not only the terms in J(h) that explicitly contain h but also terms that are implicitly influenced. -2.9 -3 -3.1 -3.2 -3.3 -3.4 -3.5 20 40 60 80 100 120 140 160 180 200 Number of Bins erocS noitadilav-ssorC 10-3 5 4 3 2 1 0 20 40 60 80 100 120 140 160 180 200 Number of Bins erocS noitadilav-ssorC 10-4 (a) One dataset (b) Average of many datasets Figure 3.12: Cross-validation score for the histogram. (a) The score of one particular dataset. (b) The scores for many different datasets generated by the same model. For the dataset we showed in Figure 3.10, the cross-validation score J(h) is shown in Figure 3.12. We can see that although the curve is noisy, there is indeed a reasonably clear minimum happening around 20≤K ≤30, which is consistent with some of the rules. The MATLAB and Python codes we used to generate Figure 3.12 are shown below. The key step is to implement Equation (3.4) inside a for-loop, where the loop goes through the range of bins we are interested in. To obtain the PMF values p ,...,p , we call hist (cid:98)1 (cid:98)K in MATLAB and np.histogram in Python. The bin width h is the number of samples n divided by the number of bins m. 119
CHAPTER 3. DISCRETE RANDOM VARIABLES % MATLAB code to perform the cross validation lambda = 1; n = 1000; X = exprnd(1/lambda,[n,1]); m = 6:200; J = zeros(1,195); for i=1:195 [num,binc] = hist(X,m(i)); h = n/m(i); J(i) = 2/((n-1)*h)-((n+1)/((n-1)*h))*sum( (num/n).^2 ); end plot(m,J,‘LineWidth’,4,‘Color’,[0.9,0.2,0.0]); # Python code to perform the cross validation import numpy as np import matplotlib.pyplot as plt lambd = 1 n = 1000 X = np.random.exponential(1/lambd, size=n) m = np.arange(5,200) J = np.zeros((195)) for i in range(0,195): hist,bins = np.histogram(X,bins=m[i]) h = n/m[i] J[i] = 2/((n-1)*h)-((n+1)/((n-1)*h))*np.sum((hist/n)**2) plt.plot(m,J); In Figure 3.12(b), we show another set of curves from the same experiment. The differencehereisthatweassumeaccesstothetruegenerativemodelsothatwecangenerate the many datasets of the same distribution. In this experiment we generated T = 1000 datasets. We compute the cross-validation score J(h) for each of the datasets, yielding T scorefunctionsJ(1)(h),...,J(T)(h).Wesubtracttheminimumbecausedifferentrealizations have different offsets. Then we compute the average: T (cid:26) (cid:27) J(h)= 1 (cid:88) J(t)(h)−min(cid:8) J(t)(h)(cid:9) . (3.5) T h t=1 This gives us a smooth red curve as shown in Figure 3.12(b). The minimum appears to be at N = 25. This is the optimal N, concerning the cross-validation score, on the average of all datasets. All rules, including cross-validation, are based on optimizing for a certain objective. Your objective could be different from our objective, and so our optimum is not necessarily youroptimum.Therefore,cross-validationmaynotbethebest.Itdependsonyourproblem. End of the discussion. 120
3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE) 3.3 Cumulative Distribution Functions (Discrete) While the probability mass function (PMF) provides a complete characterization of a dis- crete random variable, the PMFs themselves are technically not “functions” because the impulses in the histogram are essentially delta functions. More formally, a PMF p (k) X should actually be written as (cid:88) p (x)= p (k) · δ(x−k) . X X (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) k∈X(Ω) PMFvalues deltafunction Thisisatrainofdeltafunctions,wheretheheightisspecifiedbytheprobabilitymassp (k). X For example, a random variable with PMF values 1 1 1 p (0)= , p (1)= , p (2)= X 4 X 2 X 4 will be expressed as 1 1 1 p (x)= δ(x)+ δ(x−1)+ δ(x−2). X 4 2 4 Sincedeltafunctionsneedtobeintegratedtogeneratevalues,thetypicalthingswewantto do, e.g., integration and differentiation, are not as straightforward in the sense of Riemann- Stieltjes. The way to handle the unfriendliness of the delta functions is to consider mild modi- fications of the PMF. This notation of “cumulative” distribution functions will allow us to resolve the delta function problems. We will defer the technical details to the next chap- ter. For the time being, we will briefly introduce the idea to prepare you for the technical discussion later. 3.3.1 Definition of the cumulative distribution function Definition 3.3. Let X be a discrete random variable with Ω = {x ,x ,...}. The 1 2 cumulative distribution function (CDF) of X is k F (x )d =efP[X ≤x ]=(cid:88) p (x ). (3.6) X k k X (cid:96) (cid:96)=1 If Ω={...,−1,0,1,2,...}, then the CDF of X is k F (k)d =efP[X ≤k]= (cid:88) p ((cid:96)). (3.7) X X (cid:96)=−∞ A CDF is essentially the cumulative sum of a PMF from −∞ to x, where the variable x(cid:48) in the sum is a dummy variable. 121
CHAPTER 3. DISCRETE RANDOM VARIABLES Example 3.6. Consider a random variable X with PMF p (0) = 1, p (1) = 1 and X 4 X 2 p (4)= 1. The CDF of X can be computed as X 4 1 F (0)=P[X ≤0]=p (0)= , X X 4 3 F (1)=P[X ≤1]=p (0)+p (1)= , X X X 4 F (4)=P[X ≤4]=p (0)+p (1)+p (4)=1. X X X X AsshowninFigure3.13,theCDFofadiscreterandomvariableisastaircasefunction. 1 1 0.75 0.75 0.5 0.5 0.25 0.25 0 1 4 0 1 4 (a) PMF p (k) (b) CDF F (k) X X Figure 3.13: Illustration of a PMF and a CDF. The MATLAB code and the Python code used to generate Figure 3.13 are shown below. The CDF is computed using the command cumsum in MATLAB and np.cumsum in Python. % MATLAB code to generate a PMF and a CDF p = [0.25 0.5 0.25]; x = [0 1 4]; F = cumsum(p); figure(1); stem(x,p,‘.’,‘LineWidth’,4,‘MarkerSize’,50); figure(2); stairs([-4 x 10],[0 F 1],‘.-’,‘LineWidth’,4,‘MarkerSize’,50); % Python code to generate a PMF and a CDF import numpy as np import matplotlib.pyplot as plt p = np.array([0.25, 0.5, 0.25]) x = np.array([0, 1, 4]) F = np.cumsum(p) plt.stem(x,p,use_line_collection=True); plt.show() plt.step(x,F); plt.show() 122
3.3. CUMULATIVE DISTRIBUTION FUNCTIONS (DISCRETE) Why is CDF a better-defined function than PMF? There are technical reasons associ- ated with whether a function is integrable. Without going into the details of these discus- sions, a short answer is that delta functions are defined through integrations; they are not functions. A delta function is defined as a function such that δ(x)=0 everywhere except at (cid:82) x = 0, and δ(x) dx = 1. On the other hand, a staircase function is always well-defined. Ω The discontinuous points of a staircase can be well defined if we specify the gap between two consecutive steps. For example, in Figure 3.13, as soon as we specify the gap 1/4, 1/2, and 1/4, the staircase function is completely defined. Example. Figure 3.14 shows the empirical histogram of the English letters and the corre- sponding empirical CDF. We want to differentiate PMF versus histogram and CDF versus empirical CDF. The empirical CDF is the CDF computed from a finite dataset. 1 0.12 0.9 0.1 0.8 0.7 0.08 0.6 0.06 0.5 0.4 0.04 0.3 0.2 0.02 0.1 0 0 abcde f gh i j k lmnopq r s t uvwx y z abcde f gh i j k lmnopq r s t uvwx y z Figure 3.14: PMF and a CDF of the frequency of English letters. 3.3.2 Properties of the CDF WeobservefromtheexampleinFigure3.13thataCDFhasseveralproperties.First,being astaircasefunction,theCDFisnon-decreasing.Itcanstayconstantforawhile,butitnever drops. Second, the minimum value of a CDF is 0, whereas the maximum value is 1. It is 0 for any value that is smaller than the first state; it is 1 for any value that is larger than the last state. Third, the gap at each jump is exactly the probability mass at that state. Let us summarize these observations in the following theorem. Theorem3.2. IfX isadiscreterandomvariable,thentheCDFofX hasthefollowing properties: (i) The CDF is a sequence of increasing unit steps. (ii) The maximum of the CDF is when x=∞: F (+∞)=1. X (iii) The minimum of the CDF is when x=−∞: F (−∞)=0. X (iv) The unit steps have jumps at positions where p (x)>0. X Proof. Statement (i) can be seen from the summation (cid:88) F (x)= p (x(cid:48)). X X x(cid:48)≤x 123
CHAPTER 3. DISCRETE RANDOM VARIABLES Sincetheprobabilitymassfunctionisnon-negative,thevalueofF islargerwhenthevalue X of the argument is larger. That is, x≤y implies F (x)≤F (y). The second statement (ii) X X is true because the summation includes all possible states. So we have ∞ (cid:88) F (+∞)= p (x(cid:48))=1. X X x(cid:48)=−∞ Similarly, for the third statement (iii), (cid:88) F (−∞)= p (x(cid:48)). X X x(cid:48)≤−∞ The summation is taken over an empty set, and so F (−∞) = 0. Statement (iv) is true X because the cumulative sum changes only when there is a non-zero mass in the PMF. (cid:3) As we can see in the proof, the basic argument of the CDF is the cumulative sum of thePMF.Bydefinition,acumulativesumalwaysaddsmass.ThisiswhytheCDFisalways increasing, has 0 at −∞, and has 1 at +∞. This last statement deserves more attention. It implies that the unit step always has a solid dot on the left-hand side and an empty dot on the right-hand side, because when the CDF jumps, the final value is specified by the “≤” sign in Equation (3.6). The technical term for this property is right continuous. 3.3.3 Converting between PMF and CDF Theorem 3.3. IfX isadiscreterandomvariable,thenthePMFofX canbeobtained from the CDF by p (x )=F (x )−F (x ), (3.8) X k X k X k−1 whereweassumedthatX hasacountablesetofstates{x ,x ,...}.Ifthesamplespace 1 2 of the random variable X contains integers from −∞ to +∞, then the PMF can be defined as p (k)=F (k)−F (k−1). (3.9) X X X Example 3.7. Continuing with the example in Figure 3.13, if we are given the CDF 1 3 F (0)= , F (1)= , F (4)=1, X 4 X 4 X how do we find the PMF? We know that the PMF will have non-negative values only at x=0,1,4. For each of these x, we can show that 1 1 p (0)=F (0)−F (−∞)= −0= , X X X 4 4 3 1 1 p (1)=F (1)−F (0)= − = , X X X 4 4 2 3 1 p (4)=F (4)−F (1)=1− = . X X X 4 4 124
3.4. EXPECTATION 3.4 Expectation When analyzing data, it is often useful to extract certain key parameters such as the mean andthestandarddeviation.Themeanandthestandarddeviationcanbeseenfromthelens of random variables. In this section, we will formalize the idea using expectation. 3.4.1 Definition of expectation Definition 3.4. The expectation of a random variable X is (cid:88) E[X]= xp (x). (3.10) X x∈X(Ω) ExpectationisthemeanoftherandomvariableX.Intuitively,wecanthinkofp (x)asthe X percentage of times that the random variable X attains the value x. When this percentage is multiplied by x, we obtain the contribution of each x. Summing over all possible values of x then yields the mean. To see this more clearly, we can write the definition as (cid:88) E[X]= x p (x) . X (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x∈X(Ω) astateX takes thepercentage (cid:124) (cid:123)(cid:122) (cid:125) sumoverallstates Figure 3.15 illustrates a PMF that contains five states x ,...,x . Corresponding to each 1 5 state are p (x ),...,p (x ). For this PMF to make sense, we must assume that p (x )+ X 1 X 5 X 1 def ···+p (x ) = 1. To simplify notation, let us define p = p (x ). Then the expectation X 5 i X i of X is just the sum of the products: value (x ) times height (p ). This gives E[X] = i i (cid:80)5 x p (x ). i=1 i X i Figure 3.15: The expectation of a random variable is the sum of x p . i i We emphasize that the definition of the expectation is exactly the same as the usual way we calculate the average of a dataset. When we calculate the average of a dataset D ={x(1),x(2),...,x(N)},wesumuptheseN samplesanddividebythenumberofsamples. This is what we called the empirical average or the sample average: N 1 (cid:88) average= x(n). (3.11) N n=1 125
CHAPTER 3. DISCRETE RANDOM VARIABLES Of course, in a typical dataset, these N samples often take distinct values. But suppose that among these N samples there are only K different values. For example, if we throw a die a million times, every sample we record will be one of the six numbers. This situation is illustrated in Figure 3.16, where we put the samples into the correct bin storing these values. In this case, to calculate the average we are effectively doing a binning: K 1 (cid:88) average= value x × number of samples with value x . (3.12) N k k k=1 Equation(3.12)isexactly thesameasEquation(3.11),aslongasthesamplescanbegrouped into K different values. With a little calculation, we can rewrite Equation (3.12) as K average= (cid:88) value x × number of samples with value x k, (cid:124) (cid:123)(cid:122) k (cid:125) N k=1 astateX takes (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) thepercentage sumofallstates which is the same as the definition of expectation. Figure 3.16: If we have a dataset D containing N samples, and if there are only K distinct values, we caneffectivelyputtheseN samplesintoK bins.Thus,the“average”(whichisthesumdividedbythe number N) is exactly the same as our definition of expectation. The difference between E[X] and the average is that E[X] is computed from the ideal histogram,whereasaverageiscomputedfromtheempirical histogram.Whenthenumberof samples N approaches infinity, we expect the average to approximate E[X]. However, when N is small, the empirical average will have random fluctuations around E[X]. Every time we experiment, the empirical average may be slightly different. Therefore, we can regard E[X]asthetrue average ofacertainrandomvariable,andtheempiricalaverageasafinite- sample average based on the particular experiment we are working with. This summarizes Key Concept 3 of this chapter. Key Concept 3: What is expectation? Expectation = Mean = Average computed from a PMF. If we are given a dataset on a computer, computing the mean can be done by calling the command mean in MATLAB and np.mean in Python. The example below shows the case of finding the mean of 10000 uniformly distributed random numbers. 126
3.4. EXPECTATION % MATLAB code to compute the mean of a dataset X = rand(10000,1); mX = mean(X); # Python code to compute the mean of a dataset import numpy as np X = np.random.rand(10000) mX = np.mean(X) Example 3.8. Let X be a random variable with PMF p (0)=1/4, p (1)=1/2 and X X p (2)=1/4. We can show that the expectation is X (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X]=(0) +(1) +(2) =1. 4 2 4 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) pX(0) pX(1) pX(2) On MATLAB and Python, if we know the PMF then computing the expectation is straight-forward. Here is the code to compute the above example. % MATLAB code to compute the expectation p = [0.25 0.5 0.25]; x = [0 1 2]; EX = sum(p.*x); # Python code to compute the expectation import numpy as np p = np.array([0.25, 0.5, 0.25]) x = np.array([0, 1, 2]) EX = np.sum(p*x) Example 3.9. Flip an unfair coin, where the probability of getting a head is 3. Let 4 X be a random variable such that X = 1 means getting a head. Then we can show that p (1)= 3 and p (0)= 1. The expectation of X is therefore X 4 X 4 (cid:18) (cid:19) (cid:18) (cid:19) 3 1 3 E[X]=(1)p (1)+(0)p (0)=(1) +(0) = . X X 4 4 4 Center of mass. How would you interpret the result of this example? Does it mean that, on average, we will get 3/4 heads (but there is not anything called 3/4 heads!). Recall the definition of a random variable: it is a translator that translates a descriptive state to a number on the real line. Thus the expectation, which is an operation defined on the real line, can only tell us what is happening on the real line, not in the original sample 127
CHAPTER 3. DISCRETE RANDOM VARIABLES Figure 3.17: Centerofmass.Ifastatex ismoreinfluentialthananotherstatex ,thecenterofmass 2 1 E[X] will lean towards x . 2 space.Ontherealline,theexpectationcanberegardedasthecenter of mass,whichisthe point where the “forces” between the two states are “balanced”. In Figure 3.17 we depict a random variable with two states x and x . The state x has less influence (because p (x ) 1 2 1 X 1 is smaller) than x . Therefore the center of mass is shifted towards x . This result shows us 2 2 that the value E[X] is not necessarily in the sample space. E[X] is a deterministic number with nothing to do with the sample space. Example3.10.LetX bearandomvariablewithPMFp (k)= 1 ,fork =1,2,3,.... X 2k The expectation is ∞ ∞ (cid:88) (cid:88) 1 E[X]= kp (k)= k· X 2k k=1 k=1 ∞ 1(cid:88) 1 1 1 = k· = · =2. 2 2k−1 2 (1− 1)2 k=1 2 On MATLAB and Python, if you want to verify this answer you can use the following code. Here, we approximate the infinite sum by a finite sum of k =1,...,100. % MATLAB code to compute the expectation k = 1:100; p = 0.5.^k; EX = sum(p.*k); # Python code to compute the expectation import numpy as np k = np.arange(100) p = np.power(0.5,k) EX = np.sum(p*k) Example 3.11. Roll a die twice. Let X be the first roll and Y be the second roll. Let Z = max(X,Y). To compute the expectation E[Z], we first construct the sample space. Since there are two rolls, we can construct a table listing all possible pairs of outcomes.Thiswillgiveus{(1,1),(1,2),...,(6,6)}.Now,wecalculateZ,whichisthe max of the two rolls. So if we have (1,3), then the max will be 3, whereas if we have (5,2), then the max will be 5. We can complete a table as shown below. 128
3.4. EXPECTATION 1 2 3 4 5 6 1 1 2 3 4 5 6 2 2 2 3 4 5 6 3 3 3 3 4 5 6 4 4 4 4 4 5 6 5 5 5 5 5 5 6 6 6 6 6 6 6 6 This table tell us that Z has 6 states. The PMF of Z can be determined by counting the number of times a state shows up in the table. Thus, we can show that 1 3 5 p (1)= , p (2)= , p (3)= , Z 36 Z 36 Z 36 7 9 11 p (4)= , p (5)= , p (6)= . Z 36 Z 36 Z 36 The expectation of Z is therefore (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 3 5 E[Z]=(1) +(2) +(3) 36 36 36 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 7 9 11 +(4) +(5) +(6) 36 36 36 161 = . 36 Example 3.12. Consider a game in which we flip a coin 3 times. The reward of the game is • $1 if there are 2 heads • $8 if there are 3 heads • $0 if there are 0 or 1 head There is a cost associated with the game. To enter the game, the player has to pay $1.50. We want to compute the net gain, on average. To answer this question, we first note that the sample space contains 8 elements: HHH, HHT, HTH, THH, THT, TTH, HTT, TTT. Let X be the number of heads. Then the PMF of X is 1 3 3 1 p (0)= , p (1)= , p (2)= , p (3)= . X 8 X 8 X 8 X 8 WethenletY bethereward.ThePMFofY canbefoundby“adding”theprobabilities of X. This yields 4 3 1 p (0)=p (0)+p (1)= , p (1)=p (2)= , p (8)=p (3)= . Y X X 8 Y X 8 Y X 8 129
CHAPTER 3. DISCRETE RANDOM VARIABLES The expectation of Y is (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 4 3 1 11 E[X]=(0) +(1) +(8) = . 8 8 8 8 Since the cost of the game is 12, the net gain (on average) is −1. 8 8 3.4.2 Existence of expectation Does every PMF have an expectation? No, because we can construct a PMF such that the expectation is undefined. Example 3.13. Consider a random variable X with the following PMF: 6 p (k)= , k =1,2,.... X π2k2 Using a result from algebra, one can show that (cid:80)∞ 1 = π2. Therefore, p (k) is a legitimatePMFbecause(cid:80)∞ p (k)=1.Howeverk ,= t1 hek2 expec6 tationdivergesX ,because k=1 X ∞ (cid:88) E[X]= kp (k) X k=1 ∞ 6 (cid:88) 1 = →∞, π2 k k=1 where the limit is due to the harmonic seriesa: 1+ 1 + 1 +···=∞. 2 3 ahttps://en.wikipedia.org/wiki/Harmonic_series_(mathematics) A PMF has an expectation when it is absolutely summable. Definition 3.5. A discrete random variable X is absolutely summable if E[|X|]d =ef (cid:88) |x|p (x)<∞. (3.13) X x∈X(Ω) This definition tells us that not all random variables have a finite expectation. This is a very important mathematical result, but its practical implication is arguably limited. Most of the random variables we use in practice are absolutely summable. Also, note that the property of absolute summability applies to discrete random variables. For continuous random variables, we have a parallel concept called absolute integrability, which will be discussed in the next chapter. 3.4.3 Properties of expectation The expectation of a random variable has several useful properties. We list them below. Note that these properties apply to both discrete and continuous random variables. 130
3.4. EXPECTATION Theorem 3.4. The expectation of a random variable X has the following properties: (i) Function. For any function g, (cid:88) E[g(X)]= g(x)p (x). X x∈X(Ω) (ii) Linearity. For any function g and h, E[g(X)+h(X)]=E[g(X)]+E[h(X)]. (iii) Scale. For any constant c, E[cX]=cE[X]. (iv) DC Shift. For any constant c, E[X+c]=E[X]+c. Proof of (i): A pictorial proof of (i) is shown in Figure 3.18. The key idea is a change of variable. Figure 3.18: By letting g(X)=Y, the PMFs are not changed. What changes are the states. When we have a function Y =g(X), the PMF of Y will have impulses moved from x (the horizontal axis) to g(x) (the vertical axis). The PMF values (i.e., the probabilities or the height of the stems), however, are not changed. If the mapping g(X) is many-to-one, multiple PMF values will add to the same position. Therefore, when we compute E[g(X)], we compute the expectation along the vertical axis. Practice Exercise 3.3. Prove statement (iii): For any constant c, E[cX]=cE[X]. Solution. Recall the definition of expectation: (cid:88) (cid:88) E[cX]= (cx)p (x)=c xp (x)=cE[X]. X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) =E[X] Statement (iii) is illustrated in Figure 3.19. Here, we assume that the original PMF has 3 131
CHAPTER 3. DISCRETE RANDOM VARIABLES states X = 0,1,2. We multiply X by a constant c = 3. This changes X to cX = 0,3,6. However, since the probabilities are not changed, the height of the PMF values remains. Therefore, when computing the expectation, we just multiply E[X] by c to get cE[X]. Figure 3.19: PictorialrepresentationofE[cX]=cE[X].WhenwemultiplyX byc,wefixtheprobabil- ities but make the spacing between states wider/narrower. Practice Exercise 3.4. Prove statement (ii): For any function g and h, E[g(X)+ h(X)]=E[g(X)]+E[h(X)]. Solution. Recall the definition of expectation: (cid:88) E[g(X)+h(X)]= [g(x)+h(x)]p (x) X x∈X(Ω) (cid:88) (cid:88) = g(x)p (x)+ h(x)p (x) X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[g(X)] =E[h(X)] =E[g(X)]+E[h(X)]. PracticeExercise3.5.Provestatement(iv):Foranyconstantc,E[X+c]=E[X]+c. Solution. Recall the definition of expectation: (cid:88) E[X+c]= (x+c)p (x) X x∈X(Ω) (cid:88) (cid:88) = xp (x)+c· p (x) X X x∈X(Ω) x∈X(Ω) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[X] =1 =E[X]+c. This result is illustrated in Figure 3.20. As we add a constant to the random variable, itsPMFvaluesremainthesamebuttheirpositionsareshifted.Therefore,whencomputing the mean, the mean will be shifted accordingly. 132
3.4. EXPECTATION Figure3.20:PictorialrepresentationofE[X+c]=E[X]+c.WhenweaddctoX,wefixtheprobabilities and shift the entire PMF to the left or to the right. Example3.14.LetX bearandomvariablewithfourequallyprobablestates0,1,2,3. We want to compute the expectation E[cos(πX/2)]. To do so, we note that (cid:18) (cid:19) (cid:88) πX E[cos(πX/2)]= cos p (x) 2 X x∈X(Ω) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 π 1 2π 1 3π 1 =(cos0) +(cos ) +(cos ) +(cos ) 4 2 4 2 4 2 4 1+0+(−1)+0 = =0. 4 Example 3.15. Let X be a random variable with E[X]=1 and E[X2]=3. We want to find the expectation E[(aX+b)2]. To do so, we realize that E[(aX+b)2]( =a)E[a2X2+2abX+b2]( =b) a2E[X2]+2abE[X]+b2 =3a2+2ab+b2, where (a) is due to expansion of the square, and (b) holds in two steps. The first step is to apply statement (ii) for individual functions of expectations, and the second step is to apply statement (iii) for scalar multiple of the expectations. 3.4.4 Moments and variance Based on the concept of expectation, we can define a moment: Definition 3.6. The kth moment of a random variable X is (cid:88) E[Xk]= xkp (x). (3.14) X x Essentially, the kth moment is the expectation applied to Xk. The definition follows from statement (i) of the expectation’s properties. Using this definition, we note that E[X] is the first moment and E[X2] is the second moment. Higher-order moments can be defined, but in practice they are less commonly used. 133
CHAPTER 3. DISCRETE RANDOM VARIABLES Example 3.16. Flip a coin 3 times. Let X be the number of heads. Then 1 3 3 1 p (0)= , p (1)= , p (2)= , p (3)= . X 8 X 8 X 8 X 8 The second moment E[X2] is (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 3 3 1 E[X2]=(0)2 +(1)2 +(2)2 +(4)2 =3. 8 8 8 8 Example 3.17. Consider a random variable X with PMF 1 p (k)= , k =1,2,.... X 2k The second moment E[X2] is (cid:88)∞ (cid:18) 1(cid:19)k 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 E[X2]= k2 = k(k−1+1) 2 22 2 k=1 k=1 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 1 (cid:88)∞ (cid:18) 1(cid:19)k−2 = k(k−1) + k 22 2 22 2 k=1 k=1 (cid:18) (cid:19) (cid:18) (cid:19) 1 2 1 1 = + =6. 22 (1− 1)3 2 (1− 1)2 2 2 Using the second moment, we can define the variance of a random variable. Definition 3.7. The variance of a random variable X is Var[X]=E[(X−µ)2], (3.15) where µ=E[X] is the expectation of X. Wedenoteσ2 byVar[X].Thesquarerootofthevariance,σ,iscalledthestandarddeviation ofX.LiketheexpectationE[X],thevarianceVar[X]iscomputedusingtheidealhistogram PMF. It is the limiting object of the usual standard deviation we calculate from a dataset. On a computer, computing the variance of a dataset is done by calling built-in com- mandssuchasvarinMATLABandnp.varinPython.Thestandarddeviationiscomputed using std and np.std, respectively. % MATLAB code to compute the variance X = rand(10000,1); vX = var(X); sX = std(X); % Python code to compute the variance import numpy as np 134
3.4. EXPECTATION X = np.random.rand(10000) vX = np.var(X) sX = np.std(X) What does the variance mean? It is a measure of the deviation of the random variable X relative to its mean. This deviation is quantified by the squared difference (X−µ)2. The expectation operator takes the average of the deviation, giving us a deterministic number E[(X−µ)2]. Theorem 3.5. The variance of a random variable X has the following properties: (i) Moment. Var[X]=E[X2]−E[X]2. (ii) Scale. For any constant c, Var[cX]=c2Var[X]. (iii) DC Shift. For any constant c, Var[X+c]=Var[X]. Figure 3.21: Pictorial representations of Var[cX]=c2Var[X] and Var[X+c]=Var[X]. Practice Exercise 3.6. Prove Theorem 3.5 above. Solution. For statement (i), we show that Var[X]=E[(X−µ)2]=E[X2−2Xµ+µ2]=E[X2]−µ2. 135
CHAPTER 3. DISCRETE RANDOM VARIABLES Statement (ii) holds because E[cX]=cµ and Var[cX]=E[(cX−E[cX])2] =E[(cX−cµ)2]=c2E[(X−µ)2]=c2Var[X]. Statement (iii) holds because Var[X+c]=E[((X+c)−E[X+c])2]=E[(X−E[X])2]=Var[X]. Thepropertiesaboveareusefulinvariousways.Thefirststatementprovidesalinkconnect- ingvarianceandthesecondmoment.Statement(ii)impliesthatwhenX isscaledbyc,the varianceshouldbescaledbyc2 becauseofthesquareinthesecondmoment.Statement(iii) says that when X is shifted by a scalar c, the variance is unchanged. This is true because no matter how we shift the mean, the fluctuation of the random variable remains the same. Practice Exercise 3.7. Flip a coin with probability p to get a head. Let X be a random variable denoting the outcome. The PMF of X is p (0)=1−p, p (1)=p. X X Find E[X], E[X2] and Var[X]. Solution. The expectation of X is E[X]=(0)p (0)+(1)p (1)=(0)(1−p)+(1)(p)=p. X X The second moment is E[X2]=(0)2p (0)+(1)2p (1)=p. X X The variance is Var[X]=E[X2]−E[X]2 =p−p2 =p(1−p). 3.5 Common Discrete Random Variables In the previous sections, we have conveyed three key concepts: one about the random vari- able, one about the PMF, and one about the mean. The next step is to introduce a few commonlyuseddiscreterandomvariablessothatyouhavesomethingconcreteinyour“tool- box.” As we have mentioned before, these predefined random variables should be studied from a synthesis perspective (sometimes called generative). The plan for this section is to introduce several models, derive their theoretical properties, and discuss examples. Note that some extra effort will be required to understand the origins of the random variables. The origins of random variablesare usually overlooked, but they are moreimpor- tant than the equations. For example, we will shortly discuss the Poisson random variable 136
3.5. COMMON DISCRETE RANDOM VARIABLES Figure 3.22: A Bernoulli random variable has two states with probability p and 1−p. and its PMF p (k) = λke−λ. Why is the Poisson random variable defined in this way? If X k! youknowhowthePoissonPMFwasoriginallyderived,youwillunderstandtheassumptions made during the derivation. Consequently, you will know why Poisson is a good model for internettraffic,recommendationscores,andimagesensorsforcomputervisionapplications. You will also know under what situation the Poisson model will fail. Understanding the physics behind the probability models is the focus of this section. 3.5.1 Bernoulli random variable We start discussing the simplest random variable, namely the Bernoulli random variable. A Bernoulli random variable is a coin-flip random variable. The random variable has two states: either 1 or 0. The probability of getting 1 is p, and the probability of getting 0 is 1−p.SeeFigure 3.22foranillustration.Bernoullirandomvariablesareusefulforallkinds ofbinarystateevents:coinflip(HorT),binarybit(1or0),trueorfalse,yesorno,present or absent, Democrat or Republican, etc. To make these notions more precise, we define a Bernoulli random variable as follows. Definition 3.8. Let X be a Bernoulli random variable. Then, the PMF of X is p (0)=1−p, p (1)=p, X X where 0<p<1 is called the Bernoulli parameter. We write X ∼Bernoulli(p) to say that X is drawn from a Bernoulli distribution with a parameter p. Inthisdefinition,theparameterpcontrolstheprobabilityofobtaining1.Inacoin-flipevent, pisusually 1,meaningthatthecoinisfair.However,forbiasedcoinspisnotnecessarily 1. 2 2 Forothersituationssuchasbinarybits(0or1),theprobabilityofobtaining1couldbevery different from the probability of obtaining 0. In MATLAB and Python, generating Bernoulli random variables can be done by call- ing the binomial random number generator np.random.binomial (Python) and binornd (MATLAB). When the parameter n is equal to 1, the binomial random variable is equiv- alent to a Bernoulli random variable. The MATLAB and Python codes to synthesize a Bernoulli random variable are shown below. 137
CHAPTER 3. DISCRETE RANDOM VARIABLES % MATLAB code to generate 1000 Bernoulli random variables p = 0.5; n = 1; X = binornd(n,p,[1000,1]); [num, ~] = hist(X, 10); bar(linspace(0,1,10), num,‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 1000 Bernoulli random variables import numpy as np import matplotlib.pyplot as plt p = 0.5 n = 1 X = np.random.binomial(n,p,size=1000) plt.hist(X,bins=‘auto’) AnalternativemethodinPythonistocallstats.bernoulli.rvstogeneraterandom Bernoulli numbers. # Python code to call scipy.stats library import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 X = stats.bernoulli.rvs(p,size=1000) plt.hist(X,bins=‘auto’); Properties of Bernoulli random variables Let us now derive a few key statistical properties of a Bernoulli random variable. Theorem 3.6. If X ∼Bernoulli(p), then E[X]=p, E[X2]=p, Var[X]=p(1−p). Proof. The expectation can be computed as E[X]=(1)p (1)+(0)p (0)=(1)(p)+(0)(1−p)=p. X X The second moment is E[X2]=(12)(p)+(02)(1−p)=p. Therefore, the variance is Var[X]=E[X2]−µ2 =p−p2 =p(1−p). (cid:3) A useful property of the Python code is that we can construct an object rv. Then we can call rv’s attributes to determine its mean, variance, etc. 138
3.5. COMMON DISCRETE RANDOM VARIABLES # Python code to generate a Bernoulli rv object import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 rv = stats.bernoulli(p) mean, var = rv.stats(moments=‘mv’) print(mean, var) In both MATLAB and Python, we can plot the PMF of a Bernoulli random variable, suchastheoneshowninFigure3.23.TodothisinMATLAB,wecallthefunctionbinopdf, with the evaluation points specified by x. 1 0.8 0.6 0.4 0.2 0 -0.2 0 0.2 0.4 0.6 0.8 1 1.2 Figure 3.23: An example of a theoretical PMF (not the empirical histogram) plotted by MATLAB. % MATLAB code to plot the PMF of a Bernoulli p = 0.3; x = [0,1]; f = binopdf(x,1,p); stem(x, f, ‘bo’, ‘LineWidth’, 8); In Python, we construct a random variable rv. With rv, we can call its PMF rv.pmf: # Python code to plot the PMF of a Bernoulli import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.3 rv = stats.bernoulli(p) x = np.linspace(0, 1, 2) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=10); plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5); 139
CHAPTER 3. DISCRETE RANDOM VARIABLES When will a Bernoulli random variable have the maximum variance? Let us take a look at the variance of the Bernoulli random variable. For any given p, the varianceisp(1−p).Thisisaquadraticequation.IfweletV(p)=p(1−p),wecanshowthat the maximum is attained at p = 1/2. To see this, take the derivative of V(p) with respect to p. This will give us d V(p) = 1−2p. Equating to zero yields 1−2p = 0, so p = 1/2. dp We know that p = 1/2 is a maximum and not a minimum point because the second order derivative V(cid:48)(cid:48)(p) = −2, which is negative. Therefore V(p) is maximized at p = 1/2. Now, since 0 ≤ p ≤ 1, we also know that V(0) = 0 and V(1) = 0. Therefore, the variance is minimized at p=0 and p=1. Figure 3.24 shows a graph of the variance. Figure 3.24: The variance of a Bernoulli reaches maximum at p=1/2. Does this result make sense? Why is the variance maximized at p = 1/2? If we think about this problem more carefully, we realize that a Bernoulli random variable represents a coin-flip experiment. If the coin is biased such that it always gives heads, on the one hand, it is certainly a bad coin. However, on the other hand, the variance is zero because there is nothing to vary; you will certainly get heads. The same situation happens if the coin is biased towards tails. However, if the coin is fair, i.e., p = 1/2, then the variance is large because we only have a 50% chance of getting a head or a tail whenever we flip a coin. Nothing is certain in this case. Therefore, the maximum variance happening at p = 1/2 matches our intuition. Rademacher random variable A slight variation of the Bernoulli random variable is the Rademacher random variable, which has two states: +1 and −1. The probability getting +1 and −1 is 1/2. Therefore, the PMF of a Rademacher random variable is 1 1 p (−1)= , and p (+1)= . X 2 X 2 Practice Exercise 3.8. Show that if X is a Rademacher random variable then (X+1)/2∼Bernoulli(1/2).Alsoshowtheconverse:IfY ∼Bernoulli(1/2)then2Y−1 is a Rademacher random variable. Solution.SinceX caneitherbe+1or−1,weshowthatifX =+1then(X+1)/2=1 and if X =−1 then (X+1)/2=0. The probabilities of getting +1 and −1 are equal. Thus, the probabilities of getting (X+1)/2=1 and 0 are also equal. So the resulting random variable is Bernoulli(1/2). The other direction can be proved similarly. 140
3.5. COMMON DISCRETE RANDOM VARIABLES Bernoulli in social networks: the Erd˝os-R´enyi graph The study of networks is a big branch of modern data science. It includes social networks, computer networks, traffic networks, etc. The history of network science is very long, but one of the most basic models of a network is the Erd˝os-R´enyi graph, named after Paul Erd˝os and Alfr´ed R´enyi. The underlying probabilistic model of the Erd˝os-R´enyi graph is the Bernoulli random variable. To see how a graph can be constructed from a Bernoulli random variable, we first introduce the concept of a graph. A graph contains two elements: nodes and edges. For node i and node j, we denote the edge connecting i and j as A . Therefore, if we have N ij nodes, then we can construct a matrix A of size N ×N. We call this matrix the adjacency matrix. For example, the adjacency matrix   0 1 1 0 1 0 0 0 A=  1 0 0 1 0 0 1 0 will have edges for node pairs (1,2), (1,3), and (3,4). Note that in this example we assume that the adjacency matrix is symmetric, meaning that the graph is undirected. The “1” in the adjacency matrix indicates there is an edge, and “0” indicates there is no edge. So A represents a binary graph. The Erd˝os-R´enyi graph model says that the probability of getting an edge is an inde- pendent Bernoulli random variable. That is A ∼Bernoulli(p), ij for i < j. If we model the graph in this way, then the parameter p will control the density of the graph. High values of p mean that there is a higher chance for an edge to be present. p = 0.3 p = 0.5 p = 0.7 p = 0.9 4 --012 21 2 5 32 4 8 3 1 96 1 1 1 337 6 8 0 1 4 612 8 307 7 2 1 3 4 8 23 73 9 3 1 45 2 2 0 0 61 3 1 29 13 2 23 3 2 113 5 29 5 2 4 21 ---0123 321 114 33 54 3 256 2 6 1 1 9 2 9 34 03 3 2 4 1 3 93 7 20 3 1 4 28 3 6 1 2 2 1 5 1 88 1 2 1 7 23 79 2 1 3 1 262 5 8 0 10 3 3 72 ---0123 321 3 2 2 1 1 41 1 2 3 4 5 1 0 56 2 1 39 34 8 1 3 4 11 2 5 2 7 5 3 8 73 0 33 32 9 1 2 1 2 91 2 2 8 73 236 8 932 6 2 11 0 60 3 47 -02 2 7 32 220 6 2 6 3 4 15 9 1 2 5 2 2 1 3 3 6 6 1 7 1 91 345 3 1 1 3 23 1 1 2 2 338 9 74 38 9 11 3 18 38 4 2 31 0 0 2 5 4 02 72 -3 -4 -4 -4 -2 0 2 -2 0 2 -4 -2 0 2 4 -4 -2 0 2 4 Figure 3.25: The Erd˝os-R´enyi graph. [Top] The graphs. [Bottom] The adjacency matrices. 141
CHAPTER 3. DISCRETE RANDOM VARIABLES To illustrate the idea of an Erd˝os-R´enyi graph, we show in Figure 3.25 a graph of 40 nodes. The edges are randomly selected by flipping a Bernoulli random variable with parameter p=0.3,0.5,0.7,0.9. As we can see in the figure, a small value of p gives a graph with very sparse connectivity, whereas a large value of p gives a very densely connected graph. The bottom row of Figure 3.25 shows the corresponding adjacency matrices. Here, a white pixel denotes “1” in the matrix and a black pixel denotes “0” in the matrix. While Erd˝os-R´enyi graphs are elementary, their variations can be realistic models of socialnetworks.Thestochasticblockmodelisonesuchmodel.Inastochasticblockmodel, nodes form small communities within a large network. For example, there are many majors in a university. Students within the same major tend to have more interactions than with students of another major. The stochastic block model achieves this goal by partitioning the nodes into communities. Within each community, the nodes can have a high degree of connectivity.Acrossdifferentcommunities,theconnectivitywillbemuchlower.Figure3.26 illustrates a network and the corresponding adjacency matrix. In this example, the network has three communities. 4 12 10 02 2 1 7 9 1 8 1 2 31 1 1 34 9 25 1 14 8 1 75 62 2 2 0 1 6 2 1 24 53 9 76 1 9 83 6 79 6 8 8 7 00 3 8 9 2 9 8 89 9 98 4 99 2 8 859 1 8 7 6 7 7 4 9 8 8 691 4 5 1 0 7 0 7 77 5 26 9 7 7 7 8 68 706 3 43 -- 42 6 4 5 4 422 6 5 5 65 9 26 3 3 4 2 8 8 9 3 30 9 3 2 2 8 5 3 3 4 34 5 1 56 33 3 6 67 3 1 14 44 54 2 7 3 55 5 106 5 45 68 57 60 27 40 49 -6 -4 -3 -2 -1 0 1 2 3 4 Figure 3.26: A stochastic block model containing three communities. [Left] The graph. [Right] The adjacency matrix. In network analysis, one of the biggest problems is determining the community struc- ture and recovering the underlying probabilities. The former task is about grouping the nodes into blocks. This is a nontrivial problem because in practice the nodes are never arranged nicely, as shown in Figure 3.26. For example, why should Alice be node 1 and Bob be node 2? Since we never know the correct ordering of the nodes, partitioning the nodes into blocks requires various estimation techniques such as clustering or iterative esti- mation. Recovering the underlying probability is also not easy. Given an adjacency matrix, why can we assume that the underlying network is a stochastic block model? Even if the model is correct, there will be imperfect grouping in the previous step. As such, estimat- ing the underlying probability in the presence of these uncertainties would pose additional challenges. Today,networkanalysisremainsoneofthehottestareasindatascience.Itsimportance derives from its broad scope and impact. It can be used to analyze social networks, opinion polls,marketing,orevengenomeanalysis.Nevertheless,thestartingpointoftheseadvanced subjects is the Bernoulli random variable, the random variable of a coin flip! 142
3.5. COMMON DISCRETE RANDOM VARIABLES 3.5.2 Binomial random variable Supposeweflipthecoinntimescountthenumberofheads.Sinceeachcoinflipisarandom variable (Bernoulli), the sum is also a random variable. It turns out that this new random variable is the binomial random variable. Definition 3.9. Let X be a binomial random variable. Then, the PMF of X is (cid:18) (cid:19) n p (k)= pk(1−p)n−k, k =0,1,...,n, X k where 0 < p < 1 is the binomial parameter, and n is the total number of states. We write X ∼Binomial(n,p) to say that X is drawn from a binomial distribution with a parameter p of size n. To understand the meaning of a binomial random variable, consider a simple experiment consisting of flipping a coin three times. We know that all possible cases are HHH, HHT, HTH, THH, TTH, THT, HTT and TTT. Now, suppose we define X = number of heads. We want to write down the probability mass function. Effectively, we ask: What is the probability of getting 0 head, one head, two heads, and three heads? We can, of course, count and get the answer right away for a fair coin. However, suppose the coin is unfair, i.e., the probability of getting a head is p whereas that of a tail is 1−p. The probability of getting each of the 8 cases is shown in Figure 3.27 below. Figure 3.27: The probability of getting k heads out of n=3 coins. Here are the detailed calculations. Let us start with X =3. p (3)=P[{HHH}] X =P[{H}∩{H}∩{H}] ( =a)P[{H}]P[{H}]P[{H}] ( =b) p3, where (a) holds because the three events are independent. (Recall that if A and B are independent then P[A∩B] = P[A]P[B].) (b) holds because each P[{H}] = p by definition. With exactly the same argument, we can show that p (0)=P[{TTT}]=(1−p)3. X 143
CHAPTER 3. DISCRETE RANDOM VARIABLES Now, let us look at p (2), i.e., 2 heads. This probability can be calculated as follows: X p (2)=P[{HHT}∪{HTH}∪{THH}] X ( =c)P[{HHT}]+P[{HTH}]+P[{THH}] ( =d) p2(1−p)+p2(1−p)+p2(1−p)=3p2(1−p), where (c) holds because the three events HHT, HTH and THH are disjoint in the sample space. Note that we are not using the independence argument in (c) but the disjoint argu- ment. We should not confuse the two. The step in (d) uses independence, because each coin flip is independent. The above calculation shows an interesting phenomenon: Although the three events HHT, HTH, and THH are different (in fact, disjoint), the number of heads in all the cases is the same. This happens because when counting the number of heads, the ordering of the heads and tails does not matter. So the same problem can be formulated as finding the number of combinations of { 2 heads and 1 tail }, which in our case is (cid:0)3(cid:1) =3. 2 To complete the story, let us also try p (1). This probability is X p (1)=P[{TTH}∪{HTT}∪{THT}]=3p(1−p)2. X Again, we see that the combination (cid:0)3(cid:1) =3 appears in front of the p(1−p)2. 1 In general, the way to interpret the binomial random variable is to decouple the prob- abilities p, (1−p), and the number of combinations (cid:0)n(cid:1) : k (cid:18) (cid:19) n p (k)= pk (1−p)n−k . X k (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) probgettingkH’s probgettingn−kT’s numberofcombinations The running index k should go with 0,1,...,n. It starts with 0 because there could be zero heads in the sample space. Furthermore, we note that in this definition, two parameters are driving a binomial random variable: the number of Bernoulli trials n and the underlying probability for each coin flip p. As such, the notation for a binomial random variable is Binomial(n,p), with two arguments. ThehistogramofabinomialrandomvariableisshowninFigure3.28(a).Here,wecon- sidertheexamplewheren=10andp=0.5.Togeneratethehistogram,weuse5000samples. In MATLAB and Python, generating binomial random variables as in Figure 3.28(a) can be done by calling binornd and np.random.binomial. % MATLAB code to generate 5000 Binomial random variables p = 0.5; n = 10; X = binornd(n,p,[5000,1]); [num, ~] = hist(X, 10); bar( num,‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 5000 Binomial random variables import numpy as np import matplotlib.pyplot as plt 144
3.5. COMMON DISCRETE RANDOM VARIABLES 1200 0.25 1000 0.2 800 0.15 600 0.1 400 0.05 200 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.28: An example of a binomial distribution with n=10, p=0.5. p = 0.5 n = 10 X = np.random.binomial(n,p,size=5000) plt.hist(X,bins=‘auto’); Generating the ideal PMF of a binomial random variable as shown in Figure 3.28(b) can be done by calling binopdf in MATLAB. In Python, we can define a random variable rv through stats.binom, and call the PMF using rv.pmf. % MATLAB code to generate a binomial PMF p = 0.5; n = 10; x = 0:10; f = binopdf(x,n,p); stem(x, f, ’o’, ’LineWidth’, 8, ’Color’, [0.8, 0.4, 0.4]); # Python code to generate a binomial PMF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) x = np.arange(11) f = rv.pmf(x) plt.plot(x, f, ’bo’, ms=10); plt.vlines(x, 0, f, colors=’b’, lw=5, alpha=0.5); TheshapeofthebinomialPMFisshowninFigure 3.29.Inthissetoffigures,wevary one of the two parameters n and p while keeping the other fixed. In Figure 3.29(a), we fix n = 60 and plot three sets of p = 0.1,0.5,0.9. For small p the PMF is skewed towards the left, and for large p the PMF is skewed toward the right. Figure 3.29(b) shows the PMF 145
CHAPTER 3. DISCRETE RANDOM VARIABLES for a fixed p = 0.5. As we increase n, the centroid of the PMF moves towards the right. Thus we shouldexpectthe mean ofa binomialrandomvariableto increase withp. Another interestingobservationisthatasnincreases,theshapeofthePMFapproachestheGaussian function (the bell-shaped curve). We will explain the reason for this when we discuss the Central Limit Theorem. 0.4 0.2 n = 5 p = 0.1 0.15 p p = = 0 0. .5 9 0.3 n n = = 5 10 00 0.1 0.2 0.05 0.1 0 0 0 10 20 30 40 50 60 0 10 20 30 40 50 60 (a) n=60 (b) p=0.5 Figure3.29:PMFsofabinomialrandomvariableX ∼Binomial(n,p).(a)Weassumethatn=60.By varyingtheprobabilityp,weseethatthePMFshiftsfromthelefttotheright,andtheshapechanges. (b) We assume that p=0.5. By varying the number of trials, the PMF shifts and the shape becomes more “bell-shaped.” The expectation, second moment, and variance of a binomial random variable are summarized in Theorem 3.7. Theorem 3.7. If X ∼Binomial(n,p), then E[X]=np, E[X2]=np(np+(1−p)), Var[X]=np(1−p). We will prove that E[X]=np using the first principle. For E[X2] and Var[X], we will skip the proofs here and will introduce a “shortcut” later. Proof. Let us start with the definition. n (cid:18) (cid:19) (cid:88) n E[X]= k· pk(1−p)n−k k k=0 n (cid:88) n! = k· pk(1−p)n−k k!(n−k)! k=0 n n! (cid:88) n! =0· p0(1−p)n−0+ k· pk(1−p)n−k 0!(n−0)! k!(n−k)! k=1 (cid:124) (cid:123)(cid:122) (cid:125) 0 n (cid:88) n! = pk(1−p)n−k. (k−1)!(n−k)! k=1 146
3.5. COMMON DISCRETE RANDOM VARIABLES Note that we have shifted the index from k =0 to k =1. Now let us apply a trick: n (cid:88) n! E[X]= pk(1−p)n−k (k−1)!(n−k)! k=1 n (cid:88) n! = pk(1−p)n−k. (k−1)!(n−k−1+1)! k=1 Using this trick, we can show that n (cid:88) n! pk(1−p)n−k (k−1)!(n−k−1+1)! k=1 n (cid:88) n! = pk(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 n (cid:88) n(n−1)! = pk(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 n (cid:88) (n−1)! =np pk−1(1−p)n−k (k−1)!((n−1)−(k−1))! k=1 With a simple substitution of (cid:96)=k−1, the above equation can be rewritten as n−1 (cid:88) (n−1)! E[X]=np· p(cid:96)(1−p)n−1−(cid:96) (cid:96)!((n−1)−(cid:96))! (cid:96)=0 n−1(cid:18) (cid:19) (cid:88) n−1 =np· p(cid:96)(1−p)n−1−(cid:96) =np. k (cid:96)=0 (cid:124) (cid:123)(cid:122) (cid:125) summingPMFofBinomial(n−1,p) (cid:3) In MATLAB, the mean and variance of a binomial random variable can be found by calling the command binostat(n,p) (MATLAB). In Python, the command is rv = stats.binom(n,p) followed by calling rv.stats. % MATLAB code to compute the mean and var of a binomial rv p = 0.5; n = 10; [M,V] = binostat(n, p) # Python code to compute the mean and var of a binomial rv import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) M, V = rv.stats(moments=‘mv’) print(M, V) 147
CHAPTER 3. DISCRETE RANDOM VARIABLES An alternative view of the binomial random variable. As we discussed, the origin of a binomial random variable is the sum of a sequence of Bernoulli random variables. Because ofthisintrinsicdefinition,wecanderivesomeusefulresultsbyexploitingthisfact.Todoso, let us define I ,...,I as a sequence of Bernoulli random variables with I ∼ Bernoulli(p) 1 n j for all i=1,...,n. Then the resulting variable X =I +I +···+I 1 2 n is a binomial random variable of size n and parameter p. Using this definition, we can compute the expectation as follows: E[X]=E[I +I +···+I ] 1 2 n ( =a)E[I ]+E[I ]+···+E[I ] 1 2 n =p+p+···+p =np. Inthisderivation,thestep(a)dependsonausefulfactaboutexpectation(whichwehavenot yetproved):ForanytworandomvariablesX andY,itholdsthatE[X+Y]=E[X]+E[Y]. Therefore, we can show that the expectation of X is np. This line of argument not only simplifies the proof but also provides a good intuition of the expectation. If each coin flip has an expectation of E[I ]=p, then the expectation of the sum should be simply n times i of p, given np. How about the variance? Again, we are going to use a very useful fact about variance: If two random variables X and Y are independent, then Var[X +Y] = Var[X]+Var[Y]. With this result, we can show that Var[X]=Var[I +···+I ] 1 n =Var[I ]+···+Var[I ] 1 n =p(1−p)+···+p(1−p) =np(1−p). Finally, using the fact that Var[X]=E[X2]−µ2, we can show that E[X2]=Var[X]+µ2 =np(1−p)+(np)2. Practice Exercise 3.9. Show that the binomial PMF sums to 1. Solution. We use the binomial theorem to prove this result: n n (cid:18) (cid:19) (cid:88) (cid:88) n p (k)= pk(1−p)n−k =(p+(1−p))n =1. X k k=0 k=0 The CDF of the binomial random variable is not very informative. It is basically the cumulative sum of the PMF: k (cid:18) (cid:19) (cid:88) n F (k)= p(cid:96)(1−p)n−(cid:96). X (cid:96) (cid:96)=0 148
3.5. COMMON DISCRETE RANDOM VARIABLES 0.2 1 0.8 0.15 0.6 0.1 0.4 0.05 0.2 0 0 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Figure 3.30: PMF and CDF of a binomial random variable X ∼Binomial(n,p). The shapes of the PMF and the CDF is shown in Figure 3.30. In MATLAB, plotting the CDF of a binomial can be done by calling the function binocdf. You may also call f = binopdf(x,n,p), and define F = cumsum(f) as the cumu- lativesumofthePMF.InPython,thecorrespondingcommandisrv = stats.binom(n,p) followed by rv.cdf. % MATLAB code to compute the mean and var of a binomial rv x = 0:10; p = 0.5; n = 10; F = binocdf(x,n,p); figure; stairs(x,F,‘.-’,‘LineWidth’,4,‘MarkerSize’,30); # Python code to compute the mean and var of a binomial rv import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats p = 0.5 n = 10 rv = stats.binom(n,p) x = np.arange(11) F = rv.cdf(x) plt.plot(x, F, ’bo’, ms=10); plt.vlines(x, 0, F, colors=’b’, lw=5, alpha=0.5); 3.5.3 Geometric random variable In some applications, we are interested in trying a binary experiment until we succeed. For example, we may want to keep calling someone until the person picks up the call. In this case,therandomvariablecanbedefinedastheoutcomeofmanyfailuresfollowedbyafinal success. This is called the geometric random variable. Definition 3.10. Let X be a geometric random variable. Then, the PMF of X is p (k)=(1−p)k−1p, k =1,2,..., X 149
CHAPTER 3. DISCRETE RANDOM VARIABLES where 0<p<1 is the geometric parameter. We write X ∼Geometric(p) to say that X is drawn from a geometric distribution with a parameter p. A geometric random variable is easy to understand. We define it as Bernoulli trials with k−1 consecutive failures followed by one success. This can be seen from the definition: p (k)= (1−p)k−1 p . X (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) k−1failures finalsuccess Note that in geometric random variables, there is no (cid:0)n(cid:1) because we must have k − 1 k consecutivefailuresbeforeonesuccess.Thereisnoalternativecombinationofthesequence. ThehistogramandPMFofageometricrandomvariableareillustratedinFigure3.31. Here, we assume that p=0.5. 3000 0.5 2500 0.4 2000 0.3 1500 0.2 1000 0.1 500 0 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.31: An example of a geometric distribution with p=0.5. In MATLAB, generating geometric random variables can be done by calling the com- mands geornd. In Python, it is np.random.geometric. % MATLAB code to generate 1000 geometric random variables p = 0.5; X = geornd(p,[5000,1]); [num, ~] = hist(X, 0:10); bar(0:10, num, ‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 1000 geometric random variables import numpy as np import matplotlib.pyplot as plt p = 0.5 X = np.random.geometric(p,size=1000) plt.hist(X,bins=‘auto’); To generate the PMF plots, in MATLAB we call geopdf and in Python we call rv = stats.geom followed by rv.pmf. 150
3.5. COMMON DISCRETE RANDOM VARIABLES % MATLAB code to generate geometric PMF p = 0.5; x = 0:10; f = geopdf(x,p); stem(x, f, ‘o’, ‘LineWidth’, 8, ‘Color’, [0.8, 0.4, 0.4]); # Python code to generate 1000 geometric random variables import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.arange(1,11) rv = stats.geom(p) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=8, label=‘geom pmf’) plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5) Practice Exercise 3.10. Show that the geometric PMF sums to one. Solution. We can apply infinite series to show the result: ∞ ∞ (cid:88) (cid:88) p (k)= (1−p)k−1p X k=1 k=1 ∞ (cid:88) =p· (1−p)k−1, (cid:96)=k−1 k=1 ∞ (cid:88) =p· (1−p)(cid:96) (cid:96)=0 1 =p· =1. 1−(1−p) ItisinterestingtocomparetheshapeofthePMFsforvariousvaluesofp.InFigure3.32 we show the PMFs. We vary the parameter p = 0.25,0.5,0.9. For small p, the PMF starts with a low value and decays at a slow speed. The opposite happens for a large p, where the PMF starts with a high value and decays rapidly. Furthermore, we can derive the following properties of the geometric random variable. Theorem 3.8. If X ∼Geometric(p), then 1 2 1 E[X]= , E[X2]= − , (3.16) p p2 p 1−p Var[X]= . p2 Proof. We will prove that the mean is 1/p and leave the second moment and variance as 151
CHAPTER 3. DISCRETE RANDOM VARIABLES 1 1 1 p = 0.25 p = 0.5 p = 0.9 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 0.2 0.2 0 0 0 0 2 4 6 8 0 2 4 6 8 0 2 4 6 8 Figure 3.32: PMFs of a geometric random variable X ∼Geometric(p). an exercise. ∞ (cid:32) ∞ (cid:33) (cid:18) (cid:19) E[X]=(cid:88) kp(1−p)k−1 =p (cid:88) k(1−p)k−1 ( =a) p 1 = 1 , (1−(1−p))2 p k=1 k=1 where (a) follows from the infinite series identity in Chapter 1. (cid:3) 3.5.4 Poisson random variable In many physical systems, the arrivals of events are typically modeled as a Poisson ran- dom variable, e.g., photon arrivals, electron emissions, and telephone call arrivals. In social networks, the number of conversations per user can also be modeled as a Poisson. In e- commerce, the number of transactions per paying user is again modeled using a Poisson. Definition 3.11. Let X be a Poisson random variable. Then, the PMF of X is λk p (k)= e−λ, k =0,1,2,..., X k! where λ>0 is the Poisson rate. We write X ∼Poisson(λ) to say that X is drawn from a Poisson distribution with a parameter λ. In this definition, the parameter λ determines the rate of the arrival. The histogram and PMF of a Poisson random variable are illustrated in Figure 3.33. Here, we assume that λ=1. TheMATLABcodeandPythoncodeusedtogeneratethehistogramareshownbelow. % MATLAB code to generate 5000 Poisson numbers lambda = 1; X = poissrnd(lambda,[5000,1]); 152
3.5. COMMON DISCRETE RANDOM VARIABLES 2000 0.4 1500 0.3 1000 0.2 500 0.1 0 0 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 10 (a) Histogram based on 5000 samples (b) PMF Figure 3.33: An example of a Poisson distribution with λ=1. [num, ~] = hist(X, 0:10); bar(0:10, num, ‘FaceColor’,[0.4, 0.4, 0.8]); # Python code to generate 5000 Poisson random variables import numpy as np import matplotlib.pyplot as plt lambd = 1 X = np.random.poisson(lambd,size=5000) plt.hist(X,bins=‘auto’); For the PMF, in MATLAB we can call poisspdf, and in Python we can call rv.pmf with rv = stats.poisson. % MATLAB code to plot the Poisson PMF lambda = 1; x = 0:10; f = poisspdf(x,lambda); stem(x, f, ‘o’, ‘LineWidth’, 8, ‘Color’, [0.8, 0.4, 0.4]); # Python code to plot the Poisson PMF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.arange(0,11) rv = stats.poisson(lambd) f = rv.pmf(x) plt.plot(x, f, ‘bo’, ms=8, label=‘geom pmf’) plt.vlines(x, 0, f, colors=‘b’, lw=5, alpha=0.5) The shape of the Poisson PMF changes with λ. As illustrated in Figure 3.34, p (k) is X more concentrated at lower values for smaller λ and becomes spread out for larger λ. Thus, we should expect that the mean and variance of a Poisson random variable will change 153
CHAPTER 3. DISCRETE RANDOM VARIABLES together as a function of λ. In the same figure, we show the CDF of a Poisson random variable. The CDF of a Poisson is (cid:88)k λ(cid:96) F (k)=P[X ≤k]= e−λ. (3.17) X (cid:96)! (cid:96)=0 0.4 1 = 1 = 4 = 10 0.8 0.3 0.6 0.2 0.4 0.1 0.2 = 1 = 4 = 10 0 0 0 5 10 15 20 0 5 10 15 20 Figure 3.34: A Poisson random variable using different λ’s. [Left] Probability mass function p (k). X [Right] Cumulative distribution function F (k). X Example 3.18.LetX beaPoissonrandomvariablewithparameterλ.FindP[X >4] and P[X ≤5]. Solution. (cid:88)4 λk P[X >4]=1−P[X ≤4]=1− e−λ, k! k=0 (cid:88)5 λk P[X ≤5]= e−λ. k! k=0 Practice Exercise 3.11. Show that the Poisson PMF sums to 1. Solution. We use the exponential series to prove this result: (cid:88)∞ (cid:88)∞ λk (cid:88)∞ λk p (k)= e−λ =e−λ· =1. X k! k! k=0 k=0 k=0 (cid:124) (cid:123)(cid:122) (cid:125) =eλ Poisson random variables in practice (1) Computational photography. In computational photography, the Poisson random vari- able is one of the most widely used models for photon arrivals. The reason pertains to the 154
3.5. COMMON DISCRETE RANDOM VARIABLES originofthePoissonrandomvariable,whichwewilldiscussshortly.Whenphotonsareemit- ted from the source, they travel through the medium as a sequence of independent events. During the integration period of the camera, the photons are accumulated to generate a voltage that is then translated to digital bits. Figure 3.35: The Poisson random variable can be used to model photon arrivals. If we assume that the photon arrival rate is α (photons per second), and suppose that thetotalamountofintegrationtimeist,thentheaveragenumberofphotonsthatthesensor can see is αt. Let X be the number of photons seen during the integration time. Then if we follow the Poisson model, we can write down the PMF of X: (αt)k P[X =k]= e−αt. k! Therefore, if a pixel is bright, meaning that α is large, then X will have a higher likelihood of landing on a large number. (2) Traffic model. The Poisson random variable can be used in many other problems. For example, we can use it to model the number of passengers on a bus or the number of spam phone calls. The required modification to Figure 3.35 is almost trivial: merely replace the photons with your favorite cartoons, e.g., a person or a phone, as shown in Figure 3.36. In theUnitedStates,shared-rideservicessuchasUberandLyftneedtomodelthevacantcars andthepassengers.Aslongastheyhaveanarrivalrateandcertaindegreesofindependence between events, the Poisson random variable will be a good model. As you can see from these examples, the Poisson random variable has broad applica- bility. Before we continue our discussion of its applications, let us introduce a few concepts related to the Poisson random variable. Properties of a Poisson random variable We now derive the mean and variance of a Poisson random variable. Theorem 3.9. If X ∼Poisson(λ), then E[X]=λ, E[X2]=λ+λ2, (3.18) Var[X]=λ. 155
CHAPTER 3. DISCRETE RANDOM VARIABLES Figure 3.36: The Poisson random variable can be used to model passenger arrivals and the number of phone calls, and can be used by Uber or Lyft to provide shared rides. Proof. Let us first prove the mean. It can be shown that (cid:88)∞ λk (cid:88)∞ λk E[X]= k· e−λ = e−λ k! (k−1)! k=0 k=1 (cid:88)∞ λk−1 (cid:88)∞ λ(cid:96) =λe−λ =λe−λ =λe−λeλ =λ. (k−1)! (cid:96)! k=1 (cid:96)=0 The second moment can be computed as (cid:88)∞ λk E[X2]= k2· e−λ k! k=0 (cid:88)∞ λk = k· e−λ (k−1)! k=0 (cid:88)∞ λk = (k−1+1)· e−λ (k−1)! k=0 (cid:88)∞ λk (cid:88)∞ λk = (k−1)· e−λ+ e−λ (k−1)! (k−1)! k=1 k=1 (cid:88)∞ λk−2e−λ (cid:88)∞ λk−1e−λ =λ2· +λ· . (k−2)! (k−1)! k=2 k=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =1 =1 The variance can be computed using Var[X]=E[X2]−µ2. (cid:3) TocomputethemeanandvarianceofaPoissonrandomvariable,wecancallpoisstat in MATLAB and rv.stats(moments=‘mv’) in Python. % MATLAB code to compute Poisson statistics lambda = 1; [M,V] = poisstat(lambda); 156
3.5. COMMON DISCRETE RANDOM VARIABLES # Python code to compute Poisson statistics import scipy.stats as stats lambd = 1 rv = stats.poisson(lambd) M, V = rv.stats(moments=’mv’) ThePoissonrandomvariableisspecialinthesensethatthemeanandthevarianceare equal. That is, if the mean arrival number is higher, the variance is also higher. This is very differentfromsomeotherrandomvariables,e.g.,thenormalrandomvariablewherethemean andvarianceareindependent.Forcertainengineeringapplicationssuchasphotography,this playsanimportantroleindefiningthesignal-to-noiseratio.Wewillcomebacktothispoint later. Origin of the Poisson random variable We now address one of the most important questions about the Poisson random variable: Where does it come from? Answering this question is useful because the derivation process will reveal the underlying assumptions that lead to the Poisson PMF. When you change the problem setting, you will know when the Poisson PMF will hold and when the Poisson PMF will fail. Our approach to addressing this problem is to consider the photon arrival process. (As we have shown, there is conceptually no difference if you replace the photons with pedestrians,passengers,orphonecalls.)OurderivationfollowstheargumentofJ.Goodman, Statistical Optics, Section 3.7.2. To begin with, we consider a photon arrival process. The total number of photons observed over an integration time t is defined as X(t). Because X(t) is a Poisson random variable, its arguments must be integers. The probability of observing X(t)=k is therefore P[X(t)=k]. Figure 3.37 illustrates the notations and concepts. Figure 3.37: Notations for deriving the Poisson PMF. We propose three hypotheses with the photon arrival process: • For sufficiently small ∆t, the probability of a small impulse occurring in the time interval [t,t+∆t] is equal to the product of ∆t and the rate λ, i.e., P[X(t+∆t)−X(t)=1]=λ∆t. This is a linearity assumption, which typically holds for a short duration of time. 157
CHAPTER 3. DISCRETE RANDOM VARIABLES • For sufficiently small ∆t, the probability that more than one impulse falls in ∆t is negligible. Thus, we have that P[X(t+∆t)−X(t)=0]=1−λ∆t. • The number of impulses in non-overlapping time intervals is independent. The significance of these three hypotheses is that if the underlying photon arrival process violates any of these assumptions, then the Poisson PMF will not hold. One example is the presence of scattering effects, where a photon has a certain probability of going off due to thescatteringmediumandacertainprobabilityofcomingback.Inthiscase,theeventswill no longer be independent. Assumingthatthesehypotheseshold,thenattimet+∆t,theprobabilityofobserving X(t+∆t)=k can be computed as P[X(t+∆t)=k] =P[X(t)=k]· (1−λ∆t) + P[X(t)=k−1]· (λ∆t) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =P[X(t+∆t)−X(t)=0] =P[X(t+∆t)−X(t)=1] =P[X(t)=k]−P[X(t)=k]λ∆t+P[X(t)=k−1]λ∆t. By rearranging the terms we show that P[X(t+∆t)=k]−P[X(t)=k] (cid:18) (cid:19) =λ P[X(t)=k−1]−P[X(t)=k] . ∆t Setting the limit of ∆t→0, we arrive at an ordinary differential equation (cid:18) (cid:19) d P[X(t)=k]=λ P[X(t)=k−1]−P[X(t)=k] . (3.19) dt We claim that the Poisson PMF, i.e., (λt)k P[X(t)=k]= e−λt, k! wouldsolvethisdifferentialequation.Toseethis,wesubstitutethePMFintotheequation. The left-hand side gives us d d (cid:18) (λt)k (cid:19) P[X(t)=k]= e−λt dt dt k! (λt)k−1 (λt)k =λk e−λt+(−λ) e−λt k! k! (λt)k−1 (λt)k =λ e−λt−λ e−λt (k−1)! k! (cid:18) (cid:19) =λ P[X(t)=k−1]−P[X(t)=k] , which is the right-hand side of the equation. To retrieve the basic form of Poisson, we can just set t=1 in the PMF so that λk P[X(1)=k]= e−λ. k! 158
3.5. COMMON DISCRETE RANDOM VARIABLES The origin of Poisson random variables • We assume independent arrivals. • Probability of seeing one event is linear with the arrival rate. • Time interval is short enough so that you see either one event or no event. • Poisson is derived by solving a differential equation based on these assumptions. • Poisson becomes invalid when these assumptions are violated, e.g., in the case of scattering of photons due to turbid medium. There is an alternative approach to deriving the Poisson PMF. The idea is to drive the parameter n in the binomial random variable to infinity while pushing p to zero. In this limit, the binomial PMF will converge to the Poisson PMF. We will discuss this shortly. However, we recommend the physics approach we have just described because it has a rich meaning and allows us to validate our assumptions. Poisson approximation to binomial We present one additional result about the Poisson random variable. The result shows that Poisson can be regarded as a limiting distribution of a binomial random variable. Theorem 3.10. (Poisson approximation to binomial). For small p and large n, (cid:18) n(cid:19) λk pk(1−p)n−k ≈ e−λ, k k! def where λ = np. Before we prove the result, let us see how close the approximation can be. In Figure 3.38, we show a binomial distribution and a Poisson approximation. The closeness of the approx- imation can easily be seen. In MATLAB, the code to approximate a binomial distribution with a Poisson formula is shown below. Here, we draw 10,000 random binomial numbers and plot their histogram. Ontopoftheplot,weusepoisspdftocomputethePoissonPMF.ThisgivesusFigure3.38. A similar set of commands can be called in Python. % MATLAB code to approximate binomial using Poisson n = 1000; p = 0.05; X = binornd(n,p,[10000,1]); t = 0:100; [num,val] = hist(X,t); lambda = n*p; f_pois = poisspdf(t,lambda); bar(num/10000,‘FaceColor’,[0.9 0.9 0],‘BarWidth’,1); hold on; plot(f_pois, ‘LineWidth’, 4); 159
CHAPTER 3. DISCRETE RANDOM VARIABLES 0.06 0.04 0.02 0 ytilibaborP Binomial, n = 5000, p = 0.01 Poisson, = 50 0 20 40 60 80 100 120 k Figure 3.38: Poisson approximation of binomial distribution. # Python code to approximate binomial using Poisson import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats n = 1000; p = 0.05 rv1 = stats.binom(n,p) X = rv1.rvs(size=10000) plt.figure(1); plt.hist(X,bins=np.arange(0,100)); rv2 = stats.poisson(n*p) f = rv2.pmf(bin) plt.figure(2); plt.plot(f); Proof. Let λ=np. Then, (cid:18) n(cid:19) n! (cid:18) λ(cid:19)k(cid:18) λ(cid:19)n−k pk(1−p)n−k = 1− k k!(n−k)! n n λk n(n−1)···(n−k+1)(cid:18) λ(cid:19)n−k = 1− k! n·n···n n λk (cid:18) 1(cid:19) (cid:18) k−1(cid:19)(cid:18) λ(cid:19)−k(cid:18) λ(cid:19)n = (1) 1− ··· 1− 1− 1− k! n n n n (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) →1asn→∞ →1asn→∞ λk (cid:18) λ(cid:19)n = 1− . k! n We claim that (cid:0) 1− λ(cid:1)n →e−λ. This can be proved by noting that n log(1+x)≈x, x(cid:28)1. It then follows that log(cid:0) 1− λ(cid:1) ≈−λ. Hence, (cid:0) 1− λ(cid:1)n ≈e−λ n n n (cid:3) 160
3.5. COMMON DISCRETE RANDOM VARIABLES Example 3.19.Consideranopticalcommunicationsystem.Thebitarrivalrateis109 bits/sec, and the probability of having one error bit is 10−9. Suppose we want to find the probability of having five error bits in one second. Let X be the number of error bits. In one second there are 109 bits. Since we do not know the location of these 5 bits, we have to enumerate all possibilities. This leads to a binomial distribution. Using the binomial distribution, we know that the probability of having k error bits is (cid:18) (cid:19) n P[X =k]= pk(1−p)n−k k (cid:18) 109(cid:19) = (10−9)k(1−10−9)109−k. k This quantity is difficult to calculate in floating-point arithmetic. UsingthePoissontobinomialapproximation,wecanseethattheprobabilitycan be approximated by λk P[X =k]≈ e−λ, k! where λ=np=109(10−9)=1. Setting k =5 yields P[X =5]≈0.003. Photon arrival statistics Poisson random variables are useful in computer vision, but you may skip this discussion if it is your first reading of the book. The strong connection between Poisson statistics and physics makes the Poisson ran- dom variable a very good fit for many physical experiments. Here we demonstrate an appli- cation in modeling photon shot noise. Animagesensorisaphotonsensitivedevicewhichisusedtodetectincomingphotons. In the simplest setting, we can model a pixel in the object plane as X , for some 2D m,n coordinate [m,n] ∈ R2. Written as an array, an M ×N image in the object plane can be visualized as   X X ··· X 1,1 1,2 1,N X =object=  . . . . . . ... . . .  . X X ··· X M,1 M,2 M,N Without loss of generality, we assume that X is normalized so that 0 ≤ X ≤ 1 for m,n m,n every coordinate [m,n]. To model the brightness, we multiply X by a scalar α > 0. If m,n a pixel αX has a large value, then it is a bright pixel; conversely, if αX has a small m,n m,n value, then it is a dark pixel. At a particular pixel location [m,n]∈R2, the observed pixel valueY isarandomvariablefollowingthePoissonstatistics.Thissituationisillustrated m,n 161
CHAPTER 3. DISCRETE RANDOM VARIABLES in Figure 3.39, where we see that an object-plane pixel will generate an observed pixel through the Poisson PMF.1 Figure 3.39: The image formation process is governed by the Poisson random variable. Given a pixel in the object plane X , the observed pixel Y is a Poisson random variable with mean αX . m,n m,n m,n Therefore, a brighter pixel will have a higher Poisson mean, whereas a darker pixel will have a lower Poisson mean. Written as an array, the image is Y =observed image (cid:26) (cid:27) =Poisson αX   Poisson{αX } Poisson{αX } ··· Poisson{αX } 1,1 1,2 1,N Poisson{αX 2,1} Poisson{αX 2,2} ··· Poisson{αX 2,N} =   . . . . . . ... . . .   . Poisson{αX } Poisson{αX } ··· Poisson{αX } M,1 M,2 M,N Here, by Poisson{αX } we mean that Y is a random integer with probability mass m,n m,n [αX ]k P[Y =k]= m,n e−αXm,n. m,n k! Note that this model implies that the images seen by our cameras are more or less an array of Poisson random variables. (We say “more or less” because of other sources of uncertainties such as read noise, dark current, etc.) Because the observed pixels Y are m,n random variables, they fluctuate about the mean values, and hence they are noisy. We refer to this type of random fluctuation as the shot noise. The impact of the shot noise can be seeninFigure 3.40.Here,wevarythesensorgainlevelα.Weseethatforsmallαtheimage is dark and has much random fluctuation. As α increases, the image becomes brighter and the fluctuation becomes smaller. In MATLAB, simulating the Poisson photon arrival process for an image requires the image-processingtoolbox.Thecommandtoreadanimageisimread.Dependingonthedata type, the input array could be unit8 integers. To convert them to floating-point numbers between0and1,weusethecommandim2double.DrawingPoissonmeasurementsfromthe clean image is done using poissrnd. Finally, we can use imshow to display the image. 1Thecolorofanimageisoftenhandledbyacolor filter array,whichcanbethoughtofasawavelength selectorthatallowsaspecificwavelengthtopassthrough. 162
3.5. COMMON DISCRETE RANDOM VARIABLES = 10 = 100 = 1000 Figure 3.40: Illustration of the Poisson random variable in photographing images. Here, α denotes the gain level of the sensor: Larger α means that there are more photons coming to the sensor. % MATLAB code to simulate a photon arrival process x0 = im2double(imread(’cameraman.tif’)); X = poissrnd(10*x0); figure(1); imshow(x0, []); figure(2); imshow(X, []); Similar commands can be found in Python with the help of the cv2 library. When reading an image, we call cv2.imread. The option 0 is used to read a gray-scale image; otherwise, we will have a 3-channel color image. The division /255 ensures that the input array ranges between 0 to 1. Generating the Poisson random numbers can be done using np.random.poisson, or by calling the statistics library with stats.poisson.rvs(10*x0). Todisplaytheimages,wecallplt.imshow,withthecolormapoptionsettocmap = ’gray’. # Python code code to simulate a photon arrival process import numpy as np import matplotlib.pyplot as plt import cv2 x0 = cv2.imread(’./cameraman.tif’, 0)/255 plt.figure(1); plt.imshow(x0,cmap=’gray’); X = np.random.poisson(10*x0) plt.figure(2); plt.imshow(X, cmap=’gray’); Why study Poisson? What is shot noise? • The Poisson random variable is used to model photon arrivals. • Shot noise is the random fluctuation of the photon counts at the pixels. Shot noise is present even if you have an ideal sensor. Signal-to-noise ratio of Poisson Now let us answer a question we asked before. A Poisson random variable has a variance equal to the mean. Thus, if the scene is brighter, the variance will be larger. How come our 163
CHAPTER 3. DISCRETE RANDOM VARIABLES simulation in Figure 3.40 shows that the fluctuation becomes smaller as the scene becomes brighter? The answer to this question lies in the signal-to-noise ratio (SNR) of the Poisson random variable. The SNR of an image defines its quality. The higher the SNR, the better the image. The mathematical definition of SNR is the ratio between the signal power and the noise power. In our case, the SNR is signal power SNR= noise power E[Y] def = (cid:112) Var[Y] (a) λ √ = √ = λ, λ whereY =Y isoneoftheobservedpixelsandλ=αX isthethecorrespondingobject m,n m,n pixel. In this equation, the step (a) uses the properties of the Poisson random variable Y √ where E[Y] = Var[Y] = λ. The result SNR = λ is very informative. It says that if the √ underlying mean photon flux (which is λ) increases, the SNR increases at a rate of λ. So, yes, the variance becomes larger when the scene is brighter. However, the gain in signal E[Y] overrides the gain in noise (cid:112) Var[Y]. As a result, the big fluctuation in bright images is compensated by the strong signal. Thus, to minimize the shot noise one has to use a longer exposure to increase the mean photon flux. When the scene is dark and the aperture is small, shot noise is unavoidable. Poissonmodelingisusefulfordescribingtheproblem.However,theactualengineering questionisthat,givenanoiseobservationY ,howwouldyoureconstructthecleanimage m,n X ? This is a very difficult inverse problem. The typical strategy is to exploit the spatial m,n correlations between nearby pixels, e.g., usually smooth except along some sharp edges. Other information about the image, e.g., the likelihood of obtaining texture patterns, can alsobeleveraged.Modernimage-processingmethodsarerich,rangingfromclassicalfiltering techniquestodeepneuralnetworks.Staticimagesareeasiertorecoverbecausewecanoften leverage multiple measurements of the same scene to boost the SNR. Dynamic scenes are substantiallyharderwhenweneedtotrackthemotionofanyunderlyingobjects.Thereare alsonewerimagesensorswithbetterphotonsensitivity.Theproblemofimaginginthedark is an important research topic in computational imaging. New solutions are developed at the intersection of optics, signal processing, and machine learning. The end of our discussions on photon statistics. 164
3.6. SUMMARY 3.6 Summary Arandomvariableissocalledbecauseitcantakemorethanonestate.Theprobabilitymass function specifies the probability for it to land on a particular state. Therefore, whenever you think of a random variable you should immediately think of its PMF (or histogram if you prefer). The PMF is a unique characterization of a random variable. Two random variables with the same PMF are effectively the same random variables. (They are not identical because there could be measure-zero sets where the two differ.) Once you have the PMF, you can derive the CDF, expectation, moments, variance, and so on. When your boss hands a dataset to you, which random variable (which model) should you use? This is a very practical and deep question. We highlight three steps for you to consider: • (i) Model selection: Which random variable is the best fit for our problem? Some- timesweknowbyphysicsthat,forexample,photonarrivalsorinternettrafficfollowa Poisson random variable. However, not all datasets can be easily described by simple models. The models we have learned in this chapter are called the parametric mod- els because they are characterized by one or two parameters. Some datasets require nonparametric models, e.g., natural images, because they are just too complex. Some data scientists refer to deep neural networks as parametric models because the net- work weights are essentially the parameters. Some do not because when the number of parameters is on the order of millions, sometimes even more than the number of training samples, it seems more reasonable to call these models nonparametric. How- ever, putting this debate aside, shortlisting a few candidate models based on prior knowledge is essential. Even if you use deep neural networks, selecting between con- volutional structures versus long short-term memory models is still a legitimate task that requires an understanding of your problem. • (ii) Parameter estimation: Suppose that you now have a candidate model; the next taskistoestimatethemodelparameterusingtheavailabletrainingdata.Forexample, forPoissonweneedtodetermineλ,andforbinomialweneedtodetermine(n,p).The estimationproblemisaninverseproblem.OftenweneedtousethePMFtoconstruct certain optimization problems. By solving the optimization problem we will find the best parameter (for that particular candidate model). Modern machine learning is doing significantly better now than in the old days because optimization methods have advanced greatly. • (iii) Validation. When each candidate model has been optimized to best fit the data, we still need to select the best model. This is done by running various testings. For example, we can construct a validation set and check which model gives us the best performance (such as classification rate or regression error). However, a model with thebestvalidationscoreisnotnecessarilythebestmodel.Yourgoalshouldbetoseek agoodmodelandnotthebestmodelbecausedeterminingthebestrequiresaccessto the testing data, which we do not have. Everything being equal, the common wisdom is to go with a simpler model because it is generally less susceptible to overfitting. 165
CHAPTER 3. DISCRETE RANDOM VARIABLES 3.7 References Probability textbooks 3-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 2. 3-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 3. 3-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 3 and 4. 3-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 2 and3. 3-5 Sheldon Ross, A First Course in Probability, Prentice Hall, 8th Edition, 2010. Chap- ter 4. 3-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 2 and 4. Advanced probability textbooks 3-7 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 3-8 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) Cross-validation 3-9 Larry Wasserman, All of Statistics, Springer 2004. Chapter 20. 3-10 Mats Rudemo, “Empirical Choice of Histograms and Kernel Density Estimators,” Scandinavian Journal of Statistics, Vol. 9, No. 2 (1982), pp. 65-78. 3-11 DavidW.Scott,MultivariateDensityEstimation:Theory,Practice,andVisualization, Wiley, 1992. Poisson statistics 3-12 Joseph Goodman, Statistical Optics, Wiley, 2015. Chapter 3. 3-13 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Section 1.10. 166
3.8. PROBLEMS 3.8 Problems Exercise 1. (Video Solution) Consider an information source that produces numbers k in the set S = {1,2,3,4}. Find X and plot the PMF in the following cases: (a) p =p /k, for k =1,2,3,4. Hint: Find p . k 1 1 (b) p =p /2 for k =1,2,3. k+1 k (c) p =p /2k for k =1,2,3. k+1 k (d) Can the random variables in parts (a)-(c) be extended to take on values in the set {1,2,...}?Whyorwhynot?Hint:Youmayusethefactthattheseries1+1+1+··· 2 3 diverges. Exercise 2. (Video Solution) Two dice are tossed. Let X be the absolute difference in the number of dots facing up. (a) Find and plot the PMF of X. (b) Find the probability that X ≤2. (c) Find E[X] and Var[X]. Exercise 3. (Video Solution) Let X be a random variable with PMF p =c/2k for k =1,2,.... k (a) Determine the value of c. (b) Find P(X >4) and P(6≤X ≤8). (c) Find E[X] and Var[X]. Exercise 4. Let X be a random variable with PMF p =c/2k for k =−1,0,1,2,3,4,5. k (a) Determine the value of c. (b) Find P(1≤X <3) and P(1<X ≤5). (c) Find P[X3 <5]. (d) Find the PMF and the CDF of X. 167
CHAPTER 3. DISCRETE RANDOM VARIABLES Exercise 5. (Video Solution) A modem transmits a +2 voltage signal into a channel. The channel adds to this sig- nal a noise term that is drawn from the set {0,−1,−2,−3} with respective probabilities {4/10,3/10,2/10,1/10}. (a) Find the PMF of the output Y of the channel. (b) Whatistheprobabilitythatthechannel’soutputisequaltotheinputofthechannel? (c) What is the probability that the channel’s output is positive? (d) Find the expected value and variance of Y. Exercise 6. On a given day, your golf score takes values from numbers 1 through 10, with equal proba- bility of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let X , X , and X be the scores that you get, and 1 2 3 let X be the minimum of these three numbers. (a) Show that for any discrete random variable X, p (k)=P(X >k−1)−P(X >k). X (b) What is the probability P(X >k) for k =1,...,10? 1 (c) Use (a), determine the PMF p (k), for k =1,...,10. X (d) What is the average score improvement if you play just for one day compared with playing for three days and taking the minimum? Exercise 7. (Video Solution) Let (cid:40) (cid:40) 1, if X >10 X−10, if X−10>0 g(X)= and h(X)= 0, otherwise. 0, otherwise. (a) Find E[g(X)] for X as in Problem 1(a) with S ={1,...,15}. X (b) Find E[h(X)] for X as in Problem 1(b) with S ={1,...,15}. X Exercise 8. (Video Solution) A voltage X is uniformly distributed in the set {−3,...,3,4}. (a) Find the mean and variance of X. (b) Find the mean and variance of Y =−2X2+3. (c) Find the mean and variance of W =cos(πX/8). (d) Find the mean and variance of Z =cos2(πX/8). Exercise 9. (Video Solution) (a) If X is Poisson(λ), compute E[1/(X+1)]. 168
3.8. PROBLEMS (b) If X is Bernoulli(p) and Y is Bernoulli(q), compute E[(X + Y)3] if X and Y are independent. (c) Let X be a random variable with mean µ and variance σ2. Let ∆(θ) = E[(X −θ)2]. Find θ that minimizes the error ∆(θ). (d) SupposethatX ,...,X areindependentuniformrandomvariablesin{0,1,...,100}. 1 n Evaluate P[min(X ,...,X )>(cid:96)] for any (cid:96)∈{0,1,...,100}. 1 n Exercise 10. (Video Solution) (a) Consider the binomial probability mass function p (k)=(cid:0)n(cid:1) pk(1−p)n−k. Show that X k the mean is E[X]=np. (b) Consider the geometric probability mass function p (k) = p(1−p)k for k = 0,1,.... X Show that the mean is E[X]=(1−p)/p. (c) Consider the Poisson probability mass function p (k) = λke−λ. Show that the vari- X k! ance is Var[X]=λ. (d) Considertheuniformprobabilitymassfunctionp (k)= 1 fork =1,...,L.Showthat X L the variance is Var[X]= L2−1. Hint: 1+2+···+n= n(n+1) and 12+22+···+n2 = 12 2 n3 + n2 + n. 3 2 6 Exercise 11. (Video Solution) Anaudioplayerusesalow-qualityharddrive.Theprobabilitythattheharddrivefailsafter being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. Theinitialcostofbuildingtheplayeris$50,andthemanufactureroffersa1-yearwarranty. Within one year, the customer can ask for a free repair up to 12 times. (a) Let X be the number of months when the player fails. What is the PMF of X? Hint: P[X = 1] may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying X = 1 is equivalent to saying that there is only one failure in the entire 12-month period. (b) What is the average cost per player? Exercise 12. (Video Solution) A binary communication channel has a probability of bit error of p = 10−6. Suppose that transmission occurs in blocks of 10,000 bits. Let N be the number of errors introduced by the channel in a transmission block. (a) What is the PMF of N? (b) Find P[N =0] and P[N ≤3]. (c) For what value of p will the probability of 1 or more errors in a block be 99%? 169
CHAPTER 3. DISCRETE RANDOM VARIABLES Hint: Use the Poisson approximation to binomial random variables. Exercise 13. (Video Solution) The number of orders waiting to be processed is given by a Poisson random variable with parameter α=λ/nµ, where λ is the average number of orders that arrive in a day, µ is the number of orders that an employee can process per day, and n is the number of employees. Let λ = 5 and µ = 1. Find the number of employees required so the probability that more than four orders are waiting is less than 10%. Hint: You need to use trial and error for a few n’s. Exercise 14. LetX bethenumberofphotonscountedbyareceiverinanopticalcommunicationsystem. ItisknownthatX isaPoissonrandomvariablewitharateλ whenasignalispresentanda 1 Poissonrandomvariablewiththerateλ <λ whenasignalisabsent.Theprobabilitythat 0 1 the signal is present is p. Suppose that we observe X =k photons. We want to determine a threshold T such that if k ≥ T we claim that the signal is present, and if k < T we claim that the signal is absent. What is the value of T? 170
Chapter 4 Continuous Random Variables If you are coming to this chapter from Chapter 3, we invite you to take a 30-second pause and switch your mind from discrete events to continuous events. Everything is continuous now. The sample space is continuous, the event space is continuous, and the probability measure is continuous. Continuous random variables are similar in many ways to discrete random variables. They are characterized by the probability density functions (the continu- ous version of the probability mass functions); they have cumulative distribution functions; theyhavemeans,moments,andvariances.Themostsignificantdifferenceisperhapstheuse of integration instead of summation, but this change is conceptually straightforward, aside from the difficulties associated with integrating functions. So why do we need a separate chapter for continuous random variables? There are several reasons. • First,howwouldyoudefinetheprobabilityofacontinuousevent?Notethatwecannot count because a continuous event is uncountable. There is also nothing called the probability mass because there are infinitely many masses. To define the probability of continuous events, we need to go back to our “slogan”: probability is a measure of the size of a set. Because probability is a measure, we can speak meaningfully about the probability of continuous events so long as we have a well-defined measure for them. Defining such a measure requires some effort. We will develop the intuitions andtheformaldefinitionsinSection4.1.InSection4.2,wewilldiscusstheexpectation and variance of continuous random variables. • The second challenge is the unification between continuous and discrete random vari- ables. Since the two types of random variables ultimately measure the size of a set, it is natural to ask whether we can unify them. Our approach to unifying them is based on the cumulative distribution functions (CDFs), which are well-defined functions for discrete and continuous random variables. Based on the CDF and the fundamental theorem of calculus, we can show that the probability density functions and proba- bility mass functions can be derived from the derivative of the CDFs. These will be discussed in Section 4.3, and in Section 4.4 we will discuss some additional results about the mode and median. • Thethirdchallengeistounderstandseveralwidelyusedcontinuousrandomvariables. We will discuss the uniform random variable and the exponential random variable in Section 4.5. Section 4.6 deals with the important topic of the Gaussian random variable.WheredoesaGaussianrandomvariablecomefrom?Whydoesithaveabell 171
CHAPTER 4. CONTINUOUS RANDOM VARIABLES shape? Why are Gaussian random variables so popular in data science? What are the useful properties of Gaussian random variables? What are the relationships between a Gaussian random variable and other random variables? These important questions will be answered in Section 4.6. • Thefinalchallengeisthetransformationofrandomvariables.Imaginethatyouhavea randomvariableX andafunctiong.Whatwilltheprobabilitymass/densityfunction ofg(X)be?Addressingthisproblemisessentialbecausealmostallpracticalengineer- ing problems involve the transformation of random variables. For example, suppose wehavevoltagemeasurementsandwewouldliketocomputethepower.Thisrequires taking the square of the voltage. We will discuss the transformation in Section 4.7, and we will also discuss an essential application in generating random numbers in Section 4.8. 4.1 Probability Density Function 4.1.1 Some intuitions about probability density functions Let’s begin by outlining some intuitive reasoning, which is needed to define the probability of continuous events properly. These intuitions are based on the fact that probability is a measure. In the following discussion you will see a sequence of logical arguments for con- structingsuchameasureforcontinuousevents.SomeargumentsarediscussedinChapter2, but now we place them in the context of continuous random variables. Suppose we are given an event A that is a subset in the sample space Ω, as illustrated in Figure 4.1. In order to calculate the probability of A, the measure perspective suggests that we consider the relative size of the set “size” of A P[{x∈A}]= . “size” of Ω The right-hand side of this equation captures everything about the probability: It is a measure of the size of a set. It is relative to the sample space. It is a number between 0 and 1. It can be applied to discrete sets, and it can be applied to continuous sets. How do we measure the “size” of a continuous set? One possible way is by means of integrating the length, area, or volume covered by the set. Consider an example: Suppose that the sample space is the interval Ω = [0,5] and the event is A = [2,3]. To measure the “size” of A, we can integrate A to determine the length. That is, (cid:82) (cid:82)3 “size” of A dx dx 1 P[{x∈[2,3]}]= “size” of Ω = (cid:82)A dx = (cid:82)2 5 dx = 5. Ω 0 Therefore, we have translated the “size” of a set to an integration. However, this definition isaveryspecialcasebecausewhenwecalculatethe“size”ofaset,wetreatalltheelements inthesetwithequalimportance.Thisisastrongassumptionthatwillberelaxedlater.But 172
4.1. PROBABILITY DENSITY FUNCTION Figure4.1:[Left]AneventAinthesamplespaceΩ.TheprobabilitythatAhappenscanbecalculated asthe“size”ofArelativetothe“size”ofΩ.[Right]Aspecificexampleontherealline.Notethatthe samedefinitionofprobabilityapplies:TheprobabilityisthesizeoftheintervalArelativetothatofthe sample space Ω. if you agree with this line of reasoning, we can rewrite the probability as (cid:82) (cid:82) dx dx P[{x∈A}]= (cid:82)A = A dx |Ω| Ω (cid:90) 1 = dx. |Ω| A (cid:124) (cid:123)(cid:122) (cid:125) equallyimportantoverΩ This equation says that under our assumption (that all elements are equiprobable), the probability of A is calculated as the integration of A using an integrand 1/|Ω| (note that 1/|Ω| is a constant with respect to x). If we evaluate the probability of another event B, all we need to do is to replace A with B and compute (cid:82) 1 dx. B |Ω| What happens if we want to relax the “equiprobable” assumption? Perhaps we can adopt something similar to the probability mass function (PMF). Recall that a PMF p X evaluated at a point x is the probability that the state x happens, i.e., p (x) = P[X = x]. X So,p (x)istherelativefrequencyofx.Followingthesamelineofthinking,wecandefinea X function f such that f (x) tells us something related to the “relative frequency”. To this X X end, we can treat f as a continuous histogram with infinitesimal bin width as shown in X Figure 4.2.Usingthisf ,wecanreplacetheconstantfunction1/|Ω|withthenewfunction X f (x). This will give us X (cid:90) P[{x∈A}]= f (x) dx. (4.1) X A (cid:124) (cid:123)(cid:122) (cid:125) replace1/|Ω| If we compare it with a PMF, we note that when X is discrete, (cid:88) P[{x∈A}]= p (x). X x∈A Hence, f can be considered a continuous version of p , although we do not recommend X X this way of thinking for the following reason: p (x) is a legitimate probability, but f (x) is X X not a probability. Rather, f is the probability per unit length, meaning that we need to X integrate f (times dx) in order to generate a probability value. If we only look at f at X X a point x, then this point is a measure-zero set because the length of this set is zero. Equation (4.1) should be familiar to you from Chapter 2. The function f (x) is pre- X cisely the weighting function we described in that chapter. 173
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.2: [Left] A probability mass function (PMF) tells us the relative frequency of a state when computing the probability. In this example, the “size” of A is p (x )+p (x ). [Right] A probability X 2 X 3 densityfunction(PDF)istheinfinitesimalversionofthePMF.Thus,the“size”ofAistheintegration over the PDF. What is a PDF? • A PDF is the continuous version of a PMF. • We integrate a PDF to compute the probability. • We integrate instead of sum because continuous events are not countable. To summarize, we have learned that when measuring the size of a continuous event, the discrete technique (counting the number of elements) does not work. Generalizing to continuous space requires us to integrate the event. However, since different elements in an event have different relative emphases, we use the probability density function f (x) to tell X us the relative frequency for a state x to happen. This PDF serves the role of the PMF. 4.1.2 More in-depth discussion about PDFs A continuous random variable X is defined by its probability density function f . This X function has to satisfy several criteria, summarized as follows. Definition 4.1. A probability density function f of a random variable X is a map- X ping f :Ω→R, with the properties X • Non-negativity: f (x)≥0 for all x∈Ω X (cid:82) • Unity: f (x)dx=1 Ω X • Measure of a set: P[{x∈A}]=(cid:82) f (x)dx A X Ifallelementsofthesamplespaceareequiprobable,thenthePDFisf(x)=1/|Ω|.Youcan easily check that it satisfies all three criteria. Let us take a closer look at the three criteria: • Non-negativity: The non-negativity criterion f (x) ≥ 0 is reminiscent of Probability X Axiom I. It says that no matter what x we are looking at, the probability density function f evaluated at x should never give a negative value. Axiom I ensures that X we will not get a negative probability. 174
4.1. PROBABILITY DENSITY FUNCTION (cid:82) • Unity: The unity criterion f(x) dx = 1 is reminiscent of Probability Axiom II, Ω which says that measuring over the entire sample space will give 1. • Measureofaset:ThethirdcriteriongivesusawaytomeasurethesizeofaneventA. It says that since each x ∈ Ω has a different emphasis when calculating the size of A, we need to scale the elements properly. This scaling is done by the PDF f (x), X which can be regarded as a histogram with a continuous x-axis. The third criterion is a consequence of Probability Axiom III, because if there are two events A and B that are disjoint, then P[{x ∈ A}∪{x ∈ B}] = (cid:82) f (x) dx+(cid:82) f (x) dx because A X B X f (x)≥0 for all x. X If the random variable X takes real numbers in 1D, then a more “user-friendly” definition of the PDF can be given. Definition 4.2. Let X be a continuous random variable. The probability density function (PDF) of X is a function f :Ω→R that, when integrated over an interval X [a, b], yields the probability of obtaining a≤X ≤b: (cid:90) b P[a≤X ≤b]= f (x)dx. (4.2) X a This definition is just a rewriting of the previous definition by explicitly writing out the definition of A as an interval [a,b]. Here are a few examples. Example 4.1.Letf (x)=3x2 withΩ=[0,1].LetA=[0,0.5].Thentheprobability X P[{X ∈A}] is (cid:90) 0.5 1 P[0≤X ≤0.5]= 3x2 dx= . 8 0 Example 4.2.Letf (x)=1/|Ω|withΩ=[0,5].LetA=[3,5].Thentheprobability X P[{X ∈A}] is (cid:90) 5 1 (cid:90) 5 1 2 P[3≤X ≤5]= dx= dx= . |Ω| 5 5 3 3 Example 4.3. Let f (x) = 2x with Ω = [0,1]. Let A = {0.5}. Then the probability X P[{X ∈A}] is (cid:90) 0.5 P[X =0.5]=P[0.5≤X ≤0.5]= 2xdx=0. 0.5 This example shows that evaluating the probability at an isolated point for a contin- uous random variable will yield 0. 175
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Practice Exercise 4.1. Let X be the phase angle of a voltage signal. Without any priorknowledgeaboutX wemayassumethatX hasanequalprobabilityofanyvalue between 0 to 2π. Find the PDF of X and compute P[0≤X ≤π/2]. Solution. Since X has an equal probability for any value between 0 to 2π, the PDF of X is 1 f (x)= , for 0≤x≤2π. X 2π Therefore, the probability P[0≤X ≤π/2] can be computed as (cid:104) π(cid:105) (cid:90) π/2 1 1 P 0≤X ≤ = dx= . 2 2π 4 0 Looking at Equation (4.2), you may wonder: If the PDF f is analogous to PMF X p , why didn’t we require 0 ≤ f (x) ≤ 1 instead of requiring only f (x) ≥ 0? This is X X X an excellent question, and it points exactly to the difference between a PMF and a PDF. Notice that f is a mapping from the sample space Ω to the real line R. It does not map X Ω to [0,1]. On the other hand, since p (x) is the actual probability, it maps Ω to [0,1]. X Thus, f (x) can take very large values but will not explode, because we have the unity X (cid:82) constraint f (x)dx=1.Eveniff (x)takesalargevalue,itwillbecompensatedbythe Ω X X small dx. If you recall, there is nothing like dx in the definition of a PMF. Whenever there is a probability mass, we need to sum or, putting it another way, the dx in the discrete case is always 1. Therefore, while the probability mass PMF must not exceed 1, a probability density PDF can exceed 1. If f (x)≥1, then what is the meaning of f (x)? Isn’t it representing the probability X X of having an element X = x? If it were a discrete random variable, then yes; p (x) is the X probability of having X = x (so the probability mass cannot go beyond 1). However, for a continuous random variable, f (x) is not the probability of having X =x. The probability X of having X = x (i.e., exactly at x) is 0 because an isolated point has zero measure in the continuous space. Thus, even though f (x) takes a value larger than 1, the probability of X X being x is zero. AtthispointyoucanseewhywecallPDFadensity,ordensityfunction,becauseeach value f (x) is the probability per unit length. If we want to calculate the probability of X x≤X ≤x+δ, for example, then according to our definition, we have (cid:90) x+δ P[x≤X ≤x+δ]= f (x)dx≈f (x)·δ. X X x Therefore, the probability of P[x ≤ X ≤ x+δ] can be regarded as the “per unit length” density f (x) multiplied with the “length” δ. As δ →0, we can see that P[X =x]=0. See X Figure 4.3 for an illustration. Why are PDFs called a density function? • Because f (x) is the probability per unit length. X • You need to integrate f (x) to obtain a probability. X 176
4.1. PROBABILITY DENSITY FUNCTION Figure 4.3: TheprobabilityP[x≤X ≤x+δ]canbeapproximatedbythedensityf (x)multipliedby X the length δ. Example 4.4. Consider a random variable X with PDF f X(x) = 2√1 x for any 0<x≤1, and is 0 otherwise. We can show that f (x) → ∞ as x → 0. However, X f (x) remains a valid PDF because X (cid:90) ∞ (cid:90) 1 1 √ (cid:12) (cid:12)1 f X(x)dx= 2√ x dx= x(cid:12) (cid:12) =1. −∞ 0 0 Remark. Since isolated points have zero measure in the continuous space, the probability of an open interval (a,b) is the same as the probability of a closed interval: P[[a,b]]=P[(a,b)]=P[(a,b]]=P[[a,b)]. The exception is that when the PDF of f (x) has a delta function at a or b. In this case, X the probability measure at a or b will be non-zero. We will discuss this when we talk about the CDFs. Practice Exercise 4.2.Letf (x)=c(1−x2)for−1≤x≤1,and0otherwise.Find X the constant c. (cid:82) Solution. Since f (x)dx=1, it follows that Ω X (cid:90) (cid:90) 1 4c f (x)dx= c(1−x2)dx= ⇒ c=3/4. X 3 Ω −1 Practice Exercise 4.3. Let f (x)=x2 for |x|≤a, and 0 otherwise. Find a. X Solution. Note that (cid:90) f X(x)dx=(cid:90) a x2 dx= x 33(cid:12) (cid:12) (cid:12) (cid:12)a = 2 3a3 . Ω −a −a (cid:113) Setting 2a3 =1 yields a= 3 3. 3 2 177
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.1.3 Connecting with the PMF The probability density function is more general than the probability mass function. To see this, consider a discrete random variable X with a PMF p (x). Because p is defined on X X a countable set Ω, we can write it as a train of delta functions and define a corresponding PDF: (cid:88) f (x)= p (x )δ(x−x ). X X k k xk∈Ω Example 4.5. If X is aBernoulli randomvariablewith PMF p (1)=p and p (0)= X X 1−p, then the corresponding PDF can be written as f (x)=pδ(x−1)+(1−p)δ(x−0). X Example4.6.IfX isabinomialrandomvariablewithPMFp (k)=(cid:0)n(cid:1) pk(1−p)n−k, X k then the corresponding PDF can be written as n (cid:88) f (x)= p (k)δ(x−k) X X k=0 n (cid:18) (cid:19) (cid:88) n = pk(1−p)n−k δ(x−k). k k=0 Strictly speaking, delta functions are not really functions. They are defined through integrations. They satisfy the properties that δ(x−x ) = ∞ if x = x , δ(x−x ) = 0 if k k k x(cid:54)=x , and k (cid:90) xk+(cid:15) δ(x−x )dx=1, k xk−(cid:15) for any (cid:15)>0. Suppose we ignore the fact that delta functions are not functions and merely treat them as ordinary functions with some interesting properties. In this case, we can imagine that for every probability mass p (x ), there exists an interval [a,b] such that X k there is one and only one state x that lies in [a,b], as shown in Figure 4.4. k Figure 4.4: We can view a PMF as a train of impulses. When computing the probability X =x , we k integrate the PMF over the interval [a,b]. 178
4.1. PROBABILITY DENSITY FUNCTION If we want to calculate the probability of obtaining X =x , we can show that k P[X =x ]( =a)P[a≤X ≤b] k (cid:90) b = f (x)dx X a (cid:90) b (b) = p (x )δ(x−x )dx X k k a (cid:90) b (c) = p (x ) δ(x−x )dx=p (x ). X k k X k a (cid:124) (cid:123)(cid:122) (cid:125) =1 Here, step (a) holds because within [a,b], there is no other event besides X = x . Step (b) k is just the definition of our f (x) (inside the interval [a,b]). Step (c) shows that the delta X function integrates to 1, thus leaving the probability mass p (x ) as the final result. Let us X k look at an example and then comment on this intuition. Example 4.7. Let X be a discrete random variable with PMF 1 p (k)= , k =1,2,... X 2k The continuous representation of the PMF can be written as ∞ ∞ (cid:18) (cid:19) (cid:88) (cid:88) 1 f (x)= p (k)δ(x−k)= δ(x−k). X X 2k k=1 k=1 Suppose we want to compute the probability P[1≤X ≤2]. This can be computed as (cid:90) 2 (cid:90) 2 (cid:88)∞ (cid:18) 1 (cid:19) P[1≤X ≤2]= f (x)dx= δ(x−k)dx X 2k 1 1 k=1 (cid:90) 2(cid:26) 1 1 (cid:27) = δ(x−1)+ δ(x−2)+··· dx 2 4 1 1(cid:90) 2 1(cid:90) 2 = δ(x−1)dx+ δ(x−2)dx 2 4 1 1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =1 =1 1(cid:90) 2 + δ(x−3)dx+ ··· 8 (cid:124)(cid:123)(cid:122)(cid:125) 1 (cid:124) (cid:123)(cid:122) (cid:125) =0 =0 1 1 3 = + = . 2 4 4 However, if we want to compute the probability P[1 < X ≤ 2], then the integration 179
CHAPTER 4. CONTINUOUS RANDOM VARIABLES limit will not include the number 1 and so the delta function will remain 0. Thus, (cid:90) 2 P[1<X ≤2]= f (x)dx X 1+ 1(cid:90) 2 1(cid:90) 2 1 = δ(x−1)dx+ δ(x−2)dx= . 2 4 4 1+ 1+ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =0 =1 Closing remark. To summarize, we see that a PMF can be “regarded” as a PDF. We are carefultoputaquotationaround“regarded”becausePMFandPDFaredefinedfordifferent events.APMFusesadiscretemeasure(i.e.,acounter)forcountableevents,whereasaPDF usesacontinuousmeasure(i.e.,integration)forcontinuousevents.Thewaywelinkthetwois byusingthedeltafunctions.Usingthedeltafunctionsisvalid,buttheargumentweprovide hereisintuitiveratherthanrigorous.Itisnotrigorousbecausetheintegrationweuseisstill the Riemann-Stieltjes integration, which does not handle delta functions. Therefore, while you can treat a discrete PDF as a train of delta functions, it is important to remember the limitations of the integrations we use. 4.2 Expectation, Moment, and Variance 4.2.1 Definition and properties As with discrete random variables, we can define expectation for continuous random vari- ables. The definition is analogous: Just replace the summation with integration. Definition 4.3. The expectation of a continuous random variable X is (cid:90) E[X]= xf (x)dx. (4.3) X Ω Example 4.8. (Uniform random variable) Let X be a continuous random variable with PDF f (x)= 1 for a≤x≤b, and 0 otherwise. The expectation is X b−a (cid:90) (cid:90) b 1 1 (cid:90) b E[X]= xf (x)dx= x· dx= xdx X b−a b−a Ω a a (cid:124) (cid:123)(cid:122) (cid:125) =x2(cid:12) (cid:12)b 2 a 1 b2−a2 a+b = · = . b−a 2 2 180
4.2. EXPECTATION, MOMENT, AND VARIANCE Example4.9.(Exponentialrandomvariable)LetX beacontinuousrandomvariable with PDF f (x)=λe−λx, for x≥0. The expectation is X (cid:90) ∞ E[X]= xλe−λx dx 0 (cid:90) ∞ =− xde−λx 0 (cid:12)∞ (cid:90) ∞ =−xe−λx(cid:12) (cid:12) + e−λx dx (cid:12) 0 0 (cid:124) (cid:123)(cid:122) (cid:125) =0 =−1 e−λx(cid:12) (cid:12) (cid:12)∞ = 1 , λ (cid:12) λ 0 (cid:124) (cid:123)(cid:122) (cid:125) =−1 where the colored step is due to integration by parts. Ifafunctiong isappliedtotherandomvariableX,theexpectationcanbefoundusing the following theorem. Theorem 4.1. Let g :Ω→R be a function and X be a continuous random variable. Then (cid:90) E[g(X)]= g(x)f (x)dx. (4.4) X Ω Example 4.10. (Uniform random variable) Let X be a continuous random variable with f (x)= 1 for a≤x≤b, and 0 otherwise. If g(·)=(·)2, then X b−a (cid:90) E[g(X)]=E[X2]= x2f (x)dx X Ω 1 (cid:90) b a2+ab+b2 = · x2 dx= . b−a 3 a (cid:124) (cid:123)(cid:122) (cid:125) =b3−a3 3 PracticeExercise4.4.LetΘbeacontinuousrandomvariablewithPDFf (θ)= 1 Θ 2π for 0≤θ ≤2π and is 0 otherwise. Let Y =cos(ωt+Θ). Find E[Y]. Solution. Referring to Equation (4.4), the function g is g(θ)=cos(ωt+θ). 181
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Therefore, the expectation E[Y] is (cid:90) 2π E[Y]= cos(ωt+θ)f (θ)dθ Θ 0 1 (cid:90) 2π = cos(ωt+θ)dθ =0, 2π 0 where the last equality holds because the integral of a sinusoid over one period is 0. Practice Exercise 4.5. Let A⊆Ω. Let I (X) be an indicator function such that A (cid:40) 1, ifX ∈A, I (X)= A 0, ifX (cid:54)∈A. Find E[I (X)]. A Solution. The expectation is (cid:90) E[I (X)]= I (x)f (x)dx A A X Ω (cid:90) = f (x)dx X x∈A =P[X ∈A]. Sotheprobabilityof{X ∈A}canbeequivalentlyrepresentedintermsofexpectation. Practice Exercise 4.6. Is it true that E[1/X]=1/E[X]? Solution. No. This is because (cid:20) 1 (cid:21) (cid:90) 1 E = f (x)dx X x X Ω 1 (cid:54)= (cid:82) xf (x)dx Ω X 1 = . E[X] All the properties of expectation we learned in the discrete case can be translated to the continuous case. Specifically, we have that • E[aX]=aE[X]: A scalar multiple of a random variable will scale the expectation. • E[X+a]=E[X]+a:Constantadditionofarandomvariablewilloffsettheexpectation. • E[aX +b] = aE[X]+b: Affine transformation of a random variable will translate to the expectation. 182
4.2. EXPECTATION, MOMENT, AND VARIANCE Practice Exercise 4.7. Prove the above three statements. Solution. The third statement is just the sum of the first two statements, so we just need to show the first two: (cid:90) (cid:90) E[aX]= axf (x)dx=a xf (x)dx=aE[X], X X Ω Ω (cid:90) (cid:90) E[X+a]= (x+a)f (x)dx= xf (x)dx+a=E[X]+a. X X Ω Ω 4.2.2 Existence of expectation As we discussed in the discrete case, not all random variables have an expectation. Definition4.4. ArandomvariableX hasanexpectationifitis absolutelyintegrable, i.e., (cid:90) E[|X|]= |x|f (x)dx<∞. (4.5) X Ω Being absolutely integrable implies that the expectation is that E[|X|] is the upper bound of E[X]. Theorem 4.2. For any random variable X, |E[X]|≤E[|X|]. (4.6) Proof. Note that f (x)≥0. Therefore, X −|x|f (x)≤xf (x)≤|x|,f (x), ∀x. X X X Thus, integrating all three terms yields (cid:90) (cid:90) (cid:90) − |x|f (x)dx≤ xf (x)dx≤ |x|f (x)dx, X X X Ω Ω Ω which is equivalent to −E[|X|]≤E[X]≤E[|X|]. (cid:3) Example 4.11. Here is a random variable whose expectation is undefined. Let X be a random variable with PDF 1 f (x)= , x∈R. X π(1+x2) This random variable is called the Cauchy random variable. We can show that (cid:90) ∞ 1 1 (cid:90) ∞ x 1 (cid:90) 0 x E[X]= x· dx= dx+ dx. π(1+x2) π (1+x2) π (1+x2) −∞ 0 −∞ 183
CHAPTER 4. CONTINUOUS RANDOM VARIABLES The first integral gives (cid:90) ∞ x 1 (cid:12)∞ dx= log(1+x2)(cid:12) =∞, 0 (1+x2) 2 (cid:12) 0 and the second integral gives −∞. Since neither integral is finite, the expectation is undefined. We can also check the absolutely integrability criterion: (cid:90) ∞ 1 E[|X|]= |x|· dx π(1+x2) −∞ (a) (cid:90) ∞ x (cid:90) ∞ x = 2 dx≥2 dx π(1+x2) π(1+x2) 0 1 (b) (cid:90) ∞ x 1 (cid:12)∞ ≥ 2 dx= log(x)(cid:12) =∞, 1 π(x2+x2) π (cid:12) 1 where in (a) we use the fact that the function being integrated is even, and in (b) we lower-bound 1 ≥ 1 if x>1. 1+x2 x2+x2 4.2.3 Moment and variance The moment and variance of a continuous random variable can be defined analogously to the moment and variance of a discrete random variable, replacing the summations with integrations. Definition 4.5. The kth moment of a continuous random variable X is (cid:90) E[Xk]= xkf (x)dx. (4.7) X Ω Definition 4.6. The variance of a continuous random variable X is (cid:90) Var[X]=E[(X−µ)2]= (x−µ)2f (x)dx, (4.8) X Ω where µd =efE[X]. It is not difficult to show that the variance can also be expressed as Var[X]=E[X2]−µ2, because Var[X]=E[(X−µ)2] =E[X2]−2E[X]µ+µ2 =E[X2]−µ2. 184
4.3. CUMULATIVE DISTRIBUTION FUNCTION Practice Exercise 4.8. (Uniform random variable) Let X be a continuous random variable with PDF f (x)= 1 for a≤x≤b, and 0 otherwise. Find Var[X]. X b−a Solution. We have shown that E[X] = a+b and E[X2] = a2+ab+b2. Therefore, the 2 3 variance is Var[X]=E[X2]−E[X]2 a2+ab+b2 (cid:18) a+b(cid:19)2 = − 3 2 (b−a)2 = . 12 Practice Exercise 4.9. (Exponential random variable) Let X be a continuous ran- dom variable with PDF f (x)=λe−λx for x≥0, and 0 otherwise. Find Var[X]. X Solution. We have shown that E[X]= 1. The second moment is λ (cid:90) ∞ E[X2]= x2 λe−λx dx 0 (cid:90) ∞ =(cid:2) −x2e−λx(cid:3)∞ + 2xe−λx dx 0 0 2 (cid:90) ∞ = xλe−λx dx λ 0 2 1 2 = · = . λ λ λ2 Therefore, Var[X]=E[X2]−E[X]2 2 1 1 = − = . λ2 λ2 λ2 4.3 Cumulative Distribution Function When we discussed discrete random variables, we introduced the concept of cumulative distributionfunctions(CDFs).OneofthemotivationswasthatifweviewaPMFasatrain ofdeltafunctions,theyaretechnicallynotwell-definedfunctions.However,itturnsoutthat theCDFisalwaysawell-definedfunction.Inthissection,wewillcompletethestorybyfirst discussing the CDF for continuous random variables. Then, we will come back and show you how the CDF can be derived for discrete random variables. 185
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.3.1 CDF for continuous random variables Definition 4.7. Let X be a continuous random variable with a sample space Ω=R. The cumulative distribution function (CDF) of X is (cid:90) x F (x)d =efP[X ≤x]= f (x(cid:48))dx(cid:48). (4.9) X X −∞ TheinterpretationoftheCDFcanbeseenfromFigure4.5.GivenaPDFf ,theCDF X F evaluated at x is the integration of f from −∞ up to a point x. The integration of f X X X from −∞ to x is nothing but the area under the curve of f . Since f is non-negative, the X X larger value x we use to evaluate in F (x), the more area under the curve we are looking X at. In the extreme when x = −∞, we can see that F (−∞) = 0, and when x = +∞ we X (cid:82)∞ have that F (+∞)= f (x)dx=1. X −∞ X Figure 4.5: A CDF is the integral of the PDF. Thus, the height of a stem in the CDF corresponds to the area under the curve of the PDF. Practice Exercise 4.10.(Uniform random variable)LetX beacontinuousrandom variablewithPDFf (x)= 1 fora≤x≤b,andis0otherwise.FindtheCDFofX. X b−a Solution. The CDF of X is given by  0, x≤a,  F (x)= (cid:82)x f (x(cid:48))dx(cid:48) =(cid:82)x 1 dx(cid:48) = x−a, a<x≤b, X −∞ X a b−a b−a 1, x>b. Asyoucanseefromthispracticeexercise,weexplicitlybreaktheCDFintothreesegments. The first segment gives F (x) = 0 because for any x ≤ a, there is nothing to integrate, X since f (x) = 0 for any x ≤ a. Similarly, for the last segment, F (x) = 1 for all x > b X X because once x goes beyond b, the integration will cover all the non-zeros of f . Figure 4.6 X illustrates the PDF and CDF for this example. In MATLAB, we can generate the PDF and CDF using the commands pdf and cdf respectively.FortheparticularexampleshowninFigure4.6,thefollowingcodecanbeused. A similar set of commands can be implemented in Python. 186
4.3. CUMULATIVE DISTRIBUTION FUNCTION Figure 4.6: Example: f (x)=1/(b−a) for a≤x≤b. The CDF has three segments. X % MATLAB code to generate the PDF and CDF unif = makedist(’Uniform’,’lower’,-3,’upper’,4); x = linspace(-5, 10, 1500)’; f = pdf(unif, x); F = cdf(unif, x); figure(1); plot(x, f, ’LineWidth’, 6); figure(2); plot(x, F, ’LineWidth’, 6); # Python code to generate the PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,10,1500) f = stats.uniform.pdf(x,-3,4) F = stats.uniform.cdf(x,-3,4) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() Practice Exercise 4.11. (Exponential random variable) Let X be a continuous random variable with PDF f (x)=λe−λx for x≥0, and 0 otherwise. Find the CDF X of X. Solution. Clearly, for x<0, we have F (x)=0. For x≥0, we can show that X (cid:90) x (cid:90) x F (x)= f (x(cid:48))dx(cid:48) = λe−λx(cid:48) dx(cid:48) =1−e−λx. X X 0 0 Therefore, the complete CDF is (see Figure 4.7 for illustration): (cid:40) 0, x<0, F (x)= X 1−e−λx, x≥0. 187
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.7: Example: f (x)=λe−λx for x≥0. The CDF has two segments. X The MATLAB code and Python code to generate this figure are shown below. % MATLAB code to generate the PDF and CDF pd = makedist(’exp’,2); x = linspace(-5, 10, 1500)’; f = pdf(pd, x); F = cdf(pd, x); figure(1); plot(x, f, ’LineWidth’, 6); figure(2); plot(x, F, ’LineWidth’, 6); # Python code to generate the PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,10,1500) f = stats.expon.pdf(x,2) F = stats.expon.cdf(x,2) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() 4.3.2 Properties of CDF LetusnowdescribethepropertiesofaCDF.Ifwecomparethesewiththoseforthediscrete cases, we see that the continuous cases simply replace the summations by integrations. Therefore, we should expect to inherit most of the properties from the discrete cases. Proposition 4.1. Let X be a random variable (either continuous or discrete), then the CDF of X has the following properties: (i) The CDF is nondecreasing. (ii) The maximum of the CDF is when x=∞: F (+∞)=1. X (iii) The minimum of the CDF is when x=−∞: F (−∞)=0. X 188
4.3. CUMULATIVE DISTRIBUTION FUNCTION Proof. For (i), we notice that F (x)=(cid:82)x f (x(cid:48))dx(cid:48). Therefore, if s≤t then X −∞ X (cid:90) s (cid:90) t F (s)= f (x(cid:48))dx(cid:48) ≤ f (x(cid:48))dx(cid:48) =F (t). X X X X −∞ −∞ Thus it shows that F is nondecreasing. (It does not need to be increasing because a CDF X can have a steady state.) For (ii) and (iii), we can show that (cid:90) +∞ (cid:90) −∞ F (+∞)= f (x(cid:48))dx(cid:48) =1, and F (−∞)= f (x(cid:48))dx(cid:48) =0. (cid:3) X X X X −∞ −∞ Example4.12.WecanshowthattheCDFwederivedfortheuniformrandomvariable satisfies these three properties. To see this, we note that x−a F (x)= , a≤x≤b. X b−a The derivative of this function F(cid:48) (x) = 1 > 0 for a ≤ x ≤ b. Also, note that X b−a F (x) = 0 for x < a and x > b, so F is nondecreasing. The other two properties X X follow because if x = b, then F (b) =1, and if x = a then F (a) =0. Together with X X the nondecreasing property, we show (ii) and (iii). Proposition 4.2. Let X be a continuous random variable. If the CDF F is contin- X uous at any a≤x≤b, then P[a≤X ≤b]=F (b)−F (a). (4.10) X X Proof. The proof follows from the definition of the CDF, which states that (cid:90) b (cid:90) a F (b)−F (a)= f (x(cid:48))dx(cid:48)− f (x(cid:48))dx(cid:48) X X X X −∞ −∞ (cid:90) b = f (x(cid:48))dx(cid:48) =P[a≤X ≤b]. (cid:3) X a This result provides a very handy tool for calculating the probability of an event a≤X ≤b using the CDF. It says that P[a ≤ X ≤ b] is the difference between F (b) and X F (a). So, if we are given F , calculating the probability of a ≤ X ≤ b just involves X X evaluating the CDF at a and b. The result also shows that for a continuous random vari- able X, P[X =x ]=F (x )−F (x )=0. This is consistent with our arguments from the 0 X 0 X 0 measure’s point of view. Example 4.13.(Exponential random variable)Weshowedthattheexponentialran- dom variable X with a PDF f (x) = λe−λx for x ≥ 0 (and f (x) = 0 for x < 0) X X has a CDF given by F (x) = 1−e−λx for x ≥ 0. Suppose we want to calculate the X 189
CHAPTER 4. CONTINUOUS RANDOM VARIABLES probability P[1≤X ≤3]. Then the PDF approach gives us (cid:90) 3 (cid:90) 3 (cid:12)3 P[1≤X ≤3]= f X(x)dx= λe−λx dx=−e−λx(cid:12) (cid:12) =e−3λ−e−λ. (cid:12) 1 1 1 If we take the CDF approach, we can show that P[1≤X ≤3]=F (3)−F (1) X X =(1−e−λ)−(1−e−3λ)=e−3λ−e−λ, which yields the same as the PDF approach. Example 4.14. Let X be a random variable with PDF f (x) = 2x for 0 ≤ x ≤ 1, X and is 0 otherwise. We can show that the CDF is (cid:90) x (cid:90) x (cid:12)x F X(x)= f X(t)dt= 2t dt=t2(cid:12) (cid:12) =x2, 0≤x≤1. (cid:12) 0 0 0 Therefore, to compute the probability P[1/3≤X ≤1/2], we have (cid:20) 1 1(cid:21) (cid:18) 1(cid:19) (cid:18) 1(cid:19) (cid:18) 1(cid:19)2 (cid:18) 1(cid:19)2 5 P ≤X ≤ =F −F = − = . 3 2 X 2 X 3 2 3 36 (cid:3) ACDFcanbeusedforbothcontinuousanddiscreterandomvariables.However,before we can do that, we need a tool to handle the discontinuities. The following definition is a summary of the three types of continuity. Definition 4.8. A function F (x) is said to be X • Left-continuous at x=b if F (b)=F (b−)d =ef lim F (b−h); X X h→0 X • Right-continuous at x=b if F (b)=F (b+)d =ef lim F (b+h); X X h→0 X • Continuous at x = b if it is both right-continuous and left-continuous at x = b. In this case, we have lim F (b−h)= lim F (b+h)=F(b). X X h→0 h→0 Inthisdefinition,thestepsizeh>0isshrinkingtozero.Thepointb−hstaysattheleftof b,andb+hstaysattherightofb.Thus,ifwesetthelimith→0,b−hwillapproachapoint b− whereas b+h will approach a point b+. If it happens that F (b−)=F (b) then we say X X that F is left-continuous at b. If F (b+)=F (b) then we say that F is right-continuous X X X X at b. These are summarized in Figure 4.8. WheneverF hasadiscontinuouspoint,itcanbeleft-continuous,right-continuous,or X neither. (“Neither” happens if F (b) take a value other than F (b+) or F (b−). You can X X X 190
4.3. CUMULATIVE DISTRIBUTION FUNCTION Figure 4.8: The definition of left- and right-continuous at a point b. always create a nasty function that satisfies this condition.) For continuous functions, it is necessary that F (b−)=F (b+). If this happens, there is no gap between the two points. X X Theorem 4.3. For any random variable X (discrete or continuous), F (x) is always X right-continuous. That is, F (b)=F (b+)d =ef lim F (b+h) (4.11) X X X h→0 Right-continuous means that if F (x) is piecewise, it must have a solid left end and an X empty right end. Figure 4.9 shows an example of a valid CDF and an invalid CDF. Figure 4.9: A CDF must be right-continuous. The reason why F is always right-continuous is that the inequality X ≤ x has a X closed right-hand limit. Imagine the following situation: A discrete random variable X has four states: 1,2,3,4. Then, “3+h” (cid:88) lim F (3+h)= lim p (k)=p (1)+p (2)+p (3)=F (3). X X X X X X h→0 h→0 k=1 Similarly, if you have a continuous random variable X with a PDF f , then X (cid:90) b+h (cid:90) b lim F (b+h)= lim f (t)dt= f (t)dt=F (b). X X X X h→0 h→0 −∞ −∞ 191
CHAPTER 4. CONTINUOUS RANDOM VARIABLES In other words, the “≤” ensures that the rightmost state is included. If we defined CDF using<,wewouldhavegottenleft-handcontinuous,butthiswouldbeinconvenientbecause the < requires us to deal with limits whenever we evaluate X <x. Theorem 4.4. For any random variable X (discrete or continuous), P[X =b] is (cid:40) F (b)−F (b−), if F is discontinuous at x=b P[X =b]= X X X (4.12) 0, otherwise. This proposition states that when F (x) is discontinuous at x = b, then P[X = b] is X the difference between F (b) and the limit from the left. In other words, the height of the X gap determines the probability at the discontinuity. If F (x) is continuous at x = b, then X F (b)=lim F (b−h) and so P[X =b]=0. X h→0 X Figure 4.10: Illustration of Equation (4.12). Since the CDF is discontinuous at a point x=b, the gap F (b)−F (b−) will define the probability P[X =b]. X X Example 4.15. Consider a random variable X with a PDF  x, 0≤x≤1,  f (x)= 1, x=3, X 2 0, otherwise. The CDF F (x) will consist of a few segments. The first segment is 0 ≤ x < 1. We X can show that (cid:90) x (cid:90) x t2(cid:12) (cid:12)x x2 F X(x)= f X(t)dt= t dt= 2(cid:12) (cid:12) = 2 , 0≤x<1. 0 0 0 The second segment is when 1 ≤ x < 3. Since there is no new f to integrate, the X CDF stays at F (x)=F (1)= 1 for 1≤x<3. The third segment is x>3. Because X X 2 this range has covered the entire sample space, we have F (x) = 1 for x > 3. How X about x=3? We can show that F (3)=F (3+)=1. X X 192
4.3. CUMULATIVE DISTRIBUTION FUNCTION Therefore, to summarize, the CDF is  0, x<0, x2, 0≤x<1, F (x)= 2 X 11 2 ,, 1 x≤ ≥x 3.<3, A graphical illustration is shown in Figure 4.11. Figure 4.11: An example of converting a PDF to a CDF. 4.3.3 Retrieving PDF from CDF Thus far, we have only seen how to obtain F (x) from f (x). In order to go in the reverse X X direction, we recall the fundamental theorem of calculus. This states that if a function f is continuous, then d (cid:90) x f(x)= f(t)dt dx a for some constant a. Using this result for CDF and PDF, we have the following: Theorem 4.5. The probability density function (PDF) is the derivative of the cu- mulative distribution function (CDF): dF (x) d (cid:90) x f (x)= X = f (x(cid:48))dx(cid:48), (4.13) X dx dx X −∞ provided F is differentiable at x. If F is not differentiable at x=x , then, X X 0 f (x )=P[X =x ]δ(x−x ). X 0 0 0 Example 4.16. Consider a CDF (cid:40) 0, x<0, F (x)= X 1− 1e−2x, x≥0. 4 We want to find the PDF f (x). To do so, we first show that F (0) = 3. This X X 4 193
CHAPTER 4. CONTINUOUS RANDOM VARIABLES corresponds to a discontinuity at x=0, as shown in Figure 4.12. Figure 4.12: An example of converting a PDF to a CDF. Because of the discontinuity, we need to consider three cases: dFX(x), x<0,  dx f (x)= P[X =0]δ(x−0), x=0, X dFX(x), x>0. dx When x<0, F (x)=0, so dFX(x) =0. X dx When x>0, F (x)=1− 1e−2x, so X 4 dF (x) 1 X = e−2x. dx 2 When x = 0, the probability P[X = 0] is determined by the gap between the solid dot and the empty dot. This yields P[X =0]=F (0)− lim F (0−h) X X h→0 3 3 = −0= . 4 4 Therefore, the overall PDF is  0, x<0,  f (x)= 3δ(x−0), x=0, X 4 1e−2x, x>0. 2 Figure 4.12 illustrates this example. 4.3.4 CDF: Unifying discrete and continuous random variables The CDF is always a well-defined function. It is integrable everywhere. If the underlying randomvariableiscontinuous,theCDFisalsocontinuous.Iftheunderlyingrandomvariable is discrete, the CDF is a staircase function. We have seen enough CDFs for continuous random variables. Let us (re)visit a few discrete random variables. 194
4.3. CUMULATIVE DISTRIBUTION FUNCTION Example 4.17. (Geometric random variable) Consider a geometric random variable with PMF p (k)=(1−p)k−1p, for k =1,2,.... X Figure 4.13: PMF and CDF of a geometric random variable. We can show that the CDF is k (cid:88) F (k)= p ((cid:96)) X X (cid:96)=1 k (cid:88) = (1−p)(cid:96)−1p (cid:96)=1 1−(1−p)k =p· 1−(1−p) =1−(1−p)k. For a sanity check, we can try to retrieve the PMF from the CDF: p (k)=F (k)−F (k−1) X X X =(1−(1−p)k)−(1−(1−p)k−1) =(1−p)k−1p. A graphical portrayal of this example is shown in Figure 4.13. If we treat the PMFs as delta functions in the above example, then the continuous definition also applies. Since the CDF is a piecewise constant function, the derivative is exactlyadeltafunction.Forsomeproblems,itiseasiertostartwithCDFandthencompute the PMF or PDF. Here is an example. Example 4.18. Let X , X and X be three independent discrete random variables 1 2 3 with sample space Ω = {1,2,...,10}. Define X = max{X ,X ,X }. We want to 1 2 3 find the PMF of X. To tackle this problem, we first observe that the PMF for X is 1 p (k)= 1 . Thus, the CDF of X is X1 10 1 k (cid:88) k F (k)= p ((cid:96))= . X1 X1 10 (cid:96)=1 195
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Then, we can show that the CDF of X is F (k)=P[X ≤k]=P[max{X ,X ,X }≤k] X 1 2 3 ( =a)P[X ≤k ∩X ≤k ∩X ≤k] 1 2 3 ( =b)P[X ≤k]P[X ≤k]P[X ≤k] 1 2 3 (cid:18) k (cid:19)3 = , 10 wherein(a)weusethefactthatmax{X ,X ,X }≤k ifandonlyifallthreeelements 1 2 3 are less than k, and in (b) we use independence. Consequently, the PMF of X is (cid:18) k (cid:19)3 (cid:18) k−1(cid:19)3 p (k)=F (k)−F (k−1)= − . X X X 10 10 What is a CDF? • CDF is F (x)=P[X ≤x]. It is the cumulative sum of the PMF/PDF. X • CDF is either a staircase function, a smooth function, or a hybrid. Unlike a PDF, which is not defined for discrete random variables, the CDF is always well defined. d • CDF −d→x PDF. (cid:82) • CDF ←− PDF. • Gap of jump in CDF = height of delta in PDF. 4.4 Median, Mode, and Mean There are three statistical quantities that we are frequently interested in: mean, mode, and median. We all know how to compute these from a dataset. For example, to compute the median of a dataset, we sort the data and pick the number that sits in the 50th percentile. However, the median computed in this way is the empirical median, i.e., it is a value computed from a particular dataset. If the data is generated from a random variable (with a given PDF), how do we compute the mean, median, and mode? 4.4.1 Median Imagine you have a sequence of numbers as shown below. n 1 2 3 4 5 6 7 8 9 ··· 100 x 1.5 2.5 3.1 1.1 −0.4 −4.1 0.5 2.2 −3.4 ··· −1.4 n How do we compute the median? We first sort the sequence (either in ascending order or descending order), and then pick the middle one. On computer, we permute the samples {x ,x ,...,x }=sort{x ,x ,...,x }, 1(cid:48) 2(cid:48) N(cid:48) 1 2 N 196
4.4. MEDIAN, MODE, AND MEAN such that x <x <...<x is ordered. The median is the one positioned at the middle. 1(cid:48) 2(cid:48) N(cid:48) There are, of course, built-in commands such as median in MATLAB and np.median in Python to perform the median operation. Now,howdowecomputethemedianifwearegivenarandomvariableX withaPDF f (x)? The answer is by integrating the PDF. X Definition 4.9. Let X be a continuous random variable with PDF f . The median X of X is a point c∈R such that (cid:90) c (cid:90) ∞ f (x)dx= f (x)dx. (4.14) X X −∞ c (cid:82)c Why is the median defined in this way? This is because f (x) dx is the area under −∞ X (cid:82)∞ the curve on the left of c, and f (x) dx is the area under the curve on the right of c. c X The area under the curve tells us the percentage of numbers that are less than the cutoff. Therefore, if the left area equals the right area, then c must be the median. How to find the median from the PDF • Find a point c that separates the PDF into two equal areas Figure 4.14: [Left] The median is computed as the point such that the two areas under the curve are equal. [Right] The median is computed as the point such that F hits 0.5. X The median can also be evaluated from the CDF as follows. Theorem 4.6. The median of a random variable X is the point c such that 1 F (c)= . (4.15) X 2 Proof. Since F (x)=(cid:82)x f (x(cid:48))dx(cid:48), we have X −∞ X (cid:90) c (cid:90) ∞ F (c)= f (x)dx= f (x)dx=1−F (c). X X X X −∞ c Rearranging the terms shows that F (c)= 1. (cid:3) X 2 197
CHAPTER 4. CONTINUOUS RANDOM VARIABLES How to find median from CDF • Find a point c such that F (c)=0.5. X Example 4.19. (Uniform random variable) Let X be a continuous random variable with PDF f (x) = 1 for a ≤ x ≤ b, and is 0 otherwise. We know that the CDF of X b−a X is F (x) = x−a for a ≤ x ≤ b. Therefore, the median of X is the number c ∈ R X b−a suchthatF (c)= 1.SubstitutingintotheCDFyields c−a = 1,whichgivesc= a+b. X 2 b−a 2 2 Example 4.20. (Exponential random variable) Let X be a continuous random vari- able with PDF f (x) = λe−λx for x ≥ 0. We know that the CDF of X is F (x) = X X 1−e−λx for x ≥ 0. The median of X is the point c such that F (c) = 1. This gives X 2 1−e−λc = 1, which is c= log2. 2 λ 4.4.2 Mode The mode is the peak of the PDF. We can see this from the definition below. Definition 4.10. Let X be a continuous random variable. The mode is the point c such that f (x) attains the maximum: X d c=argmax f (x)=argmax F (x). (4.16) X dx X x∈Ω x∈Ω The second equality holds because f (x) = F(cid:48) (x) = d (cid:82)x f (t) dt. A pictorial illustra- X X dx −∞ X tionofmodeisgiveninFigure4.15.Notethatthemodeofarandomvariableisnotunique, e.g., a mixture of two identical Gaussians with different means has two modes. Figure4.15:[Left]ThemodeappearsatthepeakofthePDF.[Right]Themodeappearsatthesteepest slope of the CDF. 198
4.4. MEDIAN, MODE, AND MEAN How to find mode from PDF • Find a point c such that f (c) is maximized. X How to find mode from CDF • Continuous: Find a point c such that F (c) has the steepest slope. X • Discrete: Find a point c such that F (c) has the biggest gap in a jump. X Example 4.21.LetX beacontinuousrandomvariablewithPDFf (x)=6x(1−x) X for 0≤x≤1. The mode of X happens at argmax f (x). To find this maximum, we X x take the derivative of f . This gives X d d 0= f (x)= 6x(1−x)=6(1−2x). dx X dx Setting this equal to zero yields x= 1. 2 To ensure that this point is a maximum, we take the second-order derivative: d2 d f (x)= 6(1−2x)=−12<0. dx2 X dx Therefore, we conclude that x = 1 is a maximum point. Hence, the mode of X is 2 x= 1. 2 4.4.3 Mean We have defined the mean as the expectation of X. Here, we show how to compute the expectation from the CDF. To simplify the demonstration, let us first assume that X >0. Lemma 4.1. Let X >0. Then E[X] can be computed from F as X (cid:90) ∞ E[X]= (1−F (t)) dt. (4.17) X 0 Proof. The trick is to change the integration order: (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ (1−F (t)) dt= [1−P[X ≤t]] dt= P[X >t]dt X 0 0 0 (cid:90) ∞(cid:90) ∞ (cid:90) ∞(cid:90) x (a) = f (x)dxdt = f (x)dtdx X X 0 t 0 0 (cid:90) ∞(cid:90) x (cid:90) ∞ = dtf (x)dx= xf (x)dx=E[X]. X X 0 0 0 Here, step (a) is due to the change of integration order. See Figure 4.16 for an illustration. (cid:3) We draw a picture to illustrate the above lemma. As shown in Figure 4.17, the mean of a positive random variable X >0 is equivalent to the area above the CDF. 199
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Figure 4.16: The double integration can be evaluated by x then t, or t then x. Figure4.17:ThemeanofapositiverandomvariableX >0canbecalculatedbyintegratingtheCDF’s complement. Lemma 4.2. Let X <0. Then E[X] can be computed from F as X (cid:90) 0 E[X]= F (t)dt. (4.18) X −∞ Proof. The idea here is also to change the integration order. (cid:90) 0 (cid:90) 0 (cid:90) 0 (cid:90) t F (t)dt= P[X ≤t]dt= f (x)dxdt X X −∞ −∞ −∞ −∞ (cid:90) 0 (cid:90) 0 (cid:90) 0 = f (x)dtdx= xf (x)dx=E[X]. X X −∞ x −∞ (cid:3) Theorem 4.7. The mean of a random variable X can be computed from the CDF as (cid:90) ∞ (cid:90) 0 E[X]= (1−F (t)) dt− F (t)dt. (4.19) X X 0 −∞ 200
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Proof.ForanyrandomvariableX,wecanpartitionX =X+−X− whereX+ andX− are the positive and negative parts, respectively. Then, the above two lemmas will give us E[X]=E[X+−X−]=E[X+]−E[X−] (cid:90) ∞ (cid:90) 0 = (1−F (t)) dt− F (t)dt. X X 0 −∞ (cid:3) AsillustratedinFigure 4.18,thisequationisequivalenttocomputingtheareasabove and below the CDF and taking the difference. Figure 4.18: The mean of a random variable X can be calculated by computing the area in the CDF. How to find the mean from the CDF • A formula is given by Equation (4.20): (cid:90) ∞ (cid:90) 0 E[X]= (1−F (t)) dt− F (t)dt. (4.20) X X 0 −∞ • This result is not commonly used, but the proof technique of switching the inte- gration order is important. 4.5 Uniform and Exponential Random Variables Therearemanyusefulcontinuousrandomvariables.Inthissection,wediscusstwoofthem: uniform random variables and exponential random variables. In the next section, we will discuss the Gaussian random variables. Similarly to the way we discussed discrete random variables, we take a generative / synthesis perspective when studying continuous random variables. We assume we have access to the PDF of the random variables so we can derive the theoretical mean and variance. The opposite direction, namely inferring the underlying model parameters from a dataset, will be discussed later. 201
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4.5.1 Uniform random variables Definition 4.11. Let X be a continuous uniform random variable. The PDF of X is (cid:40) 1 , a≤x≤b, f (x)= b−a (4.21) X 0, otherwise, where [a,b] is the interval on which X is defined. We write X ∼Uniform(a,b) to mean that X is drawn from a uniform distribution on an interval [a,b]. 3 1.2 2.5 1 2 0.8 1.5 0.6 1 0.4 0.5 0.2 0 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) PDF (b) CDF Figure 4.19: The PDF and CDF of X ∼Uniform(0.2,0.6). The shape of the PDF of a uniform random variable is shown in Figure 4.19. In this figure, we assume that the random variables X ∼ Uniform(0.2,0.6) are taken from the sample space Ω=[0,1]. Note that the height of the uniform distribution is greater than 1, since (cid:40) 1 =2.5, 0.2≤x≤0.6, f (x)= 0.6−0.2 X 0, otherwise. ThereisnothingwrongwiththisPDF,becausef (x)istheprobabilityperunitlength.Ifwe X integratef (x)overanysub-intervalbetween0.2and0.6,wecanshowthattheprobability X is between 0 and 1. The CDF of a uniform random variable can be determined by integrating f (x): X (cid:90) x F (x)= f (t)dt X X −∞ (cid:90) x 1 = dt b−a a x−a = , a≤x≤b. b−a 202
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Therefore, the complete CDF is  0, x<a,  F (x)= x−a, a≤x≤b, X b−a 1, x>b. ThecorrespondingCDFforthePDFweshowedinFigure4.19(a)isshowninFigure4.19(b). It can be seen that although the height of the PDF exceeds 1, the CDF grows linearly and saturates at 1. Remark. The uniform distribution can also be defined for discrete random variables. In this case, the probability mass function is given by 1 p (k)= , k =a,a+1,...,b. X b−a+1 Thepresenceof“1”inthedenominatorofthePMFisbecausek runsfromatob,including the two endpoints. InMATLABandPython,generatinguniformrandomnumberscanbedonebycalling commands unifrnd (MATLAB), and stats.uniform.rvs (Python). For discrete uniform random variables, in MATLAB the command is unidrnd, and in Python the command is stats.randint. % MATLAB code to generate 1000 uniform random numbers a = 0; b = 1; X = unifrnd(a,b,[1000,1]); hist(X); # Python code to generate 1000 uniform random numbers import scipy.stats as stats a = 0; b = 1; X = stats.uniform.rvs(a,b,size=1000) plt.hist(X); To compute the empirical average and variance of the random numbers in MATLAB wecancallthecommandmeanandvar.ThecorrespondingcommandinPythonisnp.mean and np.var. We can also compute the median and mode, as shown below. % MATLAB code to compute empirical mean, var, median, mode X = unifrnd(a,b,[1000,1]); M = mean(X); V = var(X); Med = median(X); Mod = mode(X); # Python code to compute empirical mean, var, median, mode X = stats.uniform.rvs(a,b,size=1000) M = np.mean(X) V = np.var(X) 203
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Med = np.median(X) Mod = stats.mode(X) The mean and variance of a uniform random variable are given by the theorem below. Theorem 4.8. If X ∼Uniform(a,b), then a+b (b−a)2 E[X]= and Var[X]= . (4.22) 2 12 Proof. We have derived these results before. Here is a recap for completeness: (cid:90) ∞ (cid:90) b x a+b E[X]= xf (x)dx= dx= , X b−a 2 −∞ a (cid:90) ∞ (cid:90) b x2 a2+ab+b2 E[X2]= x2f (x)dx= dx= , X b−a 3 −∞ a (b−a)2 Var[X]=E[X2]−E[X]2 = . 12 (cid:3) The result should be intuitive because it says that the mean is the midpoint of the PDF. Whenwillweencounterauniformrandomvariable?Uniformrandomvariablesareone of the most elementary continuous random variables. Given a uniform random variable, we can construct any random variable by using an appropriate transformation. We will discuss this technique as part of our discussion about generating random numbers. In MATLAB, computing the mean and variance of a uniform random variable can be done using the command unifstat. The Python coommand is stats.uniform.stats. % MATLAB code to compute mean and variance a = 0; b = 1; [M,V] = unifstat(a,b) # Python code to compute mean and variance import scipy.stats as stats a = 0; b = 1; M, V = stats.uniform.stats(a,b,moments=’mv’) To evaluate the probability P[(cid:96) ≤ X ≤ u] for a uniform random variable, we can call unifcdf in MATLAB and % MATLAB code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; F = unifcdf(0.3,a,b) - unifcdf(0.2,a,b) 204
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES # Python code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; F = stats.uniform.cdf(0.3,a,b)-stats.uniform.cdf(0.2,a,b) An alternative is to define an object rv = stats.uniform, and call the CDF attribute: # Python code to compute the probability P(0.2 < X < 0.3) a = 0; b = 1; rv = stats.uniform(a,b) F = rv.cdf(0.3)-rv.cdf(0.2) 4.5.2 Exponential random variables Definition 4.12. Let X be an exponential random variable. The PDF of X is (cid:40) λe−λx, x≥0, f (x)= (4.23) X 0, otherwise, where λ>0 is a parameter. We write X ∼Exponential(λ) to mean that X is drawn from an exponential distribution of parameter λ. In this definition, the parameter λ of the exponential random variable determines the rate of decay. A large λ implies a faster decay. The PDF of an exponential random variable is illustrated in Figure 4.20. We show two values of λ. Note that the initial value f (0) is X f (0)=λe−λ0 =λ. X Therefore, as long as λ>1, f (0) will exceed 1. X The CDF of an exponential random variable can be determined by (cid:90) x F (x)= f (t)dt X X −∞ (cid:90) x = λe−λt dt=1−e−λx, x≥0. 0 Therefore, if we consider the entire real line, the CDF is (cid:40) 0, x<0, F (x)= X 1−e−λx, x≥0. The corresponding CDFs for the PDFs shown in Figure 4.20(a) are shown in Fig- ure 4.20(b).Forlargerλ,thePDFf (x)decaysfasterbuttheCDFF (x)increasesfaster. X X 205
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 6 1.2 = 2 5 = 5 1 4 0.8 3 0.6 2 0.4 1 0.2 = 2 = 5 0 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 (a) PDF (b) CDF Figure 4.20: (a) The PDF and (c) the CDF of X ∼Exponential(λ). In MATLAB, the code used to generate Figure 4.20(a) is shown below. There are multiplewaysofdoingthis.Analternativewayistocallexppdf,whichwillreturnthesame result. In Python, the corresponding command is stats.expon.pdf. Note that in Python the parameter λ is specified in scale option. % MATLAB code to plot the exponential PDF lambda1 = 1/2; lambda2 = 1/5; x = linspace(0,1,1000); f1 = pdf(’exp’,x, lambda1); f2 = pdf(’exp’,x, lambda2); plot(x, f1, ’LineWidth’, 4, ’Color’, [0 0.2 0.8]); hold on; plot(x, f2, ’LineWidth’, 4, ’Color’, [0.8 0.2 0]); # Python code to plot the exponential PDF lambd1 = 1/2 lambd2 = 1/5 x = np.linspace(0,1,1000) f1 = stats.expon.pdf(x,scale=lambd1) f2 = stats.expon.pdf(x,scale=lambd2) plt.plot(x, f1) plt.plot(x, f2) To plot the CDF, we replace pdf by cdf. Similarly, in Python we replace expon.pdf by expon.cdf. % MATLAB code to plot the exponential CDF F = cdf(’exp’,x, lambda1); plot(x, F, ’LineWidth’, 4, ’Color’, [0 0.2 0.8]); # Python code to plot the exponential CDF F = stats.expon.cdf(x,scale=lambd1) plt.plot(x, F) 206
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES Theorem 4.9. If X ∼Exponential(λ), then 1 1 E[X]= and Var[X]= . (4.24) λ λ2 Proof. We have discussed this proof before. Here is a recap for completeness: (cid:90) ∞ (cid:90) ∞ E[X]= xf (x)dx= λxe−λx dx X −∞ 0 (cid:90) ∞ =− xde−λx 0 (cid:12)∞ (cid:90) ∞ 1 =−xe−λx(cid:12) + e−λx dx= , (cid:12) 0 0 λ (cid:90) ∞ (cid:90) ∞ E[X2]= x2f (x)dx= λx2e−λx dx X −∞ 0 (cid:90) ∞ =− x2de−λx 0 (cid:12)∞ (cid:90) ∞ =−x2e−λx(cid:12) + 2xe−λx dx (cid:12) 0 0 2 2 =0+ E[X]= . λ λ2 Thus, Var[X]=E[X2]−E[X]2 = 1 . λ2 (cid:3) ComputingthemeanandvarianceofanexponentialrandomvariableinMATLABand Python follows the similar procedures that we described above. 4.5.3 Origin of exponential random variables Exponential random variables are closely related to Poisson random variables. Recall that the definition of a Poisson random variable is a random variable that describes the number ofeventsthathappeninacertainperiod,e.g.,photonarrivals,numberofpedestrians,phone calls, etc. We summarize the origin of an exponential random variable as follows. What is the origin of exponential random variables? • Anexponentialrandomvariableistheinterarrivaltimebetweentwoconsecutive Poisson events. • Thatis,anexponentialrandomvariableishowmuchtimeittakestogofromN Poisson counts to N +1 Poisson counts. An example will clarify this concept. Imagine that you are waiting for a bus, as illus- tratedinFigure 4.21.Passengersarriveatthebusstopwithanarrivalrateλperunittime. Thus, for some time t, the average number of people that arrive is λt. Let N be a random 207
CHAPTER 4. CONTINUOUS RANDOM VARIABLES variable denoting the number of people. We assume that N is Poisson with a parameter λt. That is, for any duration t, the probability of observing n people follows the PMF (λt)n P[N =n]= e−λt. n! Figure 4.21: For any fixed period of time t, the number of people N is modeled as a Poisson random variable with a parameter λt. Figure 4.22: TheinterarrivaltimeT betweentwoconsecutivePoissoneventsisanexponentialrandom variable. LetT betheinterarrivaltimebetweentwopeople,bywhichwemeanthetimebetween twoconsecutivearrivals,asshowninFigure 4.22.NotethatT isarandomvariablebecause T depends on N, which is itself a random variable. To find the PDF of T, we first find the CDF of T. We note that P[T >t]( =a)P[interarrival time >t] ( =b)P[no arrival in t]( =c)P[N =0]= (λt)0 e−λt =e−λt. 0! In this set of arguments, (a) holds because T is the interarrival time, and (b) holds be- cause interarrival time is between two consecutive arrivals. If the interarrival time is larger than t, there is no arrival during the period. Equality (c) holds because N is the number of passengers. Since P[T >t]=1−F (t), where F (t) is the CDF of T, we can show that T T F (t)=1−e−λt, T d f (t)= F (t)=λe−λt. T dt T Therefore, the interarrival time T follows an exponential distribution. SinceexponentialrandomvariablesaretightlyconnectedtoPoissonrandomvariables, weshouldexpectthemtobeusefulformodelingtemporalevents.Wediscusstwoexamples. 208
4.5. UNIFORM AND EXPONENTIAL RANDOM VARIABLES 4.5.4 Applications of exponential random variables Example 4.22. (Photon arrivals) Single-photon image sensors are designed to op- erate in the photon-limited regime. The number-one goal of using these sensors is to count the number of arriving photons precisely. However, for some applications not all single-photon image sensors are used to count photons. Some are used to measure the time between two photon arrivals, such as time-of-flight systems. In this case, we are interested in measuring the time it takes for a pulse to bounce back to the sensor. The more time it takes for a pulse to come back, the greater the distance between the object and the sensor. Other applications utilize the time information. For example, high-dynamic-range imaging can be achieved by recording the time between two pho- ton arrivals because brighter regions have a higher Poisson rate λ and darker regions have a lower λ. Thefigureaboveillustratesanexampleofhigh-dynamic-rangeimaging.Whenthe sceneisbright,thelargeλwillgeneratemorephotons.Therefore,theinterarrivaltime between the consecutive photons will be relatively short. If we plot the histogram of theinterarrivaltime,weobservethatmostoftheinterarrivaltimewillbeconcentrated atsmallvalues.Darkregionsbehaveintheoppositemanner.Theinterarrivaltimewill typically be much longer. In addition, because there is more variation in the photon arrival times, the histogram will look shorter and wider. Nevertheless, both cases are modeled by the exponential random variable. Example4.23.(Energy-efficientescalator)Manyairportstodayhaveinstalledvariable- speedescalators.Theseescalatorschangetheirspeedsaccordingtothetraffic.Ifthere are no passengers for more than a certain period (say, 60 seconds), the escalator will switch from the full-speed mode to the low-speed mode. For moderately busy esca- lators, the variable-speed configuration can save energy. The interesting data-science problem is to determine, given a traffic pattern, e.g., the one shown in Figure 4.23, whether we can predict the amount of energy savings? We will not dive into the details of this problem, but we can briefly discuss the principle.Considerafixedarrivalrateλ(say,theaveragefrom07:00to08:00).Thein- terarrival time, according to our discussion above, follows an exponential distribution. 209
CHAPTER 4. CONTINUOUS RANDOM VARIABLES So we know that f (t)=λe−λt. T Supposethattheescalatorswitchestolow-speedmodewhentheinterarrivaltime exceeds τ. Then we can define a new variable Y to denote the amount of time that the escalator will operate in the low-speed mode. This new variable is (cid:40) T −τ, T >τ, Y = 0, T ≤τ. In other words, if the interarrival time T is more than τ, then the amount of time saved Y takes the value T −τ, but if the interarrival time is less than τ, then there is no saving. Figure4.23:Thevariable-speedescalatorproblem.[Left]Wemodelthepassengersasindependent Poissonarrivals.Thus,theinterarrivaltimeisexponential.[Right]Ahypotheticalpassengerarrival rate (number of people per minute), from 06:00 to 23:00. Figure 4.24: The escalator problem requires modeling the cutoff threshold τ such that if T >τ, thesavingsareY =T−τ.IfT <τ,thenY =0.Theleft-handsideofthefigureshowshowthe PDF of Y is constructed. The PDF of Y can be computed according to Figure 4.24. There are two parts to the calculation. When Y =0, there is a probability mass such that (cid:90) τ (cid:90) τ f (0)=P[Y =0]= f (t)dt= λe−λt dt=1−e−λτ. Y T 0 0 For other values of y, we can show that f (y)=f (y+τ)=λe−λ(y+τ). Y T Therefore, to summarize, we can show that the PDF of Y is (cid:40) (1−e−λτ)δ(y), y =0, f (y)= Y λe−λ(y+τ), y >0. 210
4.6. GAUSSIAN RANDOM VARIABLES Consequently, we can compute E[Y] and Var[Y] and analyze how these values change for λ (which itself changes with the time of day). Furthermore, we can analyze the amount of savings in terms of dollars. We leave these problems as an exercise. Closing remark. The photon arrival problem and the escalator problem are two of many examples we can find in which exponential random variables are useful for modeling a problem.Wedidnotgointothedetailsoftheproblemsbecauseeachofthemrequiressome additionalmodelingtoaddresstherealpracticalproblem.Weencourageyoutoexplorethese problems further. Our message is simple: Many problems can be modeled by exponential random variables, most of which are associated with time. 4.6 Gaussian Random Variables We now discuss the most important continuous random variable — the Gaussian random variable(alsoknownasthenormalrandomvariable).Wecallitthemostimportantrandom variable because it is widely used in almost all scientific disciplines. Many of us have used Gaussian random variables before, and perhaps its bell shape is the first lesson we learn in statistics. However, there are many mysteries about Gaussian random variables which you mayhavemissed,suchas:WheredoestheGaussianrandomvariablecomefrom?Whydoes ittakeabellshape?WhatarethepropertiesofaGaussianrandomvariable?Theobjective ofthissectionistoexplaineverythingyouneedtoknowaboutaGaussianrandomvariable. 4.6.1 Definition of a Gaussian random variable Definition 4.13. A Gaussian random variable is a random variable X such that its PDF is 1 (cid:26) (x−µ)2(cid:27) f (x)= √ exp − , (4.25) X 2πσ2 2σ2 where (µ,σ2) are parameters of the distribution. We write X ∼Gaussian(µ,σ2) or X ∼N(µ,σ2) to say that X is drawn from a Gaussian distribution of parameter (µ,σ2). Gaussian random variables have two parameters (µ,σ2). It is noteworthy that the mean is µ and the variance is σ2 — these two parameters are exactly the first moment and the second central moment of the random variable. Most other random variables do not have this property. Note that a Gaussian random variable is positive from −∞ to ∞. Thus, f (x) has X a non-zero value for any x, even though the value may be extremely small. A Gaussian random variable is also symmetric about µ. If µ=0, then f (x) is an even function. X The shape of the Gaussian is illustrated in Figure 4.25. When we fix the variance and change the mean, the PDF of the Gaussian moves left or right depending on the sign of the mean. When we fix the mean and change the variance, the PDF of the Gaussian changes 211
CHAPTER 4. CONTINUOUS RANDOM VARIABLES its width. Since any PDF should integrate to unity, a wider Gaussian means that the PDF is shorter. Note also that if σ is very small, it is possible that f (x) > 1 although the X integration over Ω will still be 1. 0.5 0.5 = -3 = 0.8 = -0.3 = 1 0.4 = 0 0.4 = 2 = 1.2 = 3 = 4 = 4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 -10 -5 0 5 10 -10 -5 0 5 10 µ changes, σ =1 µ=0, σ changes Figure 4.25: A Gaussian random variable with different µ and σ. On a computer, plotting the Gaussian PDF can be done by calling the function pdf(’norm’,x) in MATLAB, and stats.norm.pdf in Python. % MATLAB to generate a Gaussian PDF x = linspace(-10,10,1000); mu = 0; sigma = 1; f = pdf(’norm’,x,mu,sigma); plot(x, f); # Python to generate a Gaussian PDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-10,10,1000) mu = 0; sigma = 1; f = stats.norm.pdf(x,mu,sigma) plt.plot(x,f) Our next result concerns the mean and variance of a Gaussian random variable. You may wonder why we need this theorem when we already know that µ is the mean and σ2 is the variance. The answer is that we have not proven these two facts. Theorem 4.10. If X ∼Gaussian(µ,σ2), then E[X]=µ, and Var[X]=σ2. (4.26) 212
4.6. GAUSSIAN RANDOM VARIABLES Proof. The expectation can be derived via substitution: E[X]= √ 1 (cid:90) ∞ xe−(x 2− σµ 2)2 dx 2πσ2 −∞ ( =a) √ 1 (cid:90) ∞ (y+µ)e− 2y σ2 2 dy 2πσ2 −∞ = √ 1 (cid:90) ∞ ye− 2y σ2 2 dy+ √ 1 (cid:90) ∞ µe− 2y σ2 2 dy 2πσ2 2πσ2 −∞ −∞ ( =b) 0+µ(cid:18) √ 1 (cid:90) ∞ e− 2y σ2 2 dy(cid:19) 2πσ2 −∞ (c) = µ, where in (a) we substitute y =x−µ, in (b) we use the fact that the first integrand is odd so that the integration is 0, and in (c) we observe that integration over the entire sample space of the PDF yields 1. The variance is also derived by substitution. Var[X]= √ 1 (cid:90) ∞ (x−µ)2e−(x 2− σµ 2)2 dx 2πσ2 −∞ ( =a) √σ2 (cid:90) ∞ y2e−y 22 dy 2π −∞ = √σ2 (cid:18) −ye−y 22(cid:12) (cid:12) (cid:12)∞ (cid:19) + √σ2 (cid:90) ∞ e−y 22 dy 2π −∞ 2π −∞ =0+σ2(cid:18) √1 (cid:90) ∞ e−y 22 dy(cid:19) 2π −∞ =σ2, where in (a) we substitute y =(x−µ)/σ. 4.6.2 Standard Gaussian WeneedtoevaluatetheprobabilityP[a≤X ≤b]ofaGaussianrandomvariableX inmany practicalsituations.ThisinvolvestheintegrationoftheGaussianPDF,i.e.,determiningthe CDF. Unfortunately, there is no closed-form expression of P[a≤X ≤b] in terms of (µ,σ2). This leads to what we call the standard Gaussian. Definition 4.14. The standard Gaussian (or standard normal) random variable X has a PDF f X(x)= √1 e−x 22 . (4.27) 2π That is, X ∼N(0,1) is a Gaussian with µ=0 and σ2 =1. The CDF of the standard Gaussian can be determined by integrating the PDF. We have a special notation for this CDF. Figure 4.26 illustrates the idea. 213
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Definition 4.15. The CDF of the standard Gaussian is defined as the Φ(·) function Φ(x)d =ef F X(x)= √1 (cid:90) x e−t 22 dt. (4.28) 2π −∞ Figure 4.26: Definition of the CDF of the standard Gaussian Φ(x). % MATLAB code to generate standard Gaussian PDF and CDF x = linspace(-5,5,1000); f = normpdf(x,0,1); F = normcdf(x,0,1); figure; plot(x, f); figure; plot(x, F); # Python code to generate standard Gaussian PDF and CDF import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-10,10,1000) f = stats.norm.pdf(x) F = stats.norm.cdf(x) plt.plot(x,f); plt.show() plt.plot(x,F); plt.show() The standard Gaussian’s CDF is related to a so-called error function defined as 2 (cid:90) x erf(x)= √ e−t2 dt. (4.29) π 0 It is easy to link Φ(x) with erf(x): 1(cid:20) (cid:18) x (cid:19)(cid:21) √ Φ(x)= 1+erf √ , and erf(x)=2Φ(x 2)−1. 2 2 WiththestandardGaussianCDF,wecandefinetheCDFofanarbitraryGaussian. 214
4.6. GAUSSIAN RANDOM VARIABLES Theorem 4.11 (CDF of an arbitrary Gaussian). Let X ∼N(µ,σ2). Then (cid:18) (cid:19) x−µ F (x)=Φ . (4.30) X σ Proof. We start by expressing F (x): X F (x)=P[X ≤x] X =(cid:90) x √ 1 e−(t 2− σµ 2)2 dt. 2πσ2 −∞ Substituting y = t−µ, and using the definition of standard Gaussian, we have σ (cid:90) x √ 1 e−(t 2− σµ 2)2 dt=(cid:90) x− σµ √1 e−y 22 dy 2πσ2 2π −∞ −∞ (cid:18) (cid:19) x−µ =Φ . (cid:3) σ If you would like to verify this on a computer, you can try the following code. % MATLAB code to verify standardized Gaussian x = linspace(-5,5,1000); mu = 3; sigma = 2; f1 = normpdf((x-mu)/sigma,0,1); % standardized f2 = normpdf(x, mu, sigma); % raw # Python code to verify standardized Gaussian import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats x = np.linspace(-5,5,1000) mu = 3; sigma = 2; f1 = stats.norm.pdf((x-mu)/sigma,0,1) # standardized f2 = stats.norm.cdf(x,mu,sigma) # raw An immediate consequence of this result is that (cid:18) (cid:19) (cid:18) (cid:19) b−µ a−µ P[a<X ≤b]=Φ −Φ . (4.31) σ σ To see this, note that P[a<X ≤b]=P[X ≤b]−P[X ≤a] (cid:18) (cid:19) (cid:18) (cid:19) b−µ a−µ =Φ −Φ . σ σ The inequality signs of the two end points are not important. That is, the statement also holds for P[a ≤ X ≤ b] or P[a < X < b], because X is a continuous random variable at every x. Thus, P[X = a] = P[X = b] = 0 for any a and b. Besides this, Φ has several properties of interest. See if you can prove these: 215
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Corollary 4.1. Let X ∼N(µ,σ2). Then the following results hold: • Φ(y)=1−Φ(−y). (cid:16) (cid:17) • P[X ≥b]=1−Φ b−µ . σ (cid:16) (cid:17) (cid:16) (cid:17) • P[|X|≥b]=1−Φ b−µ +Φ −b−µ . σ σ 4.6.3 Skewness and kurtosis In modern data analysis we are sometimes interested in high-order moments. Here we con- sider two useful quantities: skewness and kurtosis. Definition 4.16. For a random variable X with PDF f (x), define the following X central moments as mean=E[X]d =ef µ, (cid:104) (cid:105) variance=E (X−µ)2 d =ef σ2, (cid:34)(cid:18) X−µ(cid:19)3(cid:35) skewness=E d =ef γ, σ (cid:34)(cid:18) X−µ(cid:19)4(cid:35) kurtosis=E d =ef κ, excess kurtosisd =ef κ−3. σ As you can see from the definitions above, skewness is the third central moment, whereas kurtosis is the fourth central moment. Both skewness and kurtosis can be regarded as“deviations”fromastandardGaussian—notintermsofmeanandvariancebutinterms of shape. Skewness measures the asymmetry of the distribution. Figure 4.27 shows three differ- ent distributions: one with left skewness, one with right skewness, and one symmetric. The skewness of a curve is • Skewed towards left: positive • Skewed towards right: negative • Symmetric: zero What is skewness? (cid:20)(cid:16) (cid:17)3(cid:21) • E X−µ . σ • Measures the asymmetry of the distribution. • Gaussian has skewness 0. 216
4.6. GAUSSIAN RANDOM VARIABLES 0.4 positive skewness symmetric 0.3 negative skewness 0.2 0.1 0 0 5 10 15 20 Figure 4.27: Skewness of a distribution measures the asymmetry of the distribution. In this example the skewnesses are: orange = 0.8943, black = 0, blue = -1.414. Kurtosismeasureshowheavy-tailedthedistributionis.Therearetwoformsofkurtosis: oneisthestandardkurtosis,whichisthefourthcentralmoment,andtheotheristheexcess kurtosis, which is κ = κ−3. The constant 3 comes from the kurtosis of a standard excess Gaussian.Excesskurtosisismorewidelyusedindataanalysis.Theinterpretationofkurtosis is the comparison to a Gaussian. If the kurtosis is positive, the distribution has a tail that decays faster than a Gaussian. If the kurtosis is negative, the distribution has a tail that decays more slowly than a Gaussian. Figure 4.28 illustrates the (excess) kurtosis of three different distributions. 1 kurtosis > 0 0.8 kurtosis = 0 kurtosis < 0 0.6 0.4 0.2 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 4.28: Kurtosis of a distribution measures how heavy-tailed the distribution is. In this example, the (excess) kurtoses are: orange = 2.8567, black = 0, blue = −0.1242. What is kurtosis? (cid:20)(cid:16) (cid:17)4(cid:21) • κ=E X−µ . σ • Measures how heavy-tailed the distribution is. Gaussian has kurtosis 3. • Some statisticians prefer excess kurtosis κ − 3, so that Gaussian has excess kurtosis 0. 217
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Random variable Mean Variance Skewness Excess kurtosis µ σ2 γ κ−3 Bernoulli p p(1−p) √1−2p 1 + 1 −6 p(1−p) 1−p p Binomial np np(1−p) √1−2p 6p2−6p+1 np(1−p) np(1−p) Geometric 1 1−p √2−p p2−6p+6 p p2 1−p 1−p Poisson λ λ √1 1 λ λ Uniform a+b (b−a)2 0 −6 2 12 5 Exponential 1 1 2 6 λ λ2 Gaussian µ σ2 0 0 Table 4.1: The first few moments of commonly used random variables. On a computer, computing the empirical skewness and kurtosis is done by built-in commands. Their implementations are based on the finite-sample calculations γ ≈ 1 (cid:88)N (cid:18) X n−µ(cid:19)3 , N σ n=1 κ≈ 1 (cid:88)N (cid:18) X n−µ(cid:19)4 . N σ n=1 TheMATLABandPythonbuilt-incommandsareshownbelow,usingagammadistribution as an example. % MATLAB code to compute skewness and kurtosis X = random(’gamma’,3,5,[10000,1]); s = skewness(X); k = kurtosis(X); # Python code to compute skewness and kurtosis import scipy.stats as stats X = stats.gamma.rvs(3,5,size=10000) s = stats.skew(X) k = stats.kurtosis(X) Example4.24.Tofurtherillustratethebehaviorofskewnessandkurtosis,weconsider an example using the gamma random variable X. The PDF of X is given by the equation 1 f X(x)= Γ(k)θkxk−1e−x θ, (4.32) where Γ(·) is known as the gamma function. If k is an integer, the gamma function is 218
4.6. GAUSSIAN RANDOM VARIABLES just the factorial: Γ(k)=(k−1)!. A gamma random variable is parametrized by two parameters (k,θ). As k increases or decreases, the shape of the PDF will change. For example, when k =1, the distribution is simplified to an exponential distribution. Without going through the (tedious) integration, we can show that the skewness and the (excess) kurtosis of Gamma(k,θ) are 2 skewness= √ , k 6 (excess) kurtosis= . k As we can see from these results, the skewness and kurtosis diminish as k grows. This can be confirmed from the PDF of Gamma(k,θ) as shown in Figure 4.29. 0.4 k = 2 k = 5 0.3 k = 10 k = 15 k = 20 0.2 0.1 0 0 5 10 15 20 25 30 Figure 4.29: The PDF of a gamma distribution Gamma(k,θ), where θ = 1. The skewness and the kurtosis are decaying to zero. Example 4.25. Let us look at a real example. On April 15, 1912, RMS Titanic sank after hitting an iceberg. The disaster killed 1502 out of 2224 passengers and crew. A hundred years later, we want to analyze the data. At https://www.kaggle.com/c/ titanic/thereisadatasetcollectingtheidentities,age,gender,etc.,ofthepassengers. We partition the dataset into two: one for those who died and the other one for those who survived. We plot the histograms of the ages of the two groups and compute several statistics of the dataset. Figure 4.30 shows the two datasets. 40 40 30 30 20 20 10 10 0 0 0 20 40 60 80 0 20 40 60 80 age age Group 1 (died) Group 2 (survived) Figure 4.30: The Titanic dataset https://www.kaggle.com/c/titanic/. 219
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Statistics Group 1 (Died) Group 2 (Survived) Mean 30.6262 28.3437 Standard Deviation 14.1721 14.9510 Skewness 0.5835 0.1795 Excess Kurtosis 0.2652 −0.0772 Note that the two groups of people have very similar means and standard devia- tions.Inotherwords,ifweonlycomparethemeanandstandarddeviation,itisnearly impossibletodifferentiatethetwogroups.However,theskewnessandkurtosisprovide more information related to the shape of the histograms. For example, Group 1 has morepositiveskewness,whereasGroup2isalmostsymmetrical.Oneinterpretationis that more young people offered lifeboats to children and older people. The kurtosis of Group 1 is slightly positive, whereas that of Group 2 is slightly negative. Therefore, high-order moments can sometimes be useful for data analysis. 4.6.4 Origin of Gaussian random variables The Gaussian random variable has a long history. Here, we provide one perspective on why Gaussian random variables are so useful. We give some intuitive arguments but leave the formal mathematical treatment for later when we introduce the Central Limit Theorem. Let’s begin with a numerical experiment. Consider throwing a fair die. We know that this will give us a (discrete) uniform random variable X. If we repeat the experiment many timeswecanplotthehistogram,anditwillreturnusaplotof6impulseswithequalheight, as shown in Figure 4.31(a). Now, suppose we throw two dice. Call them X and X , and let Z = X +X , i.e., 1 2 1 2 the sum of two dice. We want to find the distribution of Z. To do so, we first list out all the possible outcomes in the sample space; this gives us {(1,1),(1,2),...,(6,6)}. We then sum the numbers, which gives us a list of states of Z: {2,3,4,...,12}. The probability of gettingthesestatesisshowninFigure4.31(b),whichhasatriangularshape.Thetriangular shape makes sense because to get the state “2”, we must have the pair (1,1), which is quite unlikely. However, if we want to get the state 7, it would be much easier to get a pair, e.g., (6,1),(5,2),(4,3),(3,4),(2,5),(1,6) would all do the job. Now,whatwillhappenifwethrow5diceandconsiderZ =X +X +···+X ?Itturns 1 2 5 out that the distribution will continue to evolve and give something like Figure 4.31(c). This is starting to approximate a bell shape. Finally, if we throw 100 dice and consider Z = X +X +···+X , the distribution will look like Figure 4.31(d). The shape is 1 2 100 becoming a Gaussian! This numerical example demonstrates a fascinating phenomenon: As we sum more random variables, the distribution of the sum will converge to a Gaussian. If you are curious about how we plot the above figures, the following MATLAB and Python code can be useful. % MATLAB code to show the histogram of Z = X1+X2+X3 N = 10000; X1 = randi(6,1,N); X2 = randi(6,1,N); X3 = randi(6,1,N); Z = X1 + X2 + X3; histogram(Z, 2.5:18.5); 220
4.6. GAUSSIAN RANDOM VARIABLES # Python code to show the histogram of Z = X1+X2+X3 import numpy as np import matplotlib.pyplot as plt N = 10000 X1 = np.random.randint(1,6,size=N) X2 = np.random.randint(1,6,size=N) X3 = np.random.randint(1,6,size=N) Z = X1 + X2 + X3 plt.hist(Z,bins=np.arange(2.5,18.5)) (a) X (b) X +X (c) X +···+X (d) X +···+X 1 1 2 1 5 1 100 Figure 4.31: Whenaddinguniformrandomvariables,theoveralldistributionapproachesaGaussianas the number of summed variables increase. Can we provide a more formal description of this? Yes, but we need some new mathe- maticaltoolsthatwehavenotyetdeveloped.So,forthetimebeing,wewilloutlinetheflow of the arguments and leave the technical details to a later chapter. Suppose we have two independent random variables with identical distributions, e.g., X and X , where both are 1 2 uniform.ThisgivesusPDFsf (x)andf (x)thataretwoidenticalrectangularfunctions. X1 X2 By what operation can we combine these two rectangular functions and create a triangle function?Thekeyliesintheconceptof convolution.Ifyouconvolvetworectanglefunctions, you will get a triangle function. Here we define the convolution of f as X (cid:90) ∞ (f ∗f )(x)= f (τ)f (x−τ)dτ. X X X X −∞ In fact, for any pair of random variables X and X (not necessarily uniform random vari- 1 2 ables),thesumZ =X +X willhaveaPDFgivenbytheconvolutionofthetwoPDFs.We 1 2 have not yet proven this, but if you trust what we are saying, we can effectively generalize this argument to many random variables. If we have N random variables, then the sum Z = X +X +···+X will have a PDF that is the result of N convolutions of all the 1 2 N individual PDFs. What is the PDF of X+Y? • Summing X+Y is equivalent to convolving the PDFs f ∗f . X Y • If you sum many random variables, you convolve all their PDFs. Howdoweanalyzetheseconvolutions?WeneedasecondsetoftoolsrelatedtoFourier transforms. The Fourier transform of a PDF is known as the characteristic function, which 221
CHAPTER 4. CONTINUOUS RANDOM VARIABLES we will discuss later, but the name is not important now. What matters is the important property of the Fourier transform, that a convolution in the original space is multiplication in the Fourier space. That is, F{(f ∗f ∗···∗f )}=F{f }·F{f }·····F{f }. X X X X X X Multiplication in the Fourier space is much easier to analyze. In particular, for independent and identically distributed random variables, the multiplication will easily translate to ad- dition in the exponent. Then, by truncating the exponent to the second order, we can show that the limiting object in the Fourier space is approaching a Gaussian. Finally, since the inverseFouriertransformofaGaussianremainsaGaussian,wehaveshownthattheinfinite convolution will give us a Gaussian. Here is some numerical evidence for what we have just described. Recall that the Fourier transform of a rectangle function is the sinc function. Therefore, if we have an infiniteconvolutionofrectangularfunctions,equivalently,wehaveaninfiniteproductofsinc functionsintheFourierspace.Multiplyingsincfunctionsisreasonablyeasy.SeeFigure4.32 for the first three sincs. It is evident that with just three sinc functions, the shape closely approximates a Gaussian. 1.25 (sin x)/x 1 (sin x)2/x2 0.75 (sin x)3/x3 0.5 0.25 0 -0.25 -0.5 -10 -8 -6 -4 -2 0 2 4 6 8 10 Figure 4.32: Convolving the PDF of a uniform distribution is equivalent to multiplying their Fourier transformsintheFourierspace.Asthenumberofconvolutionsgrows,theproductisgraduallybecoming Gaussian. How about distributions that are not rectangular? We invite you to numerically visu- alizetheeffectwhenyouconvolvethefunctionmanytimes.Youwillseethatasthenumber of convolutions grows, the resulting function will become more and more like a Gaussian. Regardless of what the input random variables are, as long as you add them, the sum will have a distribution that looks like a Gaussian: X +X +···+X (cid:32)Gaussian. 1 2 N We use the notation (cid:32) to emphasize that the convergence is not the usual form of conver- gence. We will make this precise later. The implication of this line of discussion is important. Regardless of the underlying true physical process, if we are only interested in the sum (or average), the distribution will be more or less Gaussian. In most engineering problems, we are looking at the sum 222
4.7. FUNCTIONS OF RANDOM VARIABLES or average. For example, when generating an image using an image sensor, the sensor will add a certain amount of read noise. Read noise is caused by the random fluctuation of the electrons in the transistors due to thermal distortions. For high-photon-flux situations, we are typically interested in the average read noise rather than the electron-level read noise. ThusGaussianrandomvariablesbecomeareasonablemodelforthat.Inotherapplications, such as imaging through a turbulent medium, the random phase distortions (which alter thephaseofthewavefront)canalsobemodeledasaGaussianrandomvariable.Hereisthe summary of the origin of a Gaussian random variable: What is the origin of Gaussian? • When we sum many independent random variables, the resulting random vari- able is a Gaussian. • This is known as the Central Limit Theorem. The theorem applies to any ran- dom variable. • Summing random variables is equivalent to convolving the PDFs. Convolving PDFs infinitely many times yields the bell shape. 4.7 Functions of Random Variables One common question we encounter in practice is the transformation of random variables. The question can be summarized as follows: Given a random variable X with PDF f (x) X and CDF F (x), and supposing that Y = g(X) for some function g, what are f (y) and X Y F (y)? This is a prevalent question. For example, we measure the voltage V, and we want Y to analyze the power P = V2/R. This involves taking the square of a random variable. Another example: We know the distribution of the phase Θ, but we want to analyze the signal cos(ωt+Θ). This involves a cosine transformation. How do we convert one variable to another? Answering this question is the goal of this section. 4.7.1 General principle We will first outline the general principle for tackling this type of problem. In the following subsection, we will give a few concrete examples. SupposewearegivenarandomvariableX withPDFf (x)andCDFF (x).LetY = X X g(X)forsomeknownandfixedfunctiong.Forsimplicity,weassumethatgismonotonically increasing. In this case, the CDF of Y can be determined as follows. F (y)( =a)P[Y ≤y]( =b)P[g(X)≤y] Y ( =c)P[X ≤g−1(y)] ( =d) F (g−1(y)). X 223
CHAPTER 4. CONTINUOUS RANDOM VARIABLES This sequence of steps is not difficult to understand. Step (a) is the definition of CDF. Step (b) substitutes g(X) for Y. Step (c) uses the fact that since g is invertible, we can apply the inverse of g to both sides of g(X)≤y to yield X ≤g−1(y). Step (d) is the definition of the CDF, but this time applied to P[X ≤♣]=F (♣), for some ♣. X ItwillbeusefultovisualizethesituationinFigure4.33.Here,weconsiderauniformly distributed X so that the CDF F (x) is a straight line. According to F , any samples X X drawn according to F are equally likely, as illustrated by the yellow dots on the x-axis. X As we transform the X’s through Y = g(X), we increase/decrease the spacing between two samples. Therefore, some samples become more concentrated while some become less concentrated. The distribution of these transformed samples (the yellow dots on the y-axis) forms a new CDF F (y). The result F (y) = F (g−1(y)) holds when we look at Y. The Y Y X samples are traveling with g−1 in order to go back to F . Therefore, we need g−1 in the X formula. Figure 4.33: When transforming a random variable X to Y = g(X), the distributions are defined accordingtothespacingbetweensamples.Inthisfigure,auniformlydistributedX willbecomesqueezed by some parts of g and widened in other parts of g. Why should we use the CDF and not the PDF in Figure 4.33? The advantage of the CDF is that it is an increasing function. Therefore, no matter what the function g is, the input and the output functions will still be increasing. If we use the PDF, then the non- monotonic behavior of the PDF will interact with another nonlinear function g. It becomes much harder to decouple the two. We can carry out the integrations to determine F (g−1(y)). It can be shown that X (cid:90) g−1(y) F (g−1(y))= f (x(cid:48))dx(cid:48), (4.33) X X −∞ and hence, by the fundamental theorem of calculus, we have d d d (cid:90) g−1(y) f (y)= F (y)= F (g−1(y))= f (x(cid:48))dx(cid:48) Y dy Y dy X dy X −∞ (cid:18) dg−1(y)(cid:19) = ·f (g−1(y)), (4.34) dy X 224
4.7. FUNCTIONS OF RANDOM VARIABLES wherethelaststepisduetothechainrule.Basedonthislineofreasoningwecansummarize a “recipe” for this problem. How to find the PDF of Y =g(X) • Step 1: Find the CDF F (y), which is F (y)=F (g−1(y)). Y Y X (cid:16) (cid:17) • Step 2: Find the PDF f (y), which is f (y)= dg−1(y) ·f (g−1(y)). Y Y dy X Thisrecipeworkswhengisaone-to-onemapping.Ifgisnotone-to-one,e.g.,g(x)=x2 √ implies g−1(y)=± y, then we will have some issues with the above two steps. When this happens, then instead of writing X ≤g−1(y) we need to determine the set {x|g(x)≤y}. 4.7.2 Examples Example 4.26.(Lineartransform)LetX bearandomvariablewithPDFf (x)and X CDF F (x). Let Y =2X+3. Find f (y) and F (y). Express the answers in terms of X Y Y f (x) and F (x). X X Solution. We first note that F (y)=P[Y ≤y] Y =P[2X+3≤y] (cid:20) (cid:21) (cid:18) (cid:19) y−3 y−3 =P X ≤ =F . 2 X 2 Therefore, the PDF is d f (y)= F (y) Y dy Y (cid:18) (cid:19) d y−3 = F dy X 2 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) y−3 d y−3 1 y−3 =F(cid:48) = f . X 2 dy 2 2 X 2 Follow-Up. (Linear transformation of a Gaussian random variable).Suppose X is a Gaus- sianrandomvariablewithzeromeanandunitvariance,andletY =aX+b.ThentheCDF and PDF of Y are respectively (cid:18) (cid:19) (cid:18) (cid:19) y−b y−b F (y)=F =Φ , Y X a a (cid:18) (cid:19) f Y(y)= a1 f X y− a b = √ 21 πae−(y 2− ab 2)2 . Follow-Up. (Linear transformation of an exponential random variable). Suppose X is an exponential random variable with parameter λ, and let Y = aX +b. Then the CDF and 225
CHAPTER 4. CONTINUOUS RANDOM VARIABLES PDF of Y are respectively (cid:18) (cid:19) y−b F (y)=F Y X a =1−e−λ a(y−b), y ≥b, (cid:18) (cid:19) 1 y−b f (y)= f Y a X a λ = e−λ a(y−b), y ≥b. a Example 4.27. Let X be a random variable with PDF f (x) and CDF F (x). Sup- X X posing that Y = X2, find f (y) and F (y). Express the answers in terms of f (x) Y Y X and F (x). X Solution. We note that √ √ F (y)=P[Y ≤y]=P[X2 ≤y]=P[− y ≤X ≤ y] Y √ √ =F ( y)−F (− y). X X Therefore, the PDF is d f (y)= F (y) Y dy Y d √ √ = (F ( y)−F (− y)) dy X X √ d √ √ d √ =F(cid:48) ( y) y−F(cid:48) (− y) (− y) X dy X dy 1 √ √ = √ (f ( y)+f (− y)). 2 y X X Figure 4.34: When transforming a random variable X to Y =X2, the CDF becomes F (y)= √ Y by −− aa and the PDF becomes f Y(y)= √ y(1 b−a). 226
4.7. FUNCTIONS OF RANDOM VARIABLES FollowUp.(Squareofauniformrandomvariable)SupposeX isauniformrandomvariable in [a,b] (assume a>0), and let Y =X2. Then the CDF and PDF of Y are respectively √ y−a F (y)= , a2 ≤y ≤b2, Y b−a 1 f (y)= √ , a2 ≤y ≤b2. Y y(b−a) Example 4.28. Let X ∼Uniform(0,2π). Suppose Y =cosX. Find f (y) and F (y). Y Y Solution. First, we need to find the CDF of X. This can be done by noting that (cid:90) x (cid:90) x 1 x F (x)= f (x(cid:48))dx(cid:48) = dx(cid:48) = . X X 2π 2π −∞ 0 Thus, the CDF of Y is F (y)=P[Y ≤y]=P[cosX ≤y] Y =P[cos−1y ≤X ≤2π−cos−1y] =F (2π−cos−1y)−F (cos−1y) X X cos−1y =1− . π The PDF of Y is d d (cid:18) cos−1y(cid:19) f (y)= F (y)= 1− Y dy Y dy π 1 = , (cid:112) π 1−y2 where we used the fact that d cos−1y = √−1 . dy 1−y2 Example 4.29. Let X be a random variable with PDF f (x)=aexe−aex . X Let Y =eX, and find f (y). Y Solution. We first note that F (y)=P[Y ≤y]=P[eX ≤y] Y (cid:90) logy =P[X ≤logy]= aexe−aex dx. −∞ 227
CHAPTER 4. CONTINUOUS RANDOM VARIABLES To find the PDF, we recall the fundamental theorem of calculus. This gives us d (cid:90) logy f (y)= aexe−aex dx Y dy −∞ (cid:18) d (cid:19)(cid:32) d (cid:90) logy (cid:33) = logy aexe−aex dx dy dlogy −∞ 1 = aelogye−aelogy =ae−ay. y Closing remark. The transformation of random variables is a fundamental technique in datascience.Theapproachwehavepresentedisthemostrudimentaryyetthemostintuitive. The key is to visualize the transformation and how the random samples are allocated after the transformation. Note that the density of the random samples is related to the slope of the CDF. Therefore, if the transformation maps many samples to similar values, the slope of the CDF will be steep. Once you understand this picture, the transformation will be a lot easier to understand. Is it possible to replace the paper-and-pencil derivation of a transformation with a computer? If the objective is to transform random realizations, then the answer is yes because your goal is to transform numbers to numbers, which can be done on a computer. √ For example, transforming a sample x to x is straightforward on a computer. However, 1 1 if the objective is to derive the theoretical expression of the PDF, then the answer is no. Why might we want to derive the theoretical PDF? We might want to analyze the mean, variance,orotherstatisticalproperties.Wemayalsowanttoreverse-engineeranddetermine a transformation that can yield a specific PDF. This would require a paper-and-pencil derivation. In what follows, we will discuss a handy application of the transformations. What are the rules of thumb for transformation of random variables? • Always find the CDF F (y) = P[g(X) ≤ y]. Ask yourself: What are the values Y of X such that g(X)≤y? Think of the cosine example. • Sometimes you do not need to solve for F (y) explicitly. The fundamental the- Y orem of calculus can help you find f (y). Y • Drawpictures.Askyourselfwhetheryouneedtosqueezeorstretchthesamples. 4.8 Generating Random Numbers Most scientific computing software nowadays has built-in random number generators. For common types of random variables, e.g., Gaussian or exponential, these random number generatorscaneasilygeneratenumbersaccordingtothechosendistribution.However,ifwe aregivenanarbitraryPDF(orPMF)thatisnotamongthelistofpredefineddistributions, how can we generate random numbers according to the PDF or PMF we want? 228
4.8. GENERATING RANDOM NUMBERS 4.8.1 General principle Generating random numbers according to the desired distribution can be formulated as an inverse problem. Suppose that we can generate uniformly random numbers according to Uniform(0,1). This is a fragile assumption, and this process can be done on almost all computers today. Let us call this random variable U and its realization u. Suppose that we also have a desired distribution f (x) (and its CDF F (x)). We can put the two random X X variables U and X on the two axes of Figure 4.35, yielding an input-output relationship. The inverse problem is: By using what transformation g, such that X =g(U), can we make sure that X is distributed according to f (x) (or F (x))? X X Figure 4.35: Generating random numbers according to a known CDF. The idea is to first generate a uniform(0,1) random variable, then do an inverse mapping F−1. X Theorem 4.12. The transformation g that can turn a uniform random variable into a random variable following a distribution F (x) is given by X g(u)=F−1(u). (4.35) X That is, if g =F−1, then g(U) will be distributed according to f (or F ). X X X Proof. First, we know that if U ∼Uniform(0,1), then f (u)=1 for 0≤u≤1, so U (cid:90) u F (u)= f (u)du=u, U U −∞ for 0≤u≤1. Let g =F−1 and define Y =g(U). Then the CDF of Y is X F (y)=P[Y ≤y]=P[g(U)≤y] Y =P[F−1(U)≤y] X =P[U ≤F (y)]=F (y). X X Therefore, we have shown that the CDF of Y is the CDF of X. (cid:3) 229
CHAPTER 4. CONTINUOUS RANDOM VARIABLES The theorem above states that if we want a distribution F , then the transformation X should be g =F−1. This suggests a two-step process for generating random numbers. X How do we generate random numbers from an arbitrary distribution F ? X • Step 1: Generate a random number U ∼Uniform(0,1). • Step 2: Let Y =F−1(U). (4.36) X Then the distribution of Y is F . X 4.8.2 Examples Example 4.30. How can we generate Gaussian random numbers with mean µ and variance σ2 from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is (cid:18) (cid:19) x−µ F (x)=Φ . X σ Therefore, the transformation g is g(U)=F−1(U)=σΦ−1(U)+µ. X In Figure 4.36, we plot the CDF of F and the transformation g. X 1 -4 10 0.9 9 0.8 8 7 0.7 6 0.6 5 0.5 4 3 0.4 2 0.3 1 0.2 0 -1 0.1 -2 0 -3 -10 -8 -6 -4 -2 0 2 4 6 8 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) F (·) (b) g(·) X Figure 4.36: To generate random numbers according to Gaussian(0,1), we plot its CDF in (a) and the transformation g in (b). To visualize the random variables before and after the transformation, we plot the histograms in Figure 4.37. 230
4.8. GENERATING RANDOM NUMBERS 400 1200 1000 300 800 200 600 400 100 200 0 0 0 0.2 0.4 0.6 0.8 1 -5 0 5 10 (a) PDF of U (b) PDF of g(U) Figure 4.37: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. The MATLAB and Python codes used to generate the histograms above are shown below. % MATLAB code to generate Gaussian from uniform mu = 3; sigma = 2; U = rand(10000,1); gU = sigma*icdf(’norm’,U,0,1)+mu; figure; hist(U); figure; hist(gU); # Python code to generate Gaussian from uniform import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats mu = 3 sigma = 2 U = stats.uniform.rvs(0,1,size=10000) gU = sigma*stats.norm.ppf(U)+mu plt.hist(U); plt.show() plt.hist(gU); plt.show() Example 4.31. How canwegenerate exponential random numbers with parameter λ from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is F (x)=1−e−λx. X 231
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Therefore, the transformation g is 1 g(U)=F−1(U)=− log(1−U). X λ TheCDFoftheexponentialrandomvariableandthetransformationg areshown in Figure 4.38. 4 1 0.9 3 0.8 2 0.7 0.6 1 0.5 0 0.4 0.3 7 0.2 6 0.1 0 5 0 1 2 3 4 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) F (·) (b) g(·) X Figure 4.38: To generate random numbers according to Exponential(1), we plot its CDF in (a) and the transformation g in (b). The PDF of the uniform random variable U and the PDF of the transformed variable g(U) are shown in Figure 4.39. 400 3000 2500 300 2000 200 1500 1000 100 500 0 0 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 (a) PDF of U (b) PDF of g(U) Figure 4.39: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. The MATLAB and Python codes for this transformation are shown below. % MATLAB code to generate exponential random variables lambda = 1; U = rand(10000,1); gU = -(1/lambda)*log(1-U); # Python code to generate exponential random variables import numpy as np 232
4.8. GENERATING RANDOM NUMBERS import scipy.stats as stats lambd = 1; U = stats.uniform.rvs(0,1,size=10000) gU = -(1/lambd)*np.log(1-U) Example 4.32. How can we generate the 4 integers 1,2,3,4, according to the his- togram [0.10.50.30.1], from uniform random numbers? First, we generate U ∼Uniform(0,1). The CDF of the ideal distribution is  0.1, x=1, 0.1+0.5=0.6, x=2, F (x)= X 0 0. .1 1+ +0 0. .5 5+ +0 0. .3 3+= 00 .. 19, =1.0, xx= =3 4, . This CDF is not invertible. However, we can still define the “inverse” mapping as g(U)=F−1(U) X  1, 0.0≤U ≤0.1, 2, 0.1<U ≤0.6, = 3 4, , 0 0. .6 9< <U U ≤ ≤0 1. .9 0, . For example, if 0.1 < U ≤ 0.6, then on the black curve shown in Figure 4.40(a), we are looking at the second vertical line from the left. This will go to “2” on the x-axis. Therefore, the inversely mapped value is 2 for 0.1<U ≤0.6. 4 1 0.9 3 0.6 2 1 0.1 0 0 1 2 3 4 5 0 0.1 0.6 0.9 1 (a) F (·) (b) g(·) X Figure 4.40: To generate random numbers according to a predefined histogram, we first define the CDF in (a) and the corresponding transformation in (b). The PDFs of the transformed variables, before and after, are shown in Fig- ure 4.41. 233
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 400 6000 5000 300 4000 200 3000 2000 100 1000 0 0 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4 5 (a) PDF of U (b) PDF of g(U) Figure 4.41: (a) PDF of the uniform random variable. (b) The PDF of the transformed random variable. In MATLAB, the above PDFs can be plotted using the commands below. In Python, weneedtousethelogicalcomparisonnp.logical_andtoidentifytheindices.Analternative is to use gU[((U<=0.5)*(U>=0.0)).astype(np.bool)]=1. % MATLAB code to generate the desired random variables U = rand(10000,1); gU = zeros(10000,1); gU((U>=0) & (U<=0.1)) = 1; gU((U>0.1) & (U<=0.6)) = 2; gU((U>0.6) & (U<=0.9)) = 3; gU((U>0.9) & (U<=1)) = 4; # Python code to generate the desired random variables import numpy as np import scipy.stats as stats U = stats.uniform.rvs(0,1,size=10000) gU = np.zeros(10000) gU[np.logical_and(U >= 0.0, U <= 0.1)] = 1 gU[np.logical_and(U > 0.1, U <= 0.6)] = 2 gU[np.logical_and(U > 0.6, U <= 0.9)] = 3 gU[np.logical_and(U > 0.9, U <= 1)] = 4 4.9 Summary Let us summarizethis chapter by revisiting thefour bullet pointsfrom the beginning of the chapter. • Definition of a continuous random variable. Continuous random variables are mea- sured bylengths,areas,andvolumes,whicharealldefinedbyintegrations.Thismakes 234
4.10. REFERENCE them different from discrete random variables, which are measured by counts (and summations).Becauseofthedifferentmeasuresbeingusedtodefinerandomvariables, we consequently have different ways of defining expectation, variance, moments, etc., all in terms of integrations. • Unification of discrete and continuous random variables. The unification is done by the CDF. The CDF of a discrete random variable can be written as a train of step functions. After taking the derivative, we will obtain the PDF, which is a train of impulses. • Origin of Gaussian random variables.TheoriginoftheGaussianrandomvariablelies inthefactthatmanyobservableeventsinengineeringaresumsofindependentevents. The summation of independent random variables is equivalent to taking convolutions of the PDFs. At the limit, they will converge to a bell-shaped function, which is the Gaussian. Gaussians are everywhere because we observe sums more often than we observe individual states. • Transformation of random variables. Transformation of random variables is done in the CDF space. The transformation can be used to generate random numbers according to a predefined distribution. Specifically, if we want to generate random numbers according to F , then the transformation is g =F−1. X X 4.10 Reference PDF, CDF, expectation 4-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 3.1, 3.2. 4-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.1 - 4.3. 4-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 4. 4-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 4.1, 4.2, 5.1, 5.3, 5.5. 4-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 4.10, 5.1, 5.2, 5.3. 4-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Chapter 2.4, 2.5, 4.1, 4.4. Gaussian random variables 4-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 3.3. 235
CHAPTER 4. CONTINUOUS RANDOM VARIABLES 4-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.4. 4-9 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 5.4. 4-10 Mark D. Ward and Ellen Gundlach, Introduction to Probability, W.H. Freeman and Company, 2016. Chapter 35. Transformation of random variables 4-11 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.1. 4-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 4.5. 4-13 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 5. 4-14 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 5.4. 4-15 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 5.7. 4-16 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd edition, 2001. Chapter 3.1, 3.2. Advanced probability textbooks 4-17 WilliamFeller,An Introduction to Probability Theory and Its Applications,Wileyand Sons, 3rd Edition, 1950. 4-18 Andrey Kolmogorov, Foundations of the Theory of Probability, 2nd English Edition, Dover 2018. (Translated from Russian to English. Originally published in 1950 by Chelsea Publishing Company New York.) 4.11 Problems Exercise 1. (Video Solution) Let X be a Gaussian random variable with µ=5 and σ2 =16. (a) Find P[X >4] and P[2≤X ≤7]. (b) If P[X <a]=0.8869, find a. (c) If P[X >b]=0.1131, find b. (d) If P[13<X ≤c]=0.0011, find c. 236
4.11. PROBLEMS Exercise 2. (Video Solution) Compute E[Y] and E[Y2] for the following random variables: (a) Y =Acos(ωt+θ), where A∼N(µ, σ2). (b) Y =acos(ωt+Θ), where Θ∼Uniform(0, 2π). (c) Y =acos(ωT +θ), where T ∼Uniform(cid:0) −π, π(cid:1) . ω ω Exercise 3. (Video Solution) Consider a CDF  0, if x<−1, 0.5, if −1≤x<0, F (x)= X ( 11 , +x)/2, i of th0 e≤ rwx is< e.1, (a) Find P[X <−1], P[−0.5<X <0.5] and P[X >0.5]. (b) Find f (x). X Exercise 4. (Video Solution) A random variable X has CDF: (cid:40) 0, if x<0, F (x)= X 1− 1e−2x, if x≥0. 4 (a) Find P[X ≤2], P[X =0], P[X <0], P[2<X <6] and P[X >10]. (b) Find f (x). X Exercise 5. (Video Solution) A random variable X has PDF (cid:40) cx(1−x2), 0≤x≤1, f (x)= X 0, otherwise. Find c, F (x), and E[X]. X Exercise 6. (Video Solution) A continuous random variable X has a cumulative distribution  0, x<0,  F (x)= 0.5+csin2(πx/2), 0≤x≤1, X 1, x>1. (a) What values can c assume? (b) Find f (x). X 237
CHAPTER 4. CONTINUOUS RANDOM VARIABLES Exercise 7. (Video Solution) A continuous random variable X is uniformly distributed in [−2, 2]. (a) Let Y =sin(πX/8). Find f (y). Y (b) Let Z =−2X2+3. Find f (z). Z Hint: Compute F (y) from F (x), and use d sin−1y = √ 1 . Y X dy 1−y2 Exercise 8. Let Y =eX. (a) Find the CDF and PDF of Y in terms of the CDF and PDF of X. (b) Find the PDF of Y when X is a Gaussian random variable. In this case, Y is said to be a lognormal random variable. Exercise 9. The random variable X has the PDF (cid:40) √1 , 0≤x≤1, f (x)= 2 x X 0, otherwise. Let Y be a new random variable  0, X <0, √ Y = X, 0≤X ≤1, 1, X >1. Find F (y) and f (y), for −∞<y <∞. Y Y Exercise 10. A random variable X has the PDF (cid:40) 2xe−x2, x≥0, f (x)= X 0, x<0. Let (cid:40) 1−e−X2, X ≥0, Y =g(X)= 0, X <0. Find the PDF of Y. Exercise 11. A random variable X has the PDF 1 f (x)= e−|x|, −∞<x<∞. X 2 238
4.11. PROBLEMS Let Y =g(X)=e−X. Find the PDF of Y. Exercise 12. A random variable X has the PDF f X(x)= √ 1 e− 2x σ2 2, −∞<x<∞. 2πσ2 Find the PDF of Y where (cid:40) X, |X|>K, Y =g(X)= −X, |X|<K. Exercise 13. A random variable X has the PDF f X(x)= √1 e−x 22 , −∞<x<∞. x2 2π Let Y =g(X)= 1. Find the PDF of Y. X Exercise 14. A random variable X has the CDF  0, x<0,  F (x)= xα, 0≤x≤1, X 1, x>1, with α>0. Find the CDF of Y if Y =g(X)=−logX. Exercise 15. Energy efficiency is an important aspect of designing electrical systems. In some modern buildings(e.g.,airports),traditionalescalatorsarebeingreplacedbyanewtypeof“smart” escalator which can automatically switch between a normal operating mode and a standby mode depending on the flow of pedestrians. (a) ThearrivalofpedestrianscanbemodeledasaPoissonrandomvariable.LetN bethe number of arrivals, and let λ be the arrival rate (people per minute). For a period of t minutes, show that the probability that there are n arrivals is (λt)n P(N =n)= e−λt. n! (b) Let T be a random variable denoting the interarrival time (i.e., the time between two consecutive arrivals). Show that P(T >t)=e−λt. Also, determine F (t) and f (t). Sketch f (t). T T T (Hint: Note that P(T >t)=P(no arrival in t minutes).) 239
CHAPTER 4. CONTINUOUS RANDOM VARIABLES (c) Suppose that the escalator will go into standby mode if there are no pedestrians for t = 30 seconds. Let Y be a random variable denoting the amount of time that the 0 escalator is in standby mode. That is, let (cid:40) 0, if T ≤t , Y = 0 T −t , if T >t . 0 0 Find E[Y]. 240
Chapter 5 Joint Distributions Whenyougotoaconcerthall,sometimesyoumaywanttoseeasoloviolinconcert,butother timesyoumaywanttoseeasymphony.Symphoniesareappealingbecausemanyinstruments areplayingtogether.Randomvariablesaresimilar.Whilesinglerandomvariablesareuseful for modeling simple events, we use multiple random variables to describe complex events. Themultiplerandomvariablescanbeeitherindependentorcorrelated.Whenmanyrandom variables are present in the problem, we enter the subject of joint distribution. What are joint distributions? In the simplest sense, joint distributions are extensions of the PDFs and PMFs we studied in the previous chapters. We summarize them as follows. Joint distributions are high-dimensional PDFs (or PMFs or CDFs). What do we mean by a high-dimensional PDF? We know that a single random variable is characterized by a 1-dimensional PDF f (x). If we have a pair of random variables, then X we use a 2-dimensional function f (x,y), and if we have a triplet of random variables, X,Y we use a 3-dimensional function f (x,y,z). In general, the dimensionality of the PDF X,Y,Z grows as the number of variables: f (x) =⇒f (x ,x )=⇒···=⇒f (x ,...,x ). X X1,X2 1 2 X1,...,XN 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) onevariable twovariables N variables For busy engineers like us, f (x ,...,x ) is not a friendly notation. A more con- X1,...,XN 1 N cise way to write f (x ,...,x ) is to define a vector of random variables X = X1,...,XN 1 N [X ,X ,...,X ]T with a vector of states x=[x ,x ,...,x ]T, and to define the PDF as 1 2 N 1 2 N f (x)=f (x ,...,x ). X X1,...,XN 1 N Under what circumstance will we encounter creatures like f (x)? Believe it or not, X these high-dimensional PDFs are everywhere. In 2010, computer-vision scientists created the ImageNet dataset, containing 14 million images with ground-truth class labels. This enormous dataset has enabled a great blossoming of machine learning over the past several 241
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.1: Joint distributions are ubiquitous in modern data analysis. For example, an image from a dataset can be represented by a high-dimensional vector x. Each vector has a certain probability of being present. This probability is described by the high-dimensional joint PDF f (x). The goal of this X chapter is to understand the properties of this f . X 0.5 0.5 0.1 0.4 0.4 0.3 0.3 0.05 0.2 0.2 0 0.1 0.1 543210- y1-2-3-4-5 -5-4-3-2 x-10 1 2 3 4 5 0 -5 -4 -3 -2 -1 x0 1 2 3 4 5 0 -5 -4 -3 -2 -1 y0 1 2 3 4 5 Figure 5.2: A 2-dimensional PDF f (x,y) of a pair of random variables (X,Y) and their respective X,Y 1D PDFs f (x) and f (y). X Y decades, in which many advances in deep learning have been made. Fundamentally, the ImageNet dataset provides a large collection of samples drawn from a latent distribution thatishigh-dimensional.EachsampleintheImageNetdatasetisa224×224×3image(the three numbers stand for the image’s height, width, and color). If we convert this image into a vector, then the sample will have a dimension of 224×224×3= 150,528. In other words, the sample is a vector x ∈ R150528×1. The probability of obtaining a particular sample x is determined by probability density function f (x). For example, it is more likely to get X an image containing trees than one containing a Ferrari. The manifold generated by f (x) X can be extremely complex, as illustrated in Figure 5.1. The story of ImageNet is just one of the many instances for which we use a joint distribution f (x). Joint distributions are ubiquitous. If you do data science, you must X understand joint distributions. However, extending a 1-dimensional function f (x) to a X 2-dimensionalfunctionf (x,y)andthentoaN-dimensionalfunctionf (x)isnottrivial. X,Y X The goal of this chapter is to guide you through these important steps. Plan of Part 1 of this chapter: Two variables This chapter is broadly divided into two halves. In the first half, we will look at a pair of random variables. • Definition of f (x,y). The first thing we need to learn is the definition of a joint X,Y distribution with two variables. Since we have two variables, the joint probability density function (or probability mass function) is a 2-dimensional function. A point 242
on this 2D function is the probability density evaluated by a pair of variables X = x and Y = y, as illustrated in Figure 5.2. However, how do we formally define this 2D function? How is it related to the probability measure? Is there a way we can retrieve f (x)andf (y)fromf (x,y),asillustratedontheright-handsidesof Figure 5.2? X Y X,Y These questions will be answered in Section 5.1. • Joint expectation E[XY]. When we have a pair of random variables, how should we definetheexpectation?InSection5.2,wewillshowthatthemostnaturalwaytodefine the joint expectation is in terms of E[XY], i.e., the expectation of the product. There isasurprisingandbeautifulconnectionbetweenthis“expectationoftheproduct”and the cosine angle between two vectors, thereby showing that E[XY] is the correlation between X and Y. • The reason for studying a pair of random variables is to spell out the cause-effect relationship between the variables. This cannot be done without conditional distri- butions; this will be explained in Section 5.3. Conditional distributions provide an extremely important computational tool for decoupling complex events into simpler events. Such decomposition allows us to solve difficult joint expectation problems via simple conditional expectations; this subject will be covered in Section 5.4. • IfyourecallourdiscussionsabouttheoriginofaGaussianrandomvariable,weclaimed that the PDF of X +Y is the convolution between f and f . Why is this so? We X Y will answer this question in terms of joint distributions in Section 5.5. Plan of Part 2 of this chapter: N variables The second half of the chapter focuses on the general case of N random variables. This requires the definitions of a random vector X =[X ,...,X ]T, a joint distribution f (x), 1 N X and the corresponding expectations E[X]. To make our discussions concrete, we will focus onthecaseof high-dimensionalGaussianrandomvariablesanddiscussthefollowingtopics. • Covariance matrices/correlation matrices. If a pair of random variables can define the correlation through the expectation of the product E[X X ], then for a vector of 1 2 random variables we can consider a matrix of correlations in the form E[X X ] E[X X ] ··· E[X X ] 1 1 1 2 1 N E[X 2X 1] E[X 2X 2] ··· E[X 2X N] R=   . . . . . . ... . . .   . E[X X ] E[X X ] ··· E[X X ] N 1 N 2 N N What are the properties of the matrix? How does it affect the shape of the high- dimensionalGaussian?Ifwehaveadatasetofvectors,howdoweestimatethismatrix from the data? We will answer these questions in Section 5.6 and Section 5.7. • Principal-component analysis. Given the covariance matrix, we can perform some very useful data analyses, such as the principal-component analysis in Section 5.8. The question we will ask is: Among the many components, which one is the principal component? If we can find the principal component(s), we can effectively perform dimensionalityreductionbyprojectingahigh-dimensionalvectorintolow-dimensional representations. We will introduce an application for face detection. 243
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.3: When there is a pair of random variables, we can regard the sample space as a set of coordinates. The random variables are 2D mappings from a coordinate ω in Ω ×Ω to another X Y coordinate X(ω) in R2. 5.1 Joint PMF and Joint PDF Probability is a measure of the size of a set. This principle applies to discrete random vari- ables,continuousrandomvariables,singlerandomvariables,andmultiplerandomvariables. In situations with a pair of random variables, the measure should be applied to the coordi- nate (X,Y) represented by the random variables X and Y. Consequently, when measuring the probability, we either count these coordinates or integrate the area covered by these coordinates. In this section, we formalize this notion of measuring 2D events. 5.1.1 Probability measure in 2D Consider two random variables X and Y. Let the sample space of X and Y be Ω and X Ω , respectively. Define the Cartesian product of Ω and Ω as Ω ×Ω = {(x,y) | x ∈ Y X Y X Y Ω andy ∈Ω }. That is, Ω ×Ω contains all possible pairs (X,Y). X Y X Y Example 5.1. If Ω = {1,2} and Ω = {4,5}, then Ω × Ω = {(1,4),(1,5), X Y X Y (2,4),(2,5)}. 244
5.1. JOINT PMF AND JOINT PDF Example 5.2. If Ω = [3,4] and Ω = [1,2], then Ω ×Ω = a rectangle with two X Y X Y diagonal vertices as (3,1) and (4,2). Random variables are mappings from the sample space to the real line. If ω ∈ Ω is X mapped to X(ω)∈R, and ξ ∈Ω is mapped to Y(ξ)∈R, then a coordinate ω =(ω,ξ) in Y thesamplespaceΩ ×Ω shouldbemappedtoacoordinate(X(ω),Y(ξ))inthe2Dplane. X Y (cid:20) (cid:21) (cid:20) (cid:21) def ω X(ω) def ω = (cid:55)−→ = X(ω). ξ Y(ξ) We denote such a vector-to-vector mapping as X(·) : Ω ×Ω → R×R, as illustrated in X Y Figure 5.3. Therefore, if we have an event A∈R2, the probability that A happens is P[A]=P[{ω |X(ω)∈A}] (cid:20)(cid:26)(cid:20) (cid:21) (cid:12) (cid:20) (cid:21) (cid:27)(cid:21) =P ω (cid:12) (cid:12) X(ω) ∈A ξ (cid:12) Y(ξ) (cid:20)(cid:26)(cid:20) (cid:21) (cid:27)(cid:21) ω =P ∈X−1(A) ξ =P[ω ∈X−1(A)]. In other words, we take the coordinate X(ω) and find its inverse image X−1(A). The size of this inverse image X−1(A) in the sample space Ω ×Ω is then the probability. We X Y summarize this general principle as follows. How to measure probability in 2D For a pair of random variables X =(X,Y), the probability of an event A is measured in the product space Ω ×Ω with the size X Y P[{ω |X−1(A)}]. Thisdefinitionisquiteabstract.Tomakeitmoreconcrete,wewilllookatdiscreteand continuous random variables. 5.1.2 Discrete random variables Suppose that the random variables X and Y are discrete. Let A = {X(ω) = x, Y(ξ) = y} be a discrete event. Then the above definition tells us that the probability of A is (cid:20) (cid:12) (cid:21) P[A]=P (ω,ξ)(cid:12) (cid:12)X(ω)=x, andY(ξ)=y =P[X =xandY =y]. (cid:12) (cid:124) (cid:123)(cid:122) (cid:125) d=efpX,Y(x,y) We define this probability as the joint probability mass function (joint PMF) p (x,y). X,Y 245
CHAPTER 5. JOINT DISTRIBUTIONS Definition 5.1. Let X and Y be two discrete random variables. The joint PMF of X and Y is defined as (cid:20) (cid:12) (cid:21) p X,Y(x,y)=P[X =xandY =y]=P (ω,ξ)(cid:12) (cid:12)X(ω)=x, andY(ξ)=y . (5.1) (cid:12) We sometimes write the joint PMF as p (x,y)=P[X =x, Y =y]. X,Y Figure 5.4: A joint PMF for a pair of discrete random variables consists of an array of impulses. To measure the size of the event A, we sum all the impulses inside A. Figure 5.4 shows a graphical portrayal of the joint PMF. In a nutshell, p (x,y) X,Y can be considered as a 2D extension of a single variable PMF. The probabilities are still representedbytheimpulses,butthedomainoftheseimpulsesisnowa2Dplane.Ifwehave an event A, then the size of the event is (cid:88) P[A]= p (x,y). X,Y (x,y)∈A Example 5.3. Let X be a coin flip, Y be a die. The sample space of X is {0,1}, whereas the sample space of Y is {1,2,3,4,5,6}. The joint PMF, according to our definition, is the probability P[X =x and Y =y], where x takes a binary state and Y takes one of the 6 states. The following table summarizes all the 12 states of the joint distribution. Y 1 2 3 4 5 6 X = 0 1 1 1 1 1 1 12 12 12 12 12 12 X = 1 1 1 1 1 1 1 12 12 12 12 12 12 In this table, since there are 12 coordinates, and each coordinate has an equal chance of appearing, the probability for each coordinate becomes 1/12. Therefore, the joint PMF of X and Y is 1 p (x,y)= , x=0,1, y =1,2,3,4,5,6. X,Y 12 246
5.1. JOINT PMF AND JOINT PDF In this example, we observe that if X and Y are not interacting with each other (for- mally,independent),thejointPMFistheproductofthetwoindividualprobabilities. Example 5.4.Inthepreviousexample,ifwedefineA={X+Y =3},theprobability P[A] is (cid:88) P[A]= p (x,y)=p (0,3)+p (1,2) X,Y X,Y X,Y (x,y)∈A 2 = . 12 If B ={min(X,Y)=1}, the probability P[B] is (cid:88) P[B]= p (x,y) X,Y (x,y)∈B =p (1,1)+p (1,2)+p (1,3) X,Y X,Y X,Y +p (1,4)+p (1,5)+p (1,6) X,Y X,Y X,Y 6 = . 12 5.1.3 Continuous random variables The continuous version of the joint PMF is called the joint probability density function (joint PDF),denotedbyf (x,y).AjointPDFisanalogoustoajointPMF.Forexample, X,Y integrating it will give us the probability. Definition 5.2. Let X and Y be two continuous random variables. The joint PDF of X and Y is a function f (x,y) that can be integrated to yield a probability X,Y (cid:90) P[A]= f (x,y)dxdy, (5.2) X,Y A for any event A⊆Ω ×Ω . X Y Pictorially, we can view f as a 2D function where the height at a coordinate (x,y) is X,Y f (x,y), as can be seen from Figure 5.5. To compute the probability that (X,Y) ∈ A, X,Y we integrate the function f with respect to the area covered by the set A. For example, X,Y if the set A is a rectangular box A=[a,b]×[c,d], then the integration becomes P[A]=P[a≤X ≤b, c≤Y ≤d] (cid:90) d(cid:90) b = f (x,y)dxdy. X,Y c a 247
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.5: A joint PDF for a pair of continuous random variables is a surface in the 2D plane. To measure the size of the event A, we integrate f (x,y) inside A. X,Y Example5.5.ConsiderauniformjointPDFf (x,y)definedon[0,2]2withf (x,y)= X,Y X,Y 1. Let A=[a,b]×[c,d]. Find P[A]. 4 Solution. P[A]=P[a≤X ≤b, c≤X ≤d] (cid:90) d(cid:90) b (cid:90) d(cid:90) b 1 (d−c)(b−a) = f (x,y)dxdy = dxdy = . X,Y 4 4 c a c a Practice Exercise 5.1. In the previous example, let B ={X+Y ≤2}. Find P[B]. Solution. (cid:90) P[B]= f (x,y)dxdy X,Y B (cid:90) 2(cid:90) 2−y = f (x,y)dxdy X,Y 0 0 (cid:90) 2(cid:90) 2−y 1 = dxdy 4 0 0 (cid:90) 2 2−y 1 = dy = . 4 2 0 Here, the limits of the integration can be determined from Figure 5.6. The inner integration (with respect to x) should start from 0 and end at 2−y, which is the line defining the set x+y ≤ 2. Since the inner integration is performed for every y, we need to enumerate all the possible y’s to complete the outer integration. This leads to the outer limit from 0 to 2. 248
5.1. JOINT PMF AND JOINT PDF Figure 5.6: To calculate P[X+Y ≤2], we perform a 2D integration over a triangle. 5.1.4 Normalization Thenormalizationpropertyofatwo-dimensionalPMFandPDFisthepropertythat,when we enumerate all outcomes of the sample space, we obtain 1. Theorem 5.1. Let Ω=Ω ×Ω . All joint PMFs and joint PDFs satisfy X Y (cid:90) (cid:88) p (x,y)=1 or f (x,y)dxdy =1. (5.3) X,Y X,Y Ω (x,y)∈Ω Example 5.6. Consider a joint uniform PDF defined in the shaded area [0,3]×[0,3] with PDF defined below. Find the constant c. (cid:40) c if (x,y)∈[0,3]×[0,3], f (x,y)= X,Y 0 otherwise. Solution. To find the constant c, we note that (cid:90) 3(cid:90) 3 1= f (x,y)dxdy X,Y 0 0 (cid:90) 3(cid:90) 3 = cdxdy =9c. 0 0 Equating the two sides gives us c= 1. 9 Practice Exercise 5.2. Consider a joint PDF (cid:40) ce−xe−y 0≤y ≤x<∞, f (x,y)= X,Y 0 otherwise. Find the constant c. Tip: Consider the area of integration as shown in Figure 5.7. 249
CHAPTER 5. JOINT DISTRIBUTIONS Solution.TherearetwowaystotaketheintegrationshowninFigure 5.7.Wechoose the inner integration w.r.t. y first. (cid:90) (cid:90) ∞(cid:90) x f (x,y)dxdy = ce−xe−y dy dx X,Y Ω 0 0 (cid:90) ∞ = ce−x(1−e−x) 0 c = . 2 Therefore, c=2. Figure 5.7: To integrate the probability P[0 ≤ Y ≤ X], we perform a 2D integration over a triangle. (cid:82) (cid:82) The two subfigures show the two ways of integrating the triangle. [Left] dx first, and then dy. (cid:82) (cid:82) [Right] dy first, and then dx. 5.1.5 Marginal PMF and marginal PDF If we only sum / integrate for one random variable, we obtain the PMF / PDF of the other random variable. The resulting PMF / PDF is called the marginal PMF / PDF. Definition 5.3. The marginal PMF is defined as (cid:88) (cid:88) p (x)= p (x,y) and p (y)= p (x,y), (5.4) X X,Y Y X,Y y∈ΩY x∈ΩX and the marginal PDF is defined as (cid:90) (cid:90) f (x)= f (x,y)dy and f (y)= f (x,y)dx. (5.5) X X,Y Y X,Y ΩY ΩX Since f (x,y) is a two-dimensional function, when integrating over y from −∞ to ∞, we X,Y project f (x,y) onto the x-axis. Therefore, the resulting function depends on x only. X,Y 250
5.1. JOINT PMF AND JOINT PDF Example5.7.ConsiderthejointPDFf (x,y)= 1 shownbelow.Findthemarginal X,Y 4 PDFs. Solution. If we integrate over x and y, we have   3, if 1<x≤2, 1 2, , i if f 1 2< <x x≤ ≤2 3, , f (x)= 1, if 2<x≤3, and f (y)= X Y 0, otherwise. 1 0, , i of th3 e< rwx is≤ e.4, So the marginal PDFs are the projection of the joint PDFs onto the x- and y-axes. Practice Exercise 5.3. A joint Gaussian random variable (X,Y) has a joint PDF given by 1 (cid:26) ((x−µ )2+(y−µ )2)(cid:27) f (x,y)= exp − X Y . X,Y 2πσ2 2σ2 Find the marginal PDFs f (x) and f (y). X Y Solution. (cid:90) ∞ (cid:90) ∞ 1 (cid:26) ((x−µ )2+(y−µ )2)(cid:27) f (x)= f (x,y)dy = exp − X Y dy X X,Y 2πσ2 2σ2 −∞ −∞ 1 (cid:26) (x−µ )2(cid:27) (cid:90) ∞ 1 (cid:26) (y−µ )2(cid:27) = √ exp − X · √ exp − Y dy. 2πσ2 2σ2 2πσ2 2σ2 −∞ Recognizing that the last integral is equal to unity because it integrates a Gaussian PDF over the real line, it follows that 1 (cid:26) (x−µ )2(cid:27) f (x)= √ exp − X . X 2πσ2 2σ2 Similarly, we have 1 (cid:26) (y−µ )2(cid:27) f (y)= √ exp − Y . Y 2πσ2 2σ2 251
CHAPTER 5. JOINT DISTRIBUTIONS 5.1.6 Independent random variables Two random variables are said to be independent if and only if the joint PMF or PDF can be factorized as a product of the marginal PMF / PDFs. Definition 5.4. Random variables X and Y are independent if and only if p (x,y)=p (x)p (y), or f (x,y)=f (x)f (y). X,Y X Y X,Y X Y This definition is consistent with the definition of independence of two events. Recall that twoeventsAandBareindependentifandonlyifP[A∩B]=P[A]P[B].LettingA={X =x} and B ={Y =y}, we see that if A and B are independent then P[X = x∩Y = y] is the product P[X =x]P[Y =y]. This is precisely the relationship p (x,y)=p (x)p (y). X,Y X Y Example 5.8. Consider two random variables with a joint PDF given by 1 (cid:26) (x−µ )2+(y−µ )2(cid:27) f (x,y)= exp − X Y . X,Y 2πσ2 2σ2 Are X and Y independent? Solution. We know that 1 (cid:26) (x−µ )2(cid:27) 1 (cid:26) (y−µ )2(cid:27) f (x,y)= √ exp − X × √ exp − Y . X,Y 2πσ 2σ2 2πσ 2σ2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fX(x) fY(y) Therefore, the random variables X and Y are independent. Practice Exercise 5.4.LetX beacoinandY beadie.ThenthejointPMFisgiven by the table below. Y 1 2 3 4 5 6 X = 0 1 1 1 1 1 1 12 12 12 12 12 12 X = 1 1 1 1 1 1 1 12 12 12 12 12 12 Are X and Y independent? Solution. For any x and y, we have that 1 1 1 p (x,y)= = × . X,Y 12 2 6 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) pX(x) pY(y) Therefore, the random variables X and Y are independent. 252
5.1. JOINT PMF AND JOINT PDF Example 5.9. Consider two random variables X and Y with a joint PDF given bya f (x,y)∝exp(cid:8) −(x−y)2(cid:9) =exp(cid:8) −x2+2xy−y2(cid:9) X,Y =exp(cid:8) −x2(cid:9) exp{2xy} exp(cid:8) −y2(cid:9) . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fX(x) extraterm fY(y) This PDF cannot be factorized into a product of two marginal PDFs. Therefore, the random variables are dependent. aWeusethenotation“∝”todenote“proportionalto”.Itimpliesthatthenormalizationconstant isomitted. We can extrapolate the definition of independence to multiple random variables. If there are many random variables X ,X ,...,X , they will have a joint PDF 1 2 N f (x ,...,x ). X1,...,XN 1 N If these random variables X ,X ,...,X are independent, then the joint PDF can be 1 2 N factorized as f (x ,...,x )=f (x )·f (x )···f (x ) X1,...,XN 1 N X1 1 X2 2 XN N N (cid:89) = f (x ). Xn n n=1 This gives us the definition of independence for N random variables. Definition 5.5. A sequence of random variables X ,...,X is independent if and 1 N only if their joint PDF (or joint PMF) can be factorized. N (cid:89) f (x ,...,x )= f (x ). (5.6) X1,...,XN 1 N Xn n n=1 Example 5.10.Throwadie4times.LetX ,X ,X andX betheoutcomes.Then, 1 2 3 4 sincethesefourthrowsareindependent,theprobabilitymassfunctionofanyquadrable (x ,x ,x ,x ) is 1 2 3 4 p (x ,x ,x ,x )=p (x )p (x )p (x )p (x ). X1,X2,X3,X4 1 2 3 4 X1 1 X2 2 X3 3 X4 4 For example, the probability of getting (1,5,2,6) is (cid:18) 1(cid:19)4 p (1,5,2,6)=p (1)p (5)p (2)p (6)= . X1,X2,X3,X4 X1 X2 X3 X4 6 The example above demonstrates an interesting phenomenon. If the N random vari- ables are independent, and if they all have the same distribution, then the joint PDF/PMF is just one of the individual PDFs taken to the power N. Random variables satisfying this property are known as independent and identically distributed random variables. 253
CHAPTER 5. JOINT DISTRIBUTIONS Definition 5.6 (Independent and Identically Distributed (i.i.d.)). A collection of random variables X ,...,X is called independent and identically distributed (i.i.d.) 1 N if • All X ,...,X are independent; and 1 N • All X ,...,X have the same distribution, i.e., f (x)=···=f (x). 1 N X1 XN If X ,...,X are i.i.d., we have that 1 N N (cid:89) f (x ,...,x )= f (x ), X1,...,XN 1 1 X1 n n=1 where the particular choice of X is unimportant because f (x)=···=f (x). 1 X1 XN Why is i.i.d. so important? • If a set of random variables are i.i.d., then the joint PDF can be written as a product of PDFs. • IntegratingajointPDFisdifficult.IntegratingaproductofPDFsismucheasier. Example 5.11.LetX ,X ,...,X beasequenceofi.i.d.Gaussianrandomvariables 1 2 N where each X has a PDF i 1 (cid:26) x2(cid:27) f (x)= √ exp − . Xi 2π 2 The joint PDF of X ,X ,...,X is 1 2 N (cid:89)N (cid:26) 1 (cid:26) x2(cid:27)(cid:27) f (x ,...,x )= √ exp − i X1,...,XN 1 N 2π 2 i=1 (cid:18) 1 (cid:19)N (cid:40) (cid:88)N x2(cid:41) = √ exp − i , 2π 2 i=1 whichisafunctiondependingnotontheindividualvaluesofx ,x ,...,x butonthe 1 2 N sum(cid:80)N x2.Sowehave“compressed”anN-dimensionalfunctionintoa1Dfunction. i=1 i Example5.12.Letθbeadeterministicnumberthatwassentthroughanoisychannel. WemodelthenoiseasanadditiveGaussianrandomvariablewithmean0andvariance σ2. Supposing we have observed measurements X = θ+W , for i = 1,...,N, where i i W ∼Gaussian(0,σ2), then the PDF of each X is i i 1 (cid:26) (x−θ)2(cid:27) f (x)= √ exp − . Xi 2πσ2 2σ2 254
5.1. JOINT PMF AND JOINT PDF Thus the joint PDF of (X ,X ,...,X ) is 1 2 N f (x ,...,x )=(cid:89)N (cid:26) √ 1 exp(cid:26) −(x i−θ)2(cid:27)(cid:27) X1,...,XN 1 N 2πσ2 2σ2 i=1 =(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x i−θ)2(cid:41) . 2πσ2 2σ2 i=1 Essentially, this joint PDF tells us the probability density of seeing sample data x ,...,x . 1 N 5.1.7 Joint CDF We now introduce the cumulative distribution function (CDF) for multiple variables. Definition 5.7. Let X and Y be two random variables. The joint CDF of X and Y is the function F (x,y) such that X,Y F (x,y)=P[X ≤x ∩ Y ≤y]. (5.7) X,Y This definition can be more explicitly written as follows. Definition 5.8. If X and Y are discrete, then (cid:88) (cid:88) F (x,y)= p (x(cid:48),y(cid:48)). (5.8) X,Y X,Y y(cid:48)≤yx(cid:48)≤x If X and Y are continuous, then (cid:90) y (cid:90) x F (x,y)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48). (5.9) X,Y X,Y −∞ −∞ If the two random variables are independent, then we have (cid:90) x (cid:90) y F (x,y)= f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) =F (x)F (y). X,Y X Y X Y −∞ −∞ Example 5.13. Let X and Y be two independent uniform random variables Uniform(0,1). Find the joint CDF. Solution. (cid:90) x (cid:90) y (cid:90) x (cid:90) y F (x,y)= f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) = 1dx(cid:48) 1dy(cid:48) =xy. X,Y X Y 0 0 0 0 255
CHAPTER 5. JOINT DISTRIBUTIONS Practice Exercise 5.5. Let X and Y be two independent uniform random variables Gaussian(µ,σ2). Find the joint CDF. Solution. Let Φ(·) be the CDF of the standard Gaussian. F (x,y)=F (x)F (y) X,Y X Y (cid:90) x (cid:90) y (cid:18) x−µ(cid:19) (cid:18) y−µ(cid:19) = f (x(cid:48))dx(cid:48) f (y(cid:48))dy(cid:48) =Φ Φ . X Y σ σ −∞ −∞ Here are a few properties of the CDF: (cid:90) −∞(cid:90) x (cid:90) x F (x,−∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) = 0dx(cid:48) =0, X,Y X,Y −∞ −∞ −∞ (cid:90) y (cid:90) −∞ (cid:90) y F (−∞,y)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) = 0dy(cid:48) =0, X,Y X,Y −∞ −∞ −∞ (cid:90) −∞(cid:90) −∞ F (−∞,−∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) =0, X,Y X,Y −∞ −∞ (cid:90) ∞ (cid:90) ∞ F (∞,∞)= f (x(cid:48),y(cid:48))dx(cid:48) dy(cid:48) =1. X,Y X,Y −∞ −∞ In addition, we can obtain the marginal CDF as follows. Proposition 5.1. Let X and Y be two random variables. The marginal CDF is F (x)=F (x,∞), (5.10) X X,Y F (y)=F (∞,y). (5.11) Y X,Y Proof. We prove only the first case. The second case is similar. (cid:90) x (cid:90) ∞ (cid:90) y F (x,∞)= f (x(cid:48),y(cid:48))dy(cid:48) dx(cid:48) = f (x(cid:48))dx(cid:48) =F (x). (cid:3) X,Y X,Y X X −∞ −∞ −∞ By the fundamental theorem of calculus, we can derive the PDF from the CDF. Definition 5.9. Let F (x,y) be the joint CDF of X and Y. Then, the joint PDF X,Y is ∂2 f (x,y)= F (x,y). (5.12) X,Y ∂y ∂x X,Y The order of the partial derivatives can be switched, yielding a symmetric result: ∂2 f (x,y)= F (x,y). X,Y ∂x∂y X,Y 256
5.2. JOINT EXPECTATION Example 5.14. Let X and Y be two uniform random variables with joint CDF F (x,y)=xy for 0≤x≤1 and 0≤y ≤1. Find the joint PDF. X,Y Solution. ∂2 ∂2 f (x,y)= F (x,y)= xy =1, X,Y ∂x∂y X,Y ∂x∂y which is consistent with the definition of a joint uniform random variable. Practice Exercise 5.6.LetX andY betwoexponentialrandomvariableswithjoint CDF F (x,y)=(1−e−λx)(1−e−λy), x≥0, y ≥0. X,Y Find the joint PDF. Solution. ∂2 ∂2 f (x,y)= F (x,y)= (1−e−λx)(1−e−λy) X,Y ∂x∂y X,Y ∂x∂y = ∂ (cid:0) (1−e−λx)(λe−λy)(cid:1) =λe−λxλe−λy. ∂x which is consistent with the definition of a joint exponential random variable. 5.2 Joint Expectation 5.2.1 Definition and interpretation When we have a single random variable, the expectation is defined as (cid:90) E[X]= xf (x)dx. X Ω For a pair of random variables, what would be a good way of defining the expectation? Certainly, we cannot just replace f (x) by f (x,y) because the integration has to be- X X,Y come a double integration. However, if it is a double integration, where should we put the variable y? It turns out that a useful way of defining the expectation for X and Y is as follows. Definition 5.10. Let X and Y be two random variables. The joint expectation is (cid:88) (cid:88) E[XY]= xy · p (x,y) (5.13) X,Y y∈ΩY x∈ΩX 257
CHAPTER 5. JOINT DISTRIBUTIONS if X and Y are discrete, or (cid:90) (cid:90) E[XY]= xy · f (x,y)dxdy (5.14) X,Y y∈ΩY x∈ΩX if X and Y are continuous. Joint expectation is also called correlation. The double summation and integration on the right-hand side of the equation is nothing but the state times the probability. Here, the state is the product xy, and the probability is the joint PMF p (x,y) (or PDF). Therefore, as long as you agree that joint expectation X,Y should be defined as E[XY], the double summation and the double integration make sense. ThebiggestmysteryhereisE[XY].Youmaywonderwhythejointexpectationshould be defined as the expectation of the product E[XY]. Why not the sum E[X +Y], or the differenceE[X−Y],orthequotientE[X/Y]?WhyarewesodeeplyinterestedinX timesY? Theseareexcellentquestions.Thatthejointexpectationisdefinedastheproducthastodo with the correlation between two random variables. We will take a small detour into linear algebra. Let us consider two discrete random variables X and Y, both with N states. So X will take the states {x ,x ,...,x } and Y will take the states {y ,y ,...,y }. Let’s define 1 2 N 1 2 N them as two vectors: xd =ef [x ,...,x ]T and y d =ef [y ,...,y ]T. Since X and Y are random 1 N 1 N variables, they have a joint PMF p (x,y). The array of the PMF values can be written X,Y as a matrix:   p (x ,y ) p (x ,y ) ··· p (x ,y ) X,Y 1 1 X,Y 1 2 X,Y 1 N PMF as a matrix=P d =ef   p X,Y(x . . .2,y 1) p X,Y(x . . .2,y 2) · ..· .· p X,Y(x . . .2,y N)   . p (x ,y ) p (x ,y ) ··· p (x ,y ) X,Y N 1 X,Y N 2 X,Y N N Let’strytowritethejointexpectationintermsofmatricesandvectors.Thedefinition of a joint expectation tells us that N N (cid:88)(cid:88) E[XY]= x y · p (x ,y ), i j X,Y i j i=1j=1 which can be written as    p (x ,y ) ··· p (x ,y ) y X,Y 1 1 X,Y 1 N 1 E[XY]=(cid:2) x 1 ··· x N(cid:3)  . . . ... . . .    . . .  =xTPy. (cid:124) (cid:123)(cid:122) (cid:125) p (x ,y ) ··· p (x ,y ) y xT X,Y N 1 X,Y N N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) P y This is a weighted inner product between x and y using the weight matrix P. Why correlation is defined as E[XY] • E[XY] is a weighted inner product between the states: E[XY]=xTPy. 258
5.2. JOINT EXPECTATION • x and y are the states of the random variables X and Y. • The inner product measures the similarity between two vectors. Example 5.15. Let X be a discrete random variable with N states, where each state hasanequalprobability.Thus,p (x)=1/N forallx.LetY =X beanothervariable. X Then the joint PMF of (X,Y) is (cid:40) 1, x=y, p (x,y)= N X,Y 0, x(cid:54)=y. It follows that the joint expectation is N N N (cid:88)(cid:88) 1 (cid:88) E[XY]= x y ·p (x ,y )= x y . i j X,Y i j N i i i=1j=1 i=1 Equivalently, we can obtain the result via the inner product by defining 1 0 ··· 0 N 0 1 ··· 0 1 P =   . . . N . . . ... . . .   = NI. 0 ··· ··· 1 N In this case, the weighted inner product is xTy 1 (cid:88)N xTPy = = x y =E[XY]. N N i i i=1 How do we understand the inner product? Ignoring the matrix P for a moment, we recall an elementary result in linear algebra. Definition 5.11. Let x ∈ RN and y ∈ RN be two vectors. Define the cosine angle cosθ as xTy cosθ = , (5.15) (cid:107)x(cid:107)(cid:107)y(cid:107) (cid:113) (cid:113) where (cid:107)x(cid:107) = (cid:80)N x2 is the norm of the vector x, and (cid:107)y(cid:107) = (cid:80)N y2 is the i=1 i i=1 i norm of the vector y. This definition can be understood as the geometry between two vectors, as illustrated in Figure 5.8. If the two vectors x and y are parallel so that x = αy for some α, then the angle θ =0. If x and y are orthogonal so that xTy =0, then θ =π/2. Therefore, the inner product xTy tells us the degree of correlation between the vectors x and y. 259
CHAPTER 5. JOINT DISTRIBUTIONS Figure5.8:Thegeometryofjointexpectation.E[XY]givesusthecosineanglebetweenthetworandom variables. This, in turn, tells us the correlation between the two random variables. Now let’s come back to our discussion about the joint expectation. The cosine angle definition tells us that if E[XY]=xTPy, the following form would make sense: xTPy E[XY] cosθ = = . (cid:107)x(cid:107)(cid:107)y(cid:107) (cid:107)x(cid:107)(cid:107)y(cid:107) That is, as long as we can find out the norms (cid:107)x(cid:107) and (cid:107)y(cid:107), we will be able to interpret E[XY] from the cosine angle perspective. But what would be a reasonable definition of (cid:107)x(cid:107) and (cid:107)y(cid:107)? We define the norm by first considering the variance of the random variable X and Y: N (cid:88) E[X2]= x x · p (x ) i i X i i=1    p (x ) ··· 0 x X 1 1 =(cid:2) x 1 ··· x N(cid:3)  . . . ... . . .    . . .   (cid:124) (cid:123)(cid:122) (cid:125) 0 ··· p (x ) x xT X N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) PX x =xTP x=(cid:107)x(cid:107)2 , X PX where P is the diagonal matrix storing the probability masses of the random variable X. X It is not difficult to show that P = diag(P1) by following the definition of the marginal X distributions(whicharethecolumnandrowsumsofthejointPMF).Similarlywecandefine N (cid:88) E[Y2]= y y · p (y ) j j Y j j=1    p (y ) ··· 0 y Y 1 1 =(cid:2) y 1 ··· y N(cid:3)  . . . ... . . .    . . .   (cid:124) (cid:123)(cid:122) (cid:125) 0 ··· p (y ) y yT Y N N (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) PY y =yTP y =(cid:107)y(cid:107)2 . Y PY 260
5.2. JOINT EXPECTATION Therefore, one way to define the cosine angle is to start with xTP y cosθ = XY , (cid:107)x(cid:107) (cid:107)y(cid:107) PX PY (cid:112) (cid:112) where P =P, (cid:107)x(cid:107) = xTP x and (cid:107)y(cid:107) = yTP y. But writing it in terms of XY PX X PY Y the expectation, we observe that this cosine angle is exactly xTP y cosθ = XY (cid:107)x(cid:107) (cid:107)y(cid:107) PX PY E[XY] = . (cid:112)E[X2](cid:112)E[Y2] Therefore,E[XY]definesthecosineanglebetweenthetworandomvariables,which,inturn, defines the correlation between the two. A large |E[XY]| means that X and Y are highly correlated,andasmall|E[XY]|meansthatX andY arenotverycorrelated.IfE[XY]=0, then the two random variables are uncorrelated. Therefore, E[XY] tells us how the two random variables are related to each other. To further convince you that √ E[X√Y] can be interpreted as a cosine angle, we E[X2] E[Y2] show that E[XY] −1≤ ≤1, (cid:112)E[X2](cid:112)E[Y2] because if this ratio can go beyond +1 and −1, it makes no sense to call it a cosine angle. The argument follows from a very well-known inequality in probability, called the Cauchy- Schwarz inequality (for expectation), which states that −1≤ √ E[X√Y] ≤1: E[X2] E[Y2] Theorem 5.2 (Cauchy-Schwarz inequality). For any random variables X and Y, (E[XY])2 ≤E[X2]E[Y2]. (5.16) The following proof can be skipped if you are reading the book the first time. Proof. Let t∈R be a constant. Consider E[(X+tY)2]=E[X2+2tXY +t2Y2]. Since E[(X+tY)2]≥0 for any t, it follows that E[X2+2tXY +t2Y2]≥0. Expanding the left-hand side yields t2E[Y2]+2tE[XY]+E[X2]≥0. 261
CHAPTER 5. JOINT DISTRIBUTIONS Thisisaquadraticequationint,andweknowthatforanyquadraticequationat2+bt+c≥0 we must have b2−4ac≤0. Therefore, in our case, we have that (2E[XY])2−4E[Y2]E[X2]≤0, which means (E[XY])2 ≤ E[X2]E[Y2]. The equality holds when E[(X +tY)2] = 0. In this case, X = −tY for some t, i.e., the random variable X is a scaled version of Y so that the vector formed by the states of X is parallel to that of Y. (cid:3) End of the proof. 5.2.2 Covariance and correlation coefficient In many practical problems, we prefer to work with central moments, i.e., E[(X−µ )2] in- X steadofE[X2].Thisessentiallymeansthatwesubtractthemeanfromtherandomvariable. If we adopt such a centralized random variable, we can define the covariance as follows. Definition 5.12. Let X and Y be two random variables. Then the covariance of X and Y is Cov(X,Y)=E[(X−µ )(Y −µ )], (5.17) X Y where µ =E[X] and µ =E[Y]. X Y It is easy to show that if X =Y, then the covariance simplifies to the variance: Cov(X,X)=E[(X−µ )(X−µ )]=Var[X]. X X Thus, covariance is a generalization of variance. The former can handle a pair of variables, whereasthelatterisonlyforasinglevariable.Wecanalsodemonstratethefollowingresult. Theorem 5.3. Let X and Y be two random variables. Then Cov(X,Y)=E[XY]−E[X]E[Y]. (5.18) Proof. Just apply the definition of covariance: Cov(X,Y)=E[(X−µ )(Y −µ )] X Y =E[XY −Xµ −Yµ +µ µ ]=E[XY]−µ µ . (cid:3) Y X X Y X Y The next theorem concerns the sum of two random variables. Theorem 5.4. For any X and Y, a. E[X+Y]=E[X]+E[Y]. b. Var[X+Y]=Var[X]+2Cov(X,Y)+Var[Y]. 262
5.2. JOINT EXPECTATION Proof. Recall the definition of joint expectation: (cid:88)(cid:88) E[X+Y]= (x+y)p (x,y) X,Y y x (cid:88)(cid:88) (cid:88)(cid:88) = xp (x,y)+ yp (x,y) X,Y X,Y y x y x (cid:32) (cid:33) (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) (cid:88) = x p (x,y) + y p (x,y) X,Y X,Y x y y x (cid:88) (cid:88) = xp (x)+ yp (y) X Y x y =E[X]+E[Y]. Similarly, Var[X+Y]=E[(X+Y)2]−E[X+Y]2 =E[(X+Y)2]−(µ +µ )2 X Y =E[X2+2XY +Y2]−(µ2 +2µ µ +µ2) X X Y Y =E[X2]−µ2 +E[Y2]−µ2 +2(E[XY]−µ µ ) X Y X Y =Var[X]+2Cov(X,Y)+Var[Y]. (cid:3) With covariance defined, we can now define the correlation coefficient ρ, which is the cosine angle of the centralized variables. That is, ρ=cosθ E[(X−µ )(Y −µ )] = X Y . (cid:112)E[(X−µ )2]E[(Y −µ )2] X Y Recognizing that the denominator of this expression is just the variance of X and Y, we define the correlation coefficient as follows. Definition 5.13. Let X and Y be two random variables. The correlation coefficient is Cov(X,Y) ρ= . (5.19) (cid:112) Var[X]Var[Y] Since −1 ≤ cosθ ≤ 1, ρ is also between −1 and 1. The difference between ρ and E[XY] is that ρ is normalized with respect to the variance of X and Y, whereas E[XY] is not normalized. The correlation coefficient has the following properties: • ρ is always between −1 and 1, i.e., −1 ≤ ρ ≤ 1. This is due to the cosine angle definition. • When X =Y (fully correlated), ρ=+1. • When X =−Y (negatively correlated), ρ=−1. • When X and Y are uncorrelated, ρ=0. 263
CHAPTER 5. JOINT DISTRIBUTIONS 5.2.3 Independence and correlation If two random variables X and Y are independent, the joint expectation can be written as a product of two individual expectations. Theorem 5.5. If X and Y are independent, then E[XY]=E[X]E[Y]. (5.20) Proof. We only prove the discrete case because the continuous can be proved similarly. If X and Y are independent, we have p (x,y)=p (x)p (y). Therefore, X,Y X Y (cid:88)(cid:88) (cid:88)(cid:88) E[XY]= xyp (x,y)= xyp (x)p (y) X,Y X Y y x y x (cid:32) (cid:33)(cid:32) (cid:33) (cid:88) (cid:88) = xp (x) yp (y) =E[X]E[Y]. X Y x y (cid:3) In general, for any two independent random variables and two functions f and g, E[f(X)g(Y)]=E[f(X)]E[g(Y)]. The following theorem illustrates a few important relationships between independence and correlation. Theorem 5.6. Consider the following two statements: a. X and Y are independent; b. Cov(X,Y)=0. Statement (a) implies statement (b), but (b) does not imply (a). Thus, independence is a stronger condition than correlation. Proof. We first prove that (a) implies (b). If X and Y are independent, then E[XY] = E[X]E[Y]. In this case, Cov(X,Y)=E[XY]−E[X]E[Y]=E[X]E[Y]−E[X]E[Y]=0. To prove that (b) does not imply (a), we show a counterexample. Consider a discrete random variable Z with PMF p (z)=(cid:2)1 1 1 1(cid:3) . Z 4 4 4 4 Let X and Y be π π X =cos Z and Y =sin Z. 2 2 264
5.2. JOINT EXPECTATION Then we can show that E[X]=0 and E[Y]=0. The covariance is Cov(X,Y)=E[(X−0)(Y −0)] (cid:104) π π (cid:105) =E cos Zsin Z 2 2 (cid:20) (cid:21) 1 =E sinπZ 2 (cid:20) (cid:21) 1 1 1 1 1 = (sinπ0) +(sinπ1) +(sinπ2) +(sinπ3) =0. 2 4 4 4 4 The next step is to show that X and Y are dependent. To this end, we only need to show that p (x,y)(cid:54)=p (x)p (y). The joint PMF p (x,y) can be found by noting that X,Y X Y X,Y Z =0⇒X =1, Y =0, Z =1⇒X =0, Y =1, Z =2⇒X =−1, Y =0, Z =3⇒X =0, Y =−1. Thus, the PMF is  0 1 0 4 p X,Y(x,y)=1 4 0 1 4. 0 1 0 4 The marginal PMFs are p (x)=(cid:2)1 1 1(cid:3) , p (y)=(cid:2)1 1 1(cid:3) . X 4 2 4 Y 4 2 4 The product p (x)p (y) is X Y  1 1 1  16 8 16 p (x)p (y)= 1 1 1 . X Y  8 4 8  1 1 1 16 8 16 Therefore, p (x,y)(cid:54)=p (x)p (y), although E[XY]=E[X]E[Y]. X,Y X Y (cid:3) What is the relationship between independent and uncorrelated? • Independent ⇒ uncorrelated. • Independent (cid:58) uncorrelated. 5.2.4 Computing correlation from data Weclosethissectionbydiscussingaverypracticalproblem:Givenadatasetcontainingtwo columns of data points, how do we determine whether the two columns are correlated? Recall that the correlation coefficient is defined as E[XY]−µ µ ρ= X Y . σ σ X Y 265
CHAPTER 5. JOINT DISTRIBUTIONS If we have a dataset containing (x ,y )N , then the correlation coefficient can be approxi- n i n=1 mated by 1 (cid:80)N x y −xy ρ= N n=1 n n , (cid:98) (cid:113) (cid:113) 1 (cid:80)N (x −x)2 1 (cid:80)N (y −y)2 N n=1 n N n=1 n where x = 1 (cid:80)N x and y = 1 (cid:80)N y are the means. This equation should not be a N n=1 n N n=1 n surprise because essentially all terms are the empirical estimates. Thus, ρ is the empirical (cid:98) correlation coefficient determined from the dataset. As N →∞, we expect ρ→ρ. (cid:98) 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 -1 -1 -1 -2 -2 -2 -3 -3 -3 -4 -4 -4 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) ρ=−0.0038 (b) ρ=0.5321 (c) ρ=0.9656 (cid:98) (cid:98) (cid:98) Figure 5.9: Visualization of correlated variables. Each of these figures represent a scattered plot of a datasetcontaining(x ,y )N .(a)isuncorrelated.(b)issomewhatcorrelated.(c)isstronglycorrelated. n n n=1 Figure 5.9 shows three example datasets. We plot the (x ,y ) pairs as coordinates in n n the 2D plane. The first dataset contains samples that are almost uncorrelated. We can see that x does not tell us anything about y . The second dataset is moderately correlated. n n The third dataset is highly correlated: If we know x , we are almost certain to know the n corresponding y , with a small number of perturbations. n On a computer, computing the correlation coefficient can be done using built-in com- mands such as corrcoef in MATLAB and stats.pearsonr in Python. The codes to gen- erate the results in Figure 5.9(b) are shown below. % MATLAB code to compute the correlation coefficient x = mvnrnd([0,0],[3 1; 1 1],1000); figure(1); scatter(x(:,1),x(:,2)); rho = corrcoef(x) # Python code to compute the correlation coefficient import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt x = stats.multivariate_normal.rvs([0,0], [[3,1],[1,1]], 10000) plt.figure(); plt.scatter(x[:,0],x[:,1]) rho,_ = stats.pearsonr(x[:,0],x[:,1]) print(rho) 266
5.3. CONDITIONAL PMF AND PDF 5.3 Conditional PMF and PDF Whenever we have a pair of random variables X and Y that are correlated, we can define their conditional distributions, which quantify the probability of X = x given Y = y. In this section, we discuss the concepts of conditional PMF and PDF. 5.3.1 Conditional PMF We start by defining the conditional PMF for a pair of discrete random variables. Definition 5.14. Let X and Y be two discrete random variables. The conditional PMF of X given Y is p (x,y) p (x|y)= X,Y . (5.21) X|Y p (y) Y The simplest way to understand this is to view p (x|y) as P[X = x|Y = y]. That is, X|Y given that Y = y, what is the probability for X = x? To see why this perspective makes sense, let us recall the definition of a conditional probability: p (x,y) p (x|y)= X,Y X|Y p (y) Y P[X =x ∩ Y =y] = =P[X =x|Y =y]. P[Y =y] Aswecansee,thelasttwoequalitiesareessentiallythedefinitionsofconditionalprobability and the joint PMF. Howshouldweunderstandthenotationp (x|y)?Isitaone-variablefunctioninxor X|Y a two-variable function in (x,y)? What does p (x|y) tell us? To answer these questions, X|Y letusfirsttrytounderstandtherandomnessexhibitedinaconditionalPMF.Inp (x|y), X|Y the random variable Y is fixed to a specific value Y =y. Therefore there is nothing random about Y. All the possibilities of Y have already been taken care of by the denominator p (y). Only the variable x in p (x|y) has randomness. What do we mean by “fixed at a Y X|Y value Y =y”? Consider the following example. Example 5.16. Suppose there are two coins. Let X =the sum of the values of two coins, Y =the value of the first coin. Clearly, X has 3 states: 0, 1, 2, and Y has two states: either 0 or 1. When we say p (x|1), we refer to the probability mass function of X when fixing Y =1. If we do X|Y not impose this condition, the probability mass of X is simple: (cid:20) (cid:21) 1 1 1 p (x)= , , . X 4 2 4 267
CHAPTER 5. JOINT DISTRIBUTIONS However, if we include the conditioning, then p (x,1) p (x|1)= X,Y X|Y p (1) Y (cid:2) 0,2,1(cid:3) (cid:20) 2 1(cid:21) = 4 4 = 0, , . 1 3 3 6 Toputthisinplainwords,whenY =1,thereisnowayforX totakethestate0.The chance for X to take the state 1 is 2/3 because either (0,1) or (1,0) can give X = 1. The chance for X to take the state 2 is 1/3 because it has to be (1,1) in order to give X = 2. Therefore, when we say “conditioned on Y = 1”, we mean that we limit our observations to cases where Y =1. Since Y is already fixed at Y =1, there is nothing random about Y. The only variable is X. This example is illustrated in Figure 5.10. Figure5.10:SupposeX isthesumoftwocoinswithPMF0.25,0.5,0.25.LetY bethefirstcoin. When X is unconditioned, the PMF is just [0.25,0.5,0.25]. When X is conditioned on Y = 1, then“X =0”cannothappen.Therefore,theresultingPMFp (x|1)onlyhastwostates.After X|Y normalization we obtain the conditional PMF [0,0.66,0.33]. Since Y is already fixed at a particular value Y = y, p (x|y) is a probability mass X|Y function of x (we want to emphasize again that it is x and not y). So p (x|y) is a one- X|Y variable function in x. It is not the same as the usual PMF p (x). p (x|y) is conditioned X X|Y on Y =y. For example, p (x|1) is the PMF of X restricted to the condition that Y =1. X|Y In fact, it follows that (cid:88) p (x|y)= (cid:88) p X,Y(x,y) X|Y p (y) Y x∈ΩX x∈ΩX (cid:80) = x∈ΩX p X,Y(x,y) = p Y(y) =1, p (y) p (y) Y Y butthistellsusthatp (x|y)isalegitimateprobabilitymassofX.Ifwesumoverthey’s X|Y instead, then we will hit a bump: (cid:88) p (x|y)= (cid:88) p X,Y(x,y) (cid:54)=1. X|Y p (y) Y y∈ΩY y∈ΩY Therefore, while p (x|y) is a legitimate probability mass function of X, it is not a prob- X|Y ability mass function of Y. 268
5.3. CONDITIONAL PMF AND PDF Example5.17.ConsiderajointPMFgiveninthefollowingtable.Findtheconditional PMF p (x|1) and the marginal PMF p (x). X|Y X Y= 1 2 3 4 X = 1 1 1 1 0 20 20 20 20 2 1 2 3 1 20 20 20 20 3 1 2 3 1 20 20 20 20 4 0 1 1 1 20 20 20 20 Solution. To find the marginal PMF, we sum over all the y’s for every x: 4 (cid:88) 1 1 1 0 3 x=1: p (1)= p (1,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 1 2 2 1 6 x=2: p (2)= p (2,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 1 3 3 1 8 x=3: p (3)= p (3,y)= + + + = , X X,Y 20 20 20 20 20 y=1 4 (cid:88) 0 1 1 1 3 x=4: p (4)= p (4,y)= + + + = . X X,Y 20 20 20 20 20 y=1 Hence, the marginal PMF is p (x)=(cid:2) 3 6 8 3 (cid:3) . X 20 20 20 20 The conditional PMF p (x|1) is X|Y p (x|1)= p X,Y(x,1) = (cid:2) 21 0 21 0 21 0 20 0(cid:3) =(cid:2)1 1 1 0(cid:3) . X|Y p (1) 3 3 3 3 Y 20 Practice Exercise 5.7. Consider two random variables X and Y defined as follows.  (cid:40) 102, with prob 5/6, 10−4Y, with prob 1/2, Y = X = 10−3Y, with prob 1/3, 104, with prob 1/6. 10−2Y, with prob 1/6. Find p (x|y), p (x) and p (x,y). X|Y X X,Y Solution.SinceY takestwodifferentstates,wecanenumerateY =102 andY =104. 269
CHAPTER 5. JOINT DISTRIBUTIONS This gives us  1/2, if x=0.01,  p (x|102)= 1/3, if x=0.1, X|Y 1/6, if x=1.  1/2, if x=1,  p (x|104)= 1/3, if x=10, X|Y 1/6, if x=100. The joint PMF p (x,y) is X,Y (cid:0)1(cid:1)(cid:0)5(cid:1) , x=0.01,  2 6 p (x,102)=p (x|102)p (102)= (cid:0)1(cid:1)(cid:0)5(cid:1) , x=0.1, X,Y X|Y Y 3 6 (cid:0)1(cid:1)(cid:0)5(cid:1) , x=1. 6 6 (cid:0)1(cid:1)(cid:0)1(cid:1) , x=1,  2 6 p (x,104)=p (x|104)p (104)= (cid:0)1(cid:1)(cid:0)1(cid:1) , x=10, X,Y X|Y Y 3 6 (cid:0)1(cid:1)(cid:0)1(cid:1) , x=100. 6 6 Therefore, the joint PMF is given by the following table. 104 0 0 1 1 1 12 18 36 102 5 5 5 0 0 12 18 36 0.01 0.1 1 10 100 The marginal PMF p (x) is thus X (cid:88) p (x)= p (x,y) X X,Y y =(cid:2) 5 5 2 1 1 (cid:3) . 12 18 9 18 36 In the previous two examples, what is the probability P[X ∈ A|Y = y] or the proba- bility P[X ∈A] for some events A? The answers are giving by the following theorem. Theorem 5.7. Let X and Y be two discrete random variables. Let A be an event. (cid:88) P[X ∈A|Y =y]= p (x|y) X|Y x∈A and (cid:88) (cid:88) (cid:88) P[X ∈A]= p (x|y)p (y)= P[X ∈A|Y =y]p (y). X|Y Y Y x∈Ay∈ΩY y∈ΩY Proof.ThefirststatementisbasedonthefactthatifAcontainsafinitenumberofelements, 270
5.3. CONDITIONAL PMF AND PDF then P[X ∈A] is equivalent to the sum (cid:80) P[X =x]. Thus, x∈A P[X ∈A∩Y =y] P[X ∈A|Y =y]= P[Y =y] (cid:80) P[X =x∩Y =y] = x∈A P[Y =y] (cid:88) = p (x|y). X|Y x∈A (cid:80) The second statement holds because the inner summation p (x|y)p (y) is just y∈ΩY X|Y Y the marginal PMF p (x). Thus the outer summation yields the probability. X (cid:3) Example 5.18. Let us follow up on Example 5.17. What is the probability that P[X >2|Y =1]? What is the probability that P[X >2]? Solution. Since the problem asks about the conditional probability, we know that it can be computed by using the conditional PMF. This gives us (cid:88) P[X >2|Y =1]= p (x|1) X|Y x>2 =(cid:24)p X(cid:24) |Y(cid:24) (1(cid:24) |(cid:24) 1)+(cid:24)p X(cid:24) |Y(cid:24) (2(cid:24) |(cid:24) 1)+p X|Y(3|1)+p X|Y(4|1)= 31 . (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) 1 0 3 The other probability is P[X >2]=(cid:88) p (x)=(cid:24)p (cid:24) ((cid:24) 1)+(cid:24)p (cid:24) ((cid:24) 2)+p (3)+p (4)= 11 . X X X X X 20 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x>2 8 3 20 20 What is the rule of thumb for conditional distribution? • The PMF/PDF should match with the probability you are finding. • If you want to find the conditional probability P[X ∈ A|Y = y], use the condi- tional PMF p (x|y). X|Y • If you want to find the probability P[X ∈A], use the marginal PMF p (x). X Finally, we define the conditional CDF for discrete random variables. Definition 5.15. Let X and Y be discrete random variables. Then the conditional CDF of X given Y =y is (cid:88) F (x|y)=P[X ≤x|Y =y]= p (x(cid:48)|y). (5.22) X|Y X|Y x(cid:48)≤x 271
CHAPTER 5. JOINT DISTRIBUTIONS 5.3.2 Conditional PDF We now discuss the conditioning of a continuous random variable. Definition 5.16. Let X and Y be two continuous random variables. The conditional PDF of X given Y is f (x,y) f (x|y)= X,Y . (5.23) X|Y f (y) Y Example 5.19. Let X and Y be two continuous random variables with a joint PDF (cid:40) 2e−xe−y, 0≤y ≤x<∞, f (x,y)= X,Y 0, otherwise. Find the conditional PDFs f (x|y) and f (y|x). X|Y Y|X Solution. We first find the marginal PDFs. (cid:90) ∞ (cid:90) x f (x)= f (x,y)dy = 2e−xe−y dy =2e−x(1−e−x), X X,Y −∞ 0 (cid:90) ∞ (cid:90) ∞ f (y)= f (x,y)dx= 2e−xe−y dx=2e−2y. Y X,Y −∞ y Thus, the conditional PDFs are f (x,y) f (x|y)= X,Y X|Y f (y) Y 2e−xe−y = =e−(x+y), x≥y, 2e−2y f (x,y) f (y|x)= X,Y Y|X f (x) X 2e−xe−y e−y = = , 0≤y <x. 2e−x(1−e−x) 1−e−x Where does the conditional PDF come from? We cannot duplicate the argument we used for the discrete case because the denominator of a conditional PMF becomes P[Y =y]=0 when Y is continuous. To answer this question, we first define the conditional CDF for continuous random variables. Definition 5.17. Let X and Y be continuous random variables. Then the conditional CDF of X given Y =y is (cid:82)x f (x(cid:48),y)dx(cid:48) F (x|y)= −∞ X,Y . (5.24) X|Y f (y) Y 272
5.3. CONDITIONAL PMF AND PDF WhyshouldtheconditionalCDFofcontinuousrandomvariablebedefinedinthisway?One waytointerpretF (x|y)isasthelimitingperspective.WecandefinetheconditionalCDF X|Y as F (x|y)= lim P(X ≤x|y ≤Y ≤y+h) X|Y h→0 P(X ≤x∩y ≤Y ≤y+h) = lim . h→0 P[y ≤Y ≤y+h] With some calculations, we have that lim P(X ≤x∩y ≤Y ≤y+h) = lim (cid:82) −x ∞(cid:82) yy+h f X,Y(x(cid:48),y(cid:48))dy(cid:48) dx(cid:48) h→0 P[y ≤Y ≤y+h] h→0 (cid:82)y+h f (y(cid:48))dy(cid:48) y Y (cid:82)x f (x(cid:48),y(cid:48))dx(cid:48)·h = lim −∞ X,Y h→0 f Y(y)·h (cid:82)x f (x(cid:48),y(cid:48))dx(cid:48) = −∞ X,Y . f (y) Y The key here is that the small step size h in the numerator and the denominator will cancel each other out. Now, given the conditional CDF, we can verify the definition of the conditional PDF. It holds that d f (x|y)= F (x|y) X|Y dx X|Y = d (cid:40)(cid:82) −x ∞f X,Y(x(cid:48),y)dx(cid:48)(cid:41) ( =a) f X,Y(x,y) , dx f (y) f (y) Y Y where (a) follows from the fundamental theorem of calculus. JustliketheconditionalPMF,wecancalculatetheprobabilitiesusingtheconditional PDFs. In particular, if we evaluate the probability where X ∈ A given that Y takes a particular value Y =y, then we can integrate the conditional PDF f (x|y), with respect X|Y to x. Theorem 5.8. Let X and Y be continuous random variables, and let A be an event. (i) P[X ∈A|Y =y]=(cid:82) f (x|y)dx, A X|Y (ii) P[X ∈A]=(cid:82) P[X ∈A|Y =y]f (y)dy. ΩY Y Example 5.20. Let X be a random bit such that (cid:40) +1, with prob 1/2, X = −1, with prob 1/2. Suppose that X is transmitted over a noisy channel so that the observed signal is Y =X+N, 273
CHAPTER 5. JOINT DISTRIBUTIONS where N ∼Gaussian(0,1) is the noise, which is independent of the signal X. Find the probabilities P[X =+1|Y >0] and P[X =−1|Y >0]. Solution. First, we know that f Y|X(y|+1)= √1 e−(y− 21)2 and f Y|X(y|−1)= √1 e−(y+ 21)2 . 2π 2π Therefore, integrating y from 0 to ∞ gives us P[Y >0|X =+1]=(cid:90) ∞ √1 e−(y− 21)2 dy 2π 0 =1−(cid:90) 0 √1 e−(y− 21)2 dy 2π −∞ (cid:18) (cid:19) 0−1 =1−Φ =1−Φ(−1). 1 Similarly, we have P[Y >0|X =−1]=1−Φ(+1). The probability we want to find is P[X =+1|Y >0], which can be determined using Bayes’ theorem. P[Y >0|X =+1]P[X =+1] P[X =+1|Y >0]= . P[Y >0] The denominator can be found by using the law of total probability: P[Y >0]=P[Y >0|X =+1]P[X =+1] +P[Y >0|X =−1]P[X =−1] 1 =1− (Φ(+1)+Φ(−1)) 2 1 = , 2 since Φ(+1)+Φ(−1)=Φ(+1)+1−Φ(+1)=1. Therefore, P[X =+1|Y >0]=1−Φ(−1) =0.8413. The implication is that if Y > 0, the probability P[X = +1|Y > 0] = 0.8413. The complement of this result gives P[X =−1|Y >0]=1−0.8413=0.1587. Practice Exercise 5.8. Find P[Y >y], where X ∼Uniform[1,2], Y |X ∼Exponential(x). Solution.Thetrickypartofthisproblemisthetendencytoconfusethetwovariables X andY.Onceyouunderstandtheirrolestheproblembecomeseasy.Firstnoticethat Y |X ∼ Exponential(x) is a conditional distribution. It says that given X = x, the 274
5.4. CONDITIONAL EXPECTATION probabilitydistributionofY isexponential,withtheparameterx.Thus,wehavethat f (y|x)=xe−xy. Y|X Why? Recall that if Y ∼ Exponential(λ) then f (y) = λe−λy. Now if we replace λ Y with x, we have xe−xy. So the role of x in this conditional density function is as a parameter. Given this property, we can compute the conditional probability: (cid:90) ∞ P[Y >y|X =x]= f (y(cid:48)|x)dy(cid:48) Y|X y (cid:90) ∞ (cid:20) (cid:21)∞ = xe−xy(cid:48) dy(cid:48) = −e−xy(cid:48) =e−xy. y y(cid:48)=y Finally, we can compute the marginal probability: (cid:90) P[Y >y]= P[Y >0|X =x(cid:48)]f (x(cid:48))dx(cid:48) X ΩX =(cid:90) 1 e−x(cid:48)y dx(cid:48) =(cid:20) 1 e−x(cid:48)y(cid:21)x(cid:48)=1 = 1(cid:0) 1−e−y(cid:1) . y y 0 x(cid:48)=0 Wecandouble-checkthisresultbynotingthattheproblemasksabouttheprobability P[Y >y]. Thus, the answer must be a function of y but not of x. 5.4 Conditional Expectation 5.4.1 Definition When dealing with two dependent random variables, at times we would like to determine the expectation of a random variable when the second random variable takes a particular state. The conditional expectation is a formal way of doing so. Definition 5.18. The conditional expectation of X given Y =y is (cid:88) E[X|Y =y]= xp (x|y) (5.25) X|Y x for discrete random variables, and (cid:90) ∞ E[X|Y =y]= xf (x|y)dx (5.26) X|Y −∞ for continuous random variables. 275
CHAPTER 5. JOINT DISTRIBUTIONS Therearetwopointstonotehere.First,theexpectationofE[X|Y =y]istakenwithrespect to f (x|y). We assume that the random variable Y is already fixed at the state Y = y. X|Y Thus, the only source of randomness is X. Secondly, since the expectation E[X|Y =y] has eliminated the randomness of X, the resulting function is in y. What is conditional expectation? • E[X|Y =y] is the expectation using f (x|y). X|Y • The integration is taken w.r.t. x, because Y =y is given and fixed. 5.4.2 The law of total expectation Theorem 5.9. The law of total expectation states that (cid:90) ∞ (cid:88) E[X]= E[X|Y =y]p (y), or E[X]= E[X|Y =y]f (y)dy. (5.27) Y Y y −∞ Proof.Wewillprovethediscretecaseonly,asthecontinuouscasecanbeprovedbyreplacing summation with integration. (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) E[X]= xp (x)= x p (x,y) X X,Y x x y (cid:88)(cid:88) = xp (x|y)p (y) X|Y Y x y (cid:32) (cid:33) (cid:88) (cid:88) (cid:88) = xp (x|y) p (y)= E[X|Y =y]p (y). X|Y Y Y y x y (cid:3) Figure 5.11illustratestheideabehindtheproof.Essentially,wedecompose theexpec- tation E[X] into “subexpectations” E[X|Y =y]. The probability of each subexpectation is p (y). By summing the subexpectation multiplied by p (y), we obtain the overall expecta- Y Y tion. What is the law of total expectation? • The law of total expectation is a decomposition rule. • It decomposes E[X] into smaller/easier conditional expectations. This law can also be written in a more compact form. Corollary 5.1. Let X and Y be two random variables. Then E[X]=E (cid:2)E [X|Y](cid:3) . (5.28) Y X|Y 276
5.4. CONDITIONAL EXPECTATION Figure 5.11: The expectation E[X] can be decomposed into a set of subexpectations. This gives us E[X]=(cid:80) E[X|Y =y]p (y). y Y Proof.TheprevioustheoremstatesthatE[X]=(cid:80) E[X|Y =y]p (y).IfwetreatE[X|Y = y Y y] as a function of y, for instance h(y), then (cid:88) (cid:88) E[X]= E[X|Y =y]p (y)= h(y)p (y)=E[h(Y)]=E[E[X|Y]]. (cid:3) Y Y y y Example 5.21. Suppose there are two classes of cars. Let X be the speed of a car and C be the class. When C =1, we know that X ∼Gaussian(µ ,σ ). We know that 1 1 P[C =1]=p. When C =2, X ∼Gaussian(µ ,σ ). Also, P[C =2]=1−p. If you see 2 2 a car on the freeway, what is its average speed? Solution. The problem has given us everything we need. In particular, we know that the conditional PDFs are: 1 (cid:26) (x−µ )2(cid:27) f (x|1)= exp − 1 , X|C (cid:112) 2πσ2 2σ2 1 1 1 (cid:26) (x−µ )2(cid:27) f (x|2)= exp − 2 . X|C (cid:112) 2πσ2 2σ2 2 2 Therefore, conditioned on C, we have two expectations: (cid:90) ∞ E[X|C =1]= xf (x|1)dx=µ , X|C 1 −∞ (cid:90) ∞ E[X|C =2]= xf (x|2)dx=µ . X|C 2 −∞ The overall expectation E[X] is E[X]=E[X|C =1]P[C =1]+E[X|C =2]P[C =2] =pµ +(1−p)µ . 1 2 277
CHAPTER 5. JOINT DISTRIBUTIONS Practice Exercise 5.9. Consider a joint PMF given by the following table. Find E[X|Y =102] and E[X|Y =104]. Y 104 0 0 1 1 1 12 18 36 102 5 5 5 0 0 12 18 36 0.01 0.1 1 10 100 X Solution. To find the conditional expectation, we first need to know the conditional PMF. p (x|102)=(cid:2)1 1 1 0 0(cid:3) , X|Y 2 3 6 p (x|104)=(cid:2) 0 0 1 1 1(cid:3) . X|Y 2 3 6 Therefore, the conditional expectations are (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X|Y =102]=(10−2) +(10−1) +(1) 2 3 6 123 = , 600 (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) 1 1 1 E[X|Y =104]=(1) +(10) +(100) 2 3 6 123 = . 6 From the conditional expectations we can also find E[X]: E[X]=E[X|Y =102]p (102) Y +E[X|Y =104]p (104) Y (cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19) 123 5 123 1 = + 600 6 6 6 =3.5875. Example 5.22. Consider two random variables X and Y. The random variable X is Gaussian-distributed with X ∼ Gaussian(µ,σ2). The random variable Y has a conditional distribution Y|X ∼Gaussian(X,X2). Find E[Y]. Solution. The notation Y|X ∼ Gaussian(X,X2) means that given the variable X, the other variable Y has a conditional distribution Gaussian(X,X2). That is, the variable Y is a Gaussian with mean X and variance X2. How can the mean be a random variable X and the variance be another random variable X2? Because X is the conditional variable. Y|X means that you have already chosen one state of X. Given that particular state, the distribution of Y follows f . Therefore, for this Y|X 278
5.4. CONDITIONAL EXPECTATION problem, we know the PDFs: 1 (cid:26) (x−µ)2(cid:27) f (x)= √ exp − , X 2πσ2 2σ2 1 (cid:26) (y−x)2(cid:27) f (y|x)= √ exp − . Y|X 2πx2 2x2 The conditional expectation of Y given X is (cid:90) ∞ 1 (cid:26) (y−x)2(cid:27) E[Y|X =x]= y√ exp − dy 2πx2 2x2 −∞ =E[Gaussian(x,x2)]=x. The last equality holds because we are computing the expectation of a Gaussian ran- domvariablewithmeanx.Finally,applyingthelawoftotalexpectation,wecanshow that (cid:90) ∞ E[Y]= E[Y|X =x]f (x)dx X −∞ (cid:90) ∞ 1 (cid:26) (x−µ)2(cid:27) = x√ exp − dx 2πσ2 2σ2 −∞ =E[Gaussian(µ,σ2)]=µ, where the last equality is based on the fact that it is the mean of a Gaussian. Practice Exercise 5.10. Find E[sin(X +Y)], if X ∼ Gaussian(0,1), and Y |X ∼ Uniform[x−π,x+π]. Solution. We know that the conditional density is 1 f (y|x)= , x−π ≤y ≤x+π. Y|X 2π Therefore, we can compute the probability (cid:90) x+π E[sin(X+Y)|X =x]= sin(x+y)f (y|x)dy Y|X x−π 1 (cid:90) x+π = sin(x+y)dy =0. 2π x−π (cid:124) (cid:123)(cid:122) (cid:125) =0 Hence, the overall expectation is E[sin(X+Y)]=(cid:90) 1 E[sin(X+Y)|X =x]√1 e−x 22 dx=0. 0 (cid:124) (cid:123)(cid:122) (cid:125) 2π =0 279
CHAPTER 5. JOINT DISTRIBUTIONS 5.5 Sum of Two Random Variables One typical problem we encounter in engineering is to determine the PDF of the sum of two random variables X and Y, i.e., X+Y. Such a problem arises naturally when we want to evaluate the average of many random variables, e.g., the sample mean of a collection of data points. This section will discuss a general principle for determining the PDF of a sum of two random variables. 5.5.1 Intuition through convolution First, consider two random variables, X and Y, both discrete uniform random variables in the range of 0,1,2,3. That is, p (x) = p (y) = [1/4,1/4,1/4,1/4]. Since this is such a X Y simpleproblemwecanenumerateallthepossiblecasesofthesumZ =X+Y.Theresulting probabilities are shown in the following table. Z =X+Y Cases, written in terms of (X, Y) Probability 0 (0,0) 1/16 1 (0,1), (1,0) 2/16 2 (1,1), (2,0), (0,2) 3/16 3 (3,0), (2,1), (1,2), (0,3) 4/16 4 (3,1), (2,2), (1,3) 3/16 5 (3,2), (2,3) 2/16 6 (3,3) 1/16 Clearly,thePMFofZ isnotf (z)=f (x)+f (y).(Caution!Donotwritethis.)The Z X Y PMF of Z looks like a triangle distribution. How can we get to this triangle distribution from two uniform distributions? The key is the idea of convolution. Let us start with the PMFofX,whichisp (x).Letusalsoflipp (y)overthey-axis.Asweshifttheflippedp , X Y Y we multiply and add the PMF values as shown in Figure 5.12. This gives us p (0)=P[X+Y =0] Z =P[(X,Y)=(0,0)] =p (0)p (0) X Y 1 = . 16 Now, if we shift towards the right by 1, we have p (1)=P[X+Y =1] Z =P[(X,Y)=(0,1)∪(0,1)] 2 =p (0)p (1)+p (1)p (0)= . X Y X Y 16 By continuing our argument, you can see that we will obtain the same PMF as the one shown in the table. 280
5.5. SUM OF TWO RANDOM VARIABLES Figure5.12:WhensummingtworandomvariablesX andY,weareeffectivelytakingtheconvolutions of the two respective PMF / PDFs. 5.5.2 Main result We can show that for any arbitrary random variable X and Y, the sum Z = X +Y has a distribution that is the convolution of two individual PDFs. Theorem 5.10. Let X and Y be two independent random variables with PDFs f (x) X and f (y) respectively. Let Z =X+Y. The PDF of Z is given by Y (cid:90) ∞ f (z)=(f ∗f )(z)= f (z−y)f (y)dy, (5.29) Z X Y X Y −∞ where “∗” denotes the convolution. Proof. We begin by analyzing the CDF of Z. The CDF of Z is F (z)=P[Z ≤z]=P[X+Y ≤z]. Z We now draw a picture to illustrate the line under which we want to integrate. As shown in Figure 5.13, the equation X+Y ≤z defines a straight line in the xy plane. You can think of it as Y ≤−X+z, so that the slope is −1 and the y-intercept is z. Now, shall we take the upper half of the triangle or the lower half? Since the equation is Y ≤−X+z, a value of Y has to be less than that of the line. Another easy way to check is to assume z > 0 so that we have a positive y-intercept. Then we check where the origin 281
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.13: The shaded region highlights the set X+Y ≤Z. To integrate the PDF over this region, we first take the inner integration over dx and then take the outer integration over dy. (0,0) belongs. In this case, if z >0, the origin (0,0) will satisfy the equation Y ≤−X +z, and so it must be included. Thus, we conclude that the area is below the line. Oncewehavedeterminedtheareatobeintegrated,wecanwritedowntheintegration: (cid:90) ∞ (cid:90) z−y P[X+Y ≤z]= f (x,y)dxdy X,Y −∞ −∞ (cid:90) ∞ (cid:90) z−y = f (x)f (y)dxdy, (independence) X Y −∞ −∞ where the integration limits are just a rewrite of X +Y ≤ z (in this case since we are integrating x first we have X ≤ −Y +z). Then, by the fundamental theorem of calculus, we can show that d d (cid:90) ∞ (cid:90) z−y f (z)= F (z)= f (x)f (y)dxdy Z dz Z dz X Y −∞ −∞ (cid:90) ∞ (cid:18) d (cid:90) z−y (cid:19) = f (x)f (y)dx dy dz X Y −∞ −∞ (cid:90) ∞ = f (z−y)f (y)dy =(f ∗f )(z), X Y X Y −∞ where “∗” denotes the convolution. How is convolution related to random variables? • If you sum X and Y, the resulting PDF is the convolution of f and f . X Y • E.g., convolving two uniform random variables gives you a triangle PDF. 5.5.3 Sum of common distributions Theorem 5.11 (SumoftwoPoissons). Let X ∼Poisson(λ ) and X ∼Poisson(λ ). 1 1 2 2 Then X +X ∼Poisson(λ +λ ). (5.30) 1 2 1 2 282
5.5. SUM OF TWO RANDOM VARIABLES Proof. Let us apply the convolution principle. p (k)=P[X +X =k] Y 1 2 =P[X =(cid:96) ∩ X =k−(cid:96)] 1 2 (cid:88)k λ(cid:96)e−λ1 λk−(cid:96)e−λ2 = 1 · 2 (cid:96)! (k−(cid:96))! (cid:96)=0 (cid:88)k λ(cid:96) λk−(cid:96) =e−(λ1+λ2) 1 · 2 (cid:96)! (k−(cid:96))! (cid:96)=0 k 1(cid:88) k! =e−(λ1+λ2)· λ(cid:96)λk−(cid:96) k! (cid:96)!(k−(cid:96))! 1 2 (cid:96)=0 (cid:124) (cid:123)(cid:122) (cid:125) =(cid:80)k (k)λ(cid:96)λk−(cid:96) (cid:96)=0 (cid:96) 1 2 (λ +λ )k = 1 2 e−(λ1+λ2), k! where the last step is based on the binomial identity (cid:80)k (cid:0)k(cid:1) a(cid:96)bk−(cid:96) =(a+b)k. (cid:96)=0 (cid:96) (cid:3) Theorem 5.12 (Sum of two Gaussians). Let X and X be two Gaussian random 1 2 variables such that X ∼Gaussian(µ ,σ2) and X ∼Gaussian(µ ,σ2). 1 1 1 2 2 2 Then X +X ∼Gaussian(µ +µ ,σ2+σ2). (5.31) 1 2 1 2 1 2 Proof. Let us apply the convolution principle. (cid:90) ∞ f (z)= f (t)f (z−t)dt Z X Y −∞ (cid:90) ∞ 1 (cid:26) (t−µ )2(cid:27) 1 (cid:26) (z−t−µ )2(cid:27) = √ exp − 1 · √ exp − 2 dt 2πσ2 2σ2 2πσ2 2σ2 −∞ 1 (cid:90) ∞ 1 (cid:26) (t−µ )2+(z−t−µ )2(cid:27) = √ √ exp − 1 2 dt. 2πσ2 2πσ2 2σ2 −∞ We now complete the square: (t−µ )2+(z−t−µ )2 =[t2−2µ t+µ2]+[t2+2t(µ −z)+(µ −z)2] 1 2 1 1 2 2 =2t2−2t(µ −µ +z)+µ2+(µ −z)2 1 2 1 2 (cid:20) (cid:21) µ −µ +z =2 t2−2t· 1 2 +µ2+(µ −z)2 2 1 2 (cid:20) µ −µ +z(cid:21)2 (cid:20) µ −µ +z(cid:21)2 =2 t− 1 2 −2 1 2 +µ2+(µ −z)2. 2 2 1 2 283
CHAPTER 5. JOINT DISTRIBUTIONS The last term can be simplified to (cid:20) µ −µ +z(cid:21)2 −2 1 2 +µ2+(µ −z)2 2 1 2 µ2−2µ (µ −z)+(µ −z)2 =− 1 1 2 2 +µ2+(µ −z)2 2 1 2 µ2+2µ (µ −z)+(µ −z)2 (µ +µ −z)2 = 1 1 2 2 = 1 2 . 2 2 Substituting these into the integral, we can show that 1 (cid:90) ∞ 1 (cid:40) 2(cid:2) t− µ1−µ2+z(cid:3)2 + (µ1+µ2−z)2(cid:41) f (z)= √ √ exp − 2 2 dt Z 2πσ2 2πσ2 2σ2 −∞ 1 (cid:26) (µ +µ −z)2(cid:27)(cid:90) ∞ 1 (cid:40) (cid:2) t− µ1−µ2+z(cid:3)2(cid:41) = √ exp − 1 2 √ exp − 2 dt 2πσ2 2(2σ2) 2πσ2 σ2 −∞ (cid:124) (cid:123)(cid:122) (cid:125) =√1 2 1 (cid:26) (µ +µ −z)2(cid:27) = exp − 1 2 . (cid:112) 2π(2σ)2 2(2σ2) Therefore, we have shown that the resulting distribution is a Gaussian with mean µ +µ 1 2 and variance 2σ2. (cid:3) Practice Exercise 5.11. Let X and Y be independent, and let (cid:40) (cid:40) xe−x, x≥0, ye−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Solution. Using the results derived above, we see that (cid:90) ∞ f (z)= f (z−y)f (y)dy Z X Y −∞ (cid:90) z = f (z−y)f (y)dy, X Y −∞ wheretheupperlimitz camefromthefactthatx≥0.Therefore,sinceZ =X+Y,we musthaveZ−Y =X ≥0andsoZ ≥Y.ThisisportrayedgraphicallyinFigure 5.14. Substituting the PDFs into the integration yields (cid:90) z z3 f (z)= (z−y)e−(z−y)ye−y dy = e−z, z ≥0. Z 6 0 For z <0, f (z)=0. Z The functions of two random variables are not limited to summation. The following example illustrates the case of the product of two random variables. 284
5.5. SUM OF TWO RANDOM VARIABLES Figure 5.14: [Left] The outer integral goes from 0 to z because the triangle stops at y =z. [Right] If the triangle is unbounded, then the integral goes from −∞ to ∞. Example 5.23. Let X and Y be two independent random variables such that (cid:40) (cid:40) 2x, if 0≤x≤1, 1, if 0≤y ≤1, f (x)= and f (y)= X Y 0, otherwise, 0, otherwise. Let Z =XY. Find f (z). Z Solution. The CDF of Z can be evaluated as (cid:90) ∞ (cid:90) z F (z)=P[Z ≤z]=P[XY ≤z]= y f (x)f (y)dxdy. Z X Y −∞ −∞ Taking the derivative yields d d (cid:90) ∞ (cid:90) yz f (z)= F (z)= f (x)f (y)dxdy Z dz Z dz X Y −∞ −∞ (a)(cid:90) ∞ 1 (cid:18) z(cid:19) = f f (y)dy, y X y Y −∞ where (a) holds by the fundamental theorem of calculus. The upper and lower limit of this integration can be determined by noting that z 0≤ =x≤1, y which implies that z ≤y. Since y ≤1, we have that z ≤y ≤1. Therefore, the PDF is (cid:90) 1 1 (cid:18) z(cid:19) f (z)= f f (y)dy Z y X y Y z (cid:90) 1 2z = dy =2(1−z), z ≥0. y2 z For z <0, f (z)=0. Z 285
CHAPTER 5. JOINT DISTRIBUTIONS Closing remark. For some random variables, summing two i.i.d. copies remain the same random variable (but with different parameters). For other random variables, summing two i.i.d. copies gives a different random variable. Table 5.1 summarizes some of the most commonly used random variable pairs. X X Sum X +X 1 2 1 2 Bernoulli(p) Bernoulli(p) Binomial(2,p) Binomial(n,p) Binomial(m,p) Binomial(m+n,p) Poisson(λ ) Poisson(λ ) Poisson(λ +λ ) 1 2 1 2 Exponential(λ) Exponential(λ) Erlang(2,λ) Gaussian(µ ,σ2) Gaussian(µ ,σ2) Gaussian(µ +µ ,σ2+σ2) 1 1 2 2 1 2 1 2 Table 5.1: Common distributions of the sum of two random variables. 5.6 Random Vectors and Covariance Matrices We now enter the second part of this chapter. In the first part, we were mainly interested in a pair of random variables. In the second part, however, we will study vectors of N random variables. To understand a vector of random variables, we will not drill down to the integrations of the PDFs (which you would certainly not enjoy). Instead, we will blend linear algebra tools and probabilistictools to learn afew practical data analysis techniques. 5.6.1 PDF of random vectors Joint distributions can be generalized to more than two random variables. The most conve- nient way is to consider a vector of random variables and their corresponding states.     X x 1 1 X 2 x 2 X = .  and x= . .  .   .   .   .  X x N N Ournotationhereisunconventionalsincebolduppercaselettersusuallyrepresentmatrices. Here,X denotesavector,specificallyarandomvector.Itsstateisavectorx.Inthischapter, we will use the following notational convention: X and Y represent random vectors while A represents a matrix. One way to think about X is to imagine that if you put your hand into the sample space, you will pick up a vector x. This random realization x has N entries, and so you need to specify the probability of getting all these entries simultaneously. Accordingly, we should expect that X is characterized by an N-dimensional PDF f (x)=f (x ,x ,...,x ). X X1,X2,...,XN 1 2 N 286
5.6. RANDOM VECTORS AND COVARIANCE MATRICES Essentially, this PDF tells us the probability density for random variable X =x , random 1 1 variable X =x , etc. It is a coordinate-wise description. For example, if X contains three 2 2 elements such that X = [X ,X ,X ]T, and if the state we are looking at is x = [3, 1, 7]T, 1 2 3 then f (x) is the probability density such that this 3D coordinate (X ,X ,X ) takes the X 1 2 3 value [3,1,7]T. To compute the probability, we can integrate f (x) with respect to x. Let A be the X event. Then (cid:90) P[X ∈A]= f (x)dx X A (cid:90) (cid:90) = ··· f (x ,...,x )dx ...dx . X1,...,XN 1 N 1 N A IftherandomcoordinatesX ,...,X areindependent,thePDFcanbewrittenasaprod- 1 N uct of N individual PDFs: f (x ,...,x )=f (x )f (x )···f (x ), and so X1,...,XN 1 N X1 1 X2 2 XN N (cid:90) (cid:90) P[X ∈A]= ··· f (x )f (x )···f (x )dx ···dx . X1 1 X2 2 XN N 1 N A However, this does not necessarily simplify the calculation unless A is separable, e.g., A= [a ,b ]×[a ,b ]×···×[a ,b ]. In this case the integration becomes 1 1 2 2 N N (cid:89)N (cid:34) (cid:90) bi (cid:35) P[X ∈A]= f (x )dx , Xi i i i=1 ai which is obviously manageable. Example 5.24.LetX =[X ,...,X ]T beavectorofzero-meanunitvarianceGaus- 1 N sian random vectors. Let A=[−1,2]N. Then (cid:90) P[X ∈A]= f (x)dx X A (cid:90) (cid:90) = ··· f (x ,...,x )dx ···dx X1,···,XN 1 N 1 N A (cid:20)(cid:90) 2 (cid:21)N = f (x )dx =[Φ(2)−Φ(−1)]N, X1 1 1 −1 where Φ(·) is the standard Gaussian CDF. As you can see from the definition of a vector random variable, computing the proba- bility typically involves integrating a high-dimensional function, which is tedious. However, thegoodnewsisthatinpracticeweseldomneedtoperformsuchcalculations.Oftenweare more interested in the mean and the covariance of the random vectors because they usually carry geometric meanings. The next subsection explores this topic. 287
CHAPTER 5. JOINT DISTRIBUTIONS 5.6.2 Expectation of random vectors LetX =[X ,...,X ]T bearandomvector.Wedefinetheexpectationofarandomvector 1 N as follows. Definition 5.19. Let X =[X ,...,X ]T be a random vector. The expectation is 1 N E[X ] 1 µd =efE[X]= E[X . 2] . (5.32)  .   .  E[X ] N The resulting vector is called the mean vector. Since the mean vector is a vector of individual elements, we need to compute the marginal PDFs before computing the expec- tations: E[X ]  (cid:82) x f (x )dx  1 Ω 1 X1 1 1 E[X]= . . = . . ,  .   .  E[X ] (cid:82) x f (x )dx N Ω N XN N N where the marginal PDF is determined by (cid:90) f (x )= f (x )dx . Xn n X\n \n \n Ω In the equation above, x = [x ,...,x ,x ,...,x ]T contains all the elements with- \n 1 n−1 n+1 N out x . For example, if the PDF is f (x ,x ,x ), then n X1,X2,X3 1 2 3 (cid:90) (cid:90) E[X ]= x f (x ,x ,x )dx dx dx . 1 1 X1,X2,X3 1 2 3 2 3 1 (cid:124) (cid:123)(cid:122) (cid:125) fX1(x1) Again, this will become tedious when there are many variables. While the definition of the expectation may be challenging to understand, some prob- lemsusingitarestraightforward.WewillfirstdemonstratethecaseofindependentPoisson random variables, and then we will discuss joint Gaussians. Example 5.25. Let X = [X ,...,X ]T be a random vector such that X are inde- 1 N n pendent Poissons with X ∼Poisson(λ ). Then n n E[X 1]  (cid:80)∞ k=0k· λk 1e k− !λ1   λ 1 E[X]=  . . .  =   . . .   =  . . .  . E[X N] (cid:80)∞ k· λk Ne−λN λ N k=0 k! On computers, computing the mean vector can be done using built-in commands such as mean in MATLAB and np.mean in Python. However, caution is needed when performing thecalculation.InMATLAB,meancomputesalongfirstdimension(rowsindex).Thus,ifwe 288
5.6. RANDOM VECTORS AND COVARIANCE MATRICES have an N ×2 array, applying mean will give us a 1×2 vector. To obtain the column mean vector of size N ×1, we need to specify the direction as mean(X,2). Similarly, in Python, when calling np.mean, we need to specify the axis. % MATLAB code to compute a mean vector X = randn(100,2); mX = mean(X,2); # Python code to compute a mean vector import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100) mX = np.mean(X,axis=1) 5.6.3 Covariance matrix Definition 5.20. The covariance matrix of a random vector X =[X ,...,X ]T is 1 N   Var[X ] Cov(X ,X ) ··· Cov(X ,X ) 1 1 2 1 N Σd =ef Cov(X)=   Cov[X . . .2,X 1] Var . . .[X 2] · ..· .· Cov(X . . .2,X N)   . (5.33) Cov(X ,X ) Cov(X ,X ) ··· Var[X ] N 1 N 2 N A more compact way of writing the covariance matrix is Σ=Cov(X)=E[(X−µ)(X−µ)T], where µ = E[X] is the mean vector. The notation abT means the outer product, defined as     a a b a b ··· a b 1 1 1 1 2 1 N abT =  . . .  (cid:2) b 1 ··· b N(cid:3) =  . . . . . . ... . . .  . a a b a b ··· a b N N 1 N 2 N N It is easy to show that Cov(X)=Cov(X)T, i.e., they are symmetric. Theorem 5.13. If the coordinates X ,...,X are independent, then the covariance 1 N matrix Cov(X)=Σ is a diagonal matrix:   Var[X ] 0 ··· 0 1  0 Var[X 2] ··· 0  Σ=Cov(X)=   . . . . . . ... . . .   . 0 0 ··· Var[X ] N 289
CHAPTER 5. JOINT DISTRIBUTIONS Proof. If all X ’s are independent, then Cov(X ,X ) = 0 for all i (cid:54)= j. Substituting this i i j into the definition of the covariance matrix, we obtain the result. (cid:3) If we ignore the mean vector µ, we obtain the autocorrelation matrix R. Definition 5.21. Let X = [X ,...,X ]T be a random vector. The autocorrelation 1 N matrix is E[X X ] E[X X ] ··· E[X X ] 1 1 1 2 1 N E[X 2X 1] E[X 2X 2] ··· E[X 2X N] R=E[XXT]=   . . . . . . ... . . .   . (5.34) E[X X ] E[X X ] ··· E[X X ] N 1 N 2 N N We state without proof that Σ=R−µµT, which corresponds to the single-variable case where σ2 =E[X2]−µ2. On computers, computing the covariance matrix is done using built-in commands cov inMATLABandnp.covinPython.Likethemeanvectors,whencomputingthecovariance, we need to specify the direction. For example, for an N ×2 data matrix X, the covariance needs to be a 2×2 matrix. If we compute the covariance along the wrong direction, we will obtain an N ×N matrix, which is incorrect. % MATLAB code to compute covariance matrix X = randn(100,2); covX = cov(X); # Python code to compute covariance matrix import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[1,0],[0,1]],100) covX = np.cov(X,rowvar=False) print(covX) 5.6.4 Multidimensional Gaussian With the above tools in hand, we can now define a high-dimensional Gaussian. The PDF of a high-dimensional Gaussian is defined as follows. Definition 5.22. A d-dimensional joint Gaussian has the PDF (cid:26) (cid:27) 1 1 f (x)= exp − (x−µ)TΣ−1(x−µ) , (5.35) X (cid:112) (2π)d|Σ| 2 where d denotes the dimensionality of the vector x. 290
5.6. RANDOM VECTORS AND COVARIANCE MATRICES ThemeanvectorandthecovariancematrixofajointGaussianisreadilyavailablefromthe definition. E[X]=µ and Cov(X)=Σ. It is easy to show that if X is a scalar X, then d = 1, µ = µ, and Σ = σ2. Substituting these into the above definition returns us the familiar 1D Gaussian. Thed-dimensionalGaussianisageneralizationofthe1DGaussian(s).SupposethatX i andX areindependentforalli(cid:54)=j.ThenE[X X ]=E[X ]E[X ]andhenceCov(X ,X )= j i j i j i j 0. Consequently, the covariance matrix Σ is a diagonal matrix:  σ2 ··· 0 1 Σ=  . . . ... . . .  , 0 ··· σ2 d where σ2 =Var[X ]. When this occurs, the exponential term in the Gaussian PDF is i i  x −µ T  σ2 ··· 0−1 x −µ  (x−µ)TΣ−1(x−µ)=  1 . . . 1     . . .1 ... . . .     1 . . . 1  =(cid:88)d (x i− σ2µ i)2 . x −µ 0 ··· σ2 x −µ i=1 i d d d d d Moreover, the determinant |Σ| is |Σ|=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)  σ . . .12 · ..· .· 0 . . .   (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)=(cid:89)d σ i2. (cid:12) (cid:12) (cid:12) 0 ··· σ2 (cid:12) i=1 d Substituting these results into the joint Gaussian PDF, we obtain f (x)=(cid:89)n 1 exp(cid:26) −(x−µ i)2(cid:27) , X (cid:112) (2π)σ2 2σ2 i=1 i i which is a product of individual Gaussians. The Gaussian has different offsets and orientations for different choices of µ and Σ. Figure 5.15 shows a few examples. Note that for Σ to be valid Σ has to be “symmetric positive semi-definite”, the meaning of which will be explained shortly. GeneratingrandomnumbersfromamultidimensionalGaussiancanbedonebycalling built-in commands. In MATLAB, we use mvnrnd. In Python, we have a similar command. % MATLAB code to generate random numbers from multivariate Gaussian mu = [0 0]; Sigma = [.25 .3; .3 1]; X = mvnrnd(mu,Sigma,100); # Python code to generate random numbers from multivariate Gaussian import numpy as np import scipy.stats as stats X = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],100) 291
CHAPTER 5. JOINT DISTRIBUTIONS 5 5 5 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 -1 -1 -1 -2 -2 -2 -3 -3 -3 -4 -4 -4 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0 5 0 1 1 −0.5 0 2 1.9 (µ,Σ)= , (µ,Σ)= , (µ,Σ)= , 2 0 0.5 2 −0.5 1 0 1.9 2 Figure 5.15: Visualization of 2D Gaussians with different means and covariances. To display the data points and overlay with the contour, we can use MATLAB com- mands such as contour. The resulting plot looks like the one shown in Figure 5.16. In Python the corresponding command is plt.contour. To set up the plotting environment we use the commands np.meshgrid. The grid points are used to evaluate the PDF values, thus giving us the contour. % MATLAB code: Overlay random numbers with the Gaussian contour. X = mvnrnd([0 0],[.25 .3; .3 1],1000); x1 = -2.5:.01:2.5; x2 = -3.5:.01:3.5; [X1,X2] = meshgrid(x1,x2); F = mvnpdf([X1(:) X2(:)],[0 0],[.25 .3; .3 1]); F = reshape(F,length(x2),length(x1)); figure(1); scatter(x(:,1),x(:,2),’rx’, ’LineWidth’, 1.5); hold on; contour(x1,x2,F,[.001 .01 .05:.1:.95 .99 .999], ’LineWidth’, 2); # Python code: Overlay random numbers with the Gaussian contour. import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt X = stats.multivariate_normal.rvs([0,0],[[0.25,0.3],[0.3,1.0]],1000) x1 = np.arange(-2.5, 2.5, 0.01) x2 = np.arange(-3.5, 3.5, 0.01) X1, X2 = np.meshgrid(x1,x2) Xpos = np.empty(X1.shape + (2,)) Xpos[:,:,0] = X1 Xpos[:,:,1] = X2 F = stats.multivariate_normal.pdf(Xpos,[0,0],[[0.25,0.3],[0.3,1.0]]) plt.scatter(X[:,0],X[:,1]) plt.contour(x1,x2,F) 292
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS 3 2 1 0 -1 -2 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 x y Figure 5.16: 1000 random numbers drawn from a 2D Gaussian, overlaid with the contour plot. 5.7 Transformation of Multidimensional Gaussians As we have seen in Figure 5.15, the shape and orientation of a multidimensional Gaussian are determined by the mean vector µ and the covariance matrix Σ. This means that if we can somehow transform the mean vector and the covariance matrix, we will get another Gaussian. A few practical questions are: • How do we shift and rotate a Gaussian random variable? • If we have an arbitrary Gaussian, how do we go back to zero-mean unit-variance Gaussian? • How do we generate random vectors according to a predefined Gaussian? These questions come up frequently in data analysis. Answering the first two questions will help us transform Gaussians back and forth, while answering the last question will help us with generating random samples. 5.7.1 Linear transformation of mean and covariance Supposewehaveanarbitrary(notnecessarilyaGaussian)randomvectorX =[X ,...,X ]T 1 N with mean µ and covariance Σ . Entries of X are not necessarily independent. Let X X A∈RN×N be a transformation, and let Y =AX. That is,      Y a a ··· a X 1 11 12 1N 1 Y 2 a 21 a 22 ··· a 2NX 2 Y =   . . .   =   . . . . . . ... . . .      . . .   =AX. Y a a ··· a X N N1 N2 NN N Then we can show the following result. 293
CHAPTER 5. JOINT DISTRIBUTIONS Theorem 5.14. The mean vector and covariance matrix of Y =AX are µ =Aµ , Σ =AΣ AT. (5.36) Y X Y X Proof. We first show the mean. Consider the nth element of Y: (cid:34) N (cid:35) N (cid:88) (cid:88) E[Y ]=E a X = a E[X ]. n nk k nk k k=1 k=1 Therefore, E[Y ] (cid:80)N a E[X ] 1 k=1 1k k µ Y =  E[Y . .2]  =  (cid:80)N k=1a . .2kE[X k]    .   .  E[Y N] (cid:80)N k=1a NkE[X k]  a a ··· a E[X ] 11 12 1N 1 a 21 a 22 ··· a 2NE[X 2] =   . . . . . . ... . . .      . . .   =Aµ X. a a ··· a E[X ] N1 N2 NN N The covariance matrix follows from the fact that Σ =E[(Y −µ )(Y −µ )T] Y Y Y =E[(AX−Aµ )(AX−Aµ )T] X X =E[A(X−µ )(X−µ )TAT] X X =AE[(X−µ )(X−µ )T]AT X X =AΣ AT. X (cid:3) What if we shift the random vector by defining Y = X +b? We state the following result without proof (try proving it as an exercise). Theorem 5.15. The mean vector and covariance matrix of Y =X+b are µ =µ +b, Σ =Σ . (5.37) Y X Y X ForaGaussianrandomvector,thelineartransformationseithershiftstheGaussianor rotates the Gaussian, as shown in Figure 5.17: • If we add b to X, the resulting operation is a translation. • If we multiply A by X, then the resulting operation is a rotation and scaling. 294
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Figure 5.17: TransformingaGaussian.[Left]Translationbyavectorb.[Right]Rotationandscalingby a matrix X. How to rotate, scale, and translate a Gaussian random variable • We rotate and scale a Gaussian by Y =AX. • We translate a Gaussian by Y =X+b. 5.7.2 Eigenvalues and eigenvectors As our next step, we need to understand eigendecomposition. You can easily find relevant background in any undergraduate linear algebra textbook. Here we provide a summary for completeness. When applying a matrix A to a vector x, a typical engineering question is: what x would be invariant to A? Or in other words, for what x can we make sure that Ax = λx, for some scalar λ? If we can find such a vector x, we say that x is the eigenvector of A. Eigenvectorsareusefulforseekingprincipalcomponentsofdatasetsorfindingefficientsignal representations. They are defined as follows: Definition 5.23. Given a square matrix A∈RN×N, the vector u∈RN (with u(cid:54)=0) is called the eigenvector of A if Au=λu, (5.38) for some λ∈R. The scalar λ is called the eigenvalue associated with u. AnN×N matrixhasN eigenvectorsandN eigenvalues.Therefore,theaboveequationcan be generalized to Au =λ u , i i i for i = 1,...,N, or more compactly as AU = ΛU. The eigenvalues λ ,...,λ are not 1 N necessarilydistinct.Therearematriceswithidenticaleigenvalues,theidentitymatrixbeing atrivialexample.Ontheotherhand,notallsquarematriceshaveeigenvectors.Forexample, (cid:20) (cid:21) 0 1 the matrix does not have an eigenvalue. Matrices that have eigenvalues must be 0 0 diagonalizable. 295
CHAPTER 5. JOINT DISTRIBUTIONS There are a number of equivalent conditions for λ to be an eigenvalue: • There exists u(cid:54)=0 such that Au=λu; • There exists u(cid:54)=0 such that (A−λI)u=0; • (A−λI) is not invertible; • det(A−λI)=0. We are mostly interested in symmetric matrices. If A is symmetric, then all the eigen- values are real, and the following result holds. Theorem 5.16. If A is symmetric, all the eigenvalues are real, and there exists U such that UTU =I and A=UΛUT. Then   λ  — uT —   1 1 a| |1 a| |2 ··· a| |N=    u || 1 u || 2 ··· u || N        λ 2 ...       — u . . .T 2 —   . (cid:124) (cid:123)(cid:122) (cid:125) λ N — uT N — A (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) U Λ UT (5.39) We call such a decomposition the eigendecomposition. In MATLAB, we can compute the eigenvaluesofamatrixbyusingtheeigcommand.InPython,thecorrespondingcommand is np.linalg.eig. Note that in our demonstration below we symmetrize the matrix. This step is needed, for otherwise the eigenvalues will contain complex numbers. % MATLAB Code to perform eigendecomposition A = randn(100,100); A = (A + A’)/2; % symmetrize because A is not symmetric [U,S] = eig(A); % eigendecomposition s = diag(S); % extract eigenvalue # Python Code to perform eigendecomposition import numpy as np A = np.random.randn(100,100) A = (A + np.transpose(A))/2 S, U = np.linalg.eig(A) s = np.diag(S) The condition that UTU = I is the result of an orthonormal matrix. Equivalently, uTu = 1 if i = j and uTu = 0 if i (cid:54)= j. Since {u }N is orthonormal, it can serve as a i j i j i i=1 basis of any vector in Rn: N (cid:88) x= α u , j j j=1 where α = uTx is called the basis coefficient. Basis vectors are useful in that they can j j provide alternative representations of a vector. 296
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Figure 5.18: The center and the radius of the ellipse is determined by µ and Σ. The geometry of the joint Gaussian is determined by its eigenvalues and eigenvectors. Consider the eigendecomposition of Σ: Σ=UΛUT  λ 0 ··· 0 − uT −   1 1 | | | 0 λ 2 ··· 0− uT 2 − =u |1 u |2 ··· u |d   . . . . . . ... . . .      . . .   , 0 ··· ··· λ − uT − d d for some unitary matrix U and diagonal matrix Λ. The columns of U are called the eigen- vectors, and the entries of Λ are called the eigenvalues. Since Σ is symmetric, all λ ’s are i real.Inaddition,sinceΣispositivesemi-definite,allλ ’sarenon-negative.Accordingly,the i volume defined by the multidimensional Gaussian is always a convex object, e.g., an ellipse in 2D or an ellipsoid in 3D. The orientation of the axes is defined by the column vectors u . In the case of d = 2, i themajoraxisisdefinedbyu andtheminoraxisisdefinedbyu .Thecorrespondingradii 1 2 ofeachaxisarespecifiedbytheeigenvaluesλ andλ .Figure 5.18providesanillustration. 1 2 5.7.3 Covariance matrices are always positive semi-definite The following subsection about positive semi-definite matrices can be skipped if it is your first time reading the book. Now that we understand eigendecomposition, what can we do with it? Here is one practical problem. Given a matrix Σ, how do you know whether this Σ is valid? For example, if we giveyouasingularmatrix,thenΣ−1 maynotexist.CheckingthevalidityofΣrequiresthe concept of positive semi-definite. GivenasquarematrixA∈RN×N,itisimportanttocheckthepositivesemi-definiteness of A. There are two practical scenarios where we need positive semi-definiteness. (1) If we are estimating the covariance matrix Σ from a dataset, we need to ensure that Σ = E[(X −µ)(X −µ)T] is positive semi-definite because all covariance matrices are positive 297
CHAPTER 5. JOINT DISTRIBUTIONS semi-definite. Otherwise, the matrix we estimate is not a legitimate covariance matrix. (2) If we solve an optimization problem involving a function f(x) = xTAx, then having A beingpositivesemi-definite,wecanguaranteethattheproblemisconvex.Convexproblems ensure that a local minimum is also global, and convex problems can be solved efficiently using known algorithms. Definition 5.24 (Positive Semi-Definite). A matrix A ∈ RN×N is positive semi- definite if xTAx≥0 (5.40) for any x∈RN. A is positive definite if xTAx>0 for any x∈RN. Usingeigendecomposition,itisnotdifficulttoshowthatpositivesemi-definitenessisequiv- alent to having non-negative eigenvalues. Theorem 5.17. A matrix A∈RN×N is positive semi-definite if and only if λ (A)≥0 (5.41) i for all i=1,...,N, where λ (A) denotes the ith eigenvalue of A. i Proof. By the definitions of eigenvalue and eigenvector, we have that Au =λ u , i i i where λ is the eigenvalue and u is the corresponding eigenvector. If A is positive semi- i i definite, then uTAu ≥0 since u is a particular vector in Rn. So we have i i i 0≤uTAu =λ(cid:107)u (cid:107)2, i i i and hence λ ≥ 0. Conversely, if λ ≥ 0 for all i, then since A = (cid:80)N λ u uT we can i i i=1 i i i conclude that (cid:32) N (cid:33) N (cid:88) (cid:88) xTAx=xT λ u uT x= λ (uTx)2 ≥0. i i i i i i=1 i=1 (cid:3) The following corollary shows that if A∈Rn×n is positive definite, it must be invert- ible. Being invertible also means that the columns of A are linearly independent. Corollary 5.2. If a matrix A ∈ RN×N is positive definite (but not semi-definite), then A must be invertible, i.e., there exists A−1 ∈RN×N such that A−1A=AA−1 =I. (5.42) The next theorem tells us that the covariance matrix is always positive semi-definite. 298
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Theorem 5.18. The covariance matrix Cov(X) = Σ is symmetric positive semi- definite, i.e., ΣT =Σ, and vTΣv ≥0, ∀v ∈Rd. Proof.Symmetryfollowsimmediatelyfromthedefinition,becauseCov(X ,X )=Cov(X ,X ). i j j i The positive semi-definiteness comes from the fact that vTΣv =vTE[(X−µ)(X−µ)T]v =E[vT(X−µ)(X−µ)Tv] =E[bTb]=E[(cid:107)b(cid:107)2]≥0, where b=(X−µ)Tv. (cid:3) End of the discussion. 5.7.4 Gaussian whitening Besides checking positive semi-definiteness, another typical problem we encounter is how to generate random samples according to some Gaussian distributions. FromGaussian(0,I)toGaussian(µ,Σ).Ifwearegivenzero-meanunit-varianceGaus- sian X ∼Gaussian(0,I), how do we generate Y ∼Gaussian(µ,Σ) from X? The idea is to define a transformation 1 Y =Σ2X+µ, where Σ21 =UΛ1 2UT. Then the mean of Y is E[Y]=E[Σ1 2X+µ] =Σ21E[X]+µ=Σ21 0+µ=µ, and the covariance matrix is E[(Y −µ)(Y −µ)T]=E[(Σ1 2X+µ−µ)(Σ1 2X+µ−µ)T] =E[(Σ21 X)(Σ21 X)T]=Σ1 2E[XXT]Σ21 1 1 =Σ2IΣ2 =Σ. Theorem 5.19. Let X be X ∼ Gaussian(0,I). Consider a mean vector µ and a covariance matrix Σ with eigendecomposition Σ=UΛUT. If 1 Y =Σ2X+µ, (5.43) where Σ21 =UΛ1 2UT, then Y ∼Gaussian(µ,Σ). 299
CHAPTER 5. JOINT DISTRIBUTIONS Therefore, the two steps for doing this Gaussian whitening are: • Step1:Generatesamples{x ,...,x }thataredistributedaccordingtoGaussian(0,I). 1 N • Step 2: Define y where n 1 y n =Σ2x n+µ. These two steps are portrayed in Figure 5.19. Figure 5.19: Generating an arbitrary Gaussian from Gaussian(0,I). Example 5.26.ConsiderasetofN =1000i.i.d.Gaussian(0,I)datapointsasshown in Figure 5.20, for example, (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.5377 −2.2588 0.3188 x = , x = , ... , x = . 1 1.8399 2 0.8622 1000 −1.3077 5 5 4 4 3 3 2 2 1 1 0 0 -1 -1 -2 -2 -3 -3 -4 -4 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) Before (b) After Figure 5.20: Generating arbitrary Gaussian random variables from Gaussian(0,I). Transform these data points so that the new distribution is a Gaussian with (cid:20) (cid:21) (cid:20) (cid:21) 1 3 −0.5 µ= and Σ= . −2 −0.5 1 300
5.7. TRANSFORMATION OF MULTIDIMENSIONAL GAUSSIANS Solution.Toperformthetransformation,wefirstperformeigendecompositionofΣ= UΛUT. Then Σ1 2 =UΛ1 2UT. For our problem, we compute (cid:20) (cid:21) 1.722 −0.1848 1 Σ2 = . −0.1848 0.9828 1 Multiplying this matrix to yield y n =Σ2x n+µ, we obtain (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 1.5870 −3.0495 1.7907 y = , y = , ... , y = . 1 −0.2971 2 −0.7351 1000 −3.3441 In MATLAB, the above whitening procedure can be realized using the following com- mands. % MATLAB code to perform the whitening x = mvnrnd([0,0],[1 0; 0 1],1000); Sigma = [3 -0.5; -0.5 1]; mu = [1; -2]; y = Sigma^(0.5)*x’ + mu; The Python implementation is similar, although one needs to be careful with the more complicated syntax. For example, Sigma^(0.5) in MATLAB does the eigen-based matrix power automatically, whereas in Python we need to call a specific built-in command fractional_matrix_power. In MATLAB, broadcasting a vector to a matrix can be rec- ognized. In Python, we need to call repmat explicitly to control the shape of the mean vectors. # Python code to perform the whitening import numpy as np import scipy.stats as stats from scipy.linalg import fractional_matrix_power x = np.random.multivariate_normal([0,0],[[1,0],[0,1]],1000) mu = np.array([1,-2]) Sigma = np.array([[3, -0.5],[-0.5, 1]]) Sigma2 = fractional_matrix_power(Sigma,0.5) y = np.dot(Sigma2, x.T) + np.matlib.repmat(mu,1000,1).T From Gaussian(µ,Σ) to Gaussian(0,I). The reverse direction can be done as follows. Supposing that we have Y ∼Gaussian(µ,Σ), we define X =Σ−1 2(Y −µ). (5.44) Then E[X]=E[Σ−1 2(Y −µ)] =Σ− 21 (E[Y]−µ)=0. 301
CHAPTER 5. JOINT DISTRIBUTIONS The covariance is Cov(X)=E[(X−µ )(X−µ )T] X X =E[XXT] (cid:104) (cid:105) =E Σ− 21 (Y −µ)(Y −µ)TΣ−T 2 =Σ− 21E(cid:2) (Y −µ)(Y −µ)T(cid:3) Σ−T 2 =Σ−1 2ΣΣ−1 2 =I. The following theorem summarizes this result. Theorem 5.20. Let Y be a Gaussian Y ∼Gaussian(µ,Σ). If X =Σ−1 2(Y −µ), (5.45) then X ∼Gaussian(0,I). Thus the two steps of doing this reversed Gaussian whitening are: • Step 1: Assuming that y ,...,y are distributed as Gaussian(µ,Σ), estimate µ 1 N and Σ. • Step 2: Define x where n 1 x n =Σ2(y n−µ). (5.46) These two steps are shown pictorially in Figure 5.21. Figure 5.21: Converting an arbitrary Gaussian back to Gaussian(0,I). In practice, if we are given {y }N , we need to estimate µ and Σ. The estimations n n=1 are quite straightforward. N 1 (cid:88) µ= y , (cid:98) N n n=1 N 1 (cid:88) Σ(cid:98) = N (y n−µ (cid:98))(y n−µ (cid:98))T. n=1 302
5.8. PRINCIPAL-COMPONENT ANALYSIS On computers, these can be obtained using the command mean and cov. Once we have calculated µ (cid:98) and Σ(cid:98), we can define x n as −1 x n =Σ(cid:98) 2(y n−µ (cid:98)). On computers, the codes for the whitening procedure that uses the estimated mean and covariance are shown below. % MATLAB code to perform whitening y = mvnrnd([1; -2],[3 -0.5; -0.5 1],100); mY = mean(y); covY = cov(y); x = covY^(-0.5)*(y-mY)’; # Python code to perform whitening import numpy as np import scipy.stats as stats from scipy.linalg import fractional_matrix_power y = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],100) mY = np.mean(y,axis=0) covY = np.cov(y,rowvar=False) covY2 = fractional_matrix_power(covY,-0.5) x = np.dot(covY2, (y-np.matlib.repmat(mY,100,1)).T) 5.8 Principal-Component Analysis We have studied the covariance matrix Σ in some depth. It has many other uses besides transforming Gaussian random variables, and in this section we present one of them, called the principal-component analysis (PCA). PCA is a widely used tool for dimension reduc- tion.Insteadofusing N featurestodescribeadatapoint,PCAallowsustouse the leading p principal components to describe the same data point. In many problems in machine learning, this makes the learning task easier and the inference task more efficient. 5.8.1 The main idea: Eigendecomposition PCA can be summarized in one sentence: The key idea of PCA is the eigendecomposition of the covariance matrix Σ. This is a condensed summary of PCA: It is just the eigendecomposition of the co- variance. However, before we discuss the computational procedure, we will explain why we would want to perform the eigendecomposition of the covariance matrix. 303
CHAPTER 5. JOINT DISTRIBUTIONS Considerasetofdatapoints{x(1),...,x(N)},whereeachx(n) ∈Rd isad-dimensional vector.Thedimensiondisoftenhigh.Forexample,ifwehaveanimageofsize1024×1024×3, then d = 3,145,728 — not a huge number, but enough to make you feel dizzy. The goal of PCA is to find a low-dimensional representation in Rp where p (cid:28) d. If we can find this low-dimensional representation, we can represent the d-dimensional input using only p coefficients.Sincep(cid:28)d,wecan“compress”thedatabyusingacompactrepresentation.In modern data science, such a dimension reduction scheme is useful for handling large-scale datasets. Mathematically, we define a set of basis vector v ,...,v , where each v ∈ Rd. Our 1 p i goal is to approximate an input data point x(n) ∈Rd by these basis vectors: p (cid:88) x(n) ≈ α v , i i i=1 where {α }p are called the representation coefficients. The representation described by i i=1 thisequationisalinearrepresentation.Linearrepresentationisextremelycommoninprac- tice. For example, a data point x(n) =[7,1,4]T can be represented as       7 1 1 1= 3 −1+ 4 1. (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) 4 0 1 α1 α2 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124)(cid:123)(cid:122)(cid:125) x(n) v1 v2 Therefore, the 3-dimensional input x(n) can now be represented by two coefficients α =3 1 and α =4. This is called dimensionality reduction. 2 Pictorially, if we have already determined the basis vectors, we can compute the co- efficients for every data point in the dataset. However, not all basis vectors are good. As illustrated in Figure 5.22, an elongated dataset will be of the greatest benefit if the basis vectors are oriented according to the data geometry. If we can find such basis vectors, then the data points will have a large coefficient and a small coefficient, corresponding to the major and the minor axes. Dimensionality reduction can thus be achieved by, for example, only keeping the larger coefficients. Figure 5.22: PCA aims at finding a low-dimensional representation of a high-dimensional dataset. In this figure, the 2D data points can be well represented by the 1D space spanned by v . 1 The challenge here is that, given the dataset {x(1),...,x(N)}, we need to determine both the basis vectors {v }p and the coefficients {α }p . Fortunately, this can be formu- i i=1 i i=1 lated as an eigendecomposition problem. 304
5.8. PRINCIPAL-COMPONENT ANALYSIS To see how this problem can be thus formulated, we consider the simplest case as illustrated in Figure 5.22, where we want to find the leading principal component. That is, we find (α,v) such that x≈αv. This amounts to solving the optimization problem (cid:13)   (cid:13)2 (cid:13) | | (cid:13) (cid:13) (cid:13) (v (cid:98),α (cid:98))= argmin (cid:13)x−αv(cid:13) . (cid:13) (cid:13) (cid:107)v(cid:107)2=1,α(cid:13) | | (cid:13) The notation “argmin” means the argument that minimizes the function. The equation says that we find the (α,v) that minimizes the distance between x and αv. The constraint (cid:107)v(cid:107) =1 limits the search to within a unit circle; otherwise our solution will not be unique. 2 Solvingtheoptimizationproblemisnotdifficult.Ifwetakethederivativew.r.t.αand set it to zero, we have that 2vT(x−αv)=0 ⇒ α=vTx. Substituting α=xTv into the objective function again, we show that (cid:26) (cid:27) (cid:8)(cid:8) argmin (cid:107)x−αv(cid:107)2 =argmin xTx−2αxTv+α2(cid:8)vTv , (cid:107)v(cid:107) =1 2 (cid:107)v(cid:107)2=1 (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmin −2αxTv+α2 , drop xTx (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmin −2(xTv)xTv+(xTv)2 , substitute α=xTv (cid:107)v(cid:107)2=1 (cid:26) (cid:27) =argmax vTxxTv , change min to max. (cid:107)v(cid:107)2=1 Letuspauseforasecond.Wehaveshownthatifwehaveone datapointx,theleading principal component v can be determined by maximizing vTxxTv. What have we gained? Wehavetransformedtheoriginaloptimization,whichcontainstwovariables(v,α),toanew optimization that contains one variable v. Thus if we know how to solve the one-variable problem we are done. However, there is one more issue we need to address before we discuss how to solve for the problem. The issue is that the formulation is about one data sample, not the entire dataset. To include all the samples, we need to assume that x is a realization of a random vector X. Then the above optimization can be formulated in the expectation sense as (cid:26) (cid:27) argmin E(cid:107)X−αv(cid:107)2 =argmaxvTE XXT v (cid:107)v(cid:107)2=1 (cid:107)v(cid:107)2=1 =argmaxvTΣv, (cid:107)v(cid:107)2=1 where Σ d =ef E[XTX].1 Therefore, if we can maximize vTΣv we will be able to determine the principal component. Now comes the main result. The following theorem shows that the maximization is equivalenttoeigendecomposition.TheproofrequiresLagrangemultipliers,whicharebeyond the scope of this book. 1Here we assume that X is zero-mean, i.e., E[X] = 0. If it is not, then we can subtract the mean by (cid:26) (cid:27) consideringargmax vTE (X−µ)(X−µ)T v. (cid:107)v(cid:107)2=1 305
CHAPTER 5. JOINT DISTRIBUTIONS Theorem 5.21. Let Σ be a d×d matrix with eigendecomposition Σ=USUT. Then the optimization v =argmax vTΣv (5.47) (cid:98) (cid:107)v(cid:107)2=1 has a solution v =u , i.e., the first column of the eigenvector matrix U. (cid:98) 1 The following proof requires an understanding of Lagrange multipliers and constrained optimizations. It is not essential for understanding this chapter. We want to prove that the solution to the problem v =argmaxvTΣv (cid:98) (cid:107)v(cid:107)2=1 is the eigenvector of the matrix Σ. To show that, we first write down the Lagrangian: L(v,λ)=vTΣv−λ((cid:107)v(cid:107)2−1) Taking the derivative w.r.t. v and setting to zero yields ∇ L(v,λ)=2Σv−2λv =0. v This is equivalent to Σv =λv. So if Σ=USUT, then by letting v =u and λ=s we can i i satisfy the condition since Σu =USUTu =USe =s u . i i i i i End of the proof. This theorem can be extended to the second (and other) principal components of the covariance matrix. In fact, given the covariance matrix Σ we can follow the procedure outlinedinFigure5.23todeterminetheprincipalcomponents.Theeigendecompositionofa d×dmatrixΣwillgiveusad×deigenvectormatrixU andaneigenvaluematrixS.Tokeep thepleadingeigenvectors,wetruncatetheU matrixtoonlyusethefirstpeigenvectors.Here, we assume that the eigenvectors are ordered according to the magnitude of the eigenvalues, from large to small. In practice, ifweare given a dataset {x(1),...,x(N)}, we can firstestimate the covari- ance matrix Σ by N 1 (cid:88) Σ(cid:98) = N (x(n)−µ (cid:98))(x(n)−µ (cid:98))T, n=1 where µ = 1 (cid:80)N x(n) is the mean vector. Afterwards, we can compute the eigendecom- (cid:98) N n=1 position of Σ(cid:98) by [U,S]=eig(Σ(cid:98)). On a computer, the principal components are obtained through eigendecomposition. A MATLAB example and a Python example are shown below. We explicitly show the two principal components in this example. The magnitudes of these two vectors are determined by the eigenvalues diag(s). 306
5.8. PRINCIPAL-COMPONENT ANALYSIS Figure 5.23: The principal components are the eigenvectors of the covariance matrix. In this figure Σ denotesthecovariancematrix,u ,...,u denotethepleadingeigenvectors,andsdenotesthediagonal 1 p of the eigenvalue matrix. % MATLAB code to perform the principal-component analysis x = mvnrnd([0,0],[2 -1.9; -1.9 2],1000); covX = cov(x); [U,S] = eig(covX); u(:,1) % Principle components u(:,2) % Principle components # Python code to perform the principal-component analysis import numpy as np x = np.random.multivariate_normal([1,-2],[[3,-0.5],[-0.5,1]],1000) covX = np.cov(x,rowvar=False) S, U = np.linalg.eig(covX) print(U) Example 5.27.SupposewehaveadatasetcontainingN =1000samples,drawnfrom an unknown distribution. The first few samples are (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.5254 −0.4040 1.4165 x = , x = , ... , x = . 1 −0.6930 2 0.3724 1000 −1.5463 We can compute the mean and covariance using MATLAB commands mean and cov. This will return us (cid:20) (cid:21) (cid:20) (cid:21) 0.0561 2.0460 −1.9394 µ (cid:98) = −0.0303 and Σ(cid:98) = −1.9394 2.0426 . Applying eigendecomposition on Σ(cid:98), we show that [U,S]=eig(Σ(cid:98)), (cid:20) (cid:21) (cid:20) (cid:21) −0.7068 −0.7074 0.1049 0 =⇒U = and S = . −0.7074 0.7068 0 3.9837 307
CHAPTER 5. JOINT DISTRIBUTIONS Therefore, we have obtained two principal components (cid:20) (cid:21) (cid:20) (cid:21) −0.7068 −0.7074 u = and u = . 1 −0.7074 2 0.7068 As seen in the figure below, these two principal components make sense. The vector u is the orange line and is the minor axis. The vector u is the blue line and is the 1 2 major axis. Again, the ordering of the vectors is determined by the eigenvalues. Since u has a larger eigenvalue (=3.9837), it is the leading principal component. 2 5 4 3 2 1 0 -1 -2 -3 -4 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 5.24: To determine the representation coefficients, we solve an inverse problem by finding the vector α in the equation x(n) =U α(n). p Why do we call our method principal component analysis? The analysis part comes from the fact that we can compress a data vector x(n) from a high dimension d to a low dimension p. Defining U =[u ,...,u ], a matrix containing the p leading eigenvectors of p 1 p the matrix U, we solve the inverse problem: x(n) =U α(n), p where the goal is to determine the coefficient vector α(n) ∈Rp. Since U is an orthonormal p matrix (i.e., UTU =I), it follows that p p UTx(n) =UTU α(n), p p p (cid:124) (cid:123)(cid:122) (cid:125) =I 308
5.8. PRINCIPAL-COMPONENT ANALYSIS as illustrated in Figure 5.24. Hence, α(n) =UTx(n). p This equation is a projection operation that projects a data point x(n) onto the space spanned by the p leading principal components. Repeating the procedure for all the data points x(1),...,x(N) in the dataset, we have compressed the dataset. Example 5.28. Using the example above, we can show that (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.1189 0.0221 0.0927 α(1) =UTx(1) = , α(2) = , ... , α(1000) = . −0.8615 0.5491 −2.0950 Theprincipal-componentanalysissaysthatsincetheleadingcomponentsrepresentthe data, we only need to keep the blue-colored values because they are the coefficients associated with the leading principal component. 5.8.2 The eigenface problem As a concrete example of PCA, we consider a computer vision problem called the eigen- face problem. In 2001, researchers at Yale University published the Yale Database, and a few years later they extended it to a larger one (http://vision.ucsd.edu/~leekc/ ExtYaleDatabase/ExtYaleB.html).Thedataset,nowknownastheYaleFaceDataset,con- tains 16,128 images of 28 human subjects under nine poses and 64 illumination conditions. Thesizesoftheimagesared=168×192=32,256pixels.TreatingtheseN =16,128images as vectors in R32,256×1, we have 16,128 of these vectors. Let us call them {x(1),...,x(N)}. Following the procedure we described above, we estimate the covariance matrix by computing N 1 (cid:88) Σ(cid:98) =E[(X−µ (cid:98))(X−µ (cid:98))T]≈ N (x(n)−µ (cid:98))(x(n)−µ (cid:98))T, (5.48) n=1 where µ = E[X] ≈ 1 (cid:80)N x(n) is the mean vector. Note that the size of µ is 32,256×1 (cid:98) N n=1 (cid:98) and the size of Σ(cid:98) is 32,256 × 32,256. Figure 5.25: The extended Yale Face Database B. Once we obtain an estimate of the covariance matrix, we can perform an eigendecom- position to get [U,S]=eig(Σ(cid:98)). The columns of U, i.e., {u i}d i=1, are the eigenvectors of Σ(cid:98). These eigenvectors are the basis of a testing face image. 309
CHAPTER 5. JOINT DISTRIBUTIONS Figure 5.26: Givenafaceimage,thelearnedbasisvectors(fromtheeigendecompositionofthecovari- ancematrix)canbeusedtocompresstheimagexintoafeaturevectorαwherethedimensionofαis significantly lower than that of x. With the basis vectors u ,...,u we can project every image in the dataset using a 1 p low-dimensional representation. Specifically, for an image x we compute the coefficients α =uTx, i=1,...,p i i or more compactly α = UTx. Note that the dimension of x is d×1 (which in our case is d = 32,526), and the dimensions of α can be as few as p = 100. Therefore, we are using a 100-dimensionalvectortorepresenta32,526-dimensionaldata.Thisisahugedimensionality reduction. Theprocessrepeatsforallthesamplesx(1),...,x(N).Thisgivesusacollectionofrep- resentationcoefficientsα(1),...,α(N),whereeachα(n)is100-dimensional(seeFigure5.26). Noticethatthebasisvectorsu appearmoreorless“faceimages,”buttheyarethefeatures i of the faces. PCA says that a real face can be written as a linear combination of these basis vectors. How to solve the eigenface problem • Compute the covariance matrix of all the images. • Apply eigendecomposition to the covariance matrix. • Project onto the basis vectors and find the coefficients. • The coefficients are the low-dimensional representation of the images. • We use the coefficients to perform downstream tasks, such as classification. 310
5.8. PRINCIPAL-COMPONENT ANALYSIS 5.8.3 What cannot be analyzed by PCA? PCA is a dimension reduction tool. It compresses a raw data vector x ∈ Rd into a smaller feature vector α∈Rp. The advantage is that the downstream learning problems are much easierbecausep(cid:28)d.Forexample,classificationusingαismoreefficientthanclassification using x since there is very little information loss from x to α. There are three limitations of PCA: • PCS fails when the raw data are not orthogonal. The basis vectors u returned i by PCA are orthogonal, meaning that uTu = 0 as long as i (cid:54)= j. As a result, if i j the data intrinsically have this orthogonality property, then PCA will work very well. However,ifthedataliveinaspacesuchasadonutshapeasillustratedinFigure5.27, then PCA will fail. Here, by failure, we mean that p is not much smaller than d. To handledatasetsbehavinglikeFigure 5.27weneedadvancedtools.Oneoftheseisthe kernel-PCA. The idea is to apply a nonlinear transformation to the data before you run PCA. Figure 5.27: [Left] PCA works when the data has redundant dimensions or is living on orthogonal spaces. [Right] PCA fails when the data does not have easily decomposable spaces. • Basis vectors returned by PCA are not interpretable. A temptation with PCA is to thinkthatthebasisvectorsu offermeaningfulinformationbecausetheyarethe“prin- i cipal components”. However, since PCA is the eigendecomposition of the covariance matrix,whichispurelyamathematicaloperation,thereisnoguaranteethatthebasis vectors contain any semantic meaning. If we look at the basis vectors shown in Fig- ure 5.26, there is almost no information one can draw. Therefore, in the data-science literaturealternativemethodssuchasnon-negativematrixfactorizationandthemore recentdeepneuralnetworkembeddingaremoreattractivebecausethefeaturevectors sometimes (not always) have meanings. • PCA does not return you the most influential “component”. Imagine that you are analyzing medical data for research on a disease, in which each data vector x(n) containsheight,weight,BMI,bloodpressure,etc.WhenyourunPCAonthedataset, you will obtain some “principal components”. However, these principal components will likely have everything, e.g., the height entry of the principal component will have some values, the weight will have some values, etc. If you have found a principal component, it does not mean that you have identified the leading risk factor of the disease. If you want to identify the leading risk factor of the disease, e.g., whether the height or weight is more important, you need to resort to advanced tools such as variable selection or the LASSO type of regression analysis (see Chapter 7). 311
CHAPTER 5. JOINT DISTRIBUTIONS Closing remark.PCAsarepowerfulcomputationaltoolsbasedonthesimplestconceptof covariancematricesbecause,asourderivationshowed,covariancematricesencodethe“vari- ation” of the data. Therefore, by finding a vector that aligns with the maximum variation of the data, we can find the principal component. 5.9 Summary As you were reading this chapter, you may have felt that the first and second parts discuss distinctly different subjects, and in fact many books treat them as separate topics. We take a different approach. We think that they are essentially the same thing if you understand the following chain of distributions: f (x) =⇒f (x ,x )=⇒···=⇒f (x ,...,x ). X X1,X2 1 2 X1,...,XN 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) onevariable twovariables N variables The first part exclusively deals with two variables. The generalization from two variables to N variables is straightforward for PDFs and CDFs: • PDF: f (x ,x )=⇒f (x ,...,x ). X1,X2 1 2 X1,...,XN 1 N • CDF: F (x ,x )=⇒F (x ,...,x ). X1,X2 1 2 X1,...,XN 1 N The joint expectation can also be generalized from two variables to N variables:  Var[X2] ··· Cov(X ,X ) (cid:20) CoV va (Xr[X ,12 X] ) Co Vv a(X r[X1, 2X ]2)(cid:21) =⇒  . . . 1 ... . . .1 N  . 2 1 2 Cov(X ,X ) ··· Var[X2] N 1 N Conditional PDFs and conditional expectations are powerful tools for decomposing complex events into simpler events. Specifically, the law of total expectation, (cid:90) E[X]= E[X|Y =y]f (y)dy =E [E [X|Y]], Y Y X|Y is instrumental for evaluating variables defined through conditional relationships. The idea is also extendable to more random variables, such as (cid:90) (cid:90) E[X ]= E[X |X =x ,X =x ]f (x ,x )dx dx , 1 1 2 2 3 3 X2,X3 2 3 2 3 where E[X |X =x ,X =x ] can be evaluated through 1 2 2 3 3 (cid:90) E[X |X =x ,X =x ]= x f (x |x ,x )dx . 1 2 2 3 3 1 X1|X2,X3 1 2 3 1 This type of chain relationship can generalize to other high-order cases. Itisimportanttorememberthatforanyhigh-dimensionalrandomvariables,thechar- acterizationisalwaysmadebythePDFf (x)(ortheCDF).Wedidnotgointothedetails X 312
5.10. REFERENCES of analyzing f (x) but have only discussed the mean vector E[X]=µ and the covariance X matrix Cov(X)=Σ. We have been focusing exclusively on the high-dimensional Gaussian random variables (cid:26) (cid:27) 1 1 f (x)= exp − (x−µ)TΣ(x−µ) , X (cid:112) (2π)d|Σ| 2 because they are ubiquitous in data science today. We discussed the linear transformations from a zero-mean unit-variance Gaussian to another Gaussian, and vice versa. 5.10 References Joint Distributions and Correlation 5-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 2.5, 3.4, 4.2. 5-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 5.1 – 5.6. 5-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 6.1 – 6.4. 5-4 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 7.1 – 7.2. 5-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapters 6.1 – 6.3. 5-6 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 2.6. Conditional Distributions and Expectations 5-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 2.6, 3.5, 3.6, 4.3. 5-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 5.7. 5-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 6.6 – 6.7. 5-10 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 7.3 – 7.5. 5-11 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapters 7.5 – 7.6. 5-12 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapter 4.2. 313
CHAPTER 5. JOINT DISTRIBUTIONS Sum of Random Variables 5-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.5. 5-14 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 7.1. 5-15 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 3.3 and 3.4. Vector Random Variables 5-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 6.1 – 6.6. 5-17 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 8.1 – 8.3, 9. 5-18 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 5.1 – 5.6. Principal-Component Analysis PCAisoftentaughtinmachinelearningcourses.Forfirst-timereaders,wesuggestreviewing thelinearalgebraictoolsinMoonandStirling.Then,thetutorialbyShlensandthechapter in Bishop would be sufficient to cover most of the materials. More advanced topics, such as kernel PCA, can be found in the following references. 5-19 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Chapter 7. 5-20 ChristopherBishop,PatternRecognitionandMachineLeanring,Springer,2006.Chap- ter 12. 5-21 Jonathon Shlens (2014) “A Tutorial on Principal Component Analysis”, https:// arxiv.org/pdf/1404.1100.pdf 5-22 PaulHoneine(2014),“Aneigenanalysisofdatacenteringinmachinelearning”,https: //arxiv.org/pdf/1407.2904.pdf 5-23 Quan Wang (2012), “Kernel Principal Component Analysis and its Applications”, https://arxiv.org/abs/1207.3538 5-24 Schlkopfetal.(2005),“KernelPrincipalComponentAnalysis”,https://link.springer. com/chapter/10.1007/BFb0020217 314
5.11. PROBLEMS 5.11 Problems Exercise 1. (Video Solution) Alex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets. (a) Find and sketch the joint PMF p (x,y). X,Y (b) Find the marginal PMF p (x) and p (y). X Y (c) Find the conditional PMF P (x|y). Does P (x|y)=P (x)? Why or why not? X|Y X|Y X Exercise 2. Two fair dice are rolled. Find the joint PMF of X and Y when (a) X is the larger value rolled, and Y is the sum of the two values. (b) X is the smaller, and Y is the larger value rolled. Exercise 3. The amplitudes of two signals X and Y have joint PDF f (x,y)=e−x/2ye−y2 XY for x>0,y >0. (a) Find the joint CDF. (b) Find P(X1/2 >Y). (c) Find the marginal PDFs. Exercise 4. (Video Solution) Find the marginal CDFs F (x) and F (y) and determine whether or not X and Y are X Y independent, if  x−1− e−y−e−xy, if 1≤x≤2,y ≥0  y F (x,y)= 1− e−y−e−2y, if x>2,y ≥0, XY y 0, otherwise. Exercise 5. (Video Solution) (a) Find the marginal PDF f (x) if X exp{−|y−x|−x2/2} f (x,y)= √ . XY 2 2π 315
CHAPTER 5. JOINT DISTRIBUTIONS (b) Find the marginal PDF f (y) if Y 4e−(x−y)2/2 f (x,y)= √ . XY y2 2π Exercise 6. (Video Solution) Let X,Y be two random variables with joint CDF y+e−x(y+1) F (x,y)= . X,Y y+1 Show that ∂2 ∂2 F (x,y)= F (x,y). ∂x∂y X,Y ∂y∂x X,Y What is the implication of this result? Exercise 7. (Video Solution) Let X and Y be two random variables with joint PDF 1 f X,Y(x,y)= 2πe−1 2(x2+y2). (a) Find the PDF of Z =max(X,Y). (b) Find the PDF of Z =min(X,Y). You may leave your answers in terms of the Φ(·) function. Exercise 8. The random vector (X,Y) has a joint PDF f (x,y)=2e−xe−2y XY for x>0, y >0. Find the probability of the following events: (a) {X+Y ≤8}. (b) {X−Y ≤10}. (c) {X2 <Y}. Exercise 9. LetX andY bezero-mean,unit-varianceindependentGaussianrandomvariables.Findthe value of r for which the probability that (X,Y) falls inside a circle of radius r is 1/2. Exercise 10. The input X to a communication channel is +1 or −1 with probabilities p and 1 − p, respectively. The received signal Y is the sum of X and noise N, which has a Gaussian distribution with zero mean and variance σ2 =0.25. 316
5.11. PROBLEMS (a) Find the joint probability P(X =j, Y ≤y). (b) Find the marginal PMF of X and the marginal PDF of Y. (c) Suppose we are given that Y >0. Which is more likely, X =1 or X =−1? Exercise 11. (Video Solution) Let (cid:40) ce−xe−y, if 0≤y ≤x<∞, f (x,y)= X,Y 0, otherwise. (a) Find c. (b) Find f (x) and f (y). X Y (c) Find E[X] and E[Y], Var[X] and Var[Y]. (d) Find E[XY], Cov(X,Y) and ρ. Exercise 12. (Video Solution) Inclass,wehaveusedtheCauchy-Schwarzinequalitytoshowthat−1≤ρ≤1.Thisexercise asks you to prove the Cauchy-Schwarz inequality: (E[XY])2 ≤E[X2]E[Y2]. Hint:ConsidertheexpectationE[(tX+Y)2].Notethatthisisaquadraticequationintand E[(tX+Y)2]≥0 for all t. Consider the discriminant of this quadratic equation. Exercise 13. (Video Solution) Let Θ∼Uniform[0,2π]. (a) If X =cosΘ, Y =sinΘ. Are X and Y uncorrelated? (b) If X =cos(Θ/4), Y =sin(Θ/4). Are X and Y uncorrelated? Exercise 14. (Video Solution) Let X and Y have a joint PDF f (x,y)=c(x+y), X,Y for 0≤x≤1 and 0≤y ≤1. (a) Find c, f (x), f (y), and E[Y]. X Y (b) Find f (y|x). Y|X (c) Find P[Y >X|X >1/2]. (d) Find E[Y|X =x]. 317
CHAPTER 5. JOINT DISTRIBUTIONS (e) Find E[E[Y|X]], and compare with the E[Y] computed in (a). Exercise 15. (Video Solution) Use the law of total expectation to compute the following: 1. E[sin(X+Y)], where X ∼N(0,1), and Y |X ∼Uniform[x−π,x+π] 2. P[Y <y], where X ∼Uniform[0,1], and Y |X ∼Exponential(x) 3. E[XeY], where X ∼Uniform[−1,1], and Y |X ∼N(0,x2) Exercise 16. LetY =X+N,whereX istheinput,N isthenoise,andY istheoutputofasystem.Assume that X and N are independent random variables. It is given that E[X] = 0, Var[X] = σ2 , X E[N]=0, and Var[N]=σ2 . N (a) Find the correlation coefficient ρ between the input X and the output Y. (b) Suppose we estimate the input X by a linear function g(Y) = aY. Find the value of a that minimizes the mean squared error E[(X−aY)2]. (c) Express the resulting mean squared error in terms of η =σ2 /σ2 . X N Exercise 17. (Video Solution) Two independent random variables X and Y have PDFs (cid:40) (cid:40) e−x, x≥0, 0, y >0, f (x)= f (y)= X 0, x<0, Y ey, y ≤0. Find the PDF of Z =X−Y. Exercise 18. Let X and Y be two independent random variables with densities (cid:40) (cid:40) xe−x, x≥0, ye−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Exercise 19. The random variables X and Y have the joint PDF f (x,y)=e−(x+y) XY for 0<y <x<1. Find the PDF of Z =X+Y. Exercise 20. The joint density function of X and Y is given by f (x,y)=e−(x+y) XY for x>0,y >0. Find the PDF of the random variable Z =X/Y. 318
Chapter 6 Sample Statistics Whenwethinkaboutprobability,thefirstthingthatlikelycomestomindisflippingacoin, throwingadie,orplayingacardgame.Theseareexcellentexamplesofthesubject.However, they seldom fit in the context of modern data science, which is concerned with drawing conclusions from data. In our opinion, the power of probability is its ability to summarize microstates using macro descriptions.Thisstatementwilltakeussomeefforttoelaborate. We study probability because we want to analyze the uncertainties. However, when we have many data points, analyzing the uncertainties of each data point (the microstates) is computationally very difficult. Probability is useful here because it allows us to bypass the microstates and summarize the macro behavior. Instead of reporting the states of each individual, we report their sample average. Instead of offering the worst-case guarantee, we offer a probabilistic guarantee. You ask: so what? If we can offer you a performance guaranteeat99.99%confidencebutone-tenthofthecostofa100%performanceguarantee, would you consider our offer? The goal of this chapter is to outline the concepts of these probabilistic arguments. The significance of sample average Imaginethatyouhaveaboxcontainingmanytinymagnets.(Youcanalsothinkofadataset containing two classes of labels.) In condensed matter physics, these are known as the spin glasses. The orientations of the magnets depend on the magnetic field. Under an extreme condition where the magnetic field is strong, all magnets will point in the same direction. When the magnetic field is not as strong, some will align with the field but some will not, as we show in Figure 6.1. If we try to study every single magnet in this box, the correlation of the magnets will force us to consider a joint distribution, since if one magnet points to the right it is likely thatanothermagnetwillalsopointtotheright.Thesimultaneousdescriptionofallmagnets is modeled through a joint probability distribution f (x ,x ,...,x ). X1,X2,...,XN 1 2 N Like any joint PDF, this PDF tells us the probability density that the magnets will take a collection of states simultaneously. If N is large (say, on the order of millions), this joint distribution will be very complicated. 319
CHAPTER 6. SAMPLE STATISTICS Figure 6.1: Imagine that we have a box of magnets and we want to measure their orientation angles. The data points have individual randomness and correlations. Studying each one individually could be computationally infeasible, as we need to estimate the joint PDF f (x ,...,x ) across all the X1,...,XN 1 N data points. Probability offers a tool to summarize these individual states using a macro description. For example, we can analyze the sample average X of the data points and derive conclusions from N the PDF of X , i.e., f (x). The objective of this chapter is to present a few probabilistic tools to N XN analyze macro descriptions, such as the sample average. Since the joint PDF is very difficult to obtain computationally, physicists proposed to study the sample statistics. Instead of looking at the individual states, they look at the sample average of the states. If we define X ,...,X as the states of the magnets, then 1 N the sample average is N 1 (cid:88) X = X . N N n n=1 Sinceeachmagnetisrandom,thesampleaverageisalsorandom,andthereforeitisgranted a PDF: f (x). XN Thus, X has a PDF, a mean, a variance, and so on. N We call X a sample statistic. It is called a statistic because it is a summary of the N microstates,andasamplestatisticbecausethestatisticisbasedonrandomsamples,noton the underlying theoretical distributions. We are interested in knowing the behavior of X N because it is the summary of the observations. If we know the PDF of X , we will know N themean,thevariance,andthevalueofX whenthemagneticfieldincreasesordecreases. N Why study the sample average X ? N • Analyzing individual variables is not feasible because the joint PDF can be ex- tremely high-dimensional. • Sample average is a macro description of the data. • If you know the behavior of the sample average, you know most of the data. Probabilistic guarantee versus worst-case guarantee Besides the sample average, we are also interested in the difference between a probabilistic guarantee and a deterministic guarantee. 320
Consider the birthday paradox (see Chapter 1 for details). Suppose there are 50 stu- dentsinaroom.Whatistheprobabilitythatatleasttwostudentshavethesamebirthday? A naive thought would suggest that we need 366 students to guarantee a pair of the same birthday because there are 365 days. So, with only 50 students, it would seem unlikely to have a pair with the same birthday. However, it turns out that with just 50 students, the probabilityofhavingatleastonepairwiththesamebirthdayismorethan97%.Figure 6.2 belowshowsacalculationbyacomputer,whereweplottheestimatedprobabilityasafunc- tion of the number of students. What is more surprising is that with as few as 23 students, the probability is greater than 50%. There is no need for there to be 365 students in order to offer a guarantee. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 10 20 30 40 50 60 70 80 90 100 Number of people ytilibaborP Figure6.2:Thebirthdayparadoxasksthequestionofhowmanypeopleweneedtoaskinordertohave atleasttwoofthemhavingthesamebirthday.Whilewetendtothinkthattheansweris366(because there are 365 days), the actual probability, as we have calculated (see Chapter 1), is more than 97%, evenifwehaveonlyasked50people.Thecurveaboveshowstheprobabilityofhavingatleastonepair of people having the same birthday as a function of the number of people. The plot highlights the gap between the worst-case performance and an average-case performance. Why does this happen? Certainly, we can trace back to the formulae in Chapter 1 and argue through the lens of combinations and permutations. However, the more important message is about the difference between the worst-case guarantee and the average-case guarantee. Worst case versus average case • Worst-case guarantee: You need to ensure that the worst one is protected. This requires an exhaustive search until hitting 100%. It is a deterministic guarantee. • Average-caseguarantee:Youguaranteethatwithahighprobability(e.g.,99.99%), the undesirable event does not happen. This is a probabilistic guarantee. Is there a difference between 99.99% and 100%? If the probability is 99.99%, there is one failure every 10,000 trials on average. You are unlikely to fail, but it is still possible. A 100% guarantee says that no matter how many trials you make you will not fail. The 99.99% guarantee is much weaker (yes, much weaker, not just a little bit weaker) than the deterministic guarantee. However, in practice, people might be willing to pay for the risk in exchange for efficiency. This is the principle behind insurance. Automobile manufacturing 321
CHAPTER 6. SAMPLE STATISTICS also uses this principle — your chance of purchasing a defective car is non-zero, but if the manufacturer can sell enough cars to compensate for the maintenance cost of fixing your car, they might be willing to offer a limited warranty in exchange for a lower selling price. Howdoweanalyzetheprobabilisticguarantee,e.g.,forthesampleaverage?Remember that the sample average X is a random variable. Since it is a random variable, it has a N mean, variance, and PDF.1 To measure the probabilistic guarantee, we consider the event def B = {|X −µ|≥(cid:15)}, N where µ = E[X ] is the true population mean, and (cid:15) > 0 is a very small number. This N probability is illustrated in Figure 6.3, assuming that X has the PDF of a Gaussian. The N probability of B is the two tails under the PDF. Therefore, B is a bad event because in principle X should be close to µ. The probability P[B] measures situations where X N N stays very far from µ. If we can show that P[B] is small (e.g., < 0.01%), then we can say that we have obtained a probabilistic guarantee at 99.99%. Figure 6.3: The probabilistic guarantee of a sample average X is established by computing the N probability of the tails. In this example, we assume that f (x) take a Gaussian shape, and we define XN (cid:15) = 1. Anything belonging to |X −µ| ≥ (cid:15) is called a undesired event B. If the probability of a N undesired event is small, we say that we can offer a probabilistic guarantee. ThemomentwecomputeP[|X −µ|≥(cid:15)],weentertheraceofprobabilisticguarantee N (e.g., 99.99%). Why? If the probability P[|X −µ|≥(cid:15)] is less than 0.01%, it still does not N exclude the possibility that something bad will happen once every 10,000 trials on average. Thechanceislow,butitisstillpossible.Wewilllearnsomemathematicaltoolsforanalyzing this type of probabilistic guarantee. Plan for this chapter Withthesetwomainthemesinmind,wenowdiscusstheorganizationofthischapter.There are four sections: two for mathematical tools and two for main results. • Moment-generating functions: We have seen in Chapter 5 that the PDF of a sum of tworandomvariablesX+Y istheconvolutionofthetwoPDFsf ∗f .Convolutions X Y are non-trivial, especially when we have more random variables to sum. The moment- generating functions provide a convenient way of summing N random variables. They are the transform domain techniques (e.g., Fourier transforms). Since convolutions in 1Not all random variables have mean and variance, e.g., a Cauchy random variable, but most of them do. 322
time are multiplications in frequency, the moment-generating functions allow us to multiply PDFs in the transformed space. In this way, we can sum as many random variables as we want. We will discuss this idea in Section 6.1. Key Concept 1: Why study moment-generating functions? Moment-generatingfunctionshelpusdeterminethePDFofX +X +···+X . 1 2 N • Probability inequalities:WhenanalyzingsamplestatisticssuchasX ,evaluatingthe N exactprobabilitycouldbedifficultbecauseitrequiresintegratingthePDFs.However, if our ultimate goal is to estimate the probability, deriving an upper bound might be sufficienttoachievethegoal.Theprobabilityinequalitiesaredesignedforthispurpose. In Section 6.2, we discuss several of the most basic probability inequalities. We will use some of them to prove the law of large numbers. Key Concept 2: How can probability inequalities be useful? Probability inequalities help us upper-bound the bad event P[|X −µ|≥(cid:15)]. N • Law of large numbers: This is the first main result of the chapter. The law of large numbers says that the sample average X converges to the population mean µ when N the number of samples grows to infinity. The law of large numbers comes in two versions: the weak law of large numbers and the strong law of large numbers. The difference is the type of convergence they guarantee. The weak law is based on con- vergence in probability,whereasthestronglawisbasedonalmost sure convergence. We will discuss these types of convergence in Section 6.3. Key Concept 3: What is the law of large numbers? There is a weak law and a strong law of large numbers. The weak law of large numbers says that X converges to the true mean µ, as N grows: N lim P[|X −µ|>(cid:15)]=0. N N→∞ • Central Limit Theorem: The Central Limit Theorem says that the probability of X can be approximated by the probability of a Gaussian. You can also think of N this as saying that the PDF of X is converging to a distribution that can be well N approximatedbyabell-shapedGaussian.Ifwehavemanyrandomvariablesandtheir sum is becoming a Gaussian, we can ignore the individual PDFs and focus on the Gaussian. Thus it explains why Gaussian is so popular. We will discuss this theorem in detail in Section 6.4. Key Concept 4: What is the Central Limit Theorem? The CDF of X can be approximated by the CDF of a Gaussian, as N grows. N 323
CHAPTER 6. SAMPLE STATISTICS 6.1 Moment-Generating and Characteristic Functions Consider two independent random variables X and Y with PDFs f (x) and f (y), respec- X Y tively. Let Z = X +Y be the sum of the two random variables. We know from Chapter 5 that the PDF of Z, f , is the convolution of f and f . However, we think you will agree Z X Y that convolutions are not easy to compute. Especially when the sum involves more random variables, computing the convolution would be tedious. So how should we proceed in this case? One approach is to use some kind of “frequency domain” method that transforms the PDFs to another domain and then perform multiplication instead of the convolution to make the calculations easy or at least easier. The moment-generating functions and the characteristic functions are designed for this purpose. 6.1.1 Moment-generating function Definition6.1. ForanyrandomvariableX,the moment-generatingfunction(MGF) M (s) is X M (s)=E(cid:2) esX(cid:3) . (6.1) X The definition says that the moment-generating function (MGF) is the expectation of the random variable taken to the power esX for some s. Effectively, it is the expectation of a function of random variables. The meaning of the expectation can be seen by writing out the definition. For the discrete case, the MGF is (cid:88) M (s)= esxp (x), (6.2) X X x∈Ω whereas in the continuous case, the MGF is (cid:90) ∞ M (s)= esxf (x)dx. (6.3) X X −∞ The continuous case should remind us of the definition of a Laplace transform. For any function f(t), the Laplace transform is (cid:90) ∞ L[f](s)= f(t)est dt. −∞ From this perspective, we can interpret the MGF as the Laplace transform of the PDF. The argument s of the output can be regarded as the coordinate in the Laplace space. If s=−jω, then M (jω) becomes the Fourier transform of the PDF. X Example 6.1. Consider a random variable X with three states 0,1,2 and with prob- ability masses 2,3,1 respectively. Find the MGF. 6 6 6 324
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Solution. The moment-generating function is 2 3 1 M (s)=E[esX]=es0· +es1· +es2· X 6 6 6 1 es e2s = + + . 3 2 6 Practice Exercise 6.1. Find the MGF for a Poisson random variable. Solution. The MGF of Poisson random variable can be found as M (s)=E[esX]=(cid:88)∞ esxλxe−λ =(cid:88)∞ (λes)x e−λ =eλes e−λ. X x! x! x=0 x=0 Practice Exercise 6.2. Find the MGF for an exponential random variable. Solution. The MGF of an exponential random variable can be found as (cid:90) ∞ (cid:90) ∞ λ M (s)=E[esX]= esxλe−λx dx= λe(s−λ)x dx= , if λ>s. X λ−s 0 0 Why are moment-generating functions so called? The following theorem reveals the reason. Theorem 6.1. The MGF has the properties that • M (0)=1, X • d M (s)| =E[X], d2 M (s)| =E[X2], ds X s=0 ds2 X s=0 • dk M (s)| =E[Xk], for any positive integer k. dsk X s=0 Proof. The first property can be proved by noting that M (0)=E[e0X]=E[1]=1. X The third property holds because dk (cid:90) ∞ dk (cid:90) ∞ M (s)= esxf (x)dx= xkesxf (x)dx. dsk X dsk X X −∞ −∞ Setting s=0 yields dk (cid:90) ∞ M (s)| = xkf (x)dx=E[Xk]. dsk X s=0 X −∞ The second property is a special case of the third property. (cid:3) 325
CHAPTER 6. SAMPLE STATISTICS The theorem tells us that if we take the derivative of the MGF and set s=0, we will obtain the moment. The order of the moment depends on the order of the derivative. As a result, the MGF can “generate moments” by taking derivatives. This happens because of the exponential function esx. Since d esx =xesx, the variable x appears whenever we take ds the derivative. Practice Exercise 6.3. Let X be a Bernoulli random variable with parameter p. Find the first two moments using MGF. Solution. The MGF of a Bernoulli random variable is M (s)=E[esX] X =es0p (0)+es1p (1) X X =(1)(1−p)+(es)(p) =1−p+pes. The first and the second moment, using the derivative approach, are (cid:12) (cid:18) (cid:19)(cid:12) (cid:12) E[X]= dd sM X(s)(cid:12) (cid:12) (cid:12) = dd s 1−p+pes (cid:12) (cid:12) (cid:12) =pes(cid:12) (cid:12) (cid:12) =p, s=0 s=0 s=0 E[X2]= dd s2 2M X(s)(cid:12) (cid:12) (cid:12) (cid:12) = dd s2 2(cid:18) 1−p+pes(cid:19)(cid:12) (cid:12) (cid:12) (cid:12) =pes(cid:12) (cid:12) (cid:12) (cid:12) =p. s=0 s=0 s=0 To facilitate our discussions of MGF, we summarize a few MGFs in the table below. Distribution PMF/PDF E[X] Var[X] M (s) X Bernoulli p (1)=pandp (0)=1−p p p(1−p) 1−p+pes X X Binomial p (k)=(cid:0)n(cid:1) pk(1−p)n−k np np(1−p) (1−p+pes)n X k 1 1−p pes Geometric p (k)=p(1−p)k−1 X p p2 1−(1−p)es λke−λ Poisson p (k)= λ λ eλ(es−1) X k! 1 (cid:26) (x−µ)2(cid:27) (cid:26) σ2s2(cid:27) Gaussian f (x)= √ exp − µ σ2 exp µs+ X 2πσ2 2σ2 2 1 1 λ Exponential f (x)=λexp{−λx} X λ λ2 λ−s 1 a+b (b−a)2 esb−esa Uniform f (x)= X b−a 2 12 s(b−a) Table 6.1: Moment-generating functions of common random variables. 326
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS 6.1.2 Sum of independent variables via MGF MGFs are most useful when analyzing the PDF of a sum of two random variables. The following theorem highlights the result. Theorem 6.2. Let X and Y be independent random variables. Let Z =X+Y. Then M (s)=M (s)M (s). (6.4) Z X Y Proof. By the definition of MGF, we have that M (s)=E(cid:104) es(X+Y)(cid:105)( =a)E(cid:2) esX(cid:3)E(cid:2) esY(cid:3) =M (s)M (s), Z X Y where (a) is valid because X and Y are independent. (cid:3) Corollary6.1. ConsiderindependentrandomvariablesX ,...,X .LetZ =(cid:80)N X 1 N n=1 n be the sum of random variables. Then the MGF of Z is N (cid:89) M (s)= M (s). (6.5) Z Xn n=1 If these random variables are further assumed to be identically distributed, the MGF is M (s)=(M (s))N. (6.6) Z X1 Proof. This follows immediately from the previous theorem: N (cid:89) M (s)=E[es(X1+···+XN)]=E[esX1]E[esX2]···E[esXN]= M (s). Z Xn n=1 If the random variables X ,...,X are i.i.d., then the product simplifies to 1 N N N (cid:89) M (s)= (cid:89) M (s)=(M (s))N. Xn X1 X1 n=1 n=1 (cid:3) Theorem 6.3 (Sum of Bernoulli = binomial). Let X , ..., X be a sequence of 1 N i.i.d. Bernoulli random variables with parameter p. Let Z =X +···+X be the sum. 1 N Then Z is a binomial random variable with parameters (N,p). Proof. Let us consider a sequence of i.i.d. Bernoulli random variables X ∼ Bernoulli(p) n for n=1,...,N. Let Z =X +···+X . The moment-generating function of Z is 1 N N (cid:89) M (s)=E[es(X1+···+XN)]= E[esXn] Z n=1 N = (cid:89)(cid:0) pes1+(1−p)es0(cid:1) =(pes+(1−p))N. n=1 327
CHAPTER 6. SAMPLE STATISTICS Now, let us check the moment-generating function of a binomial random variable: If Z ∼ Binomial(N,p), then N (cid:18) (cid:19) (cid:88) N M (s)=E[esZ]= esk pk(1−p)N−k Z k n=0 N (cid:18) (cid:19) = (cid:88) N (pes)k(1−p)N−k =(pes+(1−p))N, k n=0 where the last equality holds because (cid:80)N (cid:0)N(cid:1) akbN−k = (a + b)N. Therefore, the two n=0 k moment-generating functions are identical. (cid:3) Theorem 6.4 (Sum of binomial = binomial). Let X , ..., X be a sequence of 1 N i.i.d. binomial random variables with parameters (n,p). Let Z =X +···+X be the 1 N sum. Then Z is a binomial random variable with parameters (Nn,p). Proof. The MGF of a binomial random variable is M (s)=(pes+(1−p))n. Xi If we have N of these random variables, then Z =X +···+X will have the MGF 1 N N M (s)=(cid:89) M (s)=(pes+(1−p))Nn. Z Xi i=1 NotethatthisisjusttheMGFofanotherbinomialrandomvariablewithparameter(Nn,p). (cid:3) Theorem 6.5 (Sum of Poisson = Poisson). Let X , ..., X be a sequence of 1 N i.i.d. Poisson random variables with parameter λ. Let Z =X +···+X be the sum. 1 N Then Z is a Poisson random variable with parameters Nλ. Proof. The MGF of a Poisson random variable is (cid:88)∞ λk M (s)=E[esX]= esk e−λ X k! k=0 (cid:88)∞ (λes)k =e−λ k! k=0 =e−λeλes =eλ(es−1). AssumethatwehaveasumofN i.i.d.Poissonrandomvariables.Then,bythemaintheorem, we have that M (s)=[M (s)]N =eNλ(es−1). Z X Therefore, the resulting random variable Z is a Poisson with parameter Nλ. (cid:3) 328
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Theorem 6.6 (Sum of Gaussian = Gaussian). Let X , ..., X be a sequence of 1 N independent Gaussian random variables with parameters (µ ,σ2), ..., (µ ,σ2 ). Let 1 1 N N Z =X +···+X be the sum. Then Z is a Gaussian random variable: 1 N (cid:18) N N (cid:19) (cid:88) (cid:88) Z =Gaussian µ , σ2 . (6.7) n n n=1 n=1 Proof. We skip the proof of the MGF of a Gaussian. It can be shown that (cid:26) σ2s2(cid:27) M (s)=exp µs+ . X 2 When we have a sequence of Gaussian random variables, then M (s)=E[es(X1+···+XN)] Z =M (s)···M (s) X1 XN (cid:18) (cid:26) σ2s2(cid:27)(cid:19) (cid:18) (cid:26) σ2 s2(cid:27)(cid:19) = exp µ s+ 1 ··· exp µ s+ N 1 2 N 2 (cid:40)(cid:32) (cid:88)N (cid:33) (cid:32) (cid:88)N (cid:33) s2(cid:41) =exp µ s+ σ2 . n n 2 n=1 n=1 Therefore, the resulting random variable Z is also a Gaussian. The mean and variance of Z are (cid:80)N µ and (cid:80)N σ2, respectively. n=1 n n=1 n (cid:3) 6.1.3 Characteristic functions Moment-generating functions are the Laplace transforms of the PDFs. However, since the Laplacetransformisdefinedontheentirerighthalf-plane,notallPDFscanbetransformed. One way to mitigate this problem is to restrict s to the imaginary axis, s = jω. This will give us the characteristic function. Definition 6.2 (Usual definition). The characteristic function of a random variable X is Φ (jω)=E[ejωX]. (6.8) X However, we note that since ω can take any value in (−∞,∞), it does not matter if we consider E[e−jωX] or E[ejωX]. This leads to the following equivalent definition of the char- acteristic function: Definition 6.3 (Alternative definition (for this book)). The characteristic function of a random variable X is Φ (jω)=E[e−jωX]. (6.9) X 329
CHAPTER 6. SAMPLE STATISTICS If we follow this definition, we see that the characteristic function can be written as (cid:90) ∞ Φ (jω)=E[e−jωX]= e−jωxf (x)dx. (6.10) X X −∞ ThisisexactlytheFouriertransform ofthePDF.Thereasonforintroducingthisalternative characteristicfunctionisthatE[e−jωX]istheFouriertransformoff (x)butE[ejωX]isthe X inverse Fourier transform of f (x). The former is more convenient (in terms of notation) X forstudentswhohavetakenacourseinsignalsandsystems.However,weshouldstressthat the usual way of defining the characteristic function is E[ejωX]. A list of common Fourier transforms is shown in the table below. Additional identities can be found in standard signals and systems textbooks. Fourier Transforms f(t)←→F(ω) f(t)←→F(ω) 1. e−atu(t)←→ 1 , a>0 10. sinc2(Wt)←→ 2π∆( ω ) a+jω 2 W 2W 2. eatu(−t)←→ a−1 jω, a>0 11. e−atsin(ω 0t)u(t)←→ (a+jωω )0 2+ω2 0 3. e−a|t| ←→ 2a , a>0 12. e−atcos(ω t)u(t)←→ a+jω a2+ω2 0 (a+jω)2+ω2 0 √ 4. a2a +2 t2 ←→πae−a|ω|, a>0 13. e− 2t σ2 2 ←→ 2πσe−σ2 2ω2 5. te−atu(t)←→ 1 , a>0 14. δ(t)←→1 (a+jω)2 6. tne−atu(t)←→ n! , a>0 15. 1←→2πδ(ω) (a+jω)n+1 7. rect( τt)←→τsinc(ω 2τ) 16. δ(t−t 0)←→e−jωt0 8. sinc(Wt)←→ Wπ rect( 2w W) 17. ejω0t ←→2πδ(ω−ω 0) 9. ∆( τt)←→ τ 2sinc2(ω 4τ) 18. f(t)ejω0t ←→F(ω−ω 0) Table 6.2: Fourier transform pairs of commonly used functions. Example 6.2. Let X be a random variable with PDF f (x)=λe−λx for x≥0. Find X the characteristic function. Solution. The Fourier transform pair is (cid:26) (cid:27) 1 λe−λx −→λ·F e−λx =λ· . λ+jω Therefore, the characteristic function is Φ (jω)= λ . X λ+jω 330
6.1. MOMENT-GENERATING AND CHARACTERISTIC FUNCTIONS Example 6.3. Let X and Y be independent, and let (cid:40) (cid:40) λe−λx, x≥0, λe−λy, y ≥0, f (x)= f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =X+Y. Solution.ThecharacteristicfunctionofX andY canbefoundfromtheFouriertable: λ λ Φ (jω)= and Φ (jω)= . X λ+jω Y λ+jω Therefore, the characteristic function of Z is λ2 Φ (jω)=Φ (jω)Φ (jω)= . Z X Y (λ+jω)2 By inverse Fourier transform, we have that (cid:26) λ2 (cid:27) f (z)=F−1 =λ2ze−λz, z ≥0. Z (λ+jω)2 Why Φ (jω) but not M (s)? As we said, the function is not always defined. Recall X X that the expectation E[X] exists only when f (x) is absolutely integrable, or E[|X|] < ∞. X Foracharacteristicfunction,theexpectationisvalidbecauseE[|ejωX|]=E[1]=1.However, for a function, E[|esX|] could be unbounded. To see a counterexample, we consider the Cauchy distribution. Theorem 6.7. Consider the Cauchy distribution with PDF 1 f (x)= . (6.11) X π(x2+1) The MGF of X is undefined but the characteristic function is well defined. Proof. The MGF is (cid:90) ∞ 1 (cid:90) ∞ 1 M (s)= esx dx≥ esx dx X π(x2+1) π(x2+1) −∞ 1 (cid:90) ∞ (sx)3 (sx)3 ≥ dx, because esx ≥ 6π(x2+1) 6 1 (cid:90) ∞ (sx)3 s3 (cid:90) ∞ ≥ dx= xdx=∞. 6π(2x2) 12π 1 1 Therefore, the MGF is undefined. On the other hand, by the Fourier table we know that (cid:26) (cid:27) 1 Φ (jω)=F =e−|ω|. X π(x2+1) (cid:3) 331
CHAPTER 6. SAMPLE STATISTICS Example 6.4. Let X ,X ,... be a sequence of independent random variables with 0 1 PDF a 1 f (x)= k , a = for k =0,1,.... Xk π(a2 +x2) k 2k+1 k Find the PDF of Y, where Y =(cid:80)∞ X . k=0 k Solution. From the Fourier transform table, we know that a 1 a2 1 π(a2 +k x2) = a π · (a2 +k x2) ←F → a π ·πa ke−ak|ω| =e−ak|ω|. k k k k The characteristic function of Y is ∞ (cid:40) ∞ (cid:41) (cid:89) (cid:88) Φ (jω)= Φ (jω)=exp −|ω| a . Y Xk k k=0 k=0 Since (cid:80)∞ a = (cid:80)∞ 1 = 1 + 1 +··· = 1, the characteristic function becomes k=0 k k=0 2k+1 2 4 Φ (jω)=e−|ω|. The inverse Fourier transform gives us Y 1 1 1 e−|ω| = ·πe−|ω| ←F → · . π π 1+x2 Therefore the PDF of Y is 1 f (y)= . Y π(1+y2) Example 6.5. Two random variables X and Y have the PDFs (cid:40) (cid:40) e−x, x≥0, e−y, y ≥0, f (x)= and f (y)= X Y 0, x<0, 0, y <0. Find the PDF of Z =max(X,Y)−min(X,Y). Solution. We first show that Z =max(X,Y)−min(X,Y)=|X−Y|. Suppose X >Y, then max(X,Y)=X and min(X,Y)=Y. So Z =X−Y. If X <Y, then max(X,Y) = Y and min(X,Y) = X. So Z = Y −X. Combining the two cases gives us Z =|X−Y|. Now, consider the Fourier transform of the PDFs: 1 e−x ←F → . 1+jω 332
6.2. PROBABILITY INEQUALITIES Let U =X−Y, and let Z =|U|. The characteristic function is Φ (jω)=E[e−jω(X−Y)]=E[e−jωX]E[ejωY] U 1 1 1 1 = · = ←F → f (u)= e−|u|. 1+jω 1−jω 1+ω2 U 2 With the PDF of U, we can find the CDF of Z: F (z)=P[Z ≤z]=P[|U|≤z] Z (cid:90) z = f (u)du U −z (cid:90) z 1 = e−|u| du 2 −z (cid:90) z 1 =2 e−u du=1−e−z. 2 0 Hence, the PDF is d f (z)= F (z)=e−z. Z dz Z Closing remark. Moment-generating functions and characteristic functions are useful mathematical tools. In this section, we have confined our discussion to using them to com- pute the sum of two random variables. Later sections and chapters will explain further uses for these functions. For example, we use the MGFs when proving Chernoff’s bound and proving the Central Limit Theorem. 6.2 Probability Inequalities Moment-generatingfunctionsandcharacteristicfunctionsarepowerfultoolsforhandlingthe sum of random variables. We now introduce another set of tools, known as the probability inequalities, that allow us to do approximations. We will highlight a few basic probability inequalities in this section. 6.2.1 Union bound The first inequality is the union bound we had introduced when we discussed the axioms of probabilities. The union bound states the following: Theorem 6.8 (Union Bound). Let A ,...,A be a collection of sets. Then 1 N (cid:34) N (cid:35) N (cid:91) (cid:88) P A ≤ P[A ]. (6.12) n n n=1 n=1 333
CHAPTER 6. SAMPLE STATISTICS Proof. We can prove this by induction. First, if N =2, P[A ∪A ]=P[A ]+P[A ]−P[A ∩A ]≤P[A ]+P[A ], 1 2 1 2 1 2 1 2 because P[A ∩A ] is a probability and so it must be non-negative. Thus we have proved 1 2 the base case. Assume that the statement is true for N = K. We need to prove that the statement is also true for N =K+1. To this end, we note that (cid:34)K+1 (cid:35) (cid:34)(cid:32) K (cid:33) (cid:35) (cid:91) (cid:91) P A =P A ∪A n n K+1 n=1 n=1 (cid:34) K (cid:35) (cid:34)(cid:32) K (cid:33) (cid:35) (cid:91) (cid:91) =P A +P[A ]−P A ∩A n K+1 n K+1 n=1 n=1 (cid:34) K (cid:35) (cid:91) ≤P A +P[A ]. n K+1 n=1 Then, according to our hypothesis for N =K, it follows that (cid:34) K (cid:35) K (cid:91) (cid:88) P A ≤ P[A ]. n n n=1 n=1 Putting these together, (cid:34)K+1 (cid:35) K K+1 (cid:91) (cid:88) (cid:88) P A ≤ P[A ]+P[A ]= P[A ]. n n K+1 n n=1 n=1 n=1 Therefore, by the principle of induction, we have proved the statement. (cid:3) Remark.Thetightnessoftheunionbounddependsontheamountof overlappingbetween theeventsA ,...,A ,asillustratedinFigure6.4.Iftheeventsaredisjoint,theunionbound 1 n is tight. If the events are overlapping significantly, the union is loose. The idea of the union bound is the principle of divide and conquer. We decompose the system into smaller events forasystemofnvariablesandusetheunionboundtoupper-limittheoverallprobability.If the probability of each event is small, the union bound tells us that the overall probability of the system will also be small. Figure 6.4: Conditions under which the union bound is loose or tight. [Left] The union bound is loose when the sets are overlapping. [Right] The union bound is tight when the sets are (nearly) disjoint. 334
6.2. PROBABILITY INEQUALITIES Example 6.6. Let X ,...,X be a sequence of i.i.d. random variables with CDF 1 N F (x) and let Z =min(X ,...,X ). Find an upper bound on the CDF. Xn 1 N Solution. Note that Z = min(X ,...,X ) ≤ z is equivalent to at least one of the 1 N X ’s being less than z. Thus, we have that n Z =min(X ,...,X )≤z ⇔ X ≤z∪···∪X ≤z. 1 N 1 N Substituting this result into the CDF, F (z)=P[Z ≤z] Z =P[min(X ,...,X )≤z] 1 N =P[X ≤z∪···∪X ≤z] 1 N ≤P[X ≤z]+···+P[X ≤z] 1 N =N ·F (z). X 6.2.2 The Cauchy-Schwarz inequality ThesecondinequalitywestudyhereistheCauchy-Schwarzinequality,whichwepreviously mentioned in Chapter 5. We review it for the sake of completeness. Theorem 6.9 (Cauchy-Schwarz inequality). Let X and Y be two random variables. Then E[XY]2 ≤E[X2]E[Y2]. (6.13) Proof. Let f(s)=E[(sX+Y)2] for any real s. Then f(s)=E[(sX+Y)2] =E[s2X2+2sXY +Y2] =E[X2]s2+2E[XY]s+E[Y2]. This is a quadratic equation, and f(s)≥0 for all s because E[(sX+Y)2]≥0. Recall that for a quadratic equation φ(x)=ax2+bx+c, the function φ(x)≥0 if and only if b2−4ac≤0. Substituting this result into our problem, we show that (2E[XY])2−4E[X2]E[Y2]≤0. This implies that E[XY]2 ≤E[X2]E[Y2], which completes the proof. (cid:3) Remark. As shown in Chapter 5, the Cauchy-Schwarz inequality is useful in analyzing E[XY].Forexample,wecanusetheCauchy-Schwarzinequalitytoprovethatthecorrelation coefficient ρ is bounded between −1 and 1. 335
CHAPTER 6. SAMPLE STATISTICS 6.2.3 Jensen’s inequality Our next inequality is Jensen’s inequality. To motivate the inequality, we recall that Var[X]=E[X2]−E[X]2. Since Var[X]≥0 for any X, it follows that E[X2] ≥ E[X]2 . (6.14) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =E[g(X)] =g(E[X]) Jensen’s inequality is a generalization of the above result by recognizing that the inequality does not only hold for the function g(X) = X2 but also for any convex function g. The theorem is stated as follows: Theorem 6.10 (Jensen’s inequality). Let X be a random variable, and let g :R→R be a convex function. Then E[g(X)]≥g(E[X]). (6.15) If the function g is concave, then the inequality sign is flipped: E[g(X)] ≤ g(E[X]). The way to remember this result is to remember that E[X2]−E[X]2 =Var[X]≥0. Now,whatisaconvexfunction?Informally,afunctiongisconvexif,whenwepickany two points on the function and connect them with a straight line, the line will be above the function for that segment. This definition is illustrated in Figure 6.5. Consider an interval [x,y], and the line segment connecting g(x) and g(y). If the function g(·) is convex, then the entire line segment should be above the curve. Figure 6.5: Illustration of a convex function, a concave function, and a function that is neither convex nor concave. The definition of a convex function essentially follows the above picture: Definition 6.4. A function g is convex if g(λx+(1−λ)y)≤λg(x)+(1−λ)g(y), (6.16) for any 0≤λ≤1. Hereλrepresentsa“sweeping”constantthatgoesfromxtoy.Whenλ=1thenλx+(1−λ)y simplifies to x, and when λ=0 then λx+(1−λ)y simplifies to y. 336
6.2. PROBABILITY INEQUALITIES Thedefinitioniseasytounderstand.Theleft-handsideg(λx+(1−λ)y)isthefunction evaluated at any points in the interval [x,y]. The right-hand side is the red straight line we plotted in Figure 6.5. It connects the two points g(x) and g(y). Convexity means that the red line is entirely above the curve. For twice-differentiable 1D functions, convexity can be described by the curvature of the function. A function is convex if g(cid:48)(cid:48)(x)≥0. (6.17) This is self-explanatory because if the curvature is non-negative for all x, then the slope of g has to keep increasing. Example 6.7. The following functions are convex or concave: • g(x)=logx is concave, because g(cid:48)(x)= 1 and g(cid:48)(cid:48)(x)=− 1 ≤0 for all x. x x2 • g(x)=x2 is convex, because g(cid:48)(x)=2x and g(cid:48)(cid:48)(x)=2 is positive. • g(x)=e−x is convex, because g(cid:48)(x)=−e−x and g(cid:48)(cid:48)(x)=e−x ≥0. Why is Jensen inequality valid for a convex function? Consider the illustration in Figure 6.6. Suppose we have a random variable X taking some PDF f (x). There is a X convexfunctiong(·)thatmapstherandomvariableX tog(X).Sinceg(·)isconvex,aPDF like the one we see in Figure 6.6 will become skewed. (You can map the left tail to the new left tail, the peak to the new peak, and the right tail to the new right tail.) As you can see from the figure, the new random variable g(X) has a mean E[g(X)] that is greater than the mapped old mean g(E[X]). Jensen’s inequality captures this phenomenon by stating that E[g(X)]≥g(E[X]) for any convex function g(·). Figure6.6:Jensen’sinequalitystatesthatifthereisaconvexfunctiong(·)thatmapsarandomvariable X to a new random variable g(X), the new mean E[g(X)] will be greater than the mapped old mean g(E[X]). ProvingJensen’sinequalityisstraightforwardforatwo-statediscreterandomvariable. Define a random variable X with states x and y. The probabilities for these two states are P[X =x]=λ and P[X =y]=1−λ. Then (cid:88) E[X]= x(cid:48)p (x(cid:48))=λx+(1−λ)y. X x(cid:48)∈{x,y} 337
CHAPTER 6. SAMPLE STATISTICS Now, let g(·) be a convex function. We know from the expectation that (cid:88) E[g(X)]= g(x(cid:48))p (x(cid:48))=g(x)λ+(1−λ)g(y). X x(cid:48)∈{x,y} By convexity of the function g(·), it follows that g(λx+(1−λ)y)≤λf(x)+(1−λ)g(y), (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =g(E[X]) =E[g(X)] where in the underbrace we substitute the definitions using the expectation. Therefore, foranytwo-statediscreterandomvariables,theproofofJensen’sinequalityfollowsdirectly fromtheconvexity.Ifthediscreterandomvariabletakesmorethantwostates,wecanprove thetheorembyinduction.Forcontinuousrandomvariables,wecanprovethetheoremusing the following approach. You may skip the proof of Jensen’s inequality if this is your first time reading the book. Here we present an alternative proof of Jensen’s inequality that does not require proof by induction. The idea is to recognize that if the function g is convex we can find a tangent lineL(X)=aX+batthepointE[X]thatisuniformlylowerthan g(X),i.e.,g(X)≥L(X) for all X. Then we can prove the result with a simple geometric argument. Figure 6.7 illustrates this idea. Figure6.7:GeometricillustrationoftheproofofJensen’sinequality.Supposeg(·)isaconvexfunction. For any point X on g(·), we can find a tangent line L(X) = aX+b. Since the black curve is always above the tangent, it follows that E[g(X)]≥E[L(X)] for any X. Also, note that at a particular point E[X], the black curve and the red line touch, and so we have L(E[X])=g(E[X]). Proof of Jensen’s inequality.ConsiderL(X)asdefinedabove.Sinceg isconvex,g(X)≥ L(X) for all X. Therefore, E[g(X)]≥E[L(X)] =E[aX+b] =aE[X]+b =L(E[X])=g(E[X]), where the last equality holds because L is a tangent line to g where they meet at E[X]. (cid:3) 338
6.2. PROBABILITY INEQUALITIES What are (a,b) in the proof? By Taylor expansion, g(X)≈g(E[X])+g(cid:48)(E[X])(X−E[X]) def = L(X). Therefore, if we want to be precise, then a=g(cid:48)(E[X]) and b=g(E[X])−g(cid:48)(E[X])E[X]. The end of the proof. Example 6.8. By Jensen’s inequality, we have that (a) E[X2]≥E[X]2, because g(x)=x2 is convex. (b) E(cid:2)1(cid:3) ≥ 1 , because g(x)= 1 is convex. X E[X] x (c) E[logX]≤logE[X], because g(x)=logx is concave. 6.2.4 Markov’s inequality Our next inequality, Markov’s inequality, is an elementary inequality that links probability and expectation. Theorem 6.11 (Markov’s inequality). Let X ≥0 be a non-negative random variable. Then, for any ε>0, we have E[X] P[X ≥ε]≤ . (6.18) ε Markov’s inequality concerns the tail of the random variable. As illustrated in Figure 6.8, P[X ≥ ε] measures the probability that the random variable takes a value greater than ε. Markov’s inequality asserts that this probability P[X ≥ ε] is upper-bounded by the ratio E[X]/ε.Thisresultisusefulbecauseitrelatestheprobabilityandtheexpectation.Inmany problems the probability P[X ≥ε] could be difficult to evaluate if the PDF is complicated. The expectation, on the other hand, is usually easier to evaluate. Proof. Consider εP[X ≥ε]. It follows that (cid:90) ∞ (cid:90) ∞ εP[X ≥ε]= ε f (x)dx≤ xf (x)dx, X X ε ε where the inequality is valid because for any x ≥ ε the integrand (which is non-negative) will always increase (or at least not decrease). It then follows that (cid:90) ∞ (cid:90) ∞ xf (x)dx≤ xf (x)dx=E[X]. (cid:3) X X ε 0 A pictorial interpretation of Markov’s inequality is shown in Figure 6.9. For X >0, it is not difficult to show that E[X] = (cid:82)∞ 1−F (x) dx. Then, in the CDF plot, we see that 0 X ε·P[X ≥ε] is a rectangle covering the top left corner. This area is clearly smaller than the area covered by the function 1−F (x). X 339
CHAPTER 6. SAMPLE STATISTICS Figure6.8:Markov’sinequalityprovidesanupperboundtothetailofarandomvariable.Theinequality states that the probability P[X ≥ε] is upper bounded by the ratio E[X]/ε. Figure 6.9: The proof of Markov’s inequality follows from the fact that ε·P[X ≥ ε] occupies the top left corner marked by the yellow rectangle. The expectation is the area above the CDF so that E[X]=(cid:82)∞1−F (x)dx.Sincetheyellowrectangleissmallerthantheorangeshadedarea,itfollows 0 X that ε·P[X ≥ε]≤E[X], which is Markov’s inequality. Practice Exercise 6.4. Prove that if X >0, then E[X]=(cid:82)∞ 1−F (x)dx. 0 X Solution. We start from the right-hand side: (cid:90) ∞ (cid:90) ∞ 1−F (x)dx= 1−P[X ≤x]dx X 0 0 (cid:90) ∞ = P[X ≥x]dx 0 (cid:90) ∞(cid:90) ∞ = f (t)dtdx X 0 x (cid:90) ∞(cid:90) t = f (t)dxdt X 0 0 (cid:90) ∞ = tf (t)dt=E[X]. X 0 The change in the integration order is illustrated below. 340
6.2. PROBABILITY INEQUALITIES How tight is Markov’s inequality? It is possible to create a random variable such that the equality is met (see Exercise 6.14). However, in general, the estimate provided by the upper bound is not tight. Here is an example. Practice Exercise 6.5.LetX ∼Uniform(0,4).VerifyMarkov’sinequalityforP[X ≥ 2], P[X ≥3] and P[X ≥4]. Solution. First, we observe that E[X]=2. Then E[X] P[X ≥2]=0.5, =1, 2 E[X] P[X ≥3]=0.25, =0.67, 3 E[X] P[X ≥4]=0, =0.5. 4 Therefore, although the upper bounds are all valid, they are very loose. If Markov’s inequality is not tight, why is it useful? It turns out that while Markov’s inequality is not tight, its variations can be powerful. We will come back to this point when we discuss Chernoff’s bound. 6.2.5 Chebyshev’s inequality The next inequality is a simple extension of Markov’s inequality. The result is known as Chebyshev’s inequality. Theorem 6.12 (Chebyshev’s inequality). Let X be a random variable with mean µ. Then for any ε>0 we have Var[X] P[|X−µ|≥ε]≤ . (6.19) ε2 The tail measured by Chebyshev’s inequality is illustrated in Figure 6.10. Since the event |X −µ| ≥ ε involves an absolute value, the probability measures the two-sided tail. Chebyshev’s inequality states that this tail probability is upper-bounded by Var[X]/ε2. 341
CHAPTER 6. SAMPLE STATISTICS Figure 6.10: Chebyshev’s inequality states that the two-sided tail probability P[|X−µ|≥ε] is upper- bounded by Var[X]/ε2 Proof. We apply Markov’s inequality to show that P[|X−µ|≥ε]=P[(X−µ)2 ≥ε2] E[(X−µ)2] Var[X] ≤ = . ε2 ε2 (cid:3) An alternative form of Chebyshev’s inequality is obtained by letting ε = kσ. In this case, we have σ2 1 P[|X−µ|≥kσ]≤ = . k2σ2 k2 Therefore,ifarandomvariableisk timesthestandarddeviationawayfromthemean,then the probability bound drops to 1/k2. Practice Exercise 6.6. Let X ∼ Uniform(0,4). Find the bound of Chebyshev’s inequality for the probability P[|X−µ|≥1]. Solution. Note that E[X]=2 and σ2 =42/12=4/3. Therefore, we have σ2 4 P[|X−µ|≥1]≤ = , ε2 3 which is a valid upper bound, but quite conservative. Practice Exercise 6.7. Let X ∼ Exponential(1). Find the bound of Chebyshev’s inequality for the probability P[X ≥ε]. Solution. Note that E[X]=1 and σ2 =1. Thus we have P[X ≥ε]=P[X−µ≥ε−µ]≤P[|X−µ|≥ε−µ] σ2 1 ≤ = . (ε−µ)2 (ε−1)2 342
6.2. PROBABILITY INEQUALITIES We can compare this with the exact probability, which is P[X ≥ε]=1−F (ε)=e−ε. X Again,theestimategivenbyChebyshev’sinequalityisacceptablebuttooconservative. Corollary 6.2. Let X ,...,X be i.i.d. random variables with mean E[X ]=µ and 1 N n variance Var[X ]=σ2. Let X = 1 (cid:80)N X be the sample mean. Then n N N n=1 n P(cid:20) (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:21) ≤ Nσ (cid:15)2 2. (6.20) Proof. We can first show that E[X ]=µ and Var[X ] satisfies N N 1 (cid:88)N σ2 Var[X ]= Var[X ]= . N N2 n N n=1 Then by Chebyshev’s inequality, P(cid:20) (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:21) ≤ Var (cid:15)[X 2 N] = Nσ (cid:15)2 2. (cid:3) Theconsequenceofthiscorollaryisthattheupperboundσ2N/(cid:15)2 willconvergetozero (cid:12) (cid:12) as N → ∞. Therefore, the probability of getting the event {(cid:12)X N −µ(cid:12) > (cid:15)} is vanishing. It means that the sample average X is converging to the true population mean µ, in the N sense that the probability of failing is shrinking. 6.2.6 Chernoff’s bound We now introduce a powerful inequality or a set of general procedures that gives us some highly useful inequalities. The idea is named for Herman Chernoff, although it was actually due to his colleague Herman Rubin. Theorem 6.13 (Chernoff’s bound). Let X be a random variable. Then, for any ε≥0, we have that P[X ≥ε]≤e−ϕ(ε), (6.21) wherea (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) , (6.22) X s>0 and M (s) is the moment-generating function. X aϕ(ε)iscalledtheFenchel-LegendredualfunctionoflogMX.Seereferences[6-14]. 343
CHAPTER 6. SAMPLE STATISTICS Proof. There are two tricks in the proof of Chernoff’s bound. The first trick is a nonlinear transformation. Since esx is an increasing function for any s>0 and x, we have that P[X ≥ε]=P[esX ≥esε] (a) E[esX] ≤ esε ( =b) e−sεM (s) X =e−sε+logMX(s), where the inequality (a) is due to Markov’s inequality. Step (b) just uses the definition of MGF that E[esX]=M (s). X Now for the second trick. Note that the above result holds for all s. That means it must also hold for the s that minimizes e−sε+logMX(s). This implies that (cid:110) (cid:111) P[X ≥ε]≤min e−sε+logMX(s) . s>0 Again, since ex is increasing, the minimizer of the above probability is also the maximizer of this function: (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) . X s>0 Thus, we conclude that P[X ≥ε]≤e−ϕ(ε). (cid:3) 6.2.7 Comparing Chernoff and Chebyshev Let’s consider an example of how Chernoff’s bound can be useful. Suppose that we have a random variable X ∼Gaussian(0,σ2/N). The number N can be regarded as the number of samples. For example, if Y ,...,Y are N Gaussian random 1 N variables with mean 0 and variance σ2, then the average X = 1 (cid:80)N Y will have mean N n=1 n 0 and variance σ2/N. Therefore, as N grows, the variance of X will become smaller and smaller. First, since the random variable is Gaussian, we can show the following: Lemma 6.1. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for any N ε>0, (cid:32)√ (cid:33) Nε P[X ≥ε]=1−Φ , (6.23) σ where Φ is the standard Gaussian’s CDF. Note that this is the exact result: If you tell me ε, N, and σ, then the probability P[X ≥ε] is exactly the one shown on the right-hand side. No approximation, no randomness. 344
6.2. PROBABILITY INEQUALITIES Proof. Since X is Gaussian, the probability is (cid:90) ∞ 1 (cid:26) x2 (cid:27) P[X ≥ε]= exp − dx (cid:112) 2π(σ2/N) 2(σ2/N) ε (cid:90) ε 1 (cid:26) x2 (cid:27) =1− exp − dx (cid:112) 2π(σ2/N) 2(σ2/N) −∞ =1−(cid:90) √ σε 2/N √1 exp(cid:26) −x2(cid:27) dx 2π 2 −∞ (cid:32) (cid:33) (cid:32)√ (cid:33) ε Nε =1−Φ =1−Φ . (cid:112) σ2/N σ (cid:3) Let us compute the bound given by Chebyshev’s inequality. Lemma 6.2. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for any N ε>0, Chebyshev’s inequality implies that σ2 P[X ≥ε]≤ . (6.24) Nε2 Proof. We apply Chebyshev’s inequality by assuming that µ=0: P[X ≥ε]=P[X−µ≥ε−µ]≤P[|X−µ|≥ε−µ] E[(X−µ)2] σ2 ≤ = . (ε−µ)2 Nε2 (cid:3) We now compute Chernoff’s bound. Theorem 6.14. Let X ∼Gaussian(0,σ2) be a Gaussian random variable. Then, for N any ε>0, Chernoff’s bound implies that (cid:26) ε2N(cid:27) P[X ≥ε]≤exp − . (6.25) 2σ2 Proof.TheMGFofazero-meanGaussianrandomvariablewithvarianceσ2/N isM (s)= X (cid:110) (cid:111) exp σ2s2 . Therefore, the function ϕ can be written as 2N (cid:26) (cid:27) ϕ(ε)=max sε−logM (s) X s>0 (cid:26) σ2s2(cid:27) =max sε− . s>0 2N To maximize the function we take the derivative and set it to zero. This yields d (cid:26) σ2s2(cid:27) Nε sε− =0 ⇒ s∗ = . ds 2N σ2 345
CHAPTER 6. SAMPLE STATISTICS Note that this s∗ is a maximizer because sε− σ2s2 is a concave function. 2N Substituting s∗ into ϕ(ε), (cid:26) sε−σ2s2(cid:27) ϕ(ε)=max s>0 2N σ2(s∗)2 (cid:18) Nε(cid:19) σ2 (cid:18) Nε(cid:19)2 ε2N =s∗ε− = ε− = , 2N σ2 2N σ2 2σ2 and hence (cid:26) ε2N(cid:27) P[X ≥ε]≤e−ϕ(ε) =exp − . 2σ2 (cid:3) Figure 6.11 shows the comparison between the exact probability, the bound provided by Chebyshev’s inequality, and Chernoff’s bound: (cid:16)√ (cid:17) • Exact: P[X ≥ε]=1−Φ Nε . σ • Chebyshev: P[X ≥ε]≤ σ2 , Nε2 (cid:110) (cid:111) • Chernoff: P[X ≥ε]≤exp −ε2N . 2σ2 Inthisnumerical experiment,weset ε=0.1,and σ =1.WevarythenumberN.Aswe can see from the figure, the bound provided by Chebyshev is valid but very loose. It does not even capture the tail as N grows. On the other hand, Chernoff’s bound is reasonably tight. However, one should note that the tightness of Chernoff is only valid for large N. When N is small, it is possible to construct random variables such that Chebyshev is tighter. The MATLAB code used to generate this plot is illustrated below. % MATLAB code to compare the probability bounds epsilon = 0.1; sigma = 1; N = logspace(1,3.9,50); p_exact = 1-normcdf(sqrt(N)*epsilon/sigma); p_cheby = sigma^2./(epsilon^2*N); p_chern = exp(-epsilon^2*N/(2*sigma^2)); loglog(N, p_exact, ’-o’, ’Color’, [1 0.5 0], ’LineWidth’, 2); hold on; loglog(N, p_cheby, ’-’, ’Color’, [0.2 0.7 0.1], ’LineWidth’, 2); loglog(N, p_chern, ’-’, ’Color’, [0.2 0.0 0.8], ’LineWidth’, 2); What could go wrong if we insist on using Chebyshev’s inequality? Consider the fol- lowing example. Example 6.9. Let X ∼Gaussian(0,σ2/N). Suppose that we want the probability to be no greater than a confidence level of α: P[X ≥ε]≤α. 346
6.2. PROBABILITY INEQUALITIES 0 10 -5 10 -10 10 -15 10 1 2 3 10 10 10 N ytilibaborP Exact Chebyshev Chernoff Figure 6.11: Comparison between Chernoff’s bound and Chebyshev’s bound. The random variable we use is X ∼ Gaussian(0,σ2/N). As N grows, we show the probability bounds predicted by the two methods. Letα=0.05,ε=0.1,andσ =1.FindtheN using(i)Chebyshev’sinequalityand(ii) Chernoff’s inequality. Solution: (i) Chebyshev’s inequality implies that σ2 P[X ≥ε]≤ ≤α, Nε2 which means that σ2 N ≥ . αε2 If we plug in α=0.05, ε=0.1, and σ =1, then N ≥2000. (ii) For Chernoff’s inequality, it holds that (cid:26) ε2N(cid:27) P[X ≥ε]≤exp − ≤α, 2σ2 which means that 2σ2 N ≥− logα ε2 Plugging in α=0.05, ε=0.1, and σ =1, we have that N ≥600. This is more than 3 timessmallerthantheonepredictedbyChebyshev’sinequality.Whichoneiscorrect? Both are correct but Chebyshev’s inequality is overly conservative. If N ≥ 600 can makeP[X ≥ε]≤α,thencertainlyN ≥2000willworktoo.However,N ≥2000istoo loose. 347
CHAPTER 6. SAMPLE STATISTICS 6.2.8 Hoeffding’s inequality Chernoff’s bound can be used to derive many powerful inequalities. Here we present an inequality for bounded random variables. This result is known as Hoeffding’s inequality. Theorem 6.15 (Hoeffding’s inequality). Let X ,...,X be i.i.d. random variables 1 N with 0≤X ≤1, and E[X ]=µ. Then n n (cid:20) (cid:21) P (cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15) ≤2e−2(cid:15)2N, (6.26) where X = 1 (cid:80)N X . N N n=1 n YoumayskiptheproofofHoeffding’sinequalityifthisisyourfirsttime readingthebook. Proof. (Hoeffding’s inequality) First, we show that (cid:34) N (cid:35) (cid:34) N (cid:35) P(cid:2) X −µ>(cid:15)(cid:3) =P 1 (cid:88) X −µ>(cid:15) =P (cid:88) (X −µ)>N(cid:15) N N n n n=1 n=1 =P(cid:104) es(cid:80)N n=1(Xn−µ) ≥es(cid:15)N(cid:105) E[es(cid:80)N n=1(Xn−µ)] (cid:18)E[es(Xn−µ)](cid:19)N ≤ = . es(cid:15)N es(cid:15) Let Z = X −µ. Then −µ ≤ Z ≤ 1−µ. At this point we use Hoeffding Lemma (see n n n below) that E[esZn]≤es 82 because b−a=(1−µ)−(−µ)=1. Thus, P(cid:2) X N −µ>(cid:15)(cid:3) ≤(cid:18)E[e es sZ (cid:15)n](cid:19)N ≤(cid:32) e es s82 (cid:15)(cid:33)N =es2 8N−s(cid:15)N, ∀s. This result holds for all s, and thus it holds for the s that minimizes the right-hand side. This implies that P(cid:2) X −µ>(cid:15)(cid:3) ≤min(cid:26) exp(cid:26) s2N −s(cid:15)N(cid:27)(cid:27) . N s 8 (cid:110) (cid:111) Minimizing the exponent gives d s2N −s(cid:15)N = sN −(cid:15)N = 0. Thus we have s = 4(cid:15). ds 8 4 Hence, P(cid:2) X −µ>(cid:15)(cid:3) ≤exp(cid:26) (4(cid:15))2N −(4(cid:15))(cid:15)N(cid:27) =e−2(cid:15)2N. N 8 By symmetry, P(cid:2) X −µ<−(cid:15)(cid:3) ≤e−2(cid:15)2N. Then by union bound we show that N P(cid:2) |X −µ|>(cid:15)(cid:3) =P(cid:2) X −µ>(cid:15)(cid:3) +P(cid:2) X −µ<−(cid:15)(cid:3) N N N ≤e−2(cid:15)2N +e−2(cid:15)2N =2e−2(cid:15)2N. (cid:3) 348
6.2. PROBABILITY INEQUALITIES Lemma 6.3 (Hoeffding’s lemma). Let a ≤ X ≤ b be a random variable with E[X]=0. Then M (s)d =efE(cid:2) esX(cid:3) ≤exp(cid:26) s2(b−a)2(cid:27) . (6.27) X 8 Proof. Since a≤X ≤b, we can write X as a linear combination of a and b: X =λb+(1−λ)a, whereλ= X−a.Sinceexp(·)isaconvexfunction,itfollowsthateλb+(1−λ)a ≤λeb+(1−λ)ea. b−a (Recall that h is convex if h(λx+(1−λ)y)≤λh(x)+(1−λ)h(y).) Therefore, we have esX ≤λesb+(1−λ)esa X−a b−X = esb+ esa. b−a b−a Taking expectations on both sides of the equation, −a b E[esX]≤ esb+ esa, b−a b−a because E[X]=0. Now, if we let θ =− a , then b−a −a b esb+ esa =θesb+(1−θ)esa b−a b−a (cid:16) (cid:17) (cid:16) (cid:17) =esa 1−θ+θes(b−a) = 1−θ+θes(b−a) e−sθ(b−a) =(1−θ+θeu)e−θu =e−θu+log(1−θ+θeu), where we let u=s(b−a). This can be simplified as E[esX]≤E[eφ(u)] by defining φ(u)=−θu+log(1−θ+θeu). The final step is to approximate φ(u). To this end, we use Taylor approximation: u2 φ(u)=φ(0)+uφ(cid:48)(0)+ φ(cid:48)(cid:48)(ξ), 2 for some ξ ∈[a,b]. Since φ(0)=0, φ(cid:48)(0)=0, and φ(cid:48)(cid:48)(u)≤ 1 for all u, it follows that 4 u2 u2 s2(b−a)2 φ(u)= φ(cid:48)(cid:48)(ξ)≤ = . (cid:3) 2 8 8 End of the proof. 349
CHAPTER 6. SAMPLE STATISTICS What is so special about the Hoeffding’s inequality? • Since Hoeffding’s inequality is derived from Chernoff’s bound, it inherits the tightness. Hoeffding’s inequality is much stronger than Chebyshev’s inequality in bounding the tail distributions. • Hoeffding’sinequalityisoneofthefewinequalitiesthatdonotrequireE[X]and Var[X] on the right-hand side. • A downside of the inequality is that boundedness is not always easy to satisfy. For example, if X is a Gaussian random variable, Hoeffding does not apply. n There are more advanced inequalities for situations like these. Interpreting Hoeffding’s inequality. One way to interpret Hoeffding’s inequality is to write the equation as P(cid:2)(cid:12) (cid:12)X N −µ(cid:12) (cid:12)>(cid:15)(cid:3) ≤2e−2(cid:15)2N, (cid:124) (cid:123)(cid:122) (cid:125) δ which is equivalent to P(cid:2)(cid:12) (cid:12)X N −µ(cid:12) (cid:12)≤(cid:15)(cid:3) ≥1−δ. This means that with a probability at least 1−δ, we have X −(cid:15)≤µ≤X +(cid:15). N N If we let δ =2e−2(cid:15)2N, this becomes (cid:114) (cid:114) 1 2 1 2 X − log ≤µ≤X + log . (6.28) N 2N δ N 2N δ This inequality is a confidence interval (see Chapter 9). It says that with probability at least 1−δ, the interval [X −(cid:15), X +(cid:15)] includes the true population mean µ. N N There are two questions one can ask about the confidence interval: • Given N and δ, what is the confidence interval? Equation (6.28) tells us that if we know N, to achieve a probability of at least 1−δ the confidence interval will follow (cid:113) Equation (6.28). For example, if N = 10,000 and δ = 0.01, 1 log2 = 0.016. 2N δ Therefore,withaprobabilityatleast99%,thetruepopulationmeanµwillbeincluded in the interval X −0.16≤µ≤X +0.16. N N • If we want to achieve a certain confidence interval, what is the N we need? If we are given (cid:15) and δ, the N we need is log2 δ ≤2e−2(cid:15)2N ⇒ N ≥ δ. 2(cid:15)2 For example, if δ =0.01 and (cid:15)=0.01, the N we need is N ≥26,500. WhenisHoeffding’sinequalityused?Hoeffding’sinequalityisfundamentalinmodern machine learning theory. In this field, one often wants to quantify how well a learning 350
6.3. LAW OF LARGE NUMBERS algorithmperforms withrespectto thecomplexityof the modeland thenumber oftraining samples.Forexample,ifwechooseacomplexmodel,weshouldexpecttousemoretraining samples or overfit otherwise. Hoeffding’s inequality provides an asymptotic description of the training error, testing error, and the number of training samples. The inequality is oftenusedtocomparethetheoreticalperformancelimitofonemodelversusanothermodel. Therefore, although we do not need to use Hoeffding’s inequality in this book, we hope you appreciate its tightness. Closing Remark. We close this section by providing the historic context of Chernoff’s inequality. Herman Chernoff, the discoverer of Chernoff’s inequality, wrote the following many years after the publication of the original paper in 1952. “In working on an artificial example, I discovered that I was using the Central Limit Theorem for large deviations where it did not apply. This led me to derive the asymptotic upper and lower bounds that were needed for the tail probabilities. [Herman] Rubin claimed he could get these bounds with much less work, and I challenged him. He produced a rather simple argument, using Markov’s inequality, for the upper bound. Since that seemed to be a minor lemma in the ensuing paper I published (Chernoff, 1952), I neglected to give him credit. I now consider it a serious error in judgment, especially because his result is stronger for the upper bound than the asymptotic result I had derived.” — Herman Chernoff, “A career in statistics,” in Lin et al., Past, Present, and Future of Statistical Science (2014), p. 35. 6.3 Law of Large Numbers In this section, we present our first main result: the law of large numbers. We will discuss two versions of the law: the weak law and the strong law. We will also introduce two forms of convergence: convergence in probability and almost sure convergence. 6.3.1 Sample average The law of large numbers is a probabilistic statement about the sample average. Suppose thatwehaveacollectionofi.i.d.randomvariablesX ,...,X .Thesampleaverageofthese 1 N N random variables is defined as follows: Definition 6.5. The sample average of a sequence of random variables X ,...,X 1 N is N 1 (cid:88) X = X . (6.29) N N n n=1 If the random variables X ,...,X are i.i.d. so that they have the same population 1 N mean E[X ]=µ (for n=1,...,N), then by the linearity of the expectation, n N E(cid:2) X (cid:3) = 1 (cid:88) E[X ]=µ. N N n n=1 351
CHAPTER 6. SAMPLE STATISTICS Therefore, the mean of X is the population mean µ. N The sample average, X , plays an important role in statistics. For example, by sur- N veying 10,000 Americans, we can find a sample average of their ages. Since we never have accesstothetruepopulationmean,thesampleaverageisanestimate,andsinceX isonly N an estimate, we need to ask how good the estimate is. One reason we ask this question is that X is a finite-sample “approximation” of µ. N More importantly, the root of the problem is that X itself is a random variable because N X ,...,X are all random variables. Since X is a random variable, there is a PDF of 1 N N X ; there is a CDF of X ; there is E[X ]; and there is Var[X ]. Since X is a random N N N N N variable, it has uncertainty. To say that we are confident about X , we need to ensure that N the uncertainty is within some tolerable range. How do we control the uncertainty? We can compute the variance. If X ,...,X are 1 N i.i.d. random variables with the same variance Var[X ]=σ2 (for n=1,...,N), then n Var(cid:2) X (cid:3) = 1 (cid:88)N Var[X ]= 1 (cid:88)N σ2 = σ2 . N N2 n N2 N n=1 n=1 Therefore, the variance will shrink to 0 as N grows. In other words, the more samples we use to construct the sample average, the less deviation the random variable X will have. N Visualizing the sample average To help you visualize the randomness of X , we consider an experiment of drawing N N Bernoulli random variables X ,...,X with parameter p = 1/2. Since X is Bernoulli, it 1 N n follows that E[X ]=p and Var[X ]=p(1−p). n n WeconstructasampleaverageX = 1 (cid:80)N X .SinceX isaBernoullirandomvariable, N N n=1 n n we know everything about X . First, X is a binomial random variable, since X is the N N N sum of Bernoulli random variables. Second, the mean and variance of X are respectively N N µ d =efE[X ]= 1 (cid:88) E[X ]=p, XN N N n n=1 N σ2 d =ef Var[X ]= 1 (cid:88) Var[X ]= p(1−p) . XN N N2 n N n=1 In Figure 6.12, we plot the random variables X (the black crosses) for every N. You N canseethatateachN,e.g.,N =100,therearemanypossibleobservationsforX because N X itself is a random variable. As N increases, we see that the deviation of the random N variablesbecomessmaller.Inthesameplot,weshowtheboundsµ±3σ ,whicharethree XN standarddeviationsfromthemean.Wecanseeclearlythattheboundsprovideaverygood envelope coveringthe random variables.As N goestoinfinity,wecansee thatthestandard deviation goes to zero, and so X approaches the true mean. N For your reference, the MATLAB code and the Python code we used to generate the plot are shown below. % MATLAB code to illustrate the weak law of large numbers Nset = round(logspace(2,5,100)); 352
6.3. LAW OF LARGE NUMBERS 0.7 0.6 0.5 0.4 0.3 102 103 104 105 N egareva elpmas Figure6.12:Theweaklawoflargenumbers.Inthisplot,weassumethatX ,...,X arei.i.d.Bernoulli 1 N random variables with a parameter p. The black crosses in the plot are the sample averages X = 1 (cid:80)N X . The red curves are the ideal bounds µ ± 3σ , where µ = p and N N (cid:112) n=1 n XN XN XN σ = p(1−p)/N. As N grows, we observe that the variance shrinks to zero. Therefore, the XN sample average is converging to the true population mean. for i=1:length(Nset) N = Nset(i); p = 0.5; x(:,i) = binornd(N, p, 1000,1)/N; end y = x(1:10:end,:)’; semilogx(Nset, y, ’kx’); hold on; semilogx(Nset, p+3*sqrt(p*(1-p)./Nset), ’r’, ’LineWidth’, 4); semilogx(Nset, p-3*sqrt(p*(1-p)./Nset), ’r’, ’LineWidth’, 4); # Python code to illustrate the weak law of large numbers import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats import numpy.matlib p = 0.5 Nset = np.round(np.logspace(2,5,100)).astype(int) x = np.zeros((1000,Nset.size)) for i in range(Nset.size): N = Nset[i] x[:,i] = stats.binom.rvs(N, p, size=1000)/N Nset_grid = np.matlib.repmat(Nset, 1000, 1) plt.semilogx(Nset_grid, x,’ko’); plt.semilogx(Nset, p + 3*np.sqrt((p*(1-p))/Nset), ’r’, linewidth=6) plt.semilogx(Nset, p - 3*np.sqrt((p*(1-p))/Nset), ’r’, linewidth=6) 353
CHAPTER 6. SAMPLE STATISTICS Note the outliers for each N in Figure 6.12. For example, at N = 102 we see a point locatednear0.7onthey-axis.Thispointisoutsidethreestandarddeviations.Isitnormal? Yes. Being outside three standard deviations only says that the probability of having this outlierissmall.Itdoesnotsaythattheoutlierisimpossible.Havingasmallprobabilitydoes notexcludethepossibility.Bycontrast,ifyousaythatsomethingwillsurelynothappenyou mean that there is not even a small probability. The former is a weaker statement than the latter. Therefore, even though we establish a three standard deviation envelope, there are pointsfallingoutsidetheenvelope.AsN grows,thechanceofhavingabadoutlierbecomes smaller. Therefore, the greater the N, the smaller the chance we will get an outlier. If the random variables X are i.i.d., the above phenomenon is universal. Below is an n example of the Poisson case. Practice Exercise 6.8. Let X ∼ Poisson(λ). Define the sample average as X = n N 1 (cid:80)N X . Find the mean and variance of X . N n=1 n N Solution. Since X is Poisson, we know that E[X ]=λ and Var[X ]=λ. So n n n N N 1 (cid:88) 1 (cid:88) E[X ]= E[X ]= λ=λ, N N n N n=1 n=1 N N 1 (cid:88) 1 (cid:88) λ Var[X ]= Var[X ]= λ= . N N2 n N2 N n=1 n=1 Therefore, as N →∞, the variance Var[X ]→0. N 6.3.2 Weak law of large numbers (WLLN) The analysis of Figure 6.12 shows us something important, namely that the convergence in a probabilistic way is different from that in a deterministic way. We now describe one fundamental result related to probabilistic convergence, known as the weak law of large numbers. Theorem 6.16 (Weak law of large numbers). Let X ,...,X be a set of i.i.d. ran- 1 N domvariableswithmeanµandvarianceσ2.AssumeE[X2]<∞.LetX = 1 (cid:80)N X . N N n=1 n Then for any ε>0, (cid:20) (cid:21) lim P |X −µ|>ε =0. (6.30) N N→∞ Proof. By Chebyshev’s inequality, P(cid:2) |X −µ|>ε(cid:3) ≤ Var[X N] = Var[X n] . N ε2 Nε2 Therefore, setting N →∞ we have lim P(cid:2) |X −µ|>ε(cid:3) = lim Var[X n] =0. N→∞ N N→∞ Nε2 (cid:3) 354
6.3. LAW OF LARGE NUMBERS Example 6.10. Consider a set of i.i.d. random variables X ,...,X where 1 N X ∼Gaussian(µ,σ2). n Verify that the sample average X = 1 (cid:80)N X follows the weak law of large num- N N n=1 n bers. Solution: Since X is a Gaussian, the sample average X is also a Gaussian: n N (cid:18) σ2(cid:19) X ∼Gaussian µ, . N N Consider the probability P(cid:2) |X −µ|>ε(cid:3) for each N: N (cid:20) (cid:21) δ d =efP |X −µ|>ε N N (cid:20) (cid:21) (cid:20) (cid:21) =P X −µ>ε +P X −µ<−ε N N (cid:32) √ (cid:33) (cid:32) √ (cid:33) ε N ε N =1−Φ +Φ − σ σ (cid:32) √ (cid:33) ε N =2Φ − . σ If we set σ =1 and ε=0.1, then (cid:18) (cid:19) (cid:32) √ (cid:33) 0.1·1 0.1· 5 δ =2Φ − =0.9203, δ =2Φ − =0.8231, 1 1 5 1 (cid:32) √ (cid:33) (cid:32) √ (cid:33) 0.1· 10 0.1· 100 δ =2Φ − =0.7518, δ =2Φ − =0.3173, 10 1 100 1 (cid:32) √ (cid:33) 0.1· 1000 δ =2Φ − =0.0016. 1000 1 As you can see, the the sequence δ ,δ ,...,δ ,... rapidly converges to 0 as N grows. 1 2 N In fact, since Φ(z) is a increasing function for z <0 with Φ(−∞)=0, it follows that (cid:20) (cid:21) (cid:32) √ (cid:33) ε N lim P |X −µ|>ε = lim 2Φ − =0. N→∞ N N→∞ σ The weak law of large numbers is portrayed graphically in Figure 6.13. In this figure we draw several PDFs of the sample average X . The shapes of the PDFs are getting N narrower as the variance of the random variable shrinks. Since the PDFs become narrower, the probability P[|X −µ| > ε] becomes more unlikely. At the limit when N → ∞, the N probabilityvanishes.Theweaklawoflargenumbersassertsthatthishappensforanysetof i.i.d.randomvariables.Itsaysthatthesequenceofprobabilityvaluesδ d =efP[|X −µ|>ε] N N 355
CHAPTER 6. SAMPLE STATISTICS will converge to zero. Figure 6.13: The weak law of large numbers states that as N increases, the variance of the sample average X shrinks. As a result, the probability P[|X −µ| > ε] decreases and eventually vanishes. N N Note that the convergence here is that of the sequence of probabilities P[|X −µ|>ε], which is just N a sequence of numbers. What is the weak law of large numbers? Let X be the sample average of i.i.d. random variables X ,...,X . N 1 N (cid:20) (cid:21) lim P |X −µ|>ε =0. (6.31) N N→∞ • For details, see Theorem 6.16. • The WLLN concerns the sequence of probability values δ =P[|X −µ|>ε]. N N • The probabilities converge to zero as N grows. • It is weak because having a small probability does not exclude the possibility of happening. 6.3.3 Convergence in probability The example above tells us that in order to show convergence, we need to first compute the probability δ of each event and then take the limit of the sequence, e.g., the one shown in n the table below: δ δ δ δ δ δ 1 5 10 100 1000 10000 0.9203 0.8231 0.7518 0.3173 0.0016 1.5240×10−23 Therefore, the convergence is the convergence of the probability. Since {δ ,δ ,...} is a 1 2 sequenceofrealnumbers(between0and1),anyconvergenceresultsforrealnumbersapply here. Note that the convergence controls only the probabilities. Probability means chance. Therefore, having the limit converging to zero only means that the chance of happening is becoming smaller and smaller. However, at any N, there is still a chance that some bad event can happen. 356
6.3. LAW OF LARGE NUMBERS Whatdowemeanbyabadevent?AssumethatX arefaircoins.Thesampleaverage n X = (1/N)(cid:80)N X is more or less equal to 1/2 as N grows. However, even if N is a N n=1 n largenumber,sayN =1000,wearestillnotcertainthatthesampleaverageisexactly1/2. It is possible, though very unlikely, that we obtain 1000 heads or 1000 tails (so that the sample average is “1” or “0”). The bottom line is: Having a probability converging to zero only means that for any tolerance level we can always find an N large enough so that the probability is smaller than that tolerance. The type of convergence described by the weak law of large numbers is known as the convergence in probability. Definition 6.6. A sequence of random variables A ,...,A converges in probability 1 N to a deterministic number α if for every ε>0, lim P[|A −α|>ε]=0. (6.32) N N→∞ p We write A →α to denote convergence in probability. N The following two examples illustrate how to prove convergence in probability. Example 6.11. Let X ,...,X be i.i.d. random variables with X ∼ Uniform(0,1). 1 N n Define A =min(X ,...,X ). Show that A converges in probability to zero. N 1 N N Solution. (Without determining the PDF of A , we notice that as N increases, the N value of A will likely decrease. Therefore, we should expect A to converge to zero.) N N Pick an ε>0. It follows that P[|A −0|≥ε]=P[min(X ,...,X )≥ε], because X ≥0 N 1 N n =P[X ≥εand ··· andX ≥ε] 1 N =P(cid:0) X ≥ε(cid:1) ···P(cid:0) X ≥ε(cid:1) =(1−ε)N. 1 N Setting the limit of N →∞, we conclude that lim P[|A −0|≥ε]= lim (1−ε)N =0. N N→∞ N→∞ Therefore, A converges to zero in probability. N Practice Exercise 6.9. Let X ∼ Exponential(1). By evaluating the CDF, we know thatP[X ≥x]=e−x.LetA =X/N.ProvethatA convergestozeroinprobability. N N Solution. For any ε>0, P[|A −0|≥ε]=P[A ≥ε] N N =P[X ≥Nε] =e−Nε. 357
CHAPTER 6. SAMPLE STATISTICS Putting N →∞ on both sides of the equation gives us lim P[|A −0|≥ε]= lim e−Nε =0. N N→∞ N→∞ Thus, A converges to zero in probability. N Example 6.12.ConstructanexamplesuchthatA convergesinprobabilitytosome- N thing, but E[A ] does not converge to the same thing. N Solution. Consider a sequence of random variables A such that N  1− 1, α=0,  N P[A =α]= 1, α=N2, N N 0, otherwise. The PDF of the random variable A is shown in Figure 6.14. N Figure 6.14: Probability density function of the random variable A . N We first show that A converges in probability to zero. Let ε > 0 be a fixed N constant. Since ε>0, 1 P[A ≥ε]= N N √ for any N > ε. Therefore, we have that lim P[|A −0|≥ε]= lim P[A ≥ε] N N N→∞ N→∞ 1 = lim =0. N→∞N Hence, A converges to 0 in probability. N However, E[A ] does not converge to zero, because N (cid:18) (cid:19) 1 1 E[A ]=0· 1− +N2· N N N =N. So E[A ] goes to infinity as N grows. N 358
6.3. LAW OF LARGE NUMBERS 6.3.4 Can we prove WLLN using Chernoff’s bound? The following discussion of using Chernoff’s bound to prove WLLN can be skipped if this is your first time reading the book. InprovingWLLNweuseChebyshev’sinequality.CanweuseChernoff’sinequality(or Hoeffding’s)toprovetheresult?Yes,wecanusethem.However,noticethatthetaskhereis to prove convergence, not to find the best convergence. Finding the best convergence means findingthefastestdecayrateoftheprobabilitysequence.Chernoff’sbound(andHoeffding’s inequality)offersabetterdecayrate.However,Chernoff’sboundneedstobecustomizedfor individual random variables. For example, Chernoff’s bound for Gaussian is different from Chernoff’s bound for exponential. This result makes Chebyshev the most convenient bound because it only requires the variance to be bounded. WhatifweinsistonusingChernoff’sboundinprovingtheWLLN?Wecandothatfor specific random variables. Let’s consider two examples. The first example is the Gaussian randomvariablewhereX ∼N(0,σ2).WeknowthatX ∼N(0,σ2/N).Chernoff’sbound n N shows that P(cid:2) |X −µ|>ε(cid:3) ≤2exp(cid:26) −ε2N(cid:27) , N 2σ2 Taking the limit on both sides, we have lim P(cid:2) |X −µ|>ε(cid:3) = lim 2exp(cid:26) −ε2N(cid:27) =0. N→∞ N N→∞ 2σ2 Note that the rate of convergence here is exponential. The rate of convergence offered by Chebyshev is only linear. Of course, you may argue that since X is Gaussian we have n closed-form expressions about the probability, so we do not need Chernoff’s bound. This is alegitimatepoint,andsohereisanexamplewherewedonothaveaclosed-formexpression for the probability. Consider a sequence of arbitrary i.i.d. random variables X ,...,X with 0≤X ≤1. 1 N n Then Hoeffding’s inequality tells us that P(cid:2) |X −µ|>ε(cid:3) ≤2exp(cid:8) −2ε2N(cid:9) . N Taking the limit on both sides, we have lim P(cid:2) |X −µ|>ε(cid:3) = lim 2exp(cid:8) −2ε2N(cid:9) =0. N N→∞ N→∞ Again, we obtain a WLLN result, this time for i.i.d. random variables X ,...,X with 1 N 0≤X ≤1. n As you can see from these two examples, WLLN can be proved in multiple ways depending on how general the random variables need to be. End of the discussions. 359
CHAPTER 6. SAMPLE STATISTICS 6.3.5 Does the weak law of large numbers always hold? The following discussion of the failure of the weak law of large numbers can be skipped if this is your first time reading the book. The weak law of large numbers does not always hold. Recall that when we prove the weak law of large numbers using Chebyshev’s inequality, we implicitly require that the variance Var[X ] is finite. (Look at the condition that E[X2]<∞.) Thus for distributions N whose variance is unbounded, Chebyshev’s inequality does not hold. One example is the Cauchy distribution. The PDF of a Cauchy distribution is γ f (x)= , X π(γ2+x2) where γ is a parameter. Letting γ =1, (cid:90) ∞ x2 1 (cid:90) ∞ 1 E[X2]= dx= 1− dx π(1+x2) π 1+x2 −∞ −∞ = 1 (cid:90) ∞ dx− 1 (cid:90) ∞ 1 dx= 1(cid:20) x−tan−1(x)(cid:21)(cid:12) (cid:12) (cid:12)∞ =∞. π π 1+x2 π (cid:12) −∞ −∞ x=−∞ Since the second moment is unbounded, the variance of X will also be unbounded. A perceptive reader may observe that even if E[X2] is unbounded, it does not mean that the tail probability is unbounded. This is correct. However, for Cauchy distributions, we can show that the sample average X does not converge to the mean when N → ∞ N (and so the WLLN fails). To see this, we note that 1 ↔e−|ω|. π(1+x2) So for the sample average X = 1 (cid:80)N X , the characteristic function is N N n=1 n N E[e−jωXN]=E[e−j Nω(cid:80)N n=1Xn]= (cid:89) E[e−j NωXn]=(cid:104) e−| Nω|(cid:105)N =e−|ω|, n=1 which remains a Cauchy distribution with γ =1. Therefore, we have that (cid:90) ε 1 P[|X |≤ε]= dx N π(1+x2) −∞ (cid:90) 0 1 (cid:90) ε 1 1 1 = dx+ dx= + tan−1(ε). π(1+x2) π(1+x2) 2 π −∞ 0 Thus no matter how many samples we have, P[|X | ≤ ε] will never converge to 1 (so N P[|X |>ε] will never converge to 0). Therefore, WLLN does not hold. N End of the discussion. 360
6.3. LAW OF LARGE NUMBERS 6.3.6 Strong law of large numbers Since there is a “weak” law of large numbers, you will not be surprised to learn that there is a strong law of large numbers. The strong law is more restrictive than the weak law. Any sequence satisfying the strong law will satisfy the weak law, but not vice versa. Since the strong law is “stronger”, the proof is more involved. Theorem 6.17 (Strong law of large numbers). Let X ,...,X be a sequence of 1 N i.i.d. random variables with common mean µ and variance σ2. Assume E[X4] < ∞. Let X = 1 (cid:80)N X be the sample average. Then N N n=1 n (cid:104) (cid:105) P lim X =µ =1. (6.33) N N→∞ The strong law flips the order of limit and probability. Asyou can see,the difference betweenthestronglawandtheweaklawistheorderofthelimitandtheprobability.Inthe weak law, the limit is outside the probability, whereas, in the strong law, the limit is inside the probability. This switch in order makes the interpretation of the result fundamentally different.Inthefinalanalysis,theweaklawconcernsthelimitofasequenceofprobabilities (which are just real numbers between 0 and 1). However, the strong law concerns the limit ofasequenceofrandomvariables.Thestronglawanswersthequestion,whatisthelimiting object of the sample average as N grows? The strong law concerns the limiting object, not a sequence of numbers. What is the “limiting object”? If we denote X as the sample average using N samples, then N we know that X is a random variable, X is a random variable, and all X ’s are random 1 2 n variables.Sowehaveasequenceofrandomvariables.AsN goestoinfinity,wecanaskabout the limiting object lim X . However, even without any deep analysis, you should be N→∞ N able to see that lim X is another random variable. The strong law says that this N→∞ N limiting object will “successfully” become a deterministic number µ, after a finite number of “failures”. The strong law asserts that there are a finite number of failures. Let us explain “success” and “failure”. X is a random variable, so it fluctuates. However, as N goes to N infinity, the strong law says that the number of times where X (cid:54)= µ will be zero. That N is, there is a finite number of times where X (cid:54)= µ (i.e., fail), and afterward, you will be N perfectly fine (i.e., success). Yes, perfectly fine means 100%. The weak law only guarantees 99.99%. A good example for differentiating the weak law and the strong law is an electronic dictionary that improves itself every time you use it. The weak law says that if you use the dictionary for a long period, the probability of making an error will become small. You will still get an error once in a while, but the probability is very small. This is a 99.99% guarantee, and it is the weak law. The strong law says that the number of failures is finite. After you have gone through this finite number of failures, you will be completely free of error. This is a 100% guarantee by the strong law. When will you hit this magical number? Thestronglawdoesnotsaywhen;itonlyassertstheexistenceofthisnumber.However,this existence is already good enough in many ways. It gives a certificate of assurance, whereas the weak law still has uncertainty. 361
CHAPTER 6. SAMPLE STATISTICS Strong law (cid:54)= deterministic. If the strong law offers a 100% guarantee, does it mean that it is a deterministic guarantee? No, the strong law is still a probabilistic statement because we are still using P[·] to measure an event. The event can include measure-zero subsets,andthemeasure-zerosubsetscanbehuge.Forexample,thesetofrationalnumbers on the real line is a measure-zero set when measuring the probability using an integration. The strong law does not handle those measure-zero subsets. 6.3.7 Almost sure convergence The discussion below can be skipped if this is your first time reading the book. The type of convergence used by the strong law of large numbers is the almost sure convergence. It is defined formally as follows. Definition 6.7. A sequence of random variables A ,...,A converges almost surely 1 N to α if (cid:104) (cid:105) P lim A =α =1. (6.34) N N→∞ a.s. We write A → α to denote almost sure convergence. N Toprovealmostsureconvergence,oneneedstoshowthatthesequenceA willdemonstrate N A (cid:54)=α for a finite number of times. Afterward, A needs to demonstrate A =α. N N N Example 6.13.a Construct a sequence of events that converges almost surely. Solution. Let X ,...,X be i.i.d. random variables such that X ∼ Uniform(0,1). 1 N n Define A = min(X ,...,X ). Since A is nonincreasing and is bounded below by N 1 N N zero, it must have a limit. Let us call this limit def A = lim A . N N→∞ Then we can show that P[A≥(cid:15)]=P[min(X ,X ,...)≥(cid:15)] 1 2 (a) ≤ P[min(X ,X ,...,X )≥(cid:15)] 1 2 N ( =b)P[X ≥(cid:15)andX ≥(cid:15)and ··· andX ≥(cid:15)] 1 2 N =(1−(cid:15))N, where (a) holds because there are more elements in (X ,X ,...) than in 1 2 (X ,X ,...,X ). Therefore, the minimum value of the former is less than the mini- 1 2 N mum value of the latter. (b) holds because if min(X ,X ,...,X ) ≥ (cid:15), then X ≥ (cid:15) 1 2 N n for all n. 362
6.3. LAW OF LARGE NUMBERS Since P[A≥(cid:15)]≤(1−(cid:15))N for any N, the statement still holds as N →∞. Thus, P[A≥(cid:15)]≤ lim (1−(cid:15))N =0. N→∞ This shows P[A≥(cid:15)]=0 for any positive (cid:15). So P[A>(cid:15)]=0, and hence P[A=0]=1. Since A is the limit of A , we conclude that N (cid:104) (cid:105) P lim A =0 =P[A=0]=1. N N→∞ So A converges to 0 almost surely. N aThisexampleismodifiedfromBertsekasandTsitsiklis,IntroductiontoProbability,Chapter5.5. Example 6.14.a Constructanexamplewhereasequenceofeventsconvergesinprob- ability but does not converge almost surely. Solution.Consideradiscretetimearrivalprocess.Thesetoftimesispartitionedinto consecutive intervals of the form I ={2,3}, 1 I ={4,5,6,7}, 2 I ={8,9,10,...,15}, 3 . . . I ={2k,2k+1,...,2k+1−1}. k Therefore, the length of each interval is |I |=2, |I |=4, ..., |I |=2k. 1 2 k During each interval, there is exactly one arrival. Define Y as a binary random n variable such that for every n∈I , k (cid:40) 1, with probability 1 , Y = |Ik| n 0, with probability 1− 1 . |Ik| For example, if n∈{2,3}, then P[Y =1]= 1. If n∈{4,5,6,7}, then P[Y =1]= 1. n 2 n 4 In general, we have that 1 1 lim P[Y =1]= lim = lim =0, n→∞ n n→∞|I k| n→∞2k and hence 1 lim P[Y =0]= lim 1− =1. n→∞ n n→∞ 2k Therefore, Y converges to 0 in probability. n However, when we carry out the experiment, there is exactly one arrival per interval according to the problem conditions. Since we have an infinite number of 363
CHAPTER 6. SAMPLE STATISTICS intervals I ,I ,..., we will have an infinite number of arrivals in total. As a result, 1 2 Y = 1 for infinitely many times. We do not know which Y will equal 1 and which n n Y will equal to 0. However, we know that there are infinitely many Y that are equal n n to 1. Therefore, in the sequence Y ,Y ,...,Y ,..., we must have that the tail of the 1 2 n sequence is 1. (If Y stops being 1 after some n, then we will not have an infinite n number of arrivals in total.) Since Y =1 when n is large enough, it follows that n (cid:104) (cid:105) P lim Y =1 =1. n n→∞ Equivalently, we can say that the sequence Y will never take the value 0 when n is n large enough. Thus, (cid:104) (cid:105) P lim Y =0 =0. n n→∞ Therefore, Y does not converge to 0 almost surely. n aThisexampleismodifiedfromBertsekasandTsitsiklis,IntroductiontoProbability,Chapter5.5. End of the discussions. 6.3.8 Proof of the strong law of large numbers Thestronglawoflargenumberscanbeprovedinseveralways.Wepresentaproofbasedon Bertsekas and Tsitsiklis, Introduction to Probability, Problems 5.16 and 5.17, which require a finite fourth moment E[X4] < ∞. An alternative proof that requires only E[X ] < ∞ is n n from Billingsley, Probability and Measure, Theorem 22.1. The proof of the strong law of large numbers is beyond the scope of this book. This section is optional. Lemma 6.4. Consider non-negative random variables X ,...,X . Assume that 1 N (cid:34) ∞ (cid:35) (cid:88) E X <∞. (6.35) n n=1 a.s. Then X → 0. n Proof. Let S = (cid:80)N X . Note that S is a random variable, and our assumption is that n=1 n E[S]<∞.Thus,wearguethatS <∞withprobability1.Ifnot,thenS willhaveapositive probability of being ∞. But if this happens, we will have E[S]=∞ because (by the law of total expectation): E[S]=E[S |S =infinite]P[S =infinite]+E[S |S =finite]P[S =finite]. (cid:124) (cid:123)(cid:122) (cid:125) =∞ 364
6.3. LAW OF LARGE NUMBERS Now,sinceSisfinite,thesequence{X ,...,X ,...}mustconvergetozero.Otherwise, 1 N if X is converging to some constants c>0, then summing the tail of the sequence (which n contains infinitely many terms) gives infinity: S =X +···+X +···+. 1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =finite =infinite Since the probability of S being finite is 1, it follows that {X ,...,X } is converging 1 N to zero with probability 1. (cid:3) Theorem 6.18 (Strong law of large numbers). Let X ,...,X be a sequence of 1 N i.i.d. random variables with common mean µ and variance σ2. Assume E[X4] < ∞. n Let X = 1 (cid:80)N X be the sample average. Then N N n=1 n (cid:104) (cid:105) P lim X =µ =1. (6.36) N N→∞ Proof. We first prove the case where E[X ]=0. To establish that X →0 with probabil- n N ity 1, we use the lemma to show that (cid:34) ∞ (cid:35) (cid:88) E |X | <∞. N N=1 But to show E[(cid:80)∞ |X |] < ∞, we note that |x| ≤ 1+x4. Therefore, E[(cid:80)∞ |X |] ≤ N=1 N N=1 N 1+E[(cid:80)∞ X4 ], and hence we just need to show that N=1 N (cid:34) ∞ (cid:35) E (cid:88) X4 <∞. N N=1 Let us expand the term E[X4 ] as follows: N N N N N E[X4 ]= 1 (cid:88) (cid:88) (cid:88) (cid:88) E[X X X X ]. N N4 n1 n2 n3 n4 n1=1n2=1n3=1n4=1 There are five possibilities for E[X X X X ]: n1 n2 n3 n4 • All indices are different. Then E[X X X X ]=E[X ]E[X ]E[X ]E[X ]=0·0·0·0=0. n1 n2 n3 n4 n1 n2 n3 n4 • One index is different from other three indices. For example, if n is different from 1 n ,n ,n , then 2 3 4 E[X X X X ]=E[X ]E[X X X ]=0·E[X X X ]=0. n1 n2 n3 n4 n1 n2 n3 n4 n2 n3 n4 • Two indices are identical. For example, if n =n , and n =n , then 1 3 2 4 E[X X X X ]=E[X X ]E[X X ]=E[X2 X2 ]. n1 n2 n3 n4 n1 n3 n2 n4 n1 n2 There are altogether 3N(N −1) of these cases: N(N −1) comes from choosing N followedbychoosingN−1,and3accountsforn =n (cid:54)=n =n ,n =n (cid:54)=n =n , 1 2 3 4 1 3 2 4 and n =n (cid:54)=n =n . 1 4 2 3 365
CHAPTER 6. SAMPLE STATISTICS • Two indices are identical, and two indices are different. For example, if n = n but 1 3 n and n are different. Then 2 4 E[X X X X ]=E[X X ]E[X ]E[X ] n1 n2 n3 n4 n1 n3 n2 n4 =E[X2 ]·0·0=0. n1 • All indices are identical. If n =n =n =n , then 1 2 3 4 E[X X X X ]=E[X4 ]. n1 n2 n3 n4 n1 There are altogether N cases of this. Therefore, it follows that E[X4 ]= NE[X 14]+3N(N −1)E[X 12X 22] . N N4 Since xy ≤(x2+y2)/2, it follows that E[X2X2]≤E[(X2)2+(X2)2]/2 1 2 1 2 =E[X4+X4]/2 1 2 =E[X4]. 1 Substituting into the previous result, E[X4 ]≤ NE[X 14]+3N(N −1)E[X 14] N N4 3N2 ≤ E[X4] N4 1 3 = E[X4]. N2 1 Now, let us complete the proof. (cid:34) ∞ (cid:35) (cid:34) ∞ (cid:35) E (cid:88) X4 ≤E (cid:88) 3 E[X4] <∞, N N2 1 N=1 N=1 because (cid:80)∞ (1/N2) is the Bassel problem with a solution that (cid:80)∞ (1/N2) = π2/6. N=1 N=1 Consequently,wehaveshownthatE(cid:104) (cid:80)∞ X4 (cid:105) <∞,whichimpliesE(cid:2)(cid:80)∞ |X |(cid:3) <∞. N=1 N N=1 N Then,bythelemma,wehaveX convergingto0withprobability1,whichprovestheresult. N IfE[X ]=µ,thenjustreplaceX withY =X −µintheabovearguments.Thenwe n n n n can show that Y converges to 0 with probability 1, which is equivalent to X converging N N to µ with probability 1. End of the proof of strong law of large numbers. 366
6.4. CENTRAL LIMIT THEOREM 6.4 Central Limit Theorem The law of large numbers tells us the mean of the sample average X = (1/N)(cid:80)N X . N n=1 n However, if you recall our experiment of throwing N dice and inspecting the PDF of the sum of the numbers, you may remember that the convolution of an infinite number of uniform distributions gives us a Gaussian distribution. For example, we show a sequence of experiments in Figure 6.15. In each experiment, we throw N dice and count the sum. Therefore, if each face of the die is denoted as X , then the sum is X +···+X . We plot n 1 N the PDF of the sum. As you can see in the figure, X +···+X converges to a Gaussian. 1 N This phenomenon is explained by the Central Limit Theorem (CLT). Figure6.15:PictorialillustrationoftheCentralLimitTheorem.Supposewethrowadieandrecordthe face. [Left] If we only have one die, then the distribution of the face is uniform. [Middle] If we throw two dice, the distribution is the convolution of two uniform distributions. This will give us a triangle distribution.[Right]Ifwethrowfivedice,thedistributionisbecomingsimilartoaGaussian.TheCentral Limit Theorem says that as N goes to infinity, the distribution of the sum will converge to a Gaussian. What does the Central Limit Theorem say? Let X be the sample average, and let √ (cid:16) (cid:17) N Z = N XN−µ be the normalized variable. The Central Limit Theorem is as follows: N σ Central Limit Theorem: The CDF of Z is converging pointwise to the CDF of Gaussian(0,1). N Note that we are very careful here. We are not saying that the PDF of Z is converging to N the PDF of a Gaussian, nor are we saying that the random variable Z is converging to a N Gaussian random variable. We are only saying that the values of the CDF are converging pointwise. The difference is subtle but important. To understand the difficulty and the core ideas, we first present the concept of conver- gence in distribution. 367
CHAPTER 6. SAMPLE STATISTICS 6.4.1 Convergence in distribution Definition 6.8. Let Z ,...,Z be random variables with CDFs F ,...,F respec- 1 N Z1 ZN tively. We say that a sequence of Z ,...,Z converges in distribution to a random 1 N variable Z with CDF F if Z lim F (z)=F (z), (6.37) N→∞ ZN Z d for every continuous point z of F . We write Z → Z to denote convergence in Z N distribution. Thisdefinitioninvolvesmanyconcepts,whichwewilldiscussonebyone.However,the definition can be summarized in a nutshell as follows. Convergence in distribution = values of the CDF converge. Example 1. (Bernoulli) Consider flipping a fair coin N times. Denote each coin flip as a Bernoulli random variable X ∼Bernoulli(p), where n=1,2,...,N. Define Z as the sum n N of N Bernoulli random variables, so that N (cid:88) Z = X . N n n=1 We know that the resulting random variable Z is a binomial random variable with mean N Np and variance Np(1−p). Let us plot the PDF f (z) as shown in Figure 6.16. ZN Figure 6.16: Convergence in distribution. The convergence in distribution concerns the convergence of the values of the CDF (not the PDF). In this figure, we let Z = X +···+X , where X is a N 1 N N Bernoulli random variable with parameter p. Since a sum of Bernoulli random variables is a binomial, Z is a binomial random variable with parameters (N,p). We plot the PDF of Z , which is a train of N N deltafunctions,andcompareitwiththeGaussianPDF.Observethattheerror,max |f (z)−f (z)|, z ZN Z doesnot convergeto0.ThePDFofZ isabinomial.Abinomialisalwaysabinomial.Itwillnotturn N into a Gaussian. The first thing we notice in the figure is that as N increases, the PDF of the binomial has an envelope that is “very Gaussian”. So one temptation is to say that the random 368
6.4. CENTRAL LIMIT THEOREM variable Z is converging to another random variable Z. In addition, we would think that N the PDFs converge in the sense that for all z, (cid:18) N(cid:19) 1 (cid:26) (z−µ)2(cid:27) f (z)= pz(1−p)N−z −→ f (z)= √ exp − , ZN z Z 2πσ2 2σ2 where µ=Np and σ2 =Np(1−p). Unfortunately this argument does not work, because f (z) is continuous but f (z) Z ZN is discrete. The sample space of Z and the sample space of Z are completely different. In N fact, if we write f as an impulse train, we observe that ZN N (cid:18) (cid:19) (cid:88) N f (z)= pi(1−p)N−iδ(z−i). ZN i i=0 Clearly, no matter how big the N is, the difference |f (z)−f (z)| will never go to zero ZN Z for non-integer values of z. Mathematically, we can show that max |f (z)−f (z)|(cid:54)−→0, z ZN Z asN →∞.Z isabinomialrandomvariableregardlessofN.ItwillnotbecomeaGaussian. N If f (z) is not converging to a Gaussian PDF, how do we explain the convergence? ZN The answer is to look at the CDF. For discrete PDFs such as a binomial random variable, the CDF is a staircase function. What we can show is that (cid:88)z (cid:18) N(cid:19) (cid:90) z 1 (cid:26) (t−µ)2(cid:27) F (z)= pi(1−p)N−i −→ F (z)= √ exp − dt. ZN i Z 2πσ2 2σ2 i=0 −∞ The difference between the PDF convergence and the CDF convergence is that the PDF doesnotallowameaningful“distance”betweenadiscretefunctionandcontinuousfunction. ForCDF,thedistanceiswelldefinedbytakingthedifferencebetweenthestaircasefunction and the continuous function. For example, we can compute |F (z)−F (z)|, for all continuous points z of F , ZN Z Z and show that max|F (z)−F (z)|−→0. z ZN Z We need to pay attention to the set of z’s. We do not evaluate all z’s but only the z’s that are continuous points of F . If F is Gaussian, this does not matter because all z’s Z Z are continuous. However, for CDFs containing discontinuous points, our definition of con- vergence in distribution will ignore these discontinuous points because they have a measure zero. Example 2. (Poisson) Consider X ∼Poisson(λ), and consider X ,...,X . Define Z = n 1 N N (cid:80)N X .ItfollowsthatE[Z ]=(cid:80)N E[X ]=NλandVar[Z ]=(cid:80)N Var[X ]=Nλ. n=1 n N n=1 n N n=1 n Moreover, we know that the sum of Poissons remains a Poisson. Therefore, the PDF of Z N is (cid:88)∞ (Nλ)k 1 (cid:26) (z−µ)2(cid:27) f (z)= e−Nλδ(z−k) and f (z)= √ exp − , ZN k! Z 2πσ2 2σ2 k=0 369
CHAPTER 6. SAMPLE STATISTICS Figure 6.17: Convergence in distribution. This is the same as Figure 6.16, but this time we plot the CDFofZ .TheCDFisastaircasefunction.WecompareitwiththeGaussianCDF.Observethatthe N error,max |F (z)−F (z)|,convergestozeroasN grows.Convergenceindistributionsaysthatthe z ZN Z sequence of CDFs F (z) will converge to the limiting CDF F (z), at all continuous points of F (z). ZN Z Z where µ=Nλ and σ2 =Nλ. Again, f does not converge to f . However, if we compare ZN Z the CDF, we can see from Figure 6.18 that the CDF of the Poisson is becoming better approximated by the Gaussian. Interpreting “convergence in distribution”. After seeing two examples, you should now have some idea of what “convergence in distribution” means. This concept applies to the CDFs. When we write lim F (z)=F (z), (6.38) N→∞ ZN Z we mean that F (z) is converging to the value F (z), and this relationship holds for all ZN Z the continuous z’s of F . It does not say that the random variable Z is becoming another Z N random variable Z. d Z −→Z is equivalent to lim F (z)=F (z). N N→∞ ZN Z Example 3. (Exponential) So far, we have studied the sum of discrete random variables. Now, let’s take a look at continuous random variables. Consider X ∼Exponential(λ), and n let X ,...,X be i.i.d. copies. Define Z =(cid:80)N X . Then E[Z ]=(cid:80)N E[X ]=N/λ 1 N N n=1 n N n=1 n andVar[Z ]= N.HowaboutthePDFofZ ?Usingthecharacteristicfunctions,weknow N λ2 N that λ f (x)=λe−λx ←F → Φ (jω)= . Xn Xn λ+jω Therefore, the product is (cid:89)N λN λN (N −1)! Φ (jω)= Φ (jω)= = × ZN Xn (λ+jω)N (λ+jω)N (N −1)! n=1 λN (N −1)! λN = · ←F → zN−1e−λz =f (z). (N −1)! (λ+jω)N (N −1)! ZN 370
6.4. CENTRAL LIMIT THEOREM 0.2 0.15 0.06 Poisson Poisson Poisson Gaussian Gaussian Gaussian 0.15 0.1 0.04 0.1 0.05 0.02 0.05 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 1 1 1 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 Poisson 0.2 Poisson 0.2 Poisson Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 (a) N =4 (b) N =10 (c) N =50 Figure 6.18: Convergence in distribution for a sum of Poisson random variables. Here we assume that X ,...,X arei.i.d.Poissonwithaparameterλ.WeletZ =(cid:80)N X bethesum,andcomputethe 1 N N n=1 n correspondingPDF(toprow)andCDFs(bottomrow).Justaswiththebinomialexample,thePDFsof the Poisson do not converge but the CDFs of the Poisson converge to the CDF of a Gaussian. This resulting PDF f (z) = λN zN−1e−λz is known as the Erlang distribution. The ZN (N−1)! CDF of the Erlang distribution is (cid:90) z F (z)= f (t)dt ZN ZN −∞ (cid:90) z λN = tN−1e−λt dt (N −1)! 0 =Gamma function(z,N), where the last integral is known as the incomplete gamma function, evaluated at z. Givenallthese,wecannowcomparethePDFandtheCDFofZ versusZ.Figure6.19 N showsthePDFsandtheCDFsofZ forvariousN values.Inthisexperimentwesetλ=1. N As we can see from the experiment, the Erlang distribution’s PDF and CDF converge to a Gaussian. In fact, for continuous random variables such as exponential random variables, we indeed have the random variable Z converging to the random variable Z. This is quite N different from discrete random variables, where Z does not converge to Z but only F N ZN converges to F . Z d p Is −→ stronger than −→? Convergence in distribution is actually weaker than con- vergence in probability. Consider a continuous random variable X with a symmetric PDF f (x) such that f (x) = f (−x). It holds that the PDF of −X has the same PDF. If X X X we define the sequence Z = X if N is odd and Z = −X if N is even, and let Z = X, N N then F (z) = F (z) for every z because the PDF of X and −X are identical. There- ZN Z d p fore, Z → Z. However, Z (cid:54)→ Z because Z oscillates between the random variables X N N N and −X. These two random variables are different (although they have the same CDF) because P[X =−X]=P[{ω :X(ω)=−X(ω)}]=P[{ω :X(ω)=0}]=0. 371
CHAPTER 6. SAMPLE STATISTICS 0.25 0.15 0.06 0.2 0.1 0.04 0.15 0.1 0.05 0.02 0.05 Sum of Exponential Sum of Exponential Sum of Exponential Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 1 1 1 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 Sum of Exponential 0.2 Sum of Exponential 0.2 Sum of Exponential Gaussian Gaussian Gaussian 0 0 0 0 2 4 6 8 0 5 10 15 20 0 20 40 60 80 100 (a) N =4 (b) N =10 (c) N =50 Figure 6.19: Convergence in distribution for a sum of exponential random variables. Here we assume that X ,...,X are i.i.d. exponentials with a parameter λ. We define Z = (cid:80)N X be the sum. 1 N N n=1 n It is known that the sum of exponentials is an Erlang. We compute the corresponding PDF (top row) andCDFs(bottomrow).Unliketheprevioustwoexamples,inthisexampleweseethatbothPDFsand CDFs of the Erlang distribution are converging to a Gaussian. 6.4.2 Central Limit Theorem Theorem 6.19 (Central Limit Theorem). Let X ,...,X be i.i.d. random variables 1 N of mean E[X ] = µ and variance Var[X ] = σ2. Also, assume that E[|X3|]<∞. Let n n √ (cid:16) (cid:17)n X =(1/N)(cid:80)N X be the sample average, and let Z = N XN−µ . Then N n=1 n N σ lim F (z)=F (z), (6.39) N→∞ ZN Z where Z =Gaussian(0,1). In plain words, the Central Limit Theorem says that the sample average (which is a random variable) has a CDF converging to the CDF of a Gaussian. Therefore, if we want to evaluate probabilities associated with the sample average, we can approximate the probability by the probability of a Gaussian. As we discussed above, the Central Limit Theorem does not mean that the random variable Z is converging to a Gaussian random variable, nor does it mean that the PDF N of Z is converging to the PDF of a Gaussian. It only means that the CDF of Z is N N converging to the CDF of a Gaussian. Many people think that the Central Limit Theorem means “sample average converges to Gaussian”. This is incorrect for the above reasons. However, it is not completely wrong. For continuous random variables where both PDF and CDF are continuous, we will not run into situations where the PDF is a train of delta functions. In this case, convergence in CDF can be translated to convergence in PDF. The power of the Central Limit Theorem is that the result holds for any distribution of X ,...,X . That is, regardless of the distribution of X ,...,X , the CDF of X is 1 N 1 N N 372
6.4. CENTRAL LIMIT THEOREM approaching a Gaussian. Summary of the Central Limit Theorem • X ,...,X are i.i.d. random variables, with mean µ and variance σ2. They are 1 N not necessarily Gaussians. √ (cid:16) (cid:17) • DefinethesampleaverageasX =(1/N)(cid:80)N X ,andletZ = N X−µ . N n=1 n N σ d • The Central Limit Theorem says Z −→ Gaussian(0,1). Equivalently, the the- N orem says that NX −d →Gaussian(µ,σ2). N • So if we want to evaluate the probability of X ∈ A for some set A, we can N approximate the probability by evaluating the Gaussian: (cid:90) 1 (cid:26) (y−µ)2(cid:27) P[X ∈A]≈ exp − dy. N (cid:112) 2π(σ2/N) 2(σ2/N) A • CLT does not say that the PDF of X is becoming a Gaussian PDF. N • CLT only says that the CDF of X is becoming a Gaussian CDF. N If the set A is an interval, we can use the standard Gaussian CDF to compute the probability. Corollary 6.3. Let X ,...,X be i.i.d. random variables with mean µ and vari- 1 N ance σ2. Define the sample average as X =(1/N)(cid:80)N X . Then N n=1 n (cid:18)√ b−µ(cid:19) (cid:18)√ a−µ(cid:19) P[a≤X ≤b]≈Φ N −Φ N , (6.40) N σ σ where Φ(z)=(cid:82)z √1 e−x 22 dx is the CDF of the standard Gaussian. −∞ 2π Proof. By the Central Limit Theorem, we know that X −d →Gaussian(µ,σ2). Therefore, N N (cid:90) b 1 (cid:26) (y−µ)2(cid:27) P[a≤X ≤b]≈ exp − dy N (cid:112) 2π(σ2/N) 2(σ2/N) a √ =(cid:90) Nb− σµ √1 e−y 22 dy =Φ(cid:18)√ Nb−µ(cid:19) −Φ(cid:18)√ Na−µ(cid:19) . √ Na−µ 2π σ σ σ (cid:3) A graphical illustration of the CLT is shown in Figure 6.20, where we use a binomial randomvariable(which isthe sumof i.i.d.Bernoulli)as anexample. TheCLT does notsay thatthebinomialrandomvariableisbecomingaGaussian.Itonlysaysthattheprobability covered by the binomial can be approximated by the Gaussian. 373
CHAPTER 6. SAMPLE STATISTICS Figure6.20:TheCentralLimitTheoremsaysthatifwewanttoevaluatetheprobabilityP[a≤X ≤b], N where X = (1/N)(cid:80)N X is the sample average of i.i.d. random variables X ,...,X , we can N n=1 n 1 N approximate the probability by integrating the Gaussian PDF. The following proof of the Central Limit Theorem can be skipped if this is your first time reading the book. Proof of the Central Limit Theorem. We now give a “proof” of the Central Limit Theorem.Technicallyspeaking,thisproofdoesnotprovetheconvergenceoftheCDFasthe theorem claims; it only proves that the moment-generating function converges. The actual proof of the CDF convergence is based on the Berry-Esseen Theorem, which is beyond the scope of this book. However, what we prove below is still useful because it gives us some intuitionaboutwhyGaussianisthelimitingrandomvariableweshouldconsiderinthefirst place. √ (cid:16) (cid:17) Let Z = N XN−µ . It follows that E[Z ] = 0 and Var[Z ] = 1. Therefore, if we N σ N N canshowthatZ isconvergingtoastandardGaussianrandomvariableZ ∼Gaussian(0,1), N thenbythelineartransformationpropertyofGaussian,Y = √σ Z+µwillbeGaussian(µ,σ2/N). N Our proof is based on analyzing the moment-generating function of Z . In particular, N M ZN(s)d =efE[esZN]=E(cid:20) es√ N(cid:16)XN σ−µ(cid:17)(cid:21) = (cid:89)N E(cid:104) eσ√s N(Xn−µ)(cid:105) . n=1 Expanding the exponential term using the Taylor expansion (Chapter 1.2), N (cid:89) E(cid:104) eσ√s N(Xn−µ)(cid:105) n=1 = (cid:89)N E(cid:20) 1+ √s (X −µ)+ s2 (X −µ)2+O(cid:18) (X n− √µ)3(cid:19)(cid:21) σ N n 2σ2N n σ3N N n=1 = (cid:89)N (cid:20) 1+ √s E[X −µ]+ s2 E(cid:2) (X −µ)2(cid:3)(cid:21) =(cid:18) 1+ s2 (cid:19)N . σ N n 2σ2N n 2N n=1 374
6.4. CENTRAL LIMIT THEOREM (cid:16) (cid:17)N It remains to show that 1+ s2 →es2/2. If we can show that, we have shown that the 2N MGF of Z is also the MGF of Gaussian(0,1). To this end, we consider log(1+x). By the N Taylor approximation, we have that (cid:18) d (cid:19) (cid:18) d2 (cid:19) x2 log(1+x)≈log(1)+ logx| x+ logx| +O(x3). dx x=1 dx2 x=1 2 (cid:16) (cid:17) Therefore, we have log 1+ s2 ≈ s2 − s4 . As N →∞, the limit becomes 2N 2N 4N2 (cid:18) s2 (cid:19) s2 s4 s2 lim Nlog 1+ ≈ − lim = , N→∞ 2N 2 N→∞4N 2 and so taking the exponential on both sides yields lim N→∞(cid:16) 1+ 2s N2 (cid:17)N = es 22 . Therefore, we conclude that lim N→∞M ZN(s)=es 22 , and so Z N is converging to a Gaussian. (cid:3) Limitation of our proof. The limitation of our proof lies in the issue of whether the integration and the limit are interchangeable: (cid:26)(cid:90) (cid:27) lim M (s)= lim f (z)esz dz N→∞ ZN N→∞ ZN (cid:90) (cid:16) (cid:17) =? lim f (z) esz dz. N→∞ ZN If they were, then proving lim M (s)=M (s) is sufficient to claim f (z)→f (z). N→∞ ZN Z ZN Z However, we know that the latter is not true in general. For example, if f (z) is a train of ZN delta functions, then the limit and the integration are not interchangeable. Berry-Esseen Theorem. The formal way of proving the Central Limit Theorem is to prove the Berry-Esseen Theorem. The theorem states that (cid:12) (cid:12) (cid:12) (cid:12) β s zu ∈p R (cid:12) (cid:12)F ZN(z)−F Z(z)(cid:12) (cid:12)≤C σ3√ N, where β and C are universal constants. Here, you can more or less treat the supremum operator as the maximum. The left-hand side represents the worst-case error of the CDF F compared to the limiting CDF F . The right-hand side involves several constants C, ZN Z β, and σ, but they are fixed. As N goes to infinity, the right-hand side will converge to zero. Therefore, if we can prove this result, then we have proved the actual Central Limit Theorem. In addition, we have found the rate of convergence since the right-hand side tells us that the error √ drops at the rate of 1/ N, which is not particularly fast but is sufficient for our purpose. Unfortunately, proving the Berry-Esseen theorem is not easy. One of the difficulties, for example, is that one needs to deal with the infinite convolutions in the time domain or the frequency domain. Interpreting our proof. If our proof is not completely valid, why do we mention it? For one thing, it provides us with some useful intuition. For most of the (well-behaving) randomvariableswhosemomentsarefinite,theexponentialterminthemoment-generating 375
CHAPTER 6. SAMPLE STATISTICS function can be truncated to the second-order polynomial. Since a second-order polynomial is a Gaussian, it naturally concludes that as long as we can perform such truncation the truncated random variable will be Gaussian. To convince you that the Gaussian MGF is the second-order approximation to other MGFs, we use Bernoulli as an example. Let X ,...,X be i.i.d. Bernoulli with a parame- 1 N ter p. Then the moment-generating function of X =(1/N)(cid:80)N X would be: N n=1 n N M (s)=E[esX]=E[es N1 (cid:80)N n=1Xn]= (cid:89) E[eNsXn] XN n=1 (cid:18) (cid:18) s s2 (cid:19)(cid:19)N =(1−p+peNs)N ≈ 1−p+p 1+ + N 2N2 (cid:18) sp s2p (cid:19)N = 1+ + . N 2N2 Using the logarithmic approximation, it follows that (cid:18) sp s2p (cid:19) logM (s)=Nlog 1+ + XN N 2N2 (cid:18) sp s2p (cid:19) N (cid:18) sp s2p (cid:19)2 ≈N + − + N 2N2 2 N 2N2 s2p(1−p) def ≈sp+ = logM (s). 2N Y Taking the exponential on both sides, we have that (cid:26) s2p(1−p)(cid:27) M (s)=exp sp+ , Y 2N (cid:16) (cid:17) which is the MGF of a Gaussian random variable Y ∼Gaussian p,p(1−p) . N Figure 6.21 shows several MGFs. In each of the subfigures we plot the exact MGF M XN(s)=(1−p+peNs)N asafunctionofs.(Theparameterpinthisexampleisp=0.5.)We vary the number N, and we inspect how the shape of M (s) changes. On top of the exact XN (cid:110) (cid:111) MGFs, we plot the Gaussian approximations M (s) = exp sp+ s2p(1−p) . According to Y 2N ourcalculation,thisGaussianapproximationisthesecond-orderapproximationtotheexact MGF. The figures show the effect of the second-order approximation. For example, in (a) when N =2 the Gaussian is a quadratic approximation of the exact MGF. For (b) and (c), as N increases, the approximation improves. The reason why the second-order approximation works for Gaussian is that when N increases, the higher order moments of X vanish and only the leading first two moments N (cid:110) (cid:111) survive. The MGFs are becoming flat because M (s) = exp sp+ s2p(1−p) converges to Y 2N exp{sp}whenN →∞.TakingtheinverseLaplacetransform,M (s)=exp{sp}corresponds Y to a delta function. This makes sense because as N grows, the variance of the X shrinks. End of the discussion. 376
6.4. CENTRAL LIMIT THEOREM 105 105 105 104 Binomial MGF 104 Binomial MGF 104 Binomial MGF Gaussian MGF Gaussian MGF Gaussian MGF 103 103 103 102 102 102 101 101 101 100 100 100 10-1 10-1 10-1 10-2 10-2 10-2 -10 -5 0 5 10 -10 -5 0 5 10 -10 -5 0 5 10 (a) N =2 (b) N =4 (c) N =10 Figure6.21:ExplanationoftheCentralLimitTheoremusingthefunction.Inthissetofplots,weshow theMGFoftherandomvariableX =(1/N)(cid:80)N X ,whereX ,...,X arei.i.d.Bernoullirandom N n=1 n 1 N variables. The exact MGF of X is the binomial, whereas the approximated MGF is the Gaussian. We N observe that as N increases, the Gaussian approximation to the exact MGF improves. 6.4.3 Examples Example 6.15. Prove the equivalence of a few statements. √ (cid:16) (cid:17) • N XN−µ →d Gaussian(0,1) σ √ • N(X −µ)→d Gaussian(0,σ2) N √ • NX →d Gaussian(µ,σ2) N Solution. The proof is based on the linear transformation property of Gaussian ran- dom variables. For example, if the first statement is true, then the second statement is also true because √ (cid:20)√ (cid:18) X −µ(cid:19) z(cid:21) lim F√ (z)= lim P[ N(X −µ)≤z]= lim P N N ≤ N→∞ N(XN−µ) N→∞ N N→∞ σ σ =(cid:90) z/σ √1 e−t 22 dt=(cid:90) z √ 1 e− 2t σ2 2 dt. 2π 2πσ2 −∞ −∞ The other results can be proved similarly. Example 6.16. Suppose X ∼ Poisson(10) for n = 1,...,N, and let X be the n N sample average. Use the Central Limit Theorem to approximate P[9 ≤ X ≤ 11] for N N =20. Solution. We first show that (cid:34) N (cid:35) N 1 (cid:88) 1 (cid:88) E[X ]=E X = E[X ]=10, N N n N n n=1 n=1 N 1 (cid:88) 1 10 1 Var[X ]= Var[X ]= Var[X ]= = . N N2 n N n 20 2 n=1 377
CHAPTER 6. SAMPLE STATISTICS Therefore, the Central Limit Theorem implies that X −d → Gaussian(cid:0) 10,1(cid:1) . The N 2 probability is (cid:32) (cid:33) (cid:32) (cid:33) 11−10 9−10 P[9≤X ≤11]≈Φ −Φ N (cid:112) (cid:112) 1/2 1/2 (cid:18) (cid:19) (cid:18) (cid:19) 1 1 =Φ √ −Φ −√ =0.9214−0.0786=0.8427. 0.5 0.5 We can also do an exact calculation to verify our approximation. Let S = N (cid:80)N n=1X n so that X N = S NN. Since a sum of Poisson remains a Poisson, it follows that S ∼Poisson(10N)=Poisson(200). N Consequently, P[9≤X ≤11]=P[180≤S ≤220] N N (cid:88)220 200(cid:96)e−200 (cid:88)180 200(cid:96)e−200 = − =0.9247−0.0822=0.8425. (cid:96)! (cid:96)! (cid:96)=0 (cid:96)=0 Note that this is an exact calculation subject to numerical errors when evaluating the finite sums. Example 6.17. Suppose you have collected N = 100 data points from an unknown distribution. The only thing you know is that the true population mean is µ = 500 and the standard deviation is σ =80. (Note that this distribution is not necessarily a Gaussian.) (a) Find the probability that the sample mean will be inside the interval (490,510). (b) Find an interval such that 95% of the sample average is covered. (cid:18) (cid:16) (cid:17)2(cid:19) Solution. To solve (a), we note that X N →d Gaussian 500, √80 . Therefore, 100 (cid:32) (cid:33) (cid:32) (cid:33) 510−500 490−500 P[490≤X ≤510]=Φ −Φ N √80 √80 100 100 =Φ(1.25)−Φ(−1.25)=0.7888. To solve (b), we know that Φ(x) = 0.025 implies that x = −1.96, and Φ(x) = 0.975 implies that x=+1.96. So y−500 =±1.96 ⇒ y =484.32 or y =515.68. √80 100 Therefore, P[484.32≤X ≤515.68]=0.95. N 378
6.4. CENTRAL LIMIT THEOREM 6.4.4 Limitation of the Central Limit Theorem If we recall the statement of the Central Limit Theorem (Berry-Esseen), we observe that the theorem states only that (cid:20)√ (cid:18) X −µ(cid:19) (cid:21) lim P N N ≤ε = lim F (ε)=F (ε)=Φ(ε). N→∞ σ N→∞ ZN Z Rearranging the terms, (cid:20) (cid:21) σε lim P X ≤µ+ √ =Φ(ε). N N→∞ N This implies that the approximation is good only when the deviation ε is small. Let us consider an example to illustrate this idea. Consider a set of i.i.d. exponential random variables X ,...,X , where X ∼ Exponential(λ). Let S = X +···+X be 1 N n N 1 N the sum, and let X =S /N be the sample average. Then, according to Chapter 6.4.1, S N N is an Erlang distribution S ∼Erlang(N,λ) with a PDF N λN f (x)= xN−1e−λx. SN (N −1)! Practice Exercise 6.10. Let S ∼ Erlang(N,λ) with a PDF f (x). Show that if N SN Y =aS +b for any constants a and b, then N N (cid:18) (cid:19) 1 y−b f (y)= f . YN a SN a Solution: This is a simple transformation of random variables: F (y)=P[Y ≤y]=P[aS +b≤y]=P(cid:20) S ≤ y−b(cid:21) =(cid:90) y− ab f (x)dx. YN N N a SN −∞ Hence, using the fundamental theorem of calculus, d (cid:90) y− ab 1 (cid:18) y−b(cid:19) f (y)= f (x)dx= f . YN dy SN a SN a −∞ We are interested in knowing the statistics of X and comparing it with a Gaussian. N To this end, we construct a normalized variable X −µ Z = N√ , N σ/ N where µ=E[X ]= 1 and σ2 =Var[X ]= 1 . Then n λ n λ2 S /N −µ S −Nµ λ √ Z = N √ = N√ = √ S − N N N σ/ N σ N N 379
CHAPTER 6. SAMPLE STATISTICS √ Usingtheresultofthepracticeexercise,bymapping a= √λ andb=− N,itfollowsthat N √ (cid:32) √ (cid:33) N z+ N f (z)= f . ZN λ SN √λ N NowwecompareZ withthestandardGaussianZ ∼Gaussian(0,1).Accordingtothe N Central Limit Theorem, the standard Gaussian is a good approximation to the normalized sample average Z . To compare the two results, we conduct a numerical experiment. We N let λ = 1 and we vary N. We plot the PDF f (z) as a function of z, for different N’s, in ZN Figure 6.22. In addition, we plot the PDF f (z), which is the standard Gaussian. Z The plot in Figure 6.22 shows that while the Central Limit Theorem provides a good approximation, the approximation is only good for values that are close to the mean. For the tails, the Gaussian approximation is not as good. 100 10-2 N = 1 10-4 N = 10 N = 100 N = 1000 Gaussian 10-6 -1 0 1 2 3 4 5 Figure 6.22: CLT fails at the tails. We note that X ,...,X are i.i.d. exponential with a parameter 1 N λ=1.WeplotthePDFsofthenormalizedsampleaverageZ N = XN√−µ byvaryingN.WeplotthePDF σ/ N of the standard Gaussian Z ∼Gaussian(0,1) on the same grid. Note that the Gaussian approximation is good for values that are close to the mean. For the tails, the Gaussian approximation is not very accurate. The limitation of the Central Limit Theorem is attributable to the fact that Gaussian is a second-order approximation. If a random variable has a very large third moment, the second-order approximation may not be sufficient. In this case, we need a much larger N to drive the third moment to a small value and make the Gaussian approximation valid. When will the Central Limit Theorem fail? • The Central Limit Theorem fails when N is small. • The Central Limit Theorem fails if the third moment is large. As an extreme case,aCauchyrandomvariabledoesnothaveafinitethirdmoment.TheCentral Limit Theorem is not valid for this case. • The Central Limit Theorem can only approximate the probability for input val- ues near the mean. It does not approximate the tails, for which we need to use Chernoff’s bound. 380
6.5. SUMMARY 6.5 Summary Whydoweneedtostudythesampleaverage?Becauseitisthesummaryofthedataset.In machinelearning,oneofthemostfrequentlyaskedquestionsisaboutthenumberoftraining samplesrequiredtotrainamodel.Theanswercanbefoundbyanalyzingtheaveragenumber of successes and failures as the number of training samples grows. For example, if we define f as the classifier that takes a data point x and predicts a label f(x ), we hope that it n n will match with the true label y . If we define an error n (cid:40) 1, f(x )=y correct classification, E = n n n 0, f(x )(cid:54)=y incorrect classification, n n then E is a Bernoulli random variable, and the total loss E = 1 (cid:80)N E will be the n N n=1 n trainingloss.Butwhatis 1 (cid:80)N E ?ItisexactlythesampleaverageofE .Therefore,by N n=1 n n analyzing the sample average E we will learn something about the generalization capability of our model. Howshouldwestudythesampleaverage?Byunderstandingthelawoflargenumbers and the Central Limit Theorem, as we have seen in this chapter. • Law of large numbers: X converges to the true mean µ as N grows. • Central Limit Theorem: The CDF of X can be approximated by the CDF of a Gaussian, as N grows. Performance guarantee? The other topic we discussed in this chapter is the concept of convergence type. There are essentially four types of convergence, ranked in the order of restrictions. • Deterministic convergence: A sequence of deterministic numbers converges to anotherdeterministicnumber.Forexample,thesequence1,1,1,1,...converges 2 3 4 to 0 deterministically. There is nothing random about it. • Almost sure convergence: Randomness exists, and there is a probabilistic con- vergence.Almostsureconvergencemeansthatthereiszeroprobabilityoffailure after a finite number of failures. • Convergence in probability: The sequence of probability values converges, i.e., the chance of failure is going to zero. However, you can still fail even if your N is large. • Convergence in distribution: The probability values can be approximated by the CDF of a Gaussian. 381
CHAPTER 6. SAMPLE STATISTICS 6.6 References Moment-Generating and Characteristic Functions 6-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 4.4. 6-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 4.5 and 4.7. 6-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapters 5.5 and 7.2. 6-4 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. Chapters 4.5 and 4.7. 6-5 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 7.7. 6-6 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapter 4.3. Basic probability inequality 6-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 5.1. 6-8 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 6 and 8. 6-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-10 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.2. 6-11 Larry Wasserman, All of Statistics, Springer 2003. Chapter 4. Concentration inequalities 6-12 Larry Wasserman, All of Statistics, Springer 2003. Chapter 4. 6-13 Martin Wainwright, High-Dimensional Statistics, Cambridge University Press, 2019. Chapter 2.1. 6-14 Stephane Boucheron, Gabor Lugosi and Pascal Massart, Concentration Inequalities, Oxford University Press, 2013. Chapters 2.1 and 2.2. 382
6.6. REFERENCES Law of large numbers 6-15 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapters 5.2, 5.3, 5.5. 6-16 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapters 7.1, 7.2, 7.4 6-17 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-18 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.2, 8.4. 6-19 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 3.3, 14.1, 14.3. 6-20 Larry Wasserman, All of Statistics, Springer 2003. Chapter 5.1 - 5.3. 6-21 Patrick Billingsley, Probability and Measure, Wiley 1995. Section 22. Central Limit Theorem 6-22 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 5.4. 6-23 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 7.3. 6-24 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 7.4. 6-25 SheldonRoss,AFirstCourseinProbability,PrenticeHall,8thEdition,2010.Chapter 8.3. 6-26 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, 2006. Chapters 5.6, 14.2. 6-27 Larry Wasserman, All of Statistics, Springer 2003. Chapter 5.4. 6-28 Patrick Billingsley, Probability and Measure, Wiley 1995. Section 27. 383
CHAPTER 6. SAMPLE STATISTICS 6.7 Problems Exercise 1. (Video Solution) Let X, Y, Z be three independent random variables: X ∼Bernoulli(p), Y ∼Exponential(α), Z ∼Poisson(λ) Find the function for the following random variables. (a) U =Y +Z (b) U =2Z+3 (c) U =XY (d) U =2XY +(1−X)Z Exercise 2. (Video Solution) Two random variables X and Y have the joint PMF λn+mλm P(X =n,Y =m)= 1 2 e−(λ1+λ2), m=0,1,2,..., n≥−m. (n+m)!m! Let Z =X+Y. Find the function M (s) and the PMF of Z. Z Exercise 3. (Video Solution) Let X ,X ,... be a sequence of independent random variables with PDF 0 1 a 1 f (x)= k , a = , Xk π(a2 +x2) k 2k+1 k for k =0,1,.... Find the PDF of Y, where ∞ (cid:88) Y = X . k k=0 Hint: You may find the characteristic function useful. Exercise 4. The random variables X and Y are independent and have PDFs (cid:40) (cid:40) e−x, x≥0, 0, y >0, f (x)= and f (y)= X 0, x<0, Y et, y ≤0. 384
6.7. PROBLEMS Find the PDF of Z = X + Y. (Hint: Use the characteristic function and the moment- generating function.) Exercise 5. A discrete random variable X has a PMF 1 p (k)= , k =1,2,.... X 2k Find the characteristic function Φ (jω). X Exercise 6. Let T ,T ,... be i.i.d. random variables with PDF 1 2 (cid:40) λe−λt, t≥0, f (t)= Tk 0, t<0, for k =1,2,3,.... Let S =(cid:80)n T . Find the PDF of S . n k=1 k n Exercise 7. (Video Solution) In this exercise we will prove a variant of Chebyshev when the variance σ2 is unknown but X is bounded between a≤X ≤b. (a) Letγ ∈R.FindaγthatminimizesE[(X−γ)2].Hence,showthatE[(X−γ)2]≥Var[X] for any γ. (b) Let γ =(a+b)/2. Show that (b−a)2 E[(X−γ)2]=E[(X−a)(X−b)]+ . 4 (c) From (a) and (b), show that Var[X]≤ (b−a)2 . 4 (d) Show that for any ε>0, (b−a)2 P[|X−µ|>ε]≤ . 4ε2 Exercise 8. The random variables X and Y are independent with PDFs 1 1 f (x)= and f (y)= , X π(1+x2) Y π(1+y2) respectively. Find the PDF of Z =X−Y. (Hint: Use the characteristic function.) Exercise 9. A random variable X has the characteristic function Φ (jω)=e−jω/(1−jω). X 385
CHAPTER 6. SAMPLE STATISTICS Find the mean and variance of X. Exercise 10. Show that for any random variables X and Y, 1 P[|X−Y|>(cid:15)]≤ E[(X−Y)2]. (cid:15)2 Exercise 11. Let X be an exponential random variable with a parameter λ. Let µ = E[X] and σ2 = Var[X]. Compute P[|X −µ| ≥ kσ] for any k > 1. Compare this to the bound obtained by Chebyshev’s inequality. Exercise 12. Let X ,...,X be i.i.d. Bernoulli with a parameter p. Let α>0 and define 1 N (cid:115) (cid:18) (cid:19) 1 2 (cid:15)= log . 2N α Let X = 1 (cid:80)N X . Define an interval N N n=1 n (cid:2) (cid:3) I = X −(cid:15), X +(cid:15) . N N Use Hoeffding’s inequality to show that P[I contains p]≥1−α. Exercise 13. Let Z ∼Gaussian(0,1). Prove that for any (cid:15)>0, P[|Z|>(cid:15)]≤(cid:114) 2e−(cid:15) 22 . π (cid:15) Hint: Note that (cid:15)P[|Z| > (cid:15)] = 2(cid:15)P[Z > (cid:15)], and then follow the procedure we used to prove Markov’s inequality. Exercise 14. (a) Giveanon-negativerandomvariableX ≥0suchthatMarkov’sinequalityismetwith equality. Hint: Consider a discrete random variable. (b) Give a random variable X such that Chebyshev’s inequality is met with equality. Exercise 15. Consider a random variable X such that E[esX]≤es2 2σ2 . 386
6.7. PROBLEMS (a) Show that for any t, (cid:26) t2 (cid:27) P[X ≥t]≤exp − . 2σ2 Hint: Use Chernoff’s bound. (b) Show that E[X2]≤4σ2. Hint: First prove that E[X2]=(cid:82)∞P[X2 ≥t]dt. Then use part (a) above. 0 Exercise 16. LetX ,...,X bei.i.d.uniformrandomvariablesdistributedover[0,1].SupposeY ,...,Y 1 N 1 N are defined as follows. (a) Y =X /n n n (b) Y =(X )n n n (c) Y =max(X ,...,X ) n 1 n (d) Y =min(X ,...,X ) n 1 n For (a), (b), (c), and (d), show that Y converges in probability to some limit. Identify the n limit in each case. Exercise 17. Let λ = 1 for n=1,2,.... Let X ∼Poisson(λ ). Show that X converges in probability n n n n n to 0. Exercise 18. Let Y ,Y ,... be a sequence of random variables such that 1 2 (cid:40) 0, with probability 1− 1, Y = n n 2n, with probability 1. n Does Y converge in probability to 0? n Exercise 19. (Video Solution) A Laplace random variable has a PDF λ f (x)= e−λ|x|, λ>0, X 2 and the variance is Var[X] = 2 . Let X ,...,X be a sequence of i.i.d. Laplace random λ2 1 500 variables. Let X +···+X M = 1 500. 500 500 (a) Find E[X]. Express your answer in terms of λ. 387
CHAPTER 6. SAMPLE STATISTICS (b) Let λ=10. Using Chebyshev’s inequality, find a lower bound of P[−0.1≤M ≤0.1]. 500 (c) Let λ=10. Using the Central Limit Theorem, find the probability P[−0.1≤M ≤0.1]. 500 You may leave your answer in terms of the Φ(·) function. Exercise 20. (Video Solution) Let X ,...,X be a sequence of i.i.d. random variables such that X = ±1 with equal 1 N n probability. Let N 1 (cid:88) X = √ X . N n N n=1 ProvetheCentralLimitTheoremforthisparticularsequenceofrandomvariablesbyshowing that (a) E[X ]=0, Var[X ]=1. N N (b) The moment-generating function of X N is M XN(s)→es 22 as N →∞. Exercise 21. (Video Solution) Let X ,...,X be a sequence of i.i.d. random variables with mean and variance 1 N E[X ]=µ and Var[X ]=σ2, n=1,...,N. n n The distribution of X is, unknown. Let n N 1 (cid:88) M = X . N N n n=1 Use the Central Limit Theorem to estimate the probability P[M >2µ]. N 388
Chapter 7 Regression Starting with this chapter, we will discuss several combat skills — techniques that we use to do the actual data analysis. The theme of this topic is learning and inference, which are both at the core of modern data science. The word “learning” can be broadly interpreted as seeking the best model to explain the data, and the word “inference” refers to prediction andrecovery.Here,predictionmeansthatweusetheobserveddatatoforecastorgeneralize to unseen situations, whereas recovery means that we try to restore the missing data in our current observations. In this chapter we will learn regression, one of the most widely used learning and inference techniques. Regressionisaprocessforfindingtherelationshipbetweentheinputsandtheoutputs. In a regression problem, we consider a set of input data {x ,...,x } and a set of output 1 N def data {y ,...,y }. We call the set of these input-output pairs D = {(x ,y ),...,(x ,y )} 1 N 1 1 N N the training data. The true relationship between an x and a y is unknown. We do not n n know,youdonotknow,onlyGodknows.Wedenotethisunknownrelationshipasamapping f(·) that takes x and maps it to y , n n y =f(x ), n n as illustrated in Figure 7.1. Figure7.1:Aregressionproblemisaboutfindingthebestapproximationtotheinput-outputrelationship of the data. Since we do not know f(·), finding it from a set of finite number of data points D = {(x ,y ),...,(x ,y )} is infeasible — there are infinitely many ways we can make y = 1 1 N N n f(x ) for every n = 1,...,N. The idea of regression is to add a structure to the problem. n Insteadoflookingforf(·),wefindaproxyg (·).Thisproxyg (·)takesacertainparametric θ θ form. For example, we can postulate that (x ,y ) has a linear relationship, and so n n g (x )= θ x + θ , n=1,...,N. θ n 1 n 0 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) parameter parameter 389
CHAPTER 7. REGRESSION This equation is a straight line with a slope θ and a y-intercept θ . We call θ = [θ ,θ ] 1 0 1 0 the parameter of the model f(·). To emphasize that the function we are using here is parameterized by θ, we denote the function by g (·). θ Of course, any model we choose is our guess. It will never be the true model. There is alwaysadifferencebetweenwhatourmodeltellsusandwhatwehaveobserved.Wedenote this “difference” or “error” by e and define it as: n e =y −g (x ), n=1,...,N. n n θ n Thepurposeofregressionistofindthebestθsuchthattheerrorisminimized.Forexample, consider a minimization of the sum-square error: N θ(cid:98)=argmin (cid:88) (y n−g θ(x n))2. θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) Thesumofthesquarederrorisjustoneofthemanypossiblewayswecandefinethetraining lossE (θ).Wewilldiscussdifferentwaystodefinethetraininglossinthischapter,butthe train point should be evident. For a given dataset D = {(x ,y ),...,(x ,y )}, regression tries 1 1 N N to find a function g (·) such that the training loss is minimized. The optimization variable θ is the parameter θ. If the function g (·) is a linear function in θ, we call the regression a θ linear regression. Figure 7.2: A regression problem involves several steps: picking a model g , defining the training loss θ E (θ), and solving the optimization to update θ. train A summary of the regression process is shown in Figure 7.2. Given the training data D = {(x ,y ),...,(x ,y )}, the user picks a model g (·) to make a prediction. We com- 1 1 N N θ pare the predicted value g (x ) with the observed value y , and compute the training loss θ n n E (θ).ThetraininglossE (θ)isafunctionofthemodelparameterθ.Differentmodel train train parameters θ give different training loss. We solve an optimization problem to find the best model parameter. In practice, we often iterate the process for a few times until the training loss is settled down. 390
What is regression? Given the data points (x ,y ),...,(x ,y ), regression is the process of finding 1 1 N N the parameter θ of a function g (·) such that the training loss is minimized: θ N (cid:88) θ(cid:98)=argmin L(y n, g θ(x n)), (7.1) θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) whereL(·,·)isthelossbetweenapairoftrueobservationy andthepredictiong (x ). n θ n One common choice of L(·,·) is L(g (x ),y )=(g (x )−y )2. θ n n θ n n Example 1. Fitting the data Suppose we have a set of data points (x ,y ), (x ,y ), ..., (x ,y ), where x ’s are the 1 1 2 2 N N n inputs and y ’s are the outputs. These pairs of data points can be plotted in a scatter plot, n as shown in Figure 7.3. We want to find the curve that best fits the data. To solve this problem, we first need to choose a model, for example g (x )=θ +θ x +θ x2 +θ x3 +θ x4. θ n 0 1 n 2 n 3 n 4 n Wecallthecoefficientsθ =[θ ,θ ,θ ,θ ,θ ]theregression coefficients.Theycanbefound 0 1 2 3 4 by solving the optimization problem N (cid:18) (cid:19)2 (cid:88) minimize y −(θ +θ x +θ x2 +θ x3 +θ x4) . n 0 1 n 2 n 3 n 4 n θ0,θ1,θ2,θ3,θ4 n=1 4 3 2 1 0 -1 -2 data fitted curve -3 -1 -0.5 0 0.5 1 Figure 7.3: Regression can be used to fit the dataset using curves. In this example, we use a fourth-th order polynomial g (x)=(cid:80)4 θ xp to fit a 50-point dataset. θ p=0 p n This optimization asks for the best θ = [θ ,...,θ ]T such that the training loss is 0 4 minimized. Solving the minimization problem would require some effort, but if we imagine that we have solved it we can find the best curve, which is g (x) = (cid:80)4 θ xp with the θ p=0 p n optimal θ plugged in. The red curve in Figure 7.3 shows an example in which we have used a fourth-order polynomial to fit a dataset comprising 50 data points. We will learn how to solve the problem in this chapter. 391
CHAPTER 7. REGRESSION Example 2. Predicting the stock market Imaginethatyouhaveboughtsomesharesinthestockmarket.Youhavelookedatthepast data, and you want to predict the price of the shares over the next few days. How would you do it besides just eyeballing the data? First,youwouldplotthedatapointsonagraph.Mathematically,wecandenotethese data points as {x ,x ,...,x }, where the indices n = 1,2,...,N can be treated as time 1 2 N stamps. We assume a simple model to describe the relationship between the x ’s, say n x ≈ax +bx , n n−1 n−2 for some parameters θ = (a,b).1 This model assumes that the current value x can be n approximated by a linear combination of two previous values x and x . Therefore, if n−1 n−2 we have x and x we should be able to predict x , and if we have x and x we should be 1 2 3 2 3 able to predict x , etc. The magic of this prediction comes from the parameters a and b. If 4 we know a and b, the prediction can be done by simply plugging in the numbers. Theregressionproblemhereistoestimatetheparametersaandbfromthedata.Since we are given a set of training data {x ,x ,...,x }, we can check whether our predicted 1 2 N value x is close to the true x , and whether our predicted value x is close to the true x , (cid:98)3 3 (cid:98)4 4 etc. This leads to the optimization N (cid:18) (cid:19)2 (cid:88) ( (cid:98)a,(cid:98)b)=argmin x n−(ax n−1+bx n−2) , a,b (cid:124) (cid:123)(cid:122) (cid:125) n=1 =prediction where we use initial conditions that x = x = 0. The optimization problem requires us 0 −1 to minimize the disparity between x and the predicted value ax +bx , for all n. n n−1 n−2 By finding the (a,b) that minimizes this objective function, we will accomplish our goal of estimating the best (a,b). Figure 7.4 shows an example of predicting a random process using the above model. If the parameters a and b are properly determined, we will obtain a reasonably well-fitted curve to the data. A simple extrapolation to the future timestamp would suffice for the forecast task. Plan for this chapter What are the key ingredients of regression? • Learning: Formulate the regression problem as an optimization problem, and solve it by finding the best parameters. • Inference: Use the estimated parameters and models to predict the unseen data points. Regression is too broad a topic to be covered adequately in a single chapter. Accord- ingly, we will present a few principles and a few practical algorithmic techniques that are broadlyapplicabletomany(definitelynotall)regressiontasks.Theseincludethefollowing. 1Caution: If you lose money in the stock market by following this naive model, please do not cry. This modelisgreatlyoversimplifiedandprobablywrong. 392
1.5 1 0.5 0 -0.5 data best fit candidate -1 0 0.2 0.4 0.6 0.8 1 Figure 7.4: An autoregression model aims at learning the model parameters based on the previous samples.Thisexampleillustratesfittingthedatausingthemodelx =ax +bx ,forn=1,...,N. n n−1 n−2 • The principle of regression (Section 7.1). We explain the formulation of a regression problem via optimization. There are a few steps involved in developing this concept. First, we will exclusively focus on linear models because these models are easier to analyze than nonlinear models but are still rich enough for many practical problems. We will discuss how to solve the linear regression problem and some applications of the solutions. We then address the issue of outliers using a concept called the robust linear regression. • Overfitting (Section 7.2). The biggest practical challenge of regression is overfitting. Overfitting occurs when a model fits too closely to the training samples so that it fails to generalize. We will delve deeply into the roots of overfitting and show that overfitting depends on three factors: the number of training samples N, the model complexity d, and the magnitude of noise σ2. • Bias-variance trade-off (Section 7.3). We will present one of the most fundamental resultsinlearningtheory,knownasthebias-variancetrade-off.Itappliestoall regres- sion problems, not just to linear models. Understanding this trade-off will help you understand the fundamental limits of your problem so that you know what to expect from the model. • Regularization (Section 7.4). In this section we discuss a technique for combatting overfitting known as regularization. Regularization is carried out by adding an extra term to the regression objective function. By solving the modified optimization, the regression solution is improved in two ways: (i) regularization makes the regression solution less sensitive to noise perturbations, and (ii) it alleviates the fitting difficulty whenwehaveonlyafewtrainingsamples.Wewilldiscusstworegularizationstrategies: the ridge regression and the LASSO regression. Much of this chapter deals with optimization. If this is your first time reading this book, we encourage you to have a reference book on linear algebra at hand. 393
CHAPTER 7. REGRESSION 7.1 Principles of Regression We start by recalling our discussion in the introduction. The purpose of regression can be summarized in a simple statement: Given the data points (x ,y ),...,(x ,y ), find the parameter θ of a function g (·) 1 1 N N θ such that the training loss is minimized: N (cid:88) θ(cid:98)=argmin L(y n, g θ(x n)), (7.2) θ∈Rd n=1 (cid:124) (cid:123)(cid:122) (cid:125) traininglossEtrain(θ) whereL(·,·)isthelossbetweenapairoftrueobservationy andthepredictiong (x ). n θ n Whenthecontextmakesitclear,wewilldropthesubscriptθing (·)withtheunderstanding θ that the function g(·) is parameterized by θ. Asyoucansee,regressionfindsafunctiong(·)thatbestapproximates theinput-output relationship between x and y . There are two choices we need to make when formulating n n a regression problem: • Function g(·): What is the family of functions we want to use? This could be a line, a polynomial,orasetofbasisfunctions.Ifitisapolynomial,whatisitsorder?Weneed to make all these decisions before running the regression. A poor choice of function family can lead to a poor regression result. • Loss“L(·,·)”:Howdowemeasuretheclosenessbetweeny andg(x )?Arewemeasur- n n ingintermsofthesquarederror(y −g(x ))2,ortheabsolutedifference|y −g(x )|, n n n n or something else? Again, a poor choice of distance function can create a false sense of closeness because you might be optimizing for a wrong objective. Before we delve into the details, we need to discuss briefly the connection between regressionandprobability.Aregressionproblemcanbesolvedwithoutknowingprobability, so why is regression discussed in a book on probability? This question is related to how much we know about the statistical model and what kind of optimality we are seeking. A full answer requires some understanding of maximum likelihood estimation and maximum a posteriori estimation, which will be explained in Chapter 8. As a quick preview of our results, we summarize the key ideas below: How is regression related to probability? • Ifyouknowthestatisticalrelationshipbetweenx andy ,thenwecanconstruct n n a regression problem that maximizes the likelihood of the underlying distribu- tion. Such regression solution is optimal with respect to the likelihood. • We can construct a regression problem that can minimize the expectation of the 394
7.1. PRINCIPLES OF REGRESSION squared error. This regression solution is mean-squared optimal. • If you are a Bayesian and you know the prior distribution of x , then we can n construct a regression problem that maximizes the posterior distribution. The solution to this regression problem is Bayesian optimal. • If you know nothing about the statistics of x and y , you can still run the n n regressionandgetsomething,andthis“something”canbeveryuseful.However, you cannot claim statistical optimality of this “something”. See Chapter 8 for additional discussion. It is important to understand that a regression problem is at the intersection of op- timization and statistics. The need for optimization is clear because we need to minimize the error. The statistical need is to generalize to unknown data. If there is no statistical relationship between x and y (for all n), whatever model we obtain from the regression n n will only work for the N training samples. The model will not generalize because knowing x will not help us know y . In other words, if there is no statistical relationship between n n x and y , you can fit perfectly to the training data but you will fail miserably to fit the n n testing data. 7.1.1 Intuition: How to fit a straight line? In this subsection we want to give you the basic idea of how regression is formulated. To keep things simple, we will discuss how to fit data using a straight line. Consider a collection of data points D = {(x ,y ),...,(x ,y )}, where x ’s are the 1 1 N N n inputs and y ’s are the observations, for example, in the table below. n n x y n n 1 0.6700 3.0237 2 0.3474 2.3937 3 0.6695 3.5548 . . . . . . . . . N −1 0.2953 2.6396 N 0.6804 3.2536 Let us consider the linear regression problem. The goal of linear regression is to find the straight line that best fits the datasets. All straight lines on a 2D graph are plots of the equation g(x)=ax+b, where a is the slope of the line and b is the y-intercept of the line. We denote this line by g(·). Note that this function g is characterized by two parameters (a,b) because once (a,b) are known the line is determined. If we change (a,b), the line will change as well. Therefore, by finding the best line we are essentially searching for the best (a,b) such that the training error is minimized. The pictorial meaning of linear regression can easily be seen in Figure 7.5, which shows N = 50 data points according to some latent distributions. Given these 50 data points, we construct several possible candidates for the regression model. These candidates 395
CHAPTER 7. REGRESSION are characterized by the parameters (a,b). For example, the parameters (a,b) = (1,2) and (a,b)=(−2,3) represent two different straight lines in the candidate pool. The goal of the regression is to find the best line from these candidates. Note that since we limit ourselves to straight lines, the candidate set will not include polynomials or trigonometric functions. These functions are outside the family we are considering. 4.5 4 3.5 3 2.5 data 2 best fit candidate 1.5 0 0.2 0.4 0.6 0.8 1 Figure 7.5: Theobjectiveofleastsquaresfitting(orlinearregression)istofindalinethatbestfitsthe dataset. Given these candidate functions, we need to measure the the training loss. This can be defined in multiple ways, such as • Sum-squared loss E (θ)=(cid:80)N (y −g(x ))2. train n=1 n n • Sum-absolute loss E (θ)=(cid:80)N |y −g(x )|. train n=1 n n • Cross-entropy loss E (θ)=−(cid:80)N (y logg(x )+(1−y )log(1−g(x ))). train n=1 n n n n • Perceptual loss E (θ) = (cid:80)N max(−y g(x ),0), when y and g(x ) are binary train n=1 n n n n takingvalues±1.Thisisareasonabletrainingerrorbecauseify matcheswithg(x ), n n then y g(x )=1 and so max(−y g(x ),0)=0. But if y does not match with g(x ), n n n n n n theny g(x )=−1andhencemax(−y g(x ),0)=1.Thus,thelosscapturesthesum n n n n of all the mismatched pairs. Choosingthelossfunctionisproblem-specific.Itisalsowhereprobabilityentersthepicture because, without any knowledge about the distributions of x and y , there is no way to n n choosethebesttrainingloss.Youcanstillpickone,aswewilldo,butitwillnotbegranted any probabilistic guarantees. Among these possible choices of the training error, we are going to focus on the sum- squared loss because it is convex and differentiable. This makes the computation easy, since we can run any textbook optimization algorithm. The regression problem under the sum-squared loss is: (cid:16) (cid:17) (cid:88)N (cid:18) (cid:19)2 (cid:98)a,(cid:98)b =argmin y n−(ax n+b) . (7.3) (a,b) n=1 (cid:124) (cid:123)(cid:122) (cid:125) =g(xn) In this equation, the symbol “argmin” means “argument minimize”, which returns the ar- gument that minimizes the cost function on the right. The interpretation of the equation is 396
7.1. PRINCIPLES OF REGRESSION that we seek the (a,b) that minimize the sum (cid:80)N (y −(ax +b))2. Since we are mini- n=1 n n mizing the squared error, this linear regression problem is also known as the least squares fitting problem. The idea is summarized in the following box. What is linear least squares fitting? • Find a line g(x)=ax+b that best fits the training data {(x ,y )}N . n n n=1 • The optimality criterion is to minimize the squared error N (cid:18) (cid:19)2 (cid:88) E (θ)= y −g(x ) , (7.4) train n n n=1 where θ =(a,b) is the model parameter. • There exist other optimality criteria. Squared error is convex and differentiable. 7.1.2 Solving the linear regression problem Let’s consider how to solve the linear regression problem given by Equation (7.3). The problem is the following: (cid:16) (cid:17) (cid:98)a,(cid:98)b =argmin E train(a,b). (7.5) (a,b) As with any two-dimensional optimization problem, the optimal point ( (cid:98)a,(cid:98)b) should have a zero gradient, meaning that ∂ ∂ E (a,b)=0 and E (a,b)=0. ∂a train ∂b train This should be familiar to you, even if you have only learned basic calculus. This pair of equations says that, at a minimum point, the directional slopes should be zero no matter which direction you are looking at. The derivative with respect to a is ∂ E (a,b) ∂a train ∂ (cid:26) (cid:88)N (cid:18) (cid:19)2(cid:27) = y −(ax +b) ∂a n n n=1 ∂ (cid:26)(cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2(cid:27) = y −(ax +b) + y −(ax +b) +···+ y −(ax +b) ∂a 1 1 2 2 N N (cid:18) (cid:19) (cid:18) (cid:19) =2 y −(ax +b) (−x )+···+2 y −(ax +b) (−x ) 1 1 1 N N N (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) =2 − x y +a x2 +b x . n n n n n=1 n=1 n=1 397
CHAPTER 7. REGRESSION Similarly, the derivative with respect to b is ∂ ∂ (cid:26) (cid:88)N (cid:18) (cid:19)2(cid:27) E (a,b)= y −(ax +b) ∂b train ∂b n n n=1 (cid:18) (cid:19) (cid:18) (cid:19) =2 y −(ax +b) (−1)+···+2 y −(ax +b) (−1) 1 1 N N (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) =2 − y +a x +b 1 . n n n=1 n=1 n=1 Setting these two equations to zero, we have that (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) 2 − y x +a x2 +b x =0, n n n n n=1 n=1 n=1 (cid:32) N N N (cid:33) (cid:88) (cid:88) (cid:88) 2 − y +a x +b 1 =0. n n n=1 n=1 n=1 Rearranging the terms, the pair can be equivalently written as  N N   N  (cid:80) x2 n (cid:80) x n (cid:20) a(cid:21) (cid:80) x ny n  n=1 n=1  =n=1 . (cid:80)N  b  (cid:80)N  x N y n n n=1 n=1 Therefore, if we can solve this system of linear equations, we will have the linear regression solution. Remark.Itiseasytoseethatthesolutionachievestheminimuminsteadofthemaximum, since the second-order derivatives are positive: ∂2 (cid:88)N ∂2 (cid:88)N E (a,b)= x2 ≥0 and E (a,b)= 1>0. ∂a2 train n ∂b2 train n=1 n=1 The following theorem summarizes this intermediate result. Theorem 7.1. The solution of the problem Equation (7.5) (cid:16) (cid:17) (cid:88)N (cid:18) (cid:19)2 (cid:98)a,(cid:98)b =argmin y n−(ax n+b) (a,b) n=1 satisfies the equation  N N   N   n(cid:80) =1x2 n n(cid:80) =1x n  (cid:20) (cid:98)a(cid:21) = n(cid:80) =1x ny n  . (7.6) (cid:80)N  (cid:98)b  (cid:80)N  x N y n n n=1 n=1 398
7.1. PRINCIPLES OF REGRESSION Matrix-vector form of linear regression Solving this linear regression requires some basic linear algebra. The regression can be written as       y x 1 e 1 1 (cid:20) (cid:21) 1 . . . a .  .  =  . . +  .  .  .   . . b  .  y N x N 1 (cid:124) (cid:123)(cid:122) (cid:125) e N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) θ (cid:124) (cid:123)(cid:122) (cid:125) y X e With X, y, θ and e, we can write the linear regression problem compactly as y =Xθ+e. Therefore, the training loss E (θ) can be defined as train E (θ)=(cid:107)y−Xθ(cid:107)2 train (cid:13)    (cid:13)2 (cid:13) (cid:13) y .1 x .1 1 . (cid:20) a(cid:21)(cid:13) (cid:13) (cid:88)N (cid:18) (cid:19)2 =(cid:13) . − . . (cid:13) = y −(ax +b) . (cid:13) .   . . b (cid:13) n n (cid:13) (cid:13) (cid:13) y N x N 1 (cid:13) n=1 Now, taking the gradient with respect to θ yields2 (cid:26) (cid:27) ∇ E (θ)=∇ (cid:107)y−Xθ(cid:107)2 θ train θ =−2XT(y−Xθ). Equating this to zero, we obtain XT(y−Xθ)=0 ⇐⇒ XTXθ =XTy. (7.7) Equation (7.7) is called the normal equation. Thenormalequationisaconvenientwayofconstructingthesystemoflinearequations. Using the 2-by-2 system shown in Equation (7.6) as an example, we note that    N N  XTX =(cid:20) x 11 · ·· ·· · x 1N(cid:21)  x . . .1 1 . . . =  n(cid:80) (cid:80)N=1x2 n n(cid:80) =1x n   , x N 1 x n N n=1    N  y (cid:80) XTy =(cid:20) x 11 · ·· ·· · x 1N(cid:21)   . . .1  =  n= (cid:80)N1x ny n   . y N y n n=1 Therefore, as long as you can construct the X matrix, forming the 2-by-2 system in Equa- tion (7.6) is straightforward: start with y = Xθ and then multiply the matrix transpose XT to both sides. The resulting system is what you need. There is nothing to memorize. 2Thisisabasicvectorcalculusresult.Fordetails,youmayconsultstandardtextssuchastheUniversity ofWaterloo’smatrixcookbook.https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf 399
CHAPTER 7. REGRESSION Running linear regression on a computer On a computer, solving the linear regression for a line is straightforward. Let us look at the MATLAB code first. % MATLAB code to fit data points using a straight line N = 50; x = rand(N,1)*1; a = 2.5; % true parameter b = 1.3; % true parameter y = a*x + b + 0.2*rand(size(x)); % Synthesize training data X = [x(:) ones(N,1)]; % construct the X matrix theta = X\y(:); % solve y = X theta t = linspace(0, 1, 200); % interpolate and plot yhat = theta(1)*t + theta(2); plot(x,y,’o’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,4); InthispieceofMATLABcode,weneedtodefinethedatamatrixX.Here,x(:)isthe column vector that stores all the values (x ,...,x ). The all-one vector ones(N,1) is the 1 N second column in our X matrix. The command X\y(:) is equivalent to solving the normal equation XTXθ =XTy. The last few lines are used to plot the predicted curve. Note that theta(1) and theta(2) are the entries of the solution θ. The result of this program is exactly the plot shown in Figure 7.5 above. In Python, the program is quite similar. The command we use to solve the inversion is np.linalg.lstsq. # Python code to fit data points using a straight line import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.rand(N) a = 2.5 # true parameter b = 1.3 # true parameter y = a*x + b + 0.2*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) # construct the X matrix theta = np.linalg.lstsq(X, y, rcond=None)[0] # solve y = X theta t = np.linspace(0,1,200) # interpolate and plot yhat = theta[0]*t + theta[1] plt.plot(x,y,’o’) plt.plot(t,yhat,’r’,linewidth=4) 400
7.1. PRINCIPLES OF REGRESSION 7.1.3 Extension: Beyond a straight line Regression is a powerful technique. Although we have discussed its usefulness for fitting straight lines, the same concept can fit other curves. To generalize the regression formulation, we consider a d-dimensional regression coef- ficient vector θ =[θ ,...,θ ]T ∈Rd and a general linear model 0 d−1 d−1 (cid:88) g (x )= θ φ (x ). θ n p p n p=0 Here, the mappings {φ (·)}d−1 can be considered as a nonlinear transformation that takes p p=0 the input x and maps it to another value. For example, φ (·) = (·)p will map an input x n p to a pth power xp. We can now write the system of linear equations as        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . . (7.8)  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) y X θ e Let us look at some examples. Example 7.1. (Quadratic fitting) Consider the linear regression problem using a quadratic equation: y =ax2 +bx +c, n=1,...,N. n n n Express this equation in matrix-vector form. Solution. The matrix-vector expression is  y   x2 x 1  e  1 1 1   1 y 2 x2 2 x 2 1 a e 2   . .  =  . . . . . . b+  . .  .  .   . . . c  .  y x2 x 1 e N N N N This is again in the form of y =Xθ+e. The MATLAB and Python programs for Example 7.1 are shown below. A numerical example is illustrated in Figure 7.6. % MATLAB code to fit data using a quadratic equation N = 50; x = rand(N,1)*1; a = -2.5; b = 1.3; c = 1.2; 401
CHAPTER 7. REGRESSION 2.5 2 1.5 1 0.5 data fitted curve 0 0 0.2 0.4 0.6 0.8 1 Figure 7.6: Example: Our goal is to fit the dataset of 50 data points shown above. The model we use is g (x )=ax2 +bx +c, for n=1,...,N. θ n n n y = a*x.^2 + b*x + c + 1*rand(size(x)); N = length(x); X = [ones(N,1) x(:) x(:).^2]; beta = X\y(:); t = linspace(0, 1, 200); yhat = theta(3)*t.^2 + theta(2)*t + theta(1); plot(x,y, ’o’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,6); # Python code to fit data using a quadratic equation import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.rand(N) a = -2.5 b = 1.3 c = 1.2 y = a*x**2 + b*x + c + 0.2*np.random.randn(N) X = np.column_stack((np.ones(N), x, x**2)) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(0,1,200) yhat = theta[0] + theta[1]*t + theta[2]*t**2 plt.plot(x,y,’o’) plt.plot(t,yhat,’r’,linewidth=4) The generalization to polynomials of arbitrary order is to replace the model with d−1 (cid:88) g (x )= θ xp, θ n p p=0 402
7.1. PRINCIPLES OF REGRESSION where p = 0,1,...,d−1 represent the orders of the polynomials and θ ,...,θ are the 0 d−1 regression coefficients. In this case, the matrix system is  y   1 x ··· xd−1 θ   e  1 1 1 0 1   y . .2  =  1 . . x 2 · . .·· x 2d . .−1      θ . .1   +  e . .2  ,  .  . ··· . .  .   .  y N 1 x N ··· x Nd−1 θ d−1 e N which again is in the form of y =Xθ+e. Example 7.2. (Legendre polynomial fitting) Let {L (·)}d−1 be a set of Legendre p p=0 polynomials (see discussions below), and consider the linear regression problem using d−1 (cid:88) y = θ L (x), n=1,...,N. n p p p=0 Express this equation in matrix-vector form. Solution. The matrix-vector expression is        y L (x ) L (x ) ··· L (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 L 0(x 2) L 1(x 2) ··· L d−1(x 2) θ 1  e 2  . = . . .  . + . .  .   . . .  .   .   .   . ··· . .  .   .  y L (x ) L (x ) ··· L (x ) θ e N 0 N 1 N d−1 N d−1 N Legendre polynomials are orthogonal polynomials. In conventional polynomials, the functions {x,x2,x3,...,xp} are not orthogonal. As we increase p, the set of functions {x,x2,x3,...,xp} will have redundancy, which will eventually result in the matrix X being noninvertible. The pth-order Legendre polynomial is denoted by L (x). Using the Legendre polyno- p mials as the building block of the regression problem, the model is expressed as d−1 def(cid:88) g (x) = θ L (x) θ p p p=0 =θ L (x)+θ L (x)+θ L (x) +···+θ L (x), 0 0 1 1 2 2 d−1 d−1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =x =1(3x2−1) 2 where L (·), L (·) and L (·) are the Legendre polynomials of order 0, 1 and 2, respectively. 0 1 2 As an example, the first few leading Legendre polynomials are L (x)=1, 0 L (x)=x, 1 1 L (x)= (3x2−1), 2 2 1 L (x)= (5x3−3x). 3 2 403
CHAPTER 7. REGRESSION The order of the Legendre polynomials is always the same as that of the ordinary polyno- mials. The shapes of these polynomials are shown in Figure 7.7(a). 4 1 data 3 Legendre basis Polynomial basis 0.5 2 0 1 L (x) 0 L 1(x) 0 -0.5 L 2(x) L (x) -1 3 L (x) 4 -1 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) (b) Figure7.7:(a)Thefirst5leadingLegendrepolynomialsplottedintherangeof−1≤x≤1.(b)Fitting the data using an ordinary polynomial and a Legendre polynomial. Figure 7.7(b) demonstrates a fitting problem using the Legendre polynomials. You can see that the fitting is just as good as that of the ordinary polynomials (which should be the case). However, if we compare the coefficients, we observe that the magnitude of the Legendre coefficients is smaller (see Table 7.1). In general, as the order of polynomials increasesandthenoisegrows,theordinarypolynomialswillbecomeincreasinglydifficultto fit the data. θ θ θ θ θ 4 3 2 1 0 Ordinary polynomials 5.3061 3.3519 −3.6285 −1.8729 0.1540 Legendre polynomials 1.2128 1.3408 0.6131 0.1382 0.0057 Table 7.1: Theregressioncoefficientsof anordinary polynomialandaLegendrepolynomial. Notethat whilebothpolynomialscanfitthedata,theLegendrepolynomialcoefficientshavesmallermagnitudes. Calling Legendre polynomials for regression is not difficult in MATLAB and Python. Specifically, one can call legendreP in MATLAB and scipy.special.eval_legendre in Python. % MATLAB code to fit data using Legendre polynomials N = 50; x = 1*(rand(N,1)*2-1); a = [-0.001 0.01 +0.55 1.5 1.2]; y = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ... + a(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ... + a(5)*legendreP(4,x) + 0.5*randn(N,1); X = [legendreP(0,x(:)) legendreP(1,x(:)) ... legendreP(2,x(:)) legendreP(3,x(:)) ... 404
7.1. PRINCIPLES OF REGRESSION legendreP(4,x(:))]; beta = X\y(:); t = linspace(-1, 1, 200); yhat = beta(1)*legendreP(0,t) + beta(2)*legendreP(1,t) + ... + beta(3)*legendreP(2,t) + beta(4)*legendreP(3,t) + ... + beta(5)*legendreP(4,t); plot(x,y,’ko’,’LineWidth’,2,’MarkerSize’,10); hold on; plot(t,yhat,’LineWidth’,6,’Color’,[0.9 0 0]); import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre N = 50 x = np.linspace(-1,1,N) a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.2*np.random.randn(N) X = np.column_stack((eval_legendre(0,x), eval_legendre(1,x), \ eval_legendre(2,x), eval_legendre(3,x), \ eval_legendre(4,x))) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(-1, 1, 50); yhat = theta[0]*eval_legendre(0,t) + theta[1]*eval_legendre(1,t) + \ theta[2]*eval_legendre(2,t) + theta[3]*eval_legendre(3,t) + \ theta[4]*eval_legendre(4,t) plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=8) plt.show() The idea of fitting a set of data using the Legendre polynomials belongs to the larger family of basis functions. In general, we can use a set of basis functions to model the data: d−1 def(cid:88) g (x) = θ φ (x), θ p p p=0 where {φ (x)}d−1 are the basis functions and {θ }d−1 are the regression coefficients. The p p=0 p p=0 constant θ is often called the bias of the regression. 0 Choiceoftheφ (x)canbeextremelybroad.Onecanchoosetheordinarypolynomials p φ (x)=xp or the Legendre polynomial φ (x)=L (x). Other choices are also available: p p p • Fourier basis: φ p(x)=ejωpx, where ω p is the pth carrier frequency. • Sinusoid basis: φ (x) = sin(ω x), which is same as the Fourier basis but taking the p p imaginary part. 405
CHAPTER 7. REGRESSION • Gaussian basis: φ (x)= √1 exp(cid:110) −(x−µp)2(cid:111) , where (µ ,σ ) are the model param- p 2πσ p2 2σ p2 p p eters. Evidently,bychoosingdifferentbasisfunctionswehavedifferentwaystofitthedata.There isnodefinitiveanswerastowhichfunctionsarebetter.Statisticaltechniquessuchasmodel selections are available, but experience will tell you to align with one and not the other. It is frequently more useful to have some domain knowledge rather than resorting to various computational techniques. How to fit data using basis functions • Construct this equation:        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . , (7.9)  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) y X θ e • The functions φ (x) are the basis functions, e.g., φ (x) = xp for ordinary poly- p p nomials. • You can replace the polynomials with the Legendre polynomials. • You can also replace the polynomials with other basis functions. • Solve for θ by θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. θ Example 7.3. (Autoregressive model) Consider a two-tap autoregressive model: y =ay +by , n=1,2,...,N n n−1 n−2 where we assume y =y =0. Express this equation in the matrix-vector form. 0 −1 Solution. The matrix-vector form of the equation is       y y y e 1 0 −1 1 y 2  y 1 y 0 (cid:20) a(cid:21) e 2  . = . .  + . .  .   . .  b  .   .   . .   .  (cid:124)(cid:123)(cid:122)(cid:125) y y y e N N−1 N−2 =θ N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X In general, we can append more previous samples to predict the future. The general 406
7.1. PRINCIPLES OF REGRESSION expression is L (cid:88) y = θ y , n=1,2,...,N, n (cid:96) n−(cid:96) (cid:96)=1 where (cid:96) = 1,2,...,L denote the previous L samples of the data and {θ ,...,θ } are the 1 L regression coefficients. If we do this we see that the matrix expression is    y y y ··· y    y 0 −1 −2 1−L e 1 1        y y y . . .2 3 4       =         y y y . . .1 2 3 y y y . . .0 1 2 y y y− . . .0 11 · · · .. . .· · ·· · · y y y2 3 4− − − . . . L L L             θθ θ . . . L1 2    +       e e e . . .2 3 4       . y . (cid:124)(cid:123)(cid:122)(cid:125) e N y y y . y N N−1 N−2 N−3 N−L =θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X Observe the pattern associated with this matrix X. Each column is a one-entry shifted version of the previous column. This matrix is called a Toeplitz matrix. The MATLAB (and Python) code for calling and using the Toeplitz matrix is shown below. % MATLAB code for auto-regressive model N = 500; y = cumsum(0.2*randn(N,1)) + 0.05*randn(N,1); % generate data L = 100; % use previous 100 samples c = [0; y(1:400-1)]; r = zeros(1,L); X = toeplitz(c,r); % Toeplitz matrix theta = X\y(1:400); % solve y = X theta yhat = X*theta; % prediction plot(y(1:400), ’ko’,’LineWidth’,2);hold on; plot(yhat(1:400),’r’,’LineWidth’,4); # Python code for auto-regressive model import numpy as np import matplotlib.pyplot as plt from scipy.linalg import toeplitz N = 500 y = np.cumsum(0.2*np.random.randn(N)) + 0.05*np.random.randn(N) L = 100 c = np.hstack((0, y[0:400-1])) r = np.zeros(L) X = toeplitz(c,r) theta = np.linalg.lstsq(X, y[0:400], rcond=None)[0] yhat = np.dot(X, theta) 407
CHAPTER 7. REGRESSION plt.plot(y[0:400], ’o’) plt.plot(yhat[0:400],linewidth=4) The plots generated by the above programs are shown in Figure 7.8(a). Note that we aredoinganinterpolation,becausewearepredictingthevalueswithinthetrainingdataset. 2 2 0 0 -2 -2 -4 -4 -6 -6 0 100 200 300 400 500 0 100 200 300 400 500 (a) (b) Figure7.8:Autoregressivemodelonasimulateddataset,usingL=100coefficients.(a)Trainingdata. Notethatthemodeltrainsverywellonthisdataset.(b)Testingdata.Whentestedonfuturedata,the autoregressive model can still predict for a few samples but loses track when the time elapsed grows. We now consider extrapolation. Given the training data, we can find the regression coefficients by solving the above linear equation. This gives us θ. To predict the future samples we need to return to the equation L (cid:88) y = θ y , n=1,2,...,N, (cid:98)n (cid:96) (cid:98)n−(cid:96) (cid:124) (cid:123)(cid:122) (cid:125) (cid:96)=1 =previousestimate wherey arethepreviousestimates.Forexample,ifwearegiven100daysofstockprices, (cid:98)n−(cid:96) then predicting the 101st day’s price should be based on the L days before the 101st. A simple for-loop suffices for such a calculation. Figure7.8(b)showsanumericalexampleofextrapolatingdatausingtheautoregressive model.InthisexperimentweuseN =400samplestotrainanautoregressivemodeloforder L = 100. We then predict the data for another 100 data points. As you can see from the figure, the first few samples still look reasonable. However, as time increases, the model starts to lose track of the real trend. Isthereanywaywecanimprovetheautoregressivemodel?Asimplewayistoincrease the memory L so that we can use a long history to predict the future. This boils down to the long-term running average of the curve, which works well in many cases. However, if the testing data does not follow the same distribution as the training data (which is often the case in the real stock market because unexpected news can change the stock price), then even the long-term average will not be a good forecast. That is why data scientists on Wall Street make so much money: they have advanced mathematical tools for modeling the stockmarket.Nevertheless,wehopethattheautoregressivemodelprovidesyouwithanew perspective for analyzing data. The summary below highlights the main ideas of the autoregressive model. 408
7.1. PRINCIPLES OF REGRESSION What is the autoregressive model? • It solves this problem    y y y ··· y          yy yy . . .1 2 3     =       yy . . .0 21 y y− . . .0 11 y y− − . . .02 1 · · . . . . .· ·· · y y1 2 3− − − . . . L L L           θθ θ . . . L1 2    +      ee e e . . .1 2 3     . (7.10) N y N−1 y N−2 y N−3 . y N−L (cid:124)(cid:123)(cid:122)(cid:125) N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =θ (cid:124) (cid:123)(cid:122) (cid:125) =y =e =X • The number of taps in the past history would affect the memory and hence the long-term forecast. • Solve for θ by θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. (7.11) θ∈Rd 7.1.4 Overdetermined and underdetermined systems The sub-section requires knowledge of some concepts in linear algebra that can be found in standard references.a aCarlMeyer,Matrix Analysis and Applied Linear Algebra,SIAM,2000. Let us now consider the theoretical properties of the least squares linear regression problem, which is an optimization: θ(cid:98)=argmin(cid:107)y−Xθ(cid:107)2. (P1) θ∈Rd We observe that the objective value of this optimization problem can go to zero if and only if the minimizer θ(cid:98)is the solution of the system of linear equations Findθ such thaty =Xθ. (P2) We emphasize that Problem (P1) and Problem (P2) are two different problems. Even if we cannot solve Problem (P2), Problem (P1) is still well defined, but the objective value will not go to zero. This subsection aims to draw the connection between the two problems and discuss the respective solutions. We will start with Problem (P2) by considering two shapes of the matrix X. Overdetermined system Problem(P2)iscalledoverdeterminedifX ∈RN×distallandskinny,i.e.,N >d.Thishap- penswhenyouhavemorerowsthancolumns,orequivalentlywhenyouhavemoreequations than unknowns. When N > d, Problem (P2) has a unique solution θ(cid:98)= (XTX)−1XTy if 409
CHAPTER 7. REGRESSION andonlyifXTX isinvertible,orequivalentlyifandonlyifthecolumnsofX arelinearlyin- dependent.AtechnicaldescriptionofthisisthatX hasafullrank,denotedbyrank(X)=d. Whenrank(X)=d,Problem(P1)hasauniqueglobalminimizerθ(cid:98)=(XTX)−1XTy,which is the same as the unique solution of Problem (P2). Figure 7.9: Hierarchy of the solutions of an overdetermined system. An overdetermined system uses a tall and skinny matrix X. The rank of a matrix X is defined as the largest number of independent columnswecanfindinX.Ifrank(X)=d,thematrixXTX isinvertible,andProblem(P2)willhave a unique solution. If rank(X) < d, then the solution depends on whether the particular observation y lives in the range space of X. If yes, Problem (P2) will have infinitely many solutions because there is a nontrivial null space. If no, Problem (P2) will have no solution because the system is incompatible. If the columns of X are linearly dependent so that XTX is not invertible, we say that X is rank-deficient (denoted as rank(X) < d). In this case, Problem (P2) may not have a solution. We say that it may not have a solution because it is still possible to have a solution. It all depends on whether y can be written as a linear combination of the linearly independent columns of X. If yes, we say that y lives in the range space of X. The range space of X is defined as the set of vectors {z|z =Xα, for someα}. If rank(X)=d, all y will live in the range space of X. But if rank(X) < d, only some of the y will live in the range space of X. When this happens, the matrix X must have a nontrivial null space. The null space of X is defined as the set of vectors {z|Xz = 0}. A nontrivial null space will give us infinitely many solutions to Problem (P2). This is because if α is the solution found in the range space so that y = Xα, then we can pick any z from the null space such that Xz = 0. This will lead to another solution α+z such that X(α+z)=Xα+0=y. Since we have infinitely many choices of such z’s, there will be infinitely many solutions to Problem (P2). AlthoughthereareinfinitelymanysolutionstoProblem(P2),allofthemaretheglobal minimizers of Problem (P1). They can make the objective value equal to zero because the equality y = Xθ holds. However, the solutions to Problem (P2) are not unique since the objective function is convex but not strictly convex. If y does not live in the range space of X, we say that Problem (P2) is incompatible. If a system of linear equations is incompatible, there is no solution. However, even when this happens, we can still solve the optimization Problem (P1), but the objective value will not reach 0. The minimizer is a global minimizer because the objective function is convex, 410
7.1. PRINCIPLES OF REGRESSION but the minimizer is not unique. Underdetermined system Problem (P2) is called underdetermined if X is fat and short, i.e., N < d. This happens whenyouhavemorecolumnsthanrows,orequivalentlywhenyouhavemoreunknownsthan equations. In this case, XTX is not invertible, and so we cannot use θ(cid:98)= (XTX)−1XTy asthesolution.However,ifrank(X)=N,thenany y willliveintherangespaceofX.But because X is fat and short, there exists a nontrivial null space. Therefore, Problem (P2) will have infinitely many solutions, attributed to the vectors generated by the null space. For this set of infinitely many solutions, the corresponding Problem (P1) will have a global minimizer, and the objective value will be zero. However, the minimizer is not unique. This is the first case in Figure 7.10. Figure7.10:Hierarchyofthesolutionsofanunderdeterminedsystem.Anunderdeterminedsystemuses a fat and short matrix X. The rank of a matrix X is defined as the largest number of independent columns we can find in X. If rank(X) = N, we will have infinitely many solutions. If rank(X) < N, thenthesolutionsdependsonwhethertheparticularobservationy livesintherangespaceofX.Ifyes, Problem(P2)willhaveinfinitelymanysolutionsbecausethereisanontrivialnullspace.Ifno,Problem (P2) will have no solution because the system is incompatible. There are two other cases in Figure 7.10, which occur when rank(X)<N: • (i) If y is in the range space of X, Problem (P2) will have infinitely many solutions. Since Problem (P2) remains feasible, the objective function of Problem (P1) will go to zero. • (ii) If y is not in the range space of X, the system in Problem (P2) is incompatible andtherewillbenosolution.TheobjectivevalueofProblem(P1)willnotgotozero. Ifanunderdeterminedsystemhasinfinitelymanysolutions,weneedtopickandchoose. One of the possible approaches is to consider the optimization θ(cid:98)=argmin (cid:107)θ(cid:107)2 subjectto Xθ =y. (P3) θ∈Rd This optimization is different from Problem (P1), which is an unconstrained optimization. OurgoalistominimizethedeviationbetweenXθandy.Problem(P3)isconstrained.Since 411
CHAPTER 7. REGRESSION we assume that Problem (P2) has infinitely many solutions, the constraint set y = Xθ is feasible. Among all the feasible choices, we pick the one that minimizes the squared norm. Therefore, the solution to Problem (P3) is called the minimum-norm least squares. Theorem 7.2 below summarizes the solution. If y does not live in the range space of X, then Problem (P2) does not have a solution. Therefore, the constraint in P3 is infeasible, and hence the optimization problem does not have a minimizer. Theorem7.2. Considerthe underdeterminedlinearregressionproblemwhereN <d: θ(cid:98)=argmin (cid:107)θ(cid:107)2 subject to y =Xθ, θ∈Rd where X ∈ RN×d, θ ∈ Rd, and y ∈ RN. If rank(X) = N, then the linear regression problem will have a unique global minimum θ(cid:98)=XT(XXT)−1y. (7.12) This solution is called the minimum-norm least-squares solution. Proof. The proof of the theorem requires some knowledge of constrained optimization. Consider the Lagrangian of the problem: L(θ,λ)=(cid:107)θ(cid:107)2+λT(Xθ−y), where λ is called the Lagrange multiplier. The solution of the constrained optimization is thestationarypointoftheLagrangian.Tofindthestationarypoint,wetakethederivatives with respect to θ and λ. This yields ∇ L=2θ+XTλ=0, θ ∇ L=Xθ−y =0. λ The first equation gives us θ = −XTλ/2. Substituting it into the second equation, and assuming that rank(X)=N so that XTX is invertible, we have (cid:16) (cid:17) X −XTλ/2 −y =0, which implies that λ=−2(XXT)−1y. Therefore, θ =XT(XXT)−1y. (cid:3) The end of this subsection. Please join us again. 7.1.5 Robust linear regression This subsection is optional for a first reading of the book. The linear regression we have discussed so far is based on an important criterion, namely the squared error criterion. We chose the squared error as the training loss because 412
7.1. PRINCIPLES OF REGRESSION it is differentiable and convex. Differentiability allows us to take the derivative and locate the minimum point. Convexity allows us to claim a global minimizer (also unique if the objective function is strictly convex). However, such a nice criterion suffers from a serious drawback: the issue of outliers. Consider Figure 7.11. In Figure 7.11(a), we show a regression problem for N = 50 datapoints.Ourbasisfunctionsaretheordinarypolynomialsinthefourthorder.Everything looksfineinthefigure.Weinterveneinthedatabyrandomlyalteringafewofthemsothat their values are off. There are only a handful of these outliers. We run the same regression analysis again, but we observe (see Figure 7.11(b)) that our fitted curve has been distorted quite significantly. 4 5 4 3 3 2 2 1 1 0 0 -1 data -1 data fitted curve fitted curve -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) (·)2 without outlier (b) (·)2 with outlier Figure 7.11: Linearregressionusingthesquarederrorasthetraininglosssuffersfromoutliers.(a)The regressionperformswellwhenthereisnooutlier.(b)Byaddingonlyafewoutliers,theregressioncurve has already been distorted. This occurs because of the squared error. By the definition of a squared error, our training loss is N (cid:18) (cid:19)2 (cid:88) E (θ)= y −g (x ) . train n θ n n=1 Withoutlossofgenerality,letusassumethatoneoftheseerrortermsislargebecauseofan outlier. Then the training loss becomes (cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2 (cid:18) (cid:19)2 E (θ)= y −g (x ) + y −g (x ) + y −g (x ) +···+ y −g (x ) . train 1 θ 1 2 θ 2 3 θ 3 N θ N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) small small large small Here is the daunting fact: If one or a few of these individual error terms are large, the squareoperationwillamplify them.Asaresult,theerroryouseeisnotjustlargebutlarge2. Moreover, since we put the squares to the small errors as well, we have small2 instead of small. When you try to weigh the relative significance between the outliers and the normal datapoints,theoutlierssuddenlyhaveaverylargecontributiontotheerror.Sincethegoal of linear regression is to minimize the total loss, the presence of the outliers will drive the optimization solution to compensate for the large error. 413
CHAPTER 7. REGRESSION One possible solution is to replace the squared error by the absolute error, such that N (cid:12) (cid:12) (cid:88)(cid:12) (cid:12) E train(θ)= (cid:12)y n−g θ(x n)(cid:12). (cid:12) (cid:12) n=1 This is a simple modification, but it is very effective. The reason is that the absolute error keeps the small just as small, and keeps the large just as large. There is no amplification. Therefore, while the outliers still contribute to the overall loss, their contributions are less prominent. (If you have a lot of strong outliers, even the absolute error will fail. If this happens, you should go back to your data collection process and find out what has gone wrong.) Whenweusetheabsoluteerrorasthetrainingloss,theresultingregressionproblemis the least absolute deviation regression (or simply the robust regression). The tricky thing about the least absolute deviation is that the training loss is not differentiable. In other words, we cannot take the derivative and find the optimal solution. The good news is that there exists an alternative approach for solving this problem: using linear programming (implemented via the simplex method). Solving the robust regression problem Let us focus on the linear model g (x )=xTθ, θ n n where x = [φ (x ),...,φ (x )]T ∈ Rd is the nth input vector for some basis functions n 0 n d−1 n {φ }d−1, and θ =[θ ,...,θ ]T ∈Rd is the parameter. Substituting this into the training p p=0 0 d−1 loss, the optimization problem is N (cid:12) (cid:12) mi θn ∈im Rdize (cid:88)(cid:12) (cid:12) (cid:12)y n−xT nθ(cid:12) (cid:12) (cid:12). n=1 Hereisanimportanttrick.Theideaistoexpresstheproblemasanequivalentproblem N (cid:88) minimize u n θ∈Rd,u∈RN n=1 subjectto u =|y −xTθ|, n=1,...,N. n n n Thereisasmallbutimportantdifferencebetweenthisproblemandthepreviousone.Inthe firstproblem,thereisonlyoneoptimizationvariableθ.Inthenewproblem,weintroducean additionalvariableu=[u ,...,u ]T andaddaconstraintu =|y −xTθ|forn=1,...,N. 1 N n n n We introduce u so that we can have some additional degrees of freedom. At the optimal solution, u must equal to |y −xTθ|, and so the corresponding θ is the solution of the n n n original problem. Now we note that x=|a| is equivalent to x≥a and x≥−a. Therefore, the constraint can be equivalently written as N (cid:88) minimize u , (7.13) n θ∈Rd,u∈RN n=1 subjectto u ≥−(y −xTθ), n=1,...,N n n n u ≥(y −xTθ), n=1,...,N. n n n 414
7.1. PRINCIPLES OF REGRESSION In other words, we have rewritten the equality constraint as a pair of inequality constraints by removing the absolute signs. The optimization in Equation (7.13) is in the form of a standard linear programming problem. A linear programming problem takes the form of minimize cTx (7.14) x∈Rk subjectto Ax≤b, for some vectors c ∈ Rk, b ∈ Rm, and matrix A ∈ Rm×k. Linear programming is a stan- dard optimization problem that you can find in most optimization textbooks. On a com- puter, if we know c, b and A, solving the linear programming problem can be done using built-in commands. For MATLAB, the command is linprog. For Python, the command is scipy.optimize.linprog. We will discuss a concrete example shortly. % MATLAB command for linear programming x = linprog(c, A, b); # Python command for linear programming linprog(c, A, b, bounds=(None,None), method="revised simplex") GivenEquation(7.13),thequestionbecomeshowtoconvertitintothestandardlinear programming format. This requires two steps. The first step uses the objective function: N d−1 N (cid:88) (cid:88) (cid:88) u = (0)(θ )+ (1)(u ) n p n n=1 p=0 n=1 (cid:20) (cid:21) (cid:2) (cid:3) θ = 0 0 ··· 0 1 1 ··· 1 . u (cid:124) (cid:123)(cid:122) (cid:125) =cT Therefore, the vector c has d 0’s followed by N 1’s. The second step concerns the constraint. It can be shown that u ≥ −(y −xTθ) is n n n equivalent to xTθ−u ≤y . Written in the matrix form, we have n n n  xT −1 0 ··· 0  θ   y  1 1 xT 2 0 −1 ··· 0 u 1 y 2  . . . .  . ≤ . ,  . . . .  .   .   . . . ··· .  .   .  xT 0 0 ··· −1 u y N N N which is equivalent to (cid:20) (cid:21) (cid:2) (cid:3) θ X −I ≤y, (7.15) u where I ∈RN×N is the identity matrix. Similarly, the other constraint u ≥ (y −xTθ) is equivalent to −xTθ−u ≤ −y . n n n n n n Written in the matrix form, we have  −xT −1 0 ··· 0  θ   −y  1 1 −xT 2 0 −1 ··· 0 u 1 −y 2  . . . .  . ≤ . ,  . . . .  .   .   . . . ··· .  .   .  −xT 0 0 ··· −1 u −y N N N 415
CHAPTER 7. REGRESSION which is equivalent to (cid:20) (cid:21) (cid:2) (cid:3) θ −X −I ≤−y u Puttingeverythingtogether,wehavefinallyarrivedatthelinearprogrammingproblem (cid:20) (cid:21) (cid:2) (cid:3) θ minimize 0 1 θ∈Rd,u∈RN d N u (cid:20) (cid:21)(cid:20) (cid:21) (cid:20) (cid:21) X −I θ y subject to ≤ , −X −I u −y where 0 ∈ Rd is an all-zero vector, and 1 ∈ RN is an all-one vector. It is this problem d N that solves the robust linear regression. Let us look at how to implement linear programming to solve the robust regression optimization. As an example, we continue with the polynomial fitting problem in which there are outliers. We choose the ordinary polynomials as the basis functions. To construct the linear programming problem, we need to define the matrix A and the vectors c and b according to the linear programming form. This is done using the following MATLAB program. % MATLAB code to demonstrate robust regression N = 50; x = linspace(-1,1,N)’; a = [-0.001 0.01 0.55 1.5 1.2]; y = a(1)*legendreP(0,x) + a(2)*legendreP(1,x) + ... a(3)*legendreP(2,x) + a(4)*legendreP(3,x) + ... a(5)*legendreP(4,x) + 0.2*randn(N,1); idx = [10, 16, 23, 37, 45]; y(idx) = 5; X = [x(:).^0 x(:).^1 x(:).^2 x(:).^3 x(:).^4]; A = [X -eye(N); -X -eye(N)]; b = [y(:); -y(:)]; c = [zeros(1,5) ones(1,N)]’; theta = linprog(c, A, b); t = linspace(-1,1,200)’; yhat = theta(1) + theta(2)*t(:) + ... theta(3)*t(:).^2 + theta(4)*t(:).^3 + ... theta(5)*t(:).^4; plot(x,y, ’ko’,’LineWidth’,2); hold on; plot(t,yhat,’r’,’LineWidth’,4); In this set of commands, the basis vectors are defined as xT = [φ (x ),...,φ (x )]T, for n 4 n 0 n n=1,...,N.ThematrixI isconstructedbyusingthecommandeye(N),whichconstructs theidentitymatrixofsizeN×N.Therestofthecommandsareself-explanatory.Notethat the solution to the linear programming problem consists of both θ and u. To squeeze θ we need to locate the first d entries. The remainder is u. CommandsforPythonaresimilar,althoughweneedtocallnp.hstackandnp.vstack toconstructthematricesandvectors.Themainroutineislinproginthescipy.optimize 416
7.1. PRINCIPLES OF REGRESSION library. Note that for this particular example, the bounds are bounds=(None,None), or otherwise Python will search in the positive quadrant. # Python code to demonstrate robust regression import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre from scipy.optimize import linprog N = 50 x = np.linspace(-1,1,N) a = np.array([-0.001, 0.01, 0.55, 1.5, 1.2]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.2*np.random.randn(N) idx = [10,16,23,37,45] y[idx] = 5 X = np.column_stack((np.ones(N), x, x**2, x**3, x**4)) A = np.vstack((np.hstack((X, -np.eye(N))), np.hstack((-X, -np.eye(N))))) b = np.hstack((y,-y)) c = np.hstack((np.zeros(5), np.ones(N))) res = linprog(c, A, b, bounds=(None,None), method="revised simplex") theta = res.x t = np.linspace(-1,1,200) yhat = theta[0]*np.ones(200) + theta[1]*t + theta[2]*t**2 + \ theta[3]*t**3 + theta[4]*t**4 plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=8) plt.show() The result of this experiment is shown in Figure 7.12. It is remarkable to see that the robust regression result is almost as good as the result would be without outliers. If robust linear regression performs so well, why don’t we use it all the time? Why is least squares regression still more popular? The answer has a lot to do with the com- putational complexity and the uniqueness of the solution. Linear programming requires an algorithmforasolution.Whilewehaveveryfastlinearprogrammingsolverstoday,thecom- putational cost of solving a linear program is still much higher than solving a least-squares problem (which is essentially a matrix inversion). The other issue with robust linear regression is the uniqueness of the solution. Lin- ear programming is known to have degenerate solutions when the constraint set (a high- dimensional polygon) touches the objective function (which is a line) at one of its edges. The least-squares fitting does not have this problem because the optimization surface is a parabola. Unless the matrix XTX is noninvertible, the solution is guaranteed to be the unique global minimum. Linear programming does not have this convenient property. We canhavemultiplesolutionsθ thatgivethesameobjectivevalue.Ifyoutrytointerpretyour resultbyinspectingthemagnitudeoftheθ’s,thenonuniquenessofthesolutionwouldcause problems because your interpretation can be swiped immediately if the linear programming gives you a nonunique solution. 417
CHAPTER 7. REGRESSION 5 5 4 4 3 3 2 2 1 1 0 0 -1 -1 -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ordinary (·)2 regression with outliers (b) Robust |·| regression with outliers Figure 7.12: (a)Ordinarylinearregressionusing(·)2 asthetrainingloss.Intheabsenceofanyoutlier, the regression performs well. (b) Robust linear regression using |·| as the training loss. Note that even in the presence of outliers, the robustness regression perform reasonably well. End of this subsection. Please join us again. Closing remark. The principle of linear regression is primarily to set up a function to fit the data. This, in turn, is about finding a set of good basis functions and minimizing the appropriate training loss. Selecting the basis is usually done in several ways: • The problem forces you to choose certain basis functions. For example, suppose you are working on a disease dataset. The variates are height, weight, and BMI. You do nothaveanychoiceherebecauseyourgoalistoseewhichfactorcontributesthemost to the cause of the disease. • There are known basis functions that work. For example, suppose you are working on a speech dataset. Physics tells us that Fourier bases are excellent representations of these sinusoidal functions. So it would make more sense to use the Fourier basis than the polynomials. • Sometimesthebasiscanbelearnedfromthedata.Forexample,youcanrunprincipal- componentanalysis(PCA)toextractthebasis.Thenyoucanrunthelinearregression to compute the coefficients. This is a data-driven approach and could apply to some problems. 7.2 Overfitting The regression principle we have discussed in the previous section is a powerful technique for data analysis. However, there are many ways in which things can fall apart. We have seen the problem of outliers, where perturbations of one or a few data points would result in a big change in the regression result, and we discussed some techniques to overcome the 418
7.2. OVERFITTING outlier problem, e.g., using robust regression. In addition to outliers, there are other causes of the failure of the regression. In this section, we examine the relationship between the number of training samples and the complexity of the model. For example, if we decide to use polynomials as the basis functionsandwehaveonlyN =20datapoints,whatshouldbetheorderofthepolynomials? Shallweusethe5th-orderpolynomial,orshallweusethe20th-order?Ourgoalinthissection is to acquire an understanding of the general problem known as overfitting. Then we will discuss methods for mitigating overfitting in Section 7.4. 7.2.1 Overview of overfitting ImaginethatwehaveadatasetcontainingN =20trainingsamples.Weknowthatthedata are generated from a fourth-order polynomial with Legendre polynomials as the basis. On top of these samples, we also know that a small amount of noise corrupts each sample, for example, Gaussian noise of standard deviation σ =0.1. We have two options here for fitting the data: • Option 1: h(x)=(cid:80)4 θ L (x), which is a 4th-order polynomial. p=0 p p • Option 2: g(x)=(cid:80)50 θ L (x), which is a 50th-order polynomial. p=0 p p Model2ismoreexpressivebecauseithasmoredegreesoffreedom.Letusfitthedatausing these two models. Figure 7.13 shows the results. However, what is going on with the 50th- order polynomial? It has gone completely wild. How can the regression ever choose such a terrible model? 3 3 2 2 1 1 0 0 -1 -1 -2 data -2 data fitted curve fitted curve -3 -3 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) 4th-order polynomial (b) 50th-order polynomial Figure 7.13: Fitting data using a 4th-order polynomial and a 50th-order polynomial. Here is an even bigger surprise: If we compute the training loss, we get 1 (cid:88)N (cid:18) (cid:19)2 E (h)= y −h(x ) =0.0063, train N n n n=1 1 (cid:88)N (cid:18) (cid:19)2 E (g)= y −g(x ) =5.7811×10−24. train N n n n=1 419
CHAPTER 7. REGRESSION Thus,whileModel2lookswildinthefigure,ithasamuchlowertraininglossthanModel1. So according to the training loss, Model 2 fits better. Any sensible person at this point will object, since Model 2 cannot possibly be better, for the following reason. It is not because it “looks bad”, but because if you test the model with an unseen sample it is almost certain that the testing error will explode. For example, in Figure 7.13(a) if we look at x = 0, we would expect the predicted value to be close to y = 0. However, Figure 7.13(b) suggests that the predicted value is going to negative infinity.Itwouldbehardtobelievethatthenegativeinfinityisabetterpredictionthanthe other one. We refer to this general phenomenon of fitting very well to the training data but generalizing poorly to the testing data as overfitting. What is overfitting? Overfitting means that a model fits too closely to the training samples so that it fails to generalize. Overfittingoccursasaconsequenceofanimbalancebetweenthefollowingthreefactors: • Number of training samples N. If you have many training samples, you should learn very well, even if the model is complex. Conversely, if the model is complex but does not have enough training samples, you will overfit it. The most serious problem in regression is often insufficient training data. • Model orderd.Thisreferstothecomplexityofthemodel.Forexample,ifyourmodel uses a polynomial, d refers to the order of the polynomial. If your training set is too small, you need to use a less complex model. The general rule of thumb is: “less is more”. • Noise variance σ2. This refers to the variance of the error e you add to the data. n The model we assumed in the previous numerical experiment is that y =g(x )+e , n=1,...,N. n n n where e ∼ Gaussian(0,σ2). If σ increases, it is inevitable that the fitting will be- n come more difficult. Hence it would require more training samples, and perhaps a less complex model would work better. 7.2.2 Analysis of the linear case Let us spell out the details of these factors one by one. To make our discussion concrete, we will use linear regression as a case study. The general analysis will be presented in the next section. Notations • Ground Truth Model. To start with, we assume that we have a population set D containinginfinitelymanysamples(x,y)drawnaccordingtosomelatentdistributions. The relationship between x and y is defined through an unknown target function y =f(x)+e, 420
7.2. OVERFITTING wheree∼Gaussian(0,σ2)isthenoise.Forouranalysis,weassumethatf(·)islinear, so that f(x)=xTθ∗, whereθ∗ ∈Rd isthegroundtruthmodelparameter.Noticethatf(·)isdeterministic, but e is random. Therefore, any randomness we see in y is due to e. • Training and Testing Set. From D, we construct two datasets: the training data set D that contains training samples {(x ,y ),...,(x ,y )} and the testing dataset train 1 1 N N D that contains {(x ,y ),...,(x ,y )}. Both D and D are subsets of D. test 1 1 M M train test • Predictive Model. We consider a predictive model g (·). For simplicity, we assume θ that g (·) is also linear: θ g (x)=xTθ. θ Given the training dataset D ={(x ,y ),...,(x ,y )}, we construct a linear regres- 1 1 N N sion problem: θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2. θ∈Rd Throughout our analysis, we assume that N ≥ d so that we have more training data than the number of unknowns. We further assume that XTX is invertible, and so there is a unique global minimizer θ(cid:98)=(XTX)−1XTy. • Training Error. Given the estimated model parameter θ(cid:98), we define the in-sample prediction as y (cid:98)train =X trainθ(cid:98), where X = X is the training data matrix. The in-sample prediction is the pre- train dicted value using the trained model for the training data. The corresponding error with respect to the ground truth is called the training error: (cid:20) (cid:21) 1 E train(θ(cid:98))=E e N(cid:107)y (cid:98)train−y(cid:107)2 , where N is the number of training samples in the training dataset. Note that the expectation is taken with respect to the noise vector e, which follows the distribution Gaussian(0,σ2I). • Testing Error. During testing, we construct a testing matrix X . This gives us the test estimated values y : (cid:98)test y (cid:98)test =X testθ(cid:98). Theout-samplepredictionisthepredictedvalueusingthetrainedmodelforthetesting data. The corresponding error with respect to the ground truth is called the testing error: (cid:20) (cid:21) 1 E test(θ(cid:98))=E e M(cid:107)y (cid:98)test−y(cid:107)2 , where M is the number of testing samples in the testing dataset. 421
CHAPTER 7. REGRESSION Analysis of the training error We first analyze the training error, which we defined as (cid:20) (cid:21) 1 E =E (cid:107)y−y(cid:107)2 d =ef MSE(y,y). (7.16) train e N (cid:98) (cid:98) For this particular choice of the training error, we call it the mean squared error (MSE). It measures the difference between y and y. (cid:98) Theorem 7.3. Let θ∗ ∈ Rd be the ground truth linear model parameter, and X ∈ RN×d be a matrix such that N ≥ d and XTX is invertible. Assume that the data follows the linear model y =Xθ∗+e where e∼Gaussian(0,σ2I). Consider the linear regression problem θ(cid:98) = argmin (cid:107)Xθ −y(cid:107)2, and the predicted value y (cid:98) = Xθ(cid:98). The θ∈Rd mean squared training error of this linear model is (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y)=E (cid:107)y−y(cid:107)2 =σ2 1− . (7.17) train (cid:98) e N (cid:98) N The proof below depends on some results from linear algebra that may be difficult for first-time readers. We recommend you read the proof later. Proof. Recall that the least squares linear regression solution is θ(cid:98)=(XTX)−1XTy. Since y =Xθ∗+e, we can substitute this into the predicted value y to show that (cid:98) y (cid:98)=Xθ(cid:98)=X(XTX)−1XTy =X(XTX)−1XT(Xθ∗+e)=Xθ∗+He. (cid:124) (cid:123)(cid:122) (cid:125) =H Therefore, substituting y =Xθ∗+He into the MSE, (cid:98) (cid:20) (cid:21) (cid:20) (cid:21) 1 1 MSE(y,y)d =efE (cid:107)y−y(cid:107)2 =E (cid:107)Xθ∗+He−Xθ∗−e(cid:107)2 (cid:98) e N (cid:98) e N (cid:20) (cid:21) 1 =E (cid:107)(H −I)e(cid:107)2 . e N At this point we need to use a tool from linear algebra. One useful identity3 is that for any v ∈RN, (cid:107)v(cid:107)2 =Tr(vvT). 3Thereasonforthisidentityisthat v= n(cid:88)N =1v n2 =Tr       vv Nv 2 . . .1 v2 v1 1 vv Nv1 . . .2v 2 v2 2 ·· · . ·· · .. ·· · v v1 2 v Nv v . . . 2N N      =Tr(cid:110) vvT(cid:111) . 422
7.2. OVERFITTING Using this identity, we have that (cid:20) (cid:21) (cid:20) (cid:26) (cid:27)(cid:21) 1 1 E (cid:107)(H −I)e(cid:107)2 = E Tr (H −I)eeT(H −I)T e N N e (cid:26) (cid:27) = 1 Tr (H −I)E (cid:2) eeT(cid:3) (H −I)T N e σ2 (cid:26) (cid:27) = Tr (H −I)(H −I)T , N whereweusedthefactthatE[eeT]=σ2I.ThespecialstructureofH tellsusthatHT =H and HTH = H. Thus, we have (H −I)T(H −I) = I −H. In addition, using the cyclic property of trace Tr(AB)=Tr(BA), we have that Tr(H)=Tr(X(XTX)−1XT) =Tr((XTX)−1XTX)=Tr(I)=d. Consequently, σ2 (cid:26) (cid:27) σ2 (cid:26) (cid:27) Tr (H −I)(H −I)T = Tr I−H N N (cid:18) (cid:19) d =σ2 1− . N This completes the proof. (cid:3) The end of the proof. Please join us again. Practice Exercise 1. In the theorem above, we proved the MSE of the prediction y. In this example, we would like to prove the MSE for the parameter. Prove that (cid:20)(cid:13) (cid:13)2(cid:21) (cid:26) (cid:27) MSE(θ(cid:98),θ∗)d =efE e (cid:13) (cid:13)θ(cid:98)−θ∗(cid:13) (cid:13) =σ2Tr (XTX)−1 . Solution. Let us start with the definition: (cid:20)(cid:13) (cid:13)2(cid:21) MSE(θ(cid:98),θ∗)=E e (cid:13) (cid:13)(XTX)−1XTy−θ∗(cid:13) (cid:13) (cid:20)(cid:13) (cid:13)2(cid:21) =E (cid:13)(XTX)−1XT(Xθ∗+e)−θ∗(cid:13) e (cid:13) (cid:13) (cid:20)(cid:13) (cid:13)2(cid:21) (cid:20)(cid:13) (cid:13)2(cid:21) =E (cid:13)θ∗+(XTX)−1XTe−θ∗(cid:13) =E (cid:13)(XTX)−1XTe(cid:13) . e (cid:13) (cid:13) e (cid:13) (cid:13) 423
CHAPTER 7. REGRESSION Continuing the calculation, (cid:20)(cid:13) (cid:13)2(cid:21) (cid:20) (cid:26) (cid:27)(cid:21) E (cid:13)(XTX)−1XTe(cid:13) =E Tr (XTX)−1XTe eTX(XTX)−1 e (cid:13) (cid:13) e (cid:26) (cid:20) (cid:21) (cid:27) =Tr (XTX)−1XTE eeT X(XTX)−1 e (cid:26) (cid:27) =Tr (XTX)−1XT · σ2I · X(XTX)−1 (cid:26) (cid:27) (cid:26) (cid:27) =σ2Tr (XTX)−1XT · X(XTX)−1 =σ2Tr (XTX)−1 . Analysis of the testing error Similarlytothetrainingerror,wecananalyzethetestingerror.Thetestingerrorisdefined as (cid:20) (cid:21) 1 E =MSE(y,y(cid:48))d =efE (cid:107)y−y(cid:48)(cid:107)2 , (7.18) test (cid:98) e,e(cid:48) M (cid:98) wherey =[y ,...,y ]T isavectorofM predictedvaluesandy(cid:48) =[y(cid:48),...,y(cid:48) ]T isavector (cid:98) (cid:98)1 (cid:98)M 1 M of M true values in the testing data.4 We would like to derive something concrete. To make our analysis simple, we consider a special case in which the testing set contains (x ,y(cid:48)),...,(x ,y(cid:48) ). That is, the inputs 1 1 N N x ,...,x areidenticalforbothtrainingandtesting(forexample,supposethatyoumeasure 1 N the temperature on two different days but at the same time stamps.) In this case, we have M =N, and we have X =X =X. However, the noise added to the testing data is test train still different from the noise added to the training data. With these simplifications, we can derive the testing error as follows. Theorem 7.4. Let θ∗ ∈ Rd be the ground truth linear model parameter, and X ∈ RN×d be a matrix such that N ≥d and XTX is invertible. Assume that the training data follows the linear model y = Xθ∗ +e, where e ∼ Gaussian(0,σ2I). Consider the linear regression problem θ(cid:98)= (XTX)−1XTy, and let y (cid:98) = Xθ(cid:98). Let X test = X be the testing input data matrix, and define y(cid:48) = X θ∗ + e(cid:48) ∈ RN, with e(cid:48) ∼ test Gaussian(0,σ2I), be the testing output. Then, the mean squared testing error of this linear model is (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ . (7.19) test (cid:98) e,e(cid:48) N (cid:98) N In this definition, the expectation is taken with respect to a joint distribution of (e,e(cid:48)). This is because, in testing, the trained model is based on y of which the randomness is e. However, the testing data is based on y(cid:48), where the randomness comes from e(cid:48). We assume that e and e(cid:48) are independent i.i.d. Gaussian vectors. 4Inpractice,thenumberoftestingsamplesMcanbemuchlargerthanthenumberoftrainingsamplesN. Thisprobablydoesnotagreewithyourexperience,inwhichthetestingdatasetisoftenmuchsmallerthan thetrainingdataset.Thereasonforthisparadoxisthatthepracticaltestingdatasetisonlyafinitesubset ofallthepossibletestingsamplesavailable.Sothe“testingerror”wecomputeinpracticeapproximatesthe truetestingerror.Ifyouwanttocomputethetruetestingerror,youneedaverylargetestingdataset. 424
7.2. OVERFITTING As with the previous proof, we recommend you study this proof later. Proof. The MSE can be derived from the definition: (cid:20) (cid:21) 1 MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 (cid:98) e,e(cid:48) N (cid:98) (cid:20) (cid:21) 1 = E (cid:107)Xθ∗+He−Xθ∗−e(cid:48)(cid:107)2 N e,e(cid:48) (cid:20) (cid:21) 1 = E (cid:107)He−e(cid:48)(cid:107)2 . N e,e(cid:48) Since each noise term e and e(cid:48) is an i.i.d. copy of the same Gaussian random variable, by n n using the fact that Tr(H)=Tr(X(XTX)−1XT) =Tr((XTX)−1XTX)=Tr(I)=d, we have that E (cid:104) (cid:107)He−e(cid:48)(cid:107)2(cid:105) =E (cid:2) (cid:107)He(cid:107)2(cid:3) −E (cid:104) 2eTHTe(cid:48)(cid:105) +E (cid:2) (cid:107)e(cid:48)(cid:107)2(cid:3) e,e(cid:48) e e,e(cid:48) e(cid:48) (cid:124) (cid:123)(cid:122) (cid:125) =0 =E (cid:104) Tr(cid:110) HeeTHT(cid:111)(cid:105) +E (cid:2) Tr(cid:8) e(cid:48)e(cid:48)T(cid:9)(cid:3) e e(cid:48) =Tr(cid:110) HE (cid:2) eeT(cid:3) HT(cid:111) +Tr{E (cid:2) e(cid:48)e(cid:48)T(cid:3) } e e(cid:48) =Tr(cid:110) H ·σ2I ·HT(cid:111) +Tr(cid:8) σ2I (cid:9) N×N N×N =σ2Tr(cid:110) HHT(cid:111) +Tr(cid:8) σ2I (cid:9) N×N =σ2Tr(I )+σ2Tr{I }=σ2(d+N). d×d N×N Combining all the terms, (cid:20) (cid:21) (cid:18) (cid:19) 1 d MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ , (cid:98) e,e(cid:48) N (cid:98) N which completes the proof. (cid:3) The end of the proof. 7.2.3 Interpreting the linear analysis results Let us summarize the two main theorems. They state that, for N ≥d, (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y)=E (cid:107)y−y(cid:107)2 =σ2 1− , (7.20) train (cid:98) e N (cid:98) N (cid:20) (cid:21) (cid:18) (cid:19) 1 d E d =ef MSE(y,y(cid:48))=E (cid:107)y−y(cid:48)(cid:107)2 =σ2 1+ . (7.21) test (cid:98) e,e(cid:48) N (cid:98) N This pair of equations tells us everything about the overfitting issue. 425
CHAPTER 7. REGRESSION How do E and E change w.r.t. σ2? train test • E ↑ as σ2 ↑. Thus noisier data are harder to fit. train • E ↑ as σ2 ↑. Thus a noiser model is more difficult to generalize. test The reasons for these results should be clear from the following equations: (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . test N As σ2 increases, the training error E grows linearly w.r.t. σ2. Since the training error train measures how good your model is compared with the training data, a larger E means it train is more difficult to fit. For the testing case, E also grows linearly w.r.t. σ2. This implies test that the model would be more difficult to generalize if the model were trained using noisier data. How do E and E change w.r.t. N? train test • E ↑ as N ↑. Thus more training samples make fitting harder. train • E ↓ as N ↑. Thus more training samples improve generalization. test The reason for this should also be clear from the following equations: (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . test N AsN increases,themodelseesmoretrainingsamples.Thegoalofthemodelistominimize theerrorwithallthetrainingsamples.Thusthemoretrainingsampleswehave,theharder itwillbetomakeeveryonehappy,sothetrainingerrorgrowsasN grows.Fortesting,ifthe model is trained with more samples it is more resilient to noise. Hence the generalization improves. How do E and E change w.r.t. d? train test • E ↓ as d ↑. Thus a more complex model makes fitting easier. train • E ↑ as d ↑. Thus a more complex model makes generalization harder. test These results are perhaps less obvious than the others. The following equations tell us that (cid:18) (cid:19) d E =σ2 1− , train N (cid:18) (cid:19) d E =σ2 1+ . (7.22) test N 426
7.2. OVERFITTING For this linear regression model to work, d has to be less than N; otherwise, the matrix inversion (XTX)−1 is invalid. However, as d grows while N remains fixed, we ask the linearregressiontofitalargerandlargermodelwhilenotprovidinganyadditionaltraining samples. Equation (7.22) says that E will drop as d increases but E will increase as d train test increases. Therefore, a larger model will not generalize as well if N is fixed. If d>N, then the optimization θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2 θ∈Rd willhavemanyglobalminimizers(seeFigure 7.10),implyingthatthetrainingerrorcango to zero. Our analysis of E and E does not cover this case because our proofs require train test (XTX)−1 to exist. However, we can still extrapolate what will happen. When the training error is zero, it only means that we fit perfectly into the training data. Since the testing error grows as d grows (though not in the particular form shown in Equation (7.22)), we should expect the testing error to become worse. Learning curve TheresultswederivedabovecanbesummarizedinthelearningcurveshowninFigure7.14. In this figure we consider a simple problem where y =θ +θ x +e , n 0 1 n n for e ∼Gaussian(0,1). Therefore, according to our theoretical derivations, we have σ =1 n andd=2.ForeveryN,wecomputetheaveragetrainingerrorE andtheaveragetesting train error E , and then mark them on the figure. These are our empirical training and testing test errors. On the same figure, we calculate the theoretical training and testing error according to Equation (7.22). TheMATLABandPythoncodesusedtogeneratethislearningcurveareshownbelow. Nset = round(logspace(1,3,20)); E_train = zeros(1,length(Nset)); E_test = zeros(1,length(Nset)); a = [1.3, 2.5]; for j = 1:length(Nset) N = Nset(j); x = linspace(-1,1,N)’; E_train_temp = zeros(1,1000); E_test_temp = zeros(1,1000); X = [ones(N,1), x(:)]; for i = 1:1000 y = a(1) + a(2)*x + randn(size(x)); y1 = a(1) + a(2)*x + randn(size(x)); theta = X\y(:); yhat = theta(1) + theta(2)*x; E_train_temp(i) = mean((yhat(:)-y(:)).^2); E_test_temp(i) = mean((yhat(:)-y1(:)).^2); end E_train(j) = mean(E_train_temp); 427
CHAPTER 7. REGRESSION E_test(j) = mean(E_test_temp); end semilogx(Nset, E_train, ’kx’, ’LineWidth’, 2, ’MarkerSize’, 16); hold on; semilogx(Nset, E_test, ’ro’, ’LineWidth’, 2, ’MarkerSize’, 8); semilogx(Nset, 1-2./Nset, ’k’, ’LineWidth’, 4); semilogx(Nset, 1+2./Nset, ’r’, ’LineWidth’, 4); import numpy as np import matplotlib.pyplot as plt Nset = np.logspace(1,3,20) Nset = Nset.astype(int) E_train = np.zeros(len(Nset)) E_test = np.zeros(len(Nset)) for j in range(len(Nset)): N = Nset[j] x = np.linspace(-1,1,N) a = np.array([1, 2]) E_train_tmp = np.zeros(1000) E_test_tmp = np.zeros(1000) for i in range(1000): y = a[0] + a[1]*x + np.random.randn(N) X = np.column_stack((np.ones(N), x)) theta = np.linalg.lstsq(X, y, rcond=None)[0] yhat = theta[0] + theta[1]*x E_train_tmp[i] = np.mean((yhat-y)**2) y1 = a[0] + a[1]*x + np.random.randn(N) E_test_tmp[i] = np.mean((yhat-y1)**2) E_train[j] = np.mean(E_train_tmp) E_test[j] = np.mean(E_test_tmp) plt.semilogx(Nset, E_train, ’kx’) plt.semilogx(Nset, E_test, ’ro’) plt.semilogx(Nset, (1-2/Nset), linewidth=4, c=’k’) plt.semilogx(Nset, (1+2/Nset), linewidth=4, c=’r’) The training error curve and the testing error curve behave in opposite ways as N increases. The training error E increases as N increases, because when we have more train trainingsamplesitbecomesharderforthemodeltofitallthedata.Bycontrast,thetesting errorE decreasesasN increases,becausewhenwehavemoretrainingsamplesthemodel test becomes more robust to noise and unseen data. Therefore, the testing error improves. As N goes to infinity, both the training error and the testing error converge. This is due to the law of large numbers, which says that the empirical training and testing errors shouldconvergetotheirrespectiveexpectedvalues.Ifthetrainingerrorandthetestingerror convergetothesamevalue,thetrainingcangeneralizetotesting.Iftheydonotconvergeto the same value, there is a mismatch between the training samples and the testing samples. It is important to pay attention to the gap between the converged values. We often assume that the training samples and the testing samples are drawn from the same distri- bution, and therefore the training samples are good representatives of the testing samples. 428
7.3. BIAS AND VARIANCE TRADE-OFF 1.2 1.15 1.1 1.05 1 0.95 0.9 0.85 0.8 101 102 103 Number of training samples, N rorrE Training Error Testing Error Figure 7.14: The learning curve is a pair of functions representing the training error and the testing error.AsN increasesweexpectthetrainingerrortoincreaseandthetestingerrortodecrease.Thetwo functions will converge to the same value as N goes to infinity. If they do not converge to the same value, there is an intrinsic mismatch between the training samples and the testing samples, e.g., the training samples are not representative enough for the dataset. If the assumption is not true, there will be a gap between the converged training error and the testing error. Thus, what you claim in training cannot be transferred to the testing. Consequently, the learning curve provides you with a useful debugging tool to check how well your training compares with your testing. Closing remark. In this section we have studied a very important concept in regression, overfitting.Weemphasizethatoverfittingisnotonlycausedbythecomplexityofthemodel but a combination of the three factors σ2, N, and d. We close this section by summarizing the causes of overfitting: What is the source of overfitting? • Overfitting occurs because you have an imbalance between σ2, N and d. • Selecting the correct complexity for your model is the key to avoid overfitting. 7.3 Bias and Variance Trade-Off Our linear analysis has provided you with a rough understanding of what we experience in overfitting. However, for general regression problems where the models are not necessarily linear, we need to go deeper. The goal of this section is to explain the trade-off between bias and variance. This analysis requires some patience as it involves many equations. We recommend skipping this section on a first reading and then returning to it later. 429
CHAPTER 7. REGRESSION If it is your first time reading it, we recommend you go through it slowly. 7.3.1 Decomposing the testing error Notations As we did at the beginning of Section 7.2, we consider a ground truth model that relates an input x and an output y: y =f(x)+e, where e∼Gaussian(0,σ2) is the noise. For example, if we use a linear model, then f could be f(x)=θTx, for some regression coefficients θ. During training, we pick a prediction model g (·) and try to predict the output when θ given a training sample x: y =g (x). (cid:98) θ Forexample,wemaychooseg (x)=θTx,whichisalsoalinearmodel.Wemayalsochoose θ a linear model in another basis, e.g., g (x)=θTφ(x) for some transformations φ(·). In any θ case, the goal of training is to minimize the training error: N θ(cid:98)=argmin N1 (cid:88) (g θ(x n)−y n)2, θ n=1 wherethesumistakenoverthetrainingsamplesD ={(x ,y ),...,(x ,y )}.Because train 1 1 N N the model parameter θ(cid:98) is learned from the training dataset D train, the prediction model depends on D . To emphasize this dependency, we write train (cid:26) (cid:27) g(Dtrain) =the model trained from (x ,y ),...,(x ,y ) . 1 1 N N Duringtesting,weconsideratestingdatasetD ={(x(cid:48),y(cid:48)),...,(x(cid:48) ,y(cid:48) )}.Weput test 1 1 M M these testing samples into the trained model to predict an output: y(cid:48) =g(Dtrain)(x(cid:48) ), m=1,...,M. (predicted value) (cid:98)m m Since the goal of regression is to make g(Dtrain) as close to f as possible, it is natural to expect y(cid:48) to be close to y(cid:48) . (cid:98)m m Testing error decomposition (noise-free) So we can now compute the testing error — the error that we ultimately care about. In the noise-free condition, i.e., e=0, the testing error is defined as E t( eD sttrain) =E x(cid:48)(cid:104)(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2(cid:105) (7.23) 1 (cid:88)M (cid:18) (cid:19)2 ≈ g(Dtraing)(x(cid:48) )−f(x(cid:48) ) . M m m m=1 There are several components in this equation. First, x(cid:48) is a testing sample drawn from a certain distribution. You can think of D as a finite subset drawn from this distribution. test 430
7.3. BIAS AND VARIANCE TRADE-OFF Second,theerror(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2 measuresthedeviationbetweenourpredictedvalue and the true value. Note that this error term is specific to one testing sample x(cid:48). Therefore, we take expectation E to find the average of the error for the distribution of x(cid:48). x(cid:48) The testing error E(Dtrain) is a function that is dependent on the training set D , test train because the model g(Dtrain) is trained from D train. Therefore, as we change the training set, we will have a different model g and hence a different testing error. To eliminate the randomness of the training set, we define the overall testing error as (cid:20) (cid:21) E =E E(Dtrain) test Dtrain test =E Dtrain(cid:20) E x(cid:48)(cid:104)(cid:0) g(Dtrain)(x(cid:48))−f(x(cid:48))(cid:1)2(cid:105)(cid:21) . (7.24) Note that this definition of the testing error is consistent with the special case in Equa- tion (7.18), in which the testing error involves a joint expectation over e and e(cid:48). The ex- pectationovereaccountsforthetrainingsamples,andtheexpectationovere(cid:48) accountsfor the testing samples. Let us try to extract some meaning from the testing error. Our method will be to decompose the testing error into bias and variance. Theorem 7.5. Assume a noise-free condition. The testing error of a regression prob- lem is given by (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2+E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] , (7.25) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) where g(x(cid:48))d =efE Dtrain[g(Dtrain)(x(cid:48))]. Proof. To simplify our notation, we will drop the subscript “train” in D when the train context is clear. We have that (cid:104) (cid:104) (cid:105)(cid:105) E =E E (g(D)(x(cid:48))−f(x(cid:48)))2 test D x(cid:48) (cid:104) (cid:104) (cid:105)(cid:105) =E E (g(D)(x(cid:48))−f(x(cid:48)))2 . x(cid:48) D Continuing the calculation, (cid:104) (cid:104) (cid:105)(cid:105) E =E E (g(D)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48)))2 test x(cid:48) D (cid:20) (cid:104) (cid:105) (cid:104) (cid:105) =E E (g(D)(x(cid:48))−g(x(cid:48)))2 +2E (g(D)(x(cid:48))−g(x(cid:48)))(g(x(cid:48))−f(x(cid:48))) x(cid:48) D D (cid:104) (cid:105)(cid:21) +E (g(x(cid:48))−f(x(cid:48)))2 . D Since g(x(cid:48))d =efE [g(D)(x(cid:48))], it follows that D (cid:104) (cid:105) 2E (g(D)(x(cid:48))−g(x(cid:48)))(g(x(cid:48))−f(x(cid:48))) =0 D 431
CHAPTER 7. REGRESSION because g(x(cid:48))−f(x(cid:48)) is independent of D, and (cid:104) (cid:105) E (g(x(cid:48))−f(x(cid:48)))2 =(g(x(cid:48))−f(x(cid:48)))2. D Therefore, (cid:20) (cid:104) (cid:105) (cid:104) (cid:105)(cid:21) E =E E (g(D)(x(cid:48))−g(x(cid:48)))2 + (g(x(cid:48))−f(x(cid:48)))2 . test x(cid:48) D Thus, by defining two following terms we have proved the theorem. bias(x(cid:48))d =ef (g(x(cid:48))−f(x(cid:48)))2, var(x(cid:48))d =efE [(g(D)(x(cid:48))−g(x(cid:48)))2]. D (cid:3) Let’s consider what this theorem implies. This result is a decomposition of the testing error into bias and variance. It is a universal result that applies to all regression models, not only linear cases. To summarize the meanings of bias and variance: What are bias and variance? • Bias = how far your average is from the truth. • Variance = how much fluctuation you have around the average. Figure 7.15 gives a pictorial representation of bias and variance. In this figure, we construct four scenarios of bias and variance. Each cross represents the predictor g(Dtrain), with the true predictor f at the origin. Figure 7.15(a) shows the case with a low bias and a low variance. All these predictors g(Dtrain) are very close to the ground truth, and they have small fluctuations around their average. Figure 7.15(b) shows the case of a high bias and a low variance. It has a high bias because the entire group of g(Dtrain) is shifted to the corner.Thebias,whichisthedistancefromthetruthtotheaverage,isthereforelarge.The variance remains small because the fluctuation around the average is small. Figure 7.15(c) shows the case of a low bias but high variance. In this case, the fluctuation around the average is large. Figure 7.15 shows the case of high bias and high variance. We want to avoid this case. Bias low Bias high Bias low Bias high Var low Var low Var high Var high (a) (b) (c) (d) Figure7.15:Imaginethatyouarethrowingadartwithatargetatthecenter.Thefoursubfiguresshow the levels of bias and variance. 432
7.3. BIAS AND VARIANCE TRADE-OFF Testing error decomposition (noisy case) Let us consider a situation when there is noise. In the presence of noise, the training and testing samples will follow the relationship y =f(x)+e, where e∼Gaussian(0,σ2). We assume that the noise is Gaussian to make the proof easier. We can consider other types of noise in theory, but the theoretical results will need to be modified. In the presence of noise, the testing error is (cid:20)(cid:16) (cid:17)2(cid:21) E (x(cid:48))d =efE g(Dtrain)(x(cid:48))−f(x(cid:48))+e test Dtrain,e (cid:20)(cid:16) (cid:17)2(cid:21) =E g(Dtrain)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48))+e , Dtrain,e where we take the joint expectation over the training dataset D and the error e. Con- train tinuingthecalculation,andusingthefactthatD andeareindependent(andE[e]=0), train it follows that (cid:20)(cid:16) (cid:17)2(cid:21) E (x(cid:48))=E g(Dtrain)(x(cid:48))−g(x(cid:48))+g(x(cid:48))−f(x(cid:48))+e test Dtrain,e (cid:20)(cid:16) (cid:17)2 (cid:16) (cid:17)2 (cid:21) =E g(D)(x(cid:48))−g(x(cid:48)) + g(x(cid:48))−f(x(cid:48)) +e2 Dtrain,e (cid:20)(cid:16) (cid:17)2(cid:21) (cid:16) (cid:17)2 (cid:104) (cid:105) =E g(Dtrain)(x(cid:48))−g(x(cid:48)) + g(x(cid:48))−f(x(cid:48)) +E e2 . Dtrain e (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =var(x(cid:48)) =bias(x(cid:48)) =noise Taking the expectation of x(cid:48) over the entire testing distribution gives us E =E [E (x(cid:48))]=E [var(x(cid:48))]+E [bias(x(cid:48))]+σ2. test x(cid:48) test x(cid:48) x(cid:48) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) var bias The theorem below summarizes the results: Theorem 7.6. Assume a noisy condition where y =f(x)+e for some i.i.d. Gaussian noise e∼Gaussian(0,σ2). The testing error of a regression problem is given by (cid:20) (cid:21) (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2 +E x(cid:48) E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] +σ2, (7.26) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) where g(x(cid:48))d =efE Dtrain[g(Dtrain)(x(cid:48))]. 7.3.2 Analysis of the bias Letusexaminethebiasandvarianceinmoredetail.Todiscussbiaswemustfirstunderstand the quantity g(x(cid:48))d =efE [g(Dtrain)(x(cid:48))], (7.27) Dtrain 433
CHAPTER 7. REGRESSION whichisknownastheaverage predictor.Theaveragepredictor,astheequationsuggests,is theexpectationofthepredictorg(Dtrain).Rememberthatg(Dtrain) isapredictorconstructed from a specific training set D . If tomorrow our training set D contains other data train train (that come from the same underlying distribution), g(Dtrain) will be different. The average predictor g is the average across these random fluctuations of the dataset D . Here is an train example: Suppose we use a linear model with the ordinary polynomials as the bases. The data points are generated according to d−1 (cid:88) y = θ xp +e . (7.28) n p n n p=0 (cid:124) (cid:123)(cid:122) (cid:125) d=eff(xn)=θTxn IfweuseaparticulartrainingsetD andruntheregression,wewillbeabletoobtain train one of the regression lines, as shown in Figure 7.16. Let us call this line g(1). We repeat the experimentbydrawinganotherdataset,andcallitg(2).Wecontinueandeventuallywewill findasetofregressionlinesg(1),g(2),...,g(K),whereK denotesthenumberoftrainingsets you are using to generate all the gray curves. The average predictor g is defined as K 1 (cid:88) g(x(cid:48))=E [g(Dtrain)]≈ g(k)(x(cid:48)). Dtrain K k=1 Thus if we take the average of all these gray curves we will obtain the average predictor, which is the red curve shown in Figure 7.16. 3 2 1 0 -1 -1 -0.5 0 0.5 1 Figure 7.16: We run linear regression many times for different training datasets. Each one consists of different random realizations of noise. The gray curves are the regression lines returned by each of the training datasets. We then take the average of these gray curves to obtain the red curve, which is the average predictor. Ifyouarecuriousabouthowthisplotwasgenerated,theMATLABandPythoncodes are given below. % MATLAB code to visualize the average predictor N = 20; 434
7.3. BIAS AND VARIANCE TRADE-OFF a = [5.7, 3.7, -3.6, -2.3, 0.05]; x = linspace(-1,1,N); yhat = zeros(100,50); for i=1:100 X = [x(:).^0, x(:).^1, x(:).^2, x(:).^3, x(:).^4]; y = X*a(:) + 0.5*randn(N,1); theta = X\y(:); t = linspace(-1, 1, 50); yhat(i,:) = theta(1) + theta(2)*t(:) + theta(3)*t(:).^2 ... + theta(4)*t(:)^3 + theta(5)*t(:).^4; end figure; plot(t, yhat, ’color’, [0.6 0.6 0.6]); hold on; plot(t, mean(yhat), ’LineWidth’, 4, ’color’, [0.8 0 0]); axis([-1 1 -2 2]); import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre np.set_printoptions(precision=2, suppress=True) N = 20 x = np.linspace(-1,1,N) a = np.array([0.5, -2, -3, 4, 6]) yhat = np.zeros((50,100)) for i in range(100): y = a[0] + a[1]*x + a[2]*x**2 + \ a[3]*x**3 + a[4]*x**4 + 0.5*np.random.randn(N) X = np.column_stack((np.ones(N), x, x**2, x**3, x**4)) theta = np.linalg.lstsq(X, y, rcond=None)[0] t = np.linspace(-1,1,50) Xhat = np.column_stack((np.ones(50), t, t**2, t**3, t**4)) yhat[:,i] = np.dot(Xhat, theta) plt.plot(t, yhat[:,i], c=’gray’) plt.plot(t, np.mean(yhat, axis=1), c=’r’, linewidth=4) We now show an analytic calculation to verify Figure 7.16. Example 7.4. Consider a linear model such that y =xTθ+e. (7.29) What is the predictor g(Dtrain)(x(cid:48))? What is the average predictor g(x(cid:48))? Solution. First, consider a training dataset D = {(x ,y ),...,(x ,y )}. We train 1 1 N N assume thatthe x ’sare deterministicand fixed.Therefore, thesource ofrandomness n in the training set is caused by the noise e ∼ Gaussian(0,σ2) and hence by the noisy 435
CHAPTER 7. REGRESSION observation y. The training set gives us the equation y = Xθ + e, where X is the matrix constructed from x ’s. The regression solution to this dataset is n θ(cid:98)=(XTX)−1XTy, which should actually be θ(cid:98)(Dtrain) because y is a dataset-dependent vector. Consequently, g(Dtrain)(x(cid:48))=θ(cid:98)T x(cid:48) =(x(cid:48))T(XTX)−1XTy =(x(cid:48))T(XTX)−1XT(Xθ+e) =(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe. Since the randomness of D is caused by the noise, it follows that train g(x(cid:48))=E [g(Dtrain)(x(cid:48))]=E [(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe] Dtrain e =(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTE [e] e =(x(cid:48))Tθ+0=f(x(cid:48)). So the average predictor will return the ground truth. However, note that not all predictors will return the ground truth. In the above example, we obtained an interesting result, namely that g(x(cid:48)) = f(x(cid:48)). That is, the average predictor equals the true predictor. However, in general, g(x(cid:48)) does not necessarily equal f(x(cid:48)). If this occurs, we have a deviation (g(x(cid:48))−f(x(cid:48)))2 > 0. This deviation is called the bias. Bias is independent of the number of training samples because we have taken the average of the predictors. Therefore, bias is more of an intrinsic (or systematic) error due to the choice of the model. What is bias? • Bias is defined as bias=E [(g(x(cid:48))−f(x(cid:48)))2], where x(cid:48) is a testing sample. x(cid:48) • It is the deviation from the average predictor to the true predictor. • Bias is not necessarily a bad thing. A good predictor can have some bias as long as it helps to reduce the variance. 7.3.3 Variance The other quantity in the game is the variance. Variance at a testing sample x(cid:48) is defined as var(x(cid:48))d =efE [(g(Dtrain)(x(cid:48))−g(x(cid:48)))2]. (7.30) Dtrain As the equation suggests, the variance measures the fluctuation between the predictor g(Dtrain) and the average predictor g. Figure 7.17 illustrates the polynomial-fitting prob- lem we discussed above. In this figure we consider two levels of variance by varying the 436
7.3. BIAS AND VARIANCE TRADE-OFF noisestrengthofe .Thefigureshowsthatastheobservationbecomesnoisier,thepredictor n g(Dtrain) will have a larger fluctuation for the average predictor. 2 2 1 1 0 0 -1 -1 -2 -2 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) small variance (b) large variance Figure 7.17: Variance measures the magnitude of fluctuation between the particular predictor g(Dtrain) and the average predictor g. Example 7.5. Continuing with Example 7.4, we ask: What is the variance? Solution. We first determine the predictor and its average: g(Dtrain) =(XTX)−1XTy =θ+(XTX)−1XTe g =E[g(Dtrain)]=E [θ+(XTX)−1XTe]=θ, e so the prediction at a testing sample x(cid:48) is g(Dtrain)(x(cid:48))=(x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe g(x(cid:48))=(x(cid:48))Tθ, Consequently, the variance is (cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21) E g(Dtrain)(x(cid:48))−g(x(cid:48)) =E (x(cid:48))Tθ+(x(cid:48))T(XTX)−1XTe−(x(cid:48))Tθ Dtrain e (cid:20)(cid:16) (cid:17)2(cid:21) =E (x(cid:48))T(XTX)−1XTe . e Continuing the calculation, (cid:20)(cid:16) (cid:17)2(cid:21) E g(Dtrain)(x(cid:48))−g(x(cid:48)) =(x(cid:48))T(XTX)−1XTE [eeT]X(XTX)−1x(cid:48) Dtrain e =(x(cid:48))T(XTX)−1XTσ2IX(XTX)−1x(cid:48) =σ2(x(cid:48))T(XTX)−1x(cid:48) (cid:110) (cid:111) =σ2Tr (XTX)−1(x(cid:48))(x(cid:48))T . 437
CHAPTER 7. REGRESSION What will happen if we use more samples so that N grows? As N grows, the matrix X will havemorerows.Assumingthatthemagnitudeoftheentriesremainsunchanged,morerows in X will increase the magnitude of XTX because we are summing more terms. Consider a 2×2 ordinary polynomial system where (cid:80)N x2 (cid:80)N x  n=1 n n=1 n XTX = . (cid:80)N x N n=1 n As N grows, all the entries in the matrix grow. As a result, (XTX)−1 will shrink in mag- (cid:110) (cid:111) nitude and thus drive the variance σ2Tr (XTX)−1(x(cid:48))(x(cid:48))T to zero. What is variance? • Variance is the deviation between the predictor g(Dtrain) and its average g. • It can be reduced by using more training samples. 7.3.4 Bias and variance on the learning curve The decomposition of the testing error into bias and variance is portrayed visually by the learning curve shown in Figure 7.18. This figure shows the testing error and the training error as functions of the number of training samples. As N increases, we observe that both testing and training errors converge to the same value. At any fixed N, the testing error is composed of bias and variance: • The bias is the distance from the ground to the steady-state level. This value is fixed and is a constant w.r.t. N. In other words, regardless of how many training samples you have, the bias is always there. It is the best outcome you can achieve. • The variance is the fluctuation from the steady-state level to the instantaneous state. It drops as N increases. Figure7.18:Thelearningcurvecanbedecomposedintothesumofthebiasandthevariance.Thebias is the testing error when N =∞. For finite N, the difference between the testing error and the bias is the variance. 438
7.3. BIAS AND VARIANCE TRADE-OFF Figure 7.19 compares the learning curve of two models. The first case requires us to fit the data using a simple model (marked in purple). The training error and the testing error have small fluctuations around the steady-state because, for simple models, you need only a small number of samples to make the model happy. The second case requires us to fit the data using a complex model (marked in green). This set of curves has a much wider fluctuation because it is harder to train and harder to generalize. However, when we have enough training samples, the training error and the testing error will converge to a lower steady-state value. Therefore, you need to pay the price of using a complex model, but if you do, you will enjoy a lower testing error. 2.5 2 1.5 1 0.5 0 101 102 103 Number of training samples, N rorrE Simple Model - Training Error Simple Model - Testing Error Complex Model - Training Error Complex Model - Testing Error Figure 7.19: The generalization capability of a model is summarized by the training and testing errors ofthemodel.Ifweuseasimplemodelwewillhaveaneasiertimewiththetrainingbutthesteady-state testing error will be high. In contrast, if we use a complex model we need to have a sufficient number of training samples to train the model well. However, when the complex model is well trained, the steady-state error will be lower. The implication of all this is that you should choose the model by considering the number of data points. Never buy an expensive toy when you do not have the money! If you insist on using a complex model while you do not have enough training data, you will suffer from a poor testing error even if you feel good about it. Closing remark. We close this section by revisiting the bias-variance trade-off: (cid:20) (cid:21) (cid:20) (cid:21) E test =E x(cid:48) (g(x(cid:48))−f(x(cid:48)))2 +E x(cid:48) E Dtrain[(g(Dtrain)(x(cid:48))−g(x(cid:48)))2] +σ2. (7.31) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =bias(x(cid:48)) =var(x(cid:48)) The relationship among the three terms is summarized below: What is the trade-off offered by the bias-variance analysis? • Overfitting improves if N ↑: Variance drops as N grows. Bias is unchanged. • Overfitting worsens if σ2 ↑. If training noise grows, g(Dtrain) will have more fluc- tuations, so variance will grow. If testing noise grows, e2 grows. 439
CHAPTER 7. REGRESSION • Overfittingworsensifthetargetfunctionf istoocomplicatedtobeapproximated by g. End of the section. Please join us again. 7.4 Regularization Having discussed the source of the overfitting problem, we now discuss methods to allevi- ate overfitting. The method we focus on here is regularization. Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term to force the parameters to“behave better”. As a preview of the technique, we change the original training loss N (cid:18) d−1 (cid:19)2 (cid:88) (cid:88) E (θ)= y − θ φ (x ) , (7.32) train n p p n n=1 p=0 (cid:124) (cid:123)(cid:122) (cid:125) datafidelity which consists of only the data fidelity term, to a modified training loss N (cid:18) d−1 (cid:19)2 d−1 (cid:88) (cid:88) (cid:88) E (θ)= y − θ φ (x ) + λ· θ2 . (7.33) train n p p n p n=1 p=0 p=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) F(θ),datafidelity λ·R(θ),regularization Putting this into the matrix form, we define the data fidelity term as F(θ)=(cid:107)Xθ−y(cid:107)2. (7.34) The newly added term R(θ) is called the regularization function or the penalty function. It can take a variety of forms, e.g., • Ridge regression: R(θ)=(cid:80)d−1θ2 =(cid:107)θ(cid:107)2. p=0 p • LASSO regression: R(θ)=(cid:80)d−1|θ |=(cid:107)θ(cid:107) . p=0 p 1 In this section we aim to understand the role of the regularization functions by studying these two examples of R(θ). 7.4.1 Ridge regularization To explain the meaning of Equation (7.33) we write it in terms of matrices and vectors: minimize (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2, (7.35) θ∈Rd where λ is called the regularization parameter.It needs to be tuned by the user. We refer to Equation (7.35) as the ridge regression.5 5Insignalprocessingandoptimization,Equation(7.35)iscalledtheTikhonovregularization.Wefollow thestatisticscommunityincallingittheridgeregression. 440
7.4. REGULARIZATION How can the regularization function help to mitigate the overfitting problem? First let’s find the solution to this problem. Practice Exercise 1. Prove that the solution to Equation (7.35) is θ(cid:98)=(XTX+λI)−1XTy. (7.36) Solution. Take the derivative with respect to θ.a This yields (cid:26) (cid:27) ∇ (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 =2XT(Xθ−y)+2λθ =0. θ Rearranging the terms gives (XTX+λI)θ =XTy. Taking the inverse of the matrix on both sides yields the solution. aThesolutionhererequiressomebasicmatrixcalculus.YoumayrefertotheUniversityofWater- loo’sMatrixCookbookhttps://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf. Let us compare the ridge regression solution with the vanilla regression solutions: θ(cid:98)vanilla =(XTX)−1XTy, θ(cid:98)ridge(λ)=(XTX+λI)−1XTy. Clearly, the only difference is the presence of the parameter λ: • If λ→0, then θ(cid:98)ridge(0)=θ(cid:98)vanilla. This is because E (θ)=(cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. train (cid:124) (cid:123)(cid:122) (cid:125) =0 Hence, when λ → 0, the regression problem goes back to the vanilla version, and so does the solution. • λ→∞, then θ(cid:98)ridge(∞)=0. This happens because 1 E (θ)= (cid:107)Xθ−y(cid:107)2+(cid:107)θ(cid:107)2. train λ (cid:124) (cid:123)(cid:122) (cid:125) =0 Since we are now minimizing (cid:107)θ(cid:107)2, the solution will be θ = 0 because zero is the smallest value a squared function can achieve. For any 0 < λ < ∞, the net effect of (XTX +λI) is the constant λ added to all the eigenvalues of XTX. By taking the eigendecomposition of XTX, [U,S]=eig(XTX), we have that XTX+λI =USUT +λI =USUT +λUUT =U(S+λI)UT. 441
CHAPTER 7. REGRESSION Therefore, if the eigenvalue matrix S has a zero eigenvalue it will be offset by λ:     ♣ ♣+λ  ♥   ♥+λ  S =  −→ S+λI =   ♠   ♠+λ  0 λ As a result, even if XTX is not invertible (or close to not invertible), the new matrix XTX+λI is guaranteed to be invertible. Practice Exercise 2. You may be wondering what happens if XTX has a negative eigenvalue so that when we add a positive λ, the resulting matrix may have a zero eigenvalue. Prove that XTX will never have a negative eigenvalue, and XTX+λI always has positive eigenvalues. Solution. Eigenvalues of a matrix A are nonnegative if and only if vTAv ≥ 0 for any v. Thus we need to check whether vTXTXv ≥0 for all v. However, this is easy: vTXTXv =(cid:107)Xv(cid:107)2, which must be nonnegative for any v. Matrices satisfying this property are called positive semidefinite. Therefore, XTX is positive semidefinite. Implementation Solvingtheridgeregressioniseasy.First,weobservethattheregularizationfunctionR(θ)= (cid:107)θ(cid:107)2 is a quadratic function. Therefore, it can be combined with the data fidelity term as θ(cid:98)=argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 θ∈Rd √ =argmin (cid:107)Xθ−y(cid:107)2+(cid:107) λIθ−0(cid:107)2 θ∈Rd (cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)2 =argmin (cid:13) (cid:13) √X θ− y (cid:13) (cid:13) . θ∈Rd (cid:13) λI 0 (cid:13) Therefore, all we need to do is to concatenate the matrix X with a d×d identity operator √ λI, and concatenate y with a d×1 all-zero vector. InMATLABandPython,theimplementationoftheridgeregressionisdonebydefining a new matrix A and a new vector b, as shown below: % MATLAB command for ridge regression A = [X; sqrt(lambda)*eye(d)]; b = [y(:); zeros(d,1)]; theta = A\b; % MATLAB command for ridge regression A = np.vstack((X, np.sqrt(lambd)*np.eye(d))) b = np.hstack((y, np.zeros(d))) theta = np.linalg.lstsq(A, b, rcond=None)[0] 442
7.4. REGULARIZATION Example 7.6. Consider a dataset of N = 20 data points. These data points are constructed from the model y =0.5−2x −3x2 +4x3 +6x4 +e , n=1,...,N, n n n n N n where e ∼Gaussian(0,0.252) is the noise. Fit the data using n (a) Vanilla linear regression with a 4th-order polynomial. (b) Vanilla linear regression with a 20th-order polynomial. (c) Ridgeregressionwitha20th-orderpolynomial,byconsideringthreechoicesofλ: λ=10−6, λ=10−3, and λ=10. Solution. (a) We first fit the data using a 4th-order polynomial. This fitting is relatively straightforward. In the MATLAB / Python programs below, set d = 4 and λ=0. The result is shown in Figure 7.20(a). 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Vanilla, 4th-order polynomial (b) Vanilla, 20th-order polynomial Figure7.20:Overfittingoccurswhenthemodelistoocomplexforthenumberoftrainingsamples. When using a vanilla regression with a 20th-order polynomial, the curve overfits the data and causes a catastrophic fitting error. (b) Suppose we use a 20th-order polynomial g(x) = (cid:80)20 θ xp to fit the data. We p=0 p plot the result in Figure 7.20(b). Since the order of the polynomial is very high relativetothenumberoftrainingsamples,itcomesasnosurprisethatthefitting is poor. This is overfitting, and we know the reason. (c) Next,weconsideraridgeregressionusingthreechoicesofλ.Theresultisshown in Figure 7.21. If λ is too small, we observe that some overfitting still occurs. If λ is too large, then the curve underfits the data. For an appropriately chosen λ, it can be seen that the fitting is reasonably good. 443
CHAPTER 7. REGRESSION 4 4 4 3 3 3 2 2 2 1 1 1 0 0 0 data data data fitted curve fitted curve fitted curve -1 -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ridge, λ=10−6 (b) Ridge, λ=10−3 (c) Ridge, λ=10 Figure 7.21: Ridge regression addresses the overfitting problem by adding a regularization term tothetrainingloss.Dependingonthestrengthoftheparameterλ,thefittedcurvecanvaryfrom overfitting to underfitting. The MATLAB and Python codes used to generate the above plots are shown below. % MATLAB code to demonstrate a ridge regression example % Generate data N = 20; x = linspace(-1,1,N); a = [0.5, -2, -3, 4, 6]; y = a(1)+a(2)*x(:)+a(3)*x(:).^2+a(4)*x(:).^3+a(5)*x(:).^4+0.25*randn(N,1); % Ridge regression lambda = 0.1; d = 20; X = zeros(N, d); for p=0:d-1 X(:,p+1) = x(:).^p; end A = [X; sqrt(lambda)*eye(d)]; b = [y(:); zeros(d,1)]; theta = A\b; % Interpolate and display results t = linspace(-1, 1, 500); Xhat = zeros(length(t), d); for p=0:d-1 Xhat(:,p+1) = t(:).^p; end yhat = Xhat*theta; plot(x,y, ’ko’,’LineWidth’,2, ’MarkerSize’, 10); hold on; plot(t,yhat,’LineWidth’,4,’Color’,[0.2 0.2 0.9]); # Python code to demonstrate a ridge regression example import numpy as np import matplotlib.pyplot as plt from scipy.special import eval_legendre np.set_printoptions(precision=2, suppress=True) 444
7.4. REGULARIZATION N = 20 x = np.linspace(-1,1,N) a = np.array([0.5, -2, -3, 4, 6]) y = a[0] + a[1]*x + a[2]*x**2 + \ a[3]*x**3 + a[4]*x**4 + 0.25*np.random.randn(N) d = 20 X = np.zeros((N, d)) for p in range(d): X[:,p] = x**p lambd = 0.1 A = np.vstack((X, np.sqrt(lambd)*np.eye(d))) b = np.hstack((y, np.zeros(d))) theta = np.linalg.lstsq(A, b, rcond=None)[0] t = np.linspace(-1, 1, 500) Xhat = np.zeros((500,d)) for p in range(d): Xhat[:,p] = t**p yhat = np.dot(Xhat, theta) plt.plot(x,y,’o’,markersize=12) plt.plot(t,yhat, linewidth=4) plt.show() Why does ridge regression work? • The penalty term (cid:107)θ(cid:107)2 in θ(cid:98)ridge =argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2 θ∈Rd does not allow solutions with very (cid:107)θ(cid:107)2. • The penalty term adds a positive offset to the eigenvalues of XTX. • Since the denominator in (XTX + λI)−1XTy becomes larger than that of (XTX)−1XTy, noise in y is less amplified. Choosing the parameter How should we choose the parameter λ? The honest answer is that there is no answer because the optimal λ can only be found if we have access to the testing samples. If we do, we can plot the MSE (the testing error) with respect to λ, as shown in Figure 7.22(a). Ofcourseinrealitywedonothaveaccesstothetestingdata.However,wecanreserve a small portion of the training samples and treat them as validation samples. Then we run the ridge regression for different choices of λ. The λ that minimizes the error on these validation samples is the one that you should deploy. If the training set is small, we can 445
CHAPTER 7. REGRESSION 10-1 0.3 0.25 0.2 10-2 0.15 0.1 0.05 10-3 0 10-10 10-5 100 100 105 (a) Testing error vs λ (b) F(θ(cid:98)λ) vs R(θ(cid:98)λ) Figure 7.22: (a) Determining the optimal λ requires knowledge of the testing samples. In practice, we canreplacethetestingsampleswiththevalidationsamples,whicharesubsetsofthetrainingdata.Then by plotting the validation error as a function of λ we can determine the optimal λ. (b) The alternative is to plot F(θ(cid:98)λ) versus R(θ(cid:98)λ). The optimal λ can be found by locating the elbow point. shuffle the validation samples randomly and compute the average. This scheme is known as cross-validation. For some problems, there are “tactics” you may be able to employ for determining the optimal λ. The first approach is to ask yourself what would be the reasonable range of (cid:107)θ(cid:107)2 or (cid:107)Xθ −y(cid:107)2? Are you expecting them to be large or small? Approximately in whatorderofmagnitude?Ifyouhavesomecluesaboutthis,thenyoucanplotthefunction F(θ(cid:98)λ) = (cid:107)Xθ(cid:98)λ −y(cid:107)2 as a function of R(θ(cid:98)λ) = (cid:107)θ(cid:98)λ(cid:107)2, where θ(cid:98)λ is a shorthand notation for θ(cid:98)ridge(λ), which is the estimated parameter using a specific value of λ. Figure 7.22(b) shows an example of such a plot. As you can see, by varying λ we have different values of F(θ(cid:98)λ) and R(θ(cid:98)λ). Ifyouhavesomeideasaboutwhat(cid:107)θ(cid:107)2 shouldbe,sayyouwant(cid:107)θ(cid:107)2 ≤τ,youcango to the F(θ(cid:98)λ) versus R(θ(cid:98)λ) curve and find a point such that R(θ(cid:98)λ)≤τ. On the other hand, if you want (cid:107)Xθ−y(cid:107)2 ≤ (cid:15), you can also go to the F(θ(cid:98)λ) versus R(θ(cid:98)λ) curve and find a point such that (cid:107)Xθ−y(cid:107)2 ≤(cid:15). In either case, you have the freedom to shift the difficulty offindingλtothatoffindingτ or(cid:15).Notethatτ and(cid:15)havebetterphysicalinterpretations. The quantity (cid:15) tells us the upper bound of the prediction error, and τ tells us the upper bound of the parameter magnitude. If you have been working on your dataset long enough, the historical data (and your experience) will help you determine these values. Another feasible option suggested in the literature is finding the anchor point of the F(θ(cid:98)λ) and R(θ(cid:98)λ). The idea is that if the curve has a sharp elbow, the turning point would indicate a rapid increase/decrease in F(θ(cid:98)λ) (or R(θ(cid:98)λ)). How to determine λ • Cross-validation: Reserve a few training samples as validation samples. Check the prediction error w.r.t. these validation samples. The λ that minimizes the validation error is the one you deploy. • (cid:107)θ(cid:107)2 ≤ τ: Plot the F(θ(cid:98)λ) and R(θ(cid:98)λ). Then go along the R-axis to find the 446
7.4. REGULARIZATION position where R(θ(cid:98)λ)≤τ. • (cid:107)Xθ−y(cid:107)2 ≤ τ: Plot the F(θ(cid:98)λ) and R(θ(cid:98)λ). Then go along the F-axis to find the position where F(θ(cid:98)λ)≤(cid:15). • Find the elbow point of F(θ(cid:98)λ) and R(θ(cid:98)λ). Bias and variance trade-off for ridge regression We now discuss the bias and variance trade-off of the ridge regression. Theorem 7.7. Let y =Xθ+e be the training data, where e is zero-mean and has a covariance σ2I. Consider the ridge regression θ(cid:98)λ =argmin (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. (7.37) θ∈Rd Then the estimate has the properties that θ(cid:98)λ =(XTX+λI)−1XTXθ+(XTX+λI)−1XTe, E[θ(cid:98)λ]=(XTX+λI)−1XTXθ =W λθ, Cov[θ(cid:98)λ]=σ2(XTX+λI)−1XTX(XTX+λI)−1, (cid:26) (cid:27) MSE(θ(cid:98)λ,θ)=σ2Tr W λ(XTX)−1WT λ +θT(W λ−I)T(W λ−I)θ, where W =(XTX+λI)−1XTX. λ Proof. The proof of this theorem involves some tedious matrix operations that will be omitted here. If you are interested in the proof you can consult van Wieringen’s “Lecture notes on ridge regression”, https://arxiv.org/pdf/1509.09169.pdf. (cid:3) The results of this theorem provide a way to assess the bias and variance. Specifically, from the MSE we know that (cid:104) (cid:105) MSE(θ(cid:98)λ,θ)=E e (cid:107)θ(cid:98)λ−θ(cid:107)2 (cid:110) (cid:111) =(cid:107)E e[θ(cid:98)λ]−θ(cid:107)2+Tr Cov[θ(cid:98)λ] (cid:26) (cid:27) =θT(W −I)T(W −I)θ+σ2Tr W (XTX)−1WT . λ λ λ λ (cid:124) (cid:123)(cid:122) (cid:125) bias (cid:124) (cid:123)(cid:122) (cid:125) variance The bias and variance are defined respectively as Bias(θ(cid:98)λ,θ)=θT(W λ−I)T(W λ−I)θ, (cid:26) (cid:27) Var(θ(cid:98)λ,θ)=σ2Tr W λ(XTX)−1WT λ . We can then plot the bias and variance as a function of λ. An example is shown in Fig- ure 7.23. 447
CHAPTER 7. REGRESSION 103 102 101 100 MSE Bias Variance 10-1 10-4 10-3 10-2 10-1 Figure 7.23: Thebiasandvarianceoftheridgeregressionbehaveinoppositewaysasλincreases.The MSE is the sum of bias and variance. The result in Figure 7.23 can be summarized in three points: • Bias ↑ as λ↑. This is because a large λ pushes the solution towards θ =0. Therefore, the bias with respect to the ground truth θ will increase. • Variance ↓ as λ ↑. Since variance is caused by noise, increasing λ forces the solution θ to be small. Hence, it becomes less sensitive to noise. • MSE reaches a minimum pointsomewherein the middle. The MSE is thesum of bias and variance. Therefore, it drops to the minimum and then rises again as λ increases. With appropriate choice of λ, we can show that the ridge regression can have a lower mean squared error than the vanilla regression. The following result is due to C. M. Theobald:6 Theorem 7.8. For λ<2σ2(cid:107)θ(cid:107)−2, (cid:16) (cid:17) (cid:16) (cid:17) MSE θ(cid:98)ridge(λ),θ <MSE θ(cid:98)vanilla,θ . (7.38) This theorem says that as long as λ is small enough, the ridge regression will have a lower MSE than the vanilla regression. Thus ridge regression is almost always helpful. Of course, the optimal λ is not provided by the theorem, which only tells us where to search for a good λ. Why does ridge regression reduce the testing error? • The regularization reduces the variance (see Figure 7.23 when λ>0) • It pays the price of increasing the bias. 6Theobald, C. M. (1974). Generalizations of mean square error applied to ridge regression. Journal of the Royal Statistical Society.SeriesB(Methodological),36(1),103-106. 448
7.4. REGULARIZATION • Usually, the drop in variance outweighs the increase in bias. So the overall MSE drops. • Bias is not always a bad thing. 7.4.2 LASSO regularization Theridgeregressionwediscussedintheprevioussubsectionisjustoneofthemanypossible ways of doing regularization. One alternative is to replace (cid:107)θ(cid:107)2 by (cid:107)θ(cid:107) , where 1 d−1 (cid:88) (cid:107)θ(cid:107) = |θ |. (7.39) 1 p p=0 This change from the sum-squares to sum-absolute-values has been main driving force in data science, machine learning, and signal processing for at least the past two decades. The optimization associated with (cid:107)θ(cid:107) is 1 minimize (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107) , (7.40) 1 θ∈Rd or N (cid:18) d−1 (cid:19)2 d−1 (cid:88) (cid:88) (cid:88) E (θ)= y − θ φ (x ) + λ· |θ | . (7.41) train n p p n p n=1 p=0 p=0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) F(θ),datafidelity λ·R(θ),regularization Seeking a sparse solution To understand the choice of (cid:107)·(cid:107) , we need to introduce the concept of sparsity. 1 Definition 7.1. A vector θ is called sparse if it has only a few non-zero elements. As illustrated in Figure 7.24, a sparse θ ensures that only a very few columns of the data matrix X are active. This is an attractive property because, in some of the regression problems, it is indeed possible to have just a few dominant factors. The LASSO regression says that if our problem possesses this sparse solution, then the (cid:107)·(cid:107) can help us find the 1 sparse solution. Figure 7.24: A vector θ is sparse if it only contains a few non-zero elements. If θ is sparse, then the observation y is determined by a few active components. 449
CHAPTER 7. REGRESSION How can (cid:107)θ(cid:107) promote sparsity? If we consider the sets 1 Ω ={θ |(cid:107)θ(cid:107) ≤τ}={(θ ,θ )||θ |+|θ |≤τ}, 1 1 1 2 1 2 Ω ={θ |(cid:107)θ(cid:107)2 ≤τ}={(θ ,θ )|θ2+θ2 ≤τ}, 2 1 2 1 2 we note that Ω has a diamond shape whereas Ω has a circular shape. Since the data 1 2 fidelity term (cid:107)Xθ −y(cid:107)2 is an ellipsoid, seeking the optimal value in the presence of the regularization term can be viewed as moving the ellipsoid until it touches the set defined by the regularization. As illustrated in Figure 7.25, since {θ | (cid:107)θ(cid:107)2 ≤ τ} is a circle, the solution will be somewhere in the middle. On the other hand, since {θ | (cid:107)θ(cid:107) ≤ τ} is a 1 diamond, the solution will be one of the vertices. The difference between “somewhere in the middle” and “a vertex” is that the vertex is a sparse solution, since by the definition of a vertex one coordinate must be zero and the other coordinate must be non-zero. We can easily extrapolate this idea to the higher-dimensional spaces. In this case, we will see that the solution for the (cid:107)·(cid:107) problem has only a few non-zero entries. 1 Figure 7.25: A vector θ is sparse if it contains only a few non-zero elements. If θ is sparse, then the observation y is determined by a few active components. TheoptimizationformulatedinEquation(7.41)isknownastheleast absolute shrink- age and selection operator (LASSO). LASSO problems are difficult, but over the past two decades we have increased our understanding of the problem. The most significant break- through is that we now have algorithms to solve the LASSO problem efficiently. This is important because, unlike the ridge regression, where we have a (very simple) closed-form solution, the LASSO problem can only be solved using iterative algorithms. What is so special about LASSO? • LASSO regularization promotes a sparse solution. • If the underlying model has a sparse solution, e.g., you choose a 50th-order polynomial, but the underlying model is a third-order polynomial, then there should only be three non-zero regression coefficients in your 50th-order polyno- mial. LASSO will help in this case. 450
7.4. REGULARIZATION • If the underlying model has a dense solution, then LASSO is of limited value. A ridge regression could be better. • While (cid:107)θ(cid:107) is not differentiable (at 0), there exist polynomial-time convex algo- 1 rithms to solve the problem, e.g., interior-point methods. Solving the LASSO problem Today, there are many open-source packages to solve the LASSO problem. They are mostly developed in the convex optimization literature. One of the most user-friendly packages is the CVX package developed by S. Boyd and colleagues at Stanford University.7 Once you have downloaded and installed the package, solving the optimization can be done literally by typing in the data fidelity term and the regularization term. An example is given below. cvx_begin variable theta(d) minimize(sum_square(X*theta-y) + lambda*norm(theta,1)) cvx_end As you can see, the program is extremely simple. You start by calling cvx_begin and end it with cvx_end. Inside the box we create a variable beta(d), where d denotes the dimension of the vector theta. The main command is minimize. However, this line is almost self-explanatory. As long as you follow the syntax given by the user guidelines, you will be able to set it up properly. In Python, we can call the cvxpy library. import cvxpy as cvx theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) prob = cvx.Problem(objective) prob.solve() To see a concrete example, we use the crime rate data obtained from https://web. stanford.edu/~hastie/StatLearnSparsity/data.html. A snapshot of the data is shown in the table below. In this dataset, the vector y is the crime rate, which is the last column of the table. The feature/basis vectors are funding, hs, not-hs, college. city crime rate funding hs no-hs college 1 478 40 74 11 31 2 494 32 72 11 43 3 643 57 71 18 16 4 341 31 71 11 25 . . . . . . . . . . . . . . . . . . 50 940 66 67 26 18 7The MATLAB version is here: http://cvxr.com/cvx/. The Python version is here: https://cvxopt. org/.Followtheinstructionstoinstallthepackage. 451
CHAPTER 7. REGRESSION We consider two optimizations: θ(cid:98)1(λ)=argmin E 1(θ)d =ef (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107) 1, θ θ(cid:98)2(λ)=argmin E 2(θ)d =ef (cid:107)Xθ−y(cid:107)2+λ(cid:107)θ(cid:107)2. θ Aswehavediscussed,thefirstoptimizationusesthe(cid:107)·(cid:107) regularizedleastsquares,whichis 1 theLASSOproblem.Thesecondoptimizationisthestandard(cid:107)·(cid:107)2 regularizedleastsquares. Since both solutions depend on the parameter λ, we parameterize the solutions in terms of λ. Note that the optimal λ for θ(cid:98)1 is not necessarily the optimal λ for θ(cid:98)2. One thing we would like to demonstrate in this example is visualizing the linear re- gression coefficients θ(cid:98)1(λ) and θ(cid:98)2(λ) as λ changes. To solve the optimization, we use CVX with the MATLAB and Python implementation is shown below. data = load(’./dataset/data_crime.txt’); y = data(:,1); % The observed crime rate X = data(:,3:end); % Feature vectors [N,d]= size(X); lambdaset = logspace(-1,8,50); theta_store = zeros(d,50); for i=1:length(lambdaset) lambda = lambdaset(i); cvx_begin variable theta(d) minimize( sum_square(X*theta-y) + lambda*norm(theta,1) ) % minimize( sum_square(X*theta-y) + lambda*sum_square(theta) ) cvx_end theta_store(:,i) = theta(:); end figure(1); semilogx(lambdaset, theta_store, ’LineWidth’, 4); legend(’funding’,’% high’, ’% no high’, ’% college’, ... ’% graduate’, ’Location’,’NW’); xlabel(’lambda’); ylabel(’feature attribute’); import cvxpy as cvx import numpy as np import matplotlib.pyplot as plt data = np.loadtxt("/content/data_crime.txt") y = data[:,0] X = data[:,2:7] N,d = X.shape lambd_set = np.logspace(-1,8,50) 452
7.4. REGULARIZATION theta_store = np.zeros((d,50)) for i in range(50): lambd = lambd_set[i] theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) # objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.sum_squares(theta) ) prob = cvx.Problem(objective) prob.solve() theta_store[:,i] = theta.value for i in range(d): plt.semilogx(lambd_set, theta_store[i,:]) 14 12 10 8 6 4 2 0 -2 10-2 100 102 104 106 108 lambda etubirtta erutaef 14 funding % high 12 % no high % college 10 % graduate 8 6 4 2 0 -2 10-2 100 102 104 106 108 lambda etubirtta erutaef funding % high % no high % college % graduate (a) LASSO (b) Ridge Figure7.26:RidgeandLASSOregressiononthecrime-ratedataset.(a)TheLASSOregressionsuggests that there are only a few active components as we change λ. (b) The ridge regression returns a set of dense solutions for all choices of λ. Figure 7.26 shows some interesting differences between the two regression models. • Trajectory. For the (cid:107)·(cid:107)2 estimate θ(cid:98)2(λ), the trajectory of the regression coefficients is smooth. This is attributable to the fact that the training loss E (θ) is continuously 2 differentiable in θ, and so the solution trajectory is smooth. By contrast, the (cid:107)·(cid:107) 1 estimate θ(cid:98)1(λ) has a more disruptive trajectory. • Active members. For the LASSO problem, θ(cid:98)1(λ) switches the active member as λ changes. For example, the feature high-school is the first one being activated when λ ↓. This implies that if we limit ourselves to only one feature, then high-school is the feature we should select. The ridge regression does not have this feature-selection property.Howaboutwhenλ=106?Inthiscase,theLASSOhastwoactivemembers: funding and high-school. This suggests that if there are two contributing factors, funding and high-school are the two. As λ=104, we see that in LASSO, the green curve goes to zero but then the red curve rises. This means a correlation between 453
CHAPTER 7. REGRESSION high school and no high school, which should not be a surprise because they are complementary to each other. • Magnitude of solutions. The magnitude of the solutions does not necessarily convey a clear conclusion because the feature vectors (e.g., high school) and the observable crime rate have different units. • Limiting solutions. As λ→0, both θ(cid:98)1(λ) and θ(cid:98)2(λ) reach the same solution, because the training losses are identical when λ=0. LASSO for overfitting Does LASSO help to mitigate the overfitting problem? Not always, but it often does. In Figure 7.27 we consider fitting a dataset of N = 20 data points. The ground truth model we use is y =L (x )+0.5L (x )+0.5L (x )+1.5L (x )+L (x )+e , n 0 n 1 n 2 n 3 n 4 n n where e ∼ Gaussian(0,σ2) for σ = 0.25. When fitting the data, we purposely choose a n 20th-orderLegendrepolynomialastheregressionmodel.WithonlyN =20datapoints,we can be almost certain that there is overfitting. The MATLAB and Python codes for solving this LASSO problem are shown below. % MATLAB code to demonstrate overfitting and LASSO % Generate data N = 20; x = linspace(-1,1,N)’; a = [1, 0.5, 0.5, 1.5, 1]; y = a(1)*legendreP(0,x)+a(2)*legendreP(1,x)+a(3)*legendreP(2,x)+ ... a(4)*legendreP(3,x)+a(5)*legendreP(4,x)+0.25*randn(N,1); % Solve LASSO using CVX d = 20; X = zeros(N, d); for p=0:d-1 X(:,p+1) = reshape(legendreP(p,x),N,1); end lambda = 2; cvx_begin variable theta(d) minimize( sum_square( X*theta - y ) + lambda * norm(theta , 1) ) cvx_end % Plot results t = linspace(-1, 1, 200); Xhat = zeros(length(t), d); for p=0:d-1 Xhat(:,p+1) = reshape(legendreP(p,t),200,1); end yhat = Xhat*theta; 454
7.4. REGULARIZATION plot(x,y, ’ko’,’LineWidth’,2, ’MarkerSize’, 10); hold on; plot(t,yhat,’LineWidth’,6,’Color’,[0.2 0.5 0.2]); # Python code to demonstrate overfitting and LASSO import cvxpy as cvx import numpy as np import matplotlib.pyplot as plt # Setup the problem N = 20 x = np.linspace(-1,1,N) a = np.array([1, 0.5, 0.5, 1.5, 1]) y = a[0]*eval_legendre(0,x) + a[1]*eval_legendre(1,x) + \ a[2]*eval_legendre(2,x) + a[3]*eval_legendre(3,x) + \ a[4]*eval_legendre(4,x) + 0.25*np.random.randn(N) # Solve LASSO using CVX d = 20 lambd = 1 X = np.zeros((N, d)) for p in range(d): X[:,p] = eval_legendre(p,x) theta = cvx.Variable(d) objective = cvx.Minimize( cvx.sum_squares(X*theta-y) \ + lambd*cvx.norm1(theta) ) prob = cvx.Problem(objective) prob.solve() thetahat = theta.value # Plot the curves t = np.linspace(-1, 1, 500) Xhat = np.zeros((500,d)) for p in range(P): Xhat[:,p] = eval_legendre(p,t) yhat = np.dot(Xhat, thetahat) plt.plot(x, y, ’o’) plt.plot(t, yhat, linewidth=4) Let us compare the various regression results. Figure 7.27(b) shows the vanilla regres- sion, which as you can see fits the N = 20 data points very well. However, no one would believe that such a fitting curve can generalize to unseen data. Figure 7.27(c) shows the ridge regression result. When performing the analysis, we sweep a range of λ and pick the value λ=0.5 so that the fitted curve is neither too “wild” nor too “flat”. We can see that the fitting is improved. However, since the ridge regression only penalizes large-magnitude coefficients, the fitting is still not ideal. Figure 7.27(d) shows the LASSO regression result. Sincethetruemodelisa4th-orderpolynomialandweusea20th-orderpolynomial,thetrue solution is sparse. Therefore, LASSO is helpful, and hence we can pick a sparse solution. The significance of LASSO is often not about the fitting of the data points but the 455
CHAPTER 7. REGRESSION 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (a) Ground truth model (b) Vanilla regression 4 4 3 3 2 2 1 1 0 0 data data fitted curve fitted curve -1 -1 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 (c) Ridge (d) LASSO Figure 7.27: We fit a dataset of N = 20 data points. (a) The ground truth model that generates the data. The model is a 4th-order ordinary polynomial. (b) Vanilla regression result, without any regularization.Notethatthereissevereoverfittingbecausethemodelcomplexityistoohigh.(c)Ridge regression result, by setting λ=0.5. (d) LASSO regression result, by setting λ=2. number of active coefficients. In Figure 7.28 we show a comparison between the ground truthcoefficients,thevanillaregressioncoefficients,theridgeregressioncoefficients,andthe LASSOregressioncoefficients.ItisevidentthattheLASSOsolutioncontainsamuchsmaller number of non-zeros compared to the ridge regression. Most of the high-order coefficients are zero. By contrast, the vanilla regression coefficients are wild. The ridge regression is better, but there are many non-zero high-order coefficients. Closing remark. In this section, we discussed two regularization techniques: ridge regres- sionandLASSOregression.Bothtechniquesareaboutaddingapenaltytermtothetraining loss to constrain the regression coefficients. In the optimization literature, writings on ridge and LASSO regression are abundant, covering both algorithms and theoretical properties. An example of a theoretical question addressed in the literature is: Under what conditions isLASSOguaranteedtorecoverthecorrectsupportofthesolution,i.e.,locatingthecorrect positions of the non-zeros? Problems like these are beyond the scope of this book. 456
7.5. SUMMARY 2 15 10 1 5 0 0 -5 -1 -10 -2 -15 5 10 15 20 5 10 15 20 (a) Ground truth model (b) Vanilla regression 2 2 1 1 0 0 -1 -1 -2 -2 5 10 15 20 5 10 15 20 (c) Ridge (d) LASSO Figure 7.28: Coefficients of the regression models. (a) The ground truth model, which is a 4th-order polynomial. There are only 5 non-zero coefficients. (b) The vanilla regression coefficients. Note that the values are wild and large, although the curve fits the training data points very well. (c) The ridge regression coefficients. While the overall magnitudes are significantly improved from the vanilla, some high-order coefficients are still non-zero. (d) The LASSO regression coefficients. There are very few non-zeros, and the non-zeros match well with the ground truth. 7.5 Summary Regressionisoneofthemostwidelyusedtechniquesindatascience.Theformulationofthe regression problem is as simple as setting up a system of linear equations: minimize (cid:107)Xθ−y(cid:107)2, (7.42) θ∈Rd which has a closed-form solution. The biggest problems in practice are outliers, lack of training samples, and poor choice of the regression model. • Outliers: We always recommend plotting the data whenever possible to check if there are obvious outliers. There are also statistical tests in which you can evaluate the validity of your samples. One simple way to debug outliers is to run the regression 457
CHAPTER 7. REGRESSION and check the prediction error against each training sample. If you have an outlier, and if your model is of reasonably low complexity, then a sample with an excessively large prediction error is an outlier. For example, if most of the training samples are within one standard deviation from your prediction but a few are substantially off, youwillknowwhichonesaretheoutliers.Robustlinearregressionisonetechniquefor countering outliers, but an experienced data scientist can often reject outliers before runninganyregressionalgorithms.Domainknowledgeisofgreatvalueforthispurpose. • Lack of training samples: As we have discussed in the overfitting section, it is ex- tremelyimportanttoensurethatyourmodelcomplexityisappropriateforthenumber of training samples. If the training set is small, do not use a complex model. Regu- larization techniques are valuable tools to mitigate overfitting. However, choosing a good regularization requires domain knowledge. For example, if you know that some featuresarenotimportant,youneedtoscalethemproperlysoasnottoover-influence the regression solution. • Wrongmodel:Wehavementionedseveraltimesthatregressioncanalwaysreturnyou a result because regression is an optimization problem. However, whether that result is meaningful depends on how meaningful your regression problem is. For example, if the noise is i.i.d. Gaussian, a data fidelity term with (cid:107)·(cid:107)2 would be a good choice; however, if the noise is i.i.d. Poisson, (cid:107)·(cid:107)2 would become a very bad model. We need a tighter connection with the statistics of the underlying data-generation model for problems like these. This is the subject of our next chapter, on parameter estimation. 7.6 References Linear regression Treatmentofstandardlinearregressionisabundant.Inthecontextofmachinelearningand data science, the following references are useful. 7-1 GarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani,AnIntroduction to Statistical Learning with Applications in R, Springer 2013, Chapter 3. 7-2 StephenBoydandLievenVandenberghe,Convex Optimization,CambridgeUniversity Press, 2004. Chapter 6. 7-3 Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning, Springer, 2001. Chapter 3. 7-4 ChristopherBishop,PatternRecognitionandMachineLearning,Springer2006.Chap- ter 3.1. 7-5 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data, AML Book, 2012. Chapter 3.2 Overfitting and Bias/Variance Thetheoryofoverfittingandthetrade-offbetweenbiasandvariancecanbefoundinmultiple references. The following are basic treatments of the subject. 458
7.7. PROBLEMS 7-6 Yaser Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin, Learning from Data, AML Book, 2012. Chapter 4. 7-7 ChristopherBishop,PatternRecognitionandMachineLearning,Springer2006.Chap- ter 3.2. Ridge and LASSO regression RidgeandLASSOregressionareimportanttoolsinstatisticallearningtoday.Thefollowing two textbooks cover some of the perspectives of the statistical community and the signal processing community. 7-8 Trevor Hastie, Robert Tibshirani, and Martin Wainwright, Statistical Learning with Sparsity: The LASSO and Generalizations, CRC Press, 2015. 7-9 Michael Elad, Sparse and Redundant Representations, Springer, 2010. Chapters 1 and 3. 7.7 Problems Exercise 1. (a) Construct a dataset with N =20 samples, following the model d−1 (cid:88) y = θ L (x )+e , (7.43) n p p n n p=0 where θ =1, θ =0.5, θ =0.5, θ =1.5, θ =1, for −1<x<1. Here, L (x) is the 0 1 2 3 4 p Legendrepolynomialofthepthorder.TheN =20samplesarerandomuniformlysam- pled from the interval [−1,1]. The noise samples e are i.i.d. Gaussian with variance n σ2 =0.252. Plot the dataset using the MATLAB or Python command scatter. (b) Run the regression using the same model where d = 5, without any regularization. Plot the predicted curve and overlay with the training samples. (c) Repeat (b) by running the regression with d=20. Explain your observations. (d) Increase the number of training samples N to N =50, N =500, and N =5000, and repeat (c). Explain your observations. (e) ConstructatestingdatasetwithM =1000testingsamples.Foreachoftheregression models trained in (b)-(d), compute the testing error. Exercise 2. Consider a data generation model N−1 x n = (cid:88) c ke−j2π Nkn,n=0,...,N −1. k=0 459
CHAPTER 7. REGRESSION (a) Write the above equation in matrix-vector form x=Wc. What are the vectors c and x, and what is the matrix W? (b) Show that W is orthogonal, i.e.,, WHW =I, where WH is the conjugate transpose of W. (c) Using (b), derive the least squares regression solution. Exercise 3. Consider a simplified LASSO regression problem: θ(cid:98)=argmin (cid:107)y−θ(cid:107)2+λ(cid:107)θ(cid:107) 1. (7.44) θ∈Rd Show that the solution is given by θ(cid:98)=sign(y)·max(|y|−λ,0), (7.45) where · is the elementwise multiplication. Exercise 4. A one-dimensional signal is corrupted by blur and noise: L−1 (cid:88) y = h x +e . n (cid:96) n−(cid:96) n (cid:96)=0 (a) Formulate the least squares regression problem in matrix-vector form y = Hx+e. Find x, y and H. (b) Consider a regularization function N (cid:88) R(x)= (x −x )2. n n−1 n=2 Show that this regularization is equivalent to R(x)=(cid:107)Dx(cid:107)2 for some D. Find D. (c) Using the regularization in (b), derive the regularized least squares regression result: minimize (cid:107)y−Hx(cid:107)2+λ(cid:107)Dx(cid:107)2. x Exercise 5. Let σ(·) be the sigmoid function 1 σ(a)= . 1+ea We want to use σ(a) as a basis function. 460
7.7. PROBLEMS (a) Show that the tanh function and the sigmoid function are related by tanh(a)=2σ(2a)−1. (b) Show that a linear combination of sigmoid functions d−1 (cid:18) (cid:19) y =θ +(cid:88) θ σ x n−µ j n 0 p s p=1 is equivalent to a linear combination of tanh functions d−1 (cid:18) (cid:19) y =α +(cid:88) α tanh x n−µ j . n 0 p 2s p=1 (c) Find the relationship between θ and α . p p Exercise 6. (NHANES Part 1)(Data download) The National Health and Nutrition Examination Survey (NHANES) is a program to assess thehealthandnutritionalstatusofadultsandchildrenintheUnitedStates8.Thecomplete survey result contains over 4,000 samples of health-related data of individuals who partici- pated in the survey between 2011 and 2014. In the following exercises, we will focus on two categories of the data for each individual: height (in mm) and body mass index (BMI). The data is divided into two classes based on gender. Table 1 contains snippets of the data. index female bmi female stature mm index male bmi male stature mm 0 28.2 1563 0 30 1679 1 22.2 1716 1 25.6 1586 2 27.1 1484 2 24.2 1773 3 28.1 1651 3 27.4 1816 Table 7.2: Male and Female Data Snippets Use csv.reader to read the training data files for the two data classes. Important! Before proceeding to the problems, • normalizethenumberinmale_stature_mmandfemale_stature_mmbydividingthem by 1000, and • normalize that of male_bmi and female_bmi by dividing them by 10. This will significantly reduce the numerical error. Consider a linear model: g =θTx, (7.46) θ 8https://www.cdc.gov/nchs/nhanes/index.htm 461
CHAPTER 7. REGRESSION The regression problem we want to solve is N θ(cid:98)=argmin (cid:88) (y n−g θ(x n))2, θ∈Rd n=1 whereD ={(x ,y )}N isthetrainingdataset.Puttingtheequationintothematrixform, n n n=1 we know that the optimization is equivalent to θ(cid:98)=argmin (cid:107)y−Xθ(cid:107)2. θ∈Rd (cid:124) (cid:123)(cid:122) (cid:125) Etrain(θ) (a) Derive the solution θ(cid:98). State the conditions under which the solution is the unique global minimum in terms of the rank of X. Suggest two techniques that can be used when XTX is not invertible. (b) For the NHANES dataset, assign y = +1 if the nth sample is a male and y = −1 n n if the nth sample is a female. Implement your answer in (a) with Python to solve the problem. Report your answer. (c) Repeat (b), but this time use CVXPY. Report your answer, and compare with (b). Exercise 7. (NHANES Part 2)(Data download) We want to do a classification based on the linear model we found in the previous exercise. The classifier we will use is predicted label=sign(g (x)), (7.47) θ where x∈Rd is the a test sample. Here, we label +1 for male and −1 for female. Because the dataset we consider in this exercise has only two columns, the linear model is g (x)=θ +θ x +θ x , θ 0 1 1 2 2 where x=[1,x ,x ]T is the input data and θ =[θ ,θ ,θ ]T is the parameter vector. 1 2 0 1 2 (a) First, we want to visualize the classifier. (i) Plotthetrainingdatapointsofthemaleandfemaleclasses.Markthemaleclass with blue circles and the female class with red dots. (ii) Plot the decision boundary g (·) and overlay it with the data plotted in (a). θ Hint: g (·) is a straight line in 2D. You can express x in terms of x and other θ 2 1 parameters. (b) (This problem requires knowledge of the content of Chapter 9). Report the classifica- tion accuracy. To do so, take testing data x and compute the prediction according to Equation (7.47). (i) WhatistheType1error(FalseAlarm)ofclassifyingmales?Thatis,whatisthe percentage of testing samples that should be female but a male was predicted? (ii) What is the Type 2 error (Miss) of classifying males? That is, what is the per- centage of testing samples that should be male but a female was predicted? 462
7.7. PROBLEMS (iii) Whatistheprecisionandrecallforthisclassifier?Forthedefinitionsofprecision and recall, refer to Chapter 9.5.4. Exercise 8. (NHANES Part 3)(Data download) Thisexerciserequiressomebackgroundinoptimization.PleaserefertoReference[7.2,Chap- ter 9 and 10]. Consider the following three optimization problems: θ(cid:98)λ =argmin (cid:107)Xθ−y(cid:107)2 2+λ(cid:107)θ(cid:107)2 2, (7.48) θ∈Rd θ(cid:98)α =argmin (cid:107)Xθ−y(cid:107)2 2 subjectto (cid:107)θ(cid:107)2 2 ≤α, (7.49) θ∈Rd θ(cid:98)(cid:15) =argmin (cid:107)θ(cid:107)2 2 subjectto (cid:107)Xθ−y(cid:107)2 2 ≤(cid:15). (7.50) θ∈Rd (a) Set lambd = np.arange(0.1,10,0.1). Plot • (cid:107)Xθ(cid:98)λ−y(cid:107)2 2 as a function of (cid:107)θ(cid:98)λ(cid:107)2 2. • (cid:107)Xθ(cid:98)λ−y(cid:107)2 2 as a function of λ. • (cid:107)θ(cid:98)λ(cid:107)2 2 as a function of λ. (b) (i) Write down the Lagrangian for each of the three problems. Note that the first problem does not have any Lagrange multiplier. For the second and third prob- lems you may use the following notations: • γ = the Lagrange multiplier of Equation (7.49), and α • γ = the Lagrange multiplier of Equation (7.50). (cid:15) (ii) State the first-order optimality conditions (the Karush-Kuhn-Tucker or KKT conditions) for each of the three problems. Express your answers in terms of X, θ, y, λ, α, (cid:15), and the two Lagrange multipliers γ , γ . α (cid:15) (iii) Fix λ>0. We can solve Equation (7.48) to obtain θ(cid:98)λ. Find α and the Lagrange multiplier γ α in Equation (7.49) such that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.49). (iv) Fix λ>0. We can solve Equation (7.48) to obtain θ(cid:98)λ. Find (cid:15) and the Lagrange multiplier γ (cid:15) in Equation (7.50) such that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.50). (v) Fix λ>0. By using the α and γ α you found in (iii), you can show that θ(cid:98)λ would satisfy the KKT conditions of Equation (7.49). Is it enough to claim that θ(cid:98)λ is thesolutionofEquation(7.49)?Ifyes,why?Ifno,whatelsedoweneedtoshow? Please elaborate through a proof, if needed. Exercise 9. ConsideratrainingdatasetD ={(x ,y ),...,(x ,y )}andaweightw =[w ,...,w ]T. train 1 1 N N 1 N Findtheregressionsolutiontothefollowingproblemanddiscusshowyouwouldchoosethe weight. N θ(cid:98)=argmin (cid:88) w n(cid:0) y n−xT nθ(cid:1)2 . (7.51) θ∈Rd n=1 463
CHAPTER 7. REGRESSION Exercise 10. Consider a training dataset D = {(x ,y ),...,(x ,y )}. Suppose that the input data train 1 1 N N x is corrupted by i.i.d. Gaussian noise e ∼ Gaussian(0,σ2I ) so that the training set n n d becomes D ={(x +e ,y ),...,(x +e ,y )}. Show that the (vanilla) least squares train 1 1 1 N N N linear regression by taking the expectation over e , n N θ(cid:98)=argmin (cid:88) E en(cid:104)(cid:0) y n−(x n+e n)Tθ(cid:1)2(cid:105) , (7.52) θ∈Rd n=1 is equivalent to a ridge regression. 464
Chapter 8 Estimation Inthischapter,wediscussanothersetofimportantcombatskillsindatascience,namelyes- timation.Estimationhasacloserelationshipwithregression.Regressionprimarilytakesthe optimization route, while estimation takes the probabilistic route. As we will see, at a cer- tainpointthetwowillmerge.Thatis,undersomespecificstatisticalconditions,estimation processes will coincide with the regression. EstimationissummarizedpictoriallyinFigure8.1.Imaginethatwehavesomerandom samples X ,...,X . These samples are drawn from a distribution f (x;θ), where θ is a 1 N X parameter that characterizes the distribution. The parameter θ is not known to us. The goal of estimation is to solve an inverse problem to recover the parameter based on the observations X ,...,X . 1 N Figure 8.1: Estimation is an inverse problem of recovering the unknown parameters that were used by the distribution. In this figure, the PDF of X using a parameter θ is denoted as f (x;θ). The forward X data-generationprocesstakestheparameterθandcreatestherandomsamplesX ,...,X .Estimation 1 N takes these observed random samples and recovers the underlying model parameter θ. What is estimation? Estimation is an inverse problem with the goal of recovering the underlying pa- rameter θ of a distribution f (x;θ) based on the observed samples X ,...,X . X 1 N 465
CHAPTER 8. ESTIMATION What are parameters? Before we discuss the methods of estimation, let us clarify the meaning of the parameter θ. Allprobabilitydensityfunctions(PDFs)haveparameters.Forexample,aBernoullirandom variableischaracterizedbyaparameterpthatdefinestheprobabilityofgettinga“head”.A Gaussian random variable is characterized by two parameters: the mean µ and variance σ2. Example 8.1. (Parameter of a Bernoulli) If X is a Bernoulli random variable, then n the PMF has a parameter θ: p (x ; θ)=θxn(1−θ)1−xn. Xn n Remark. The PMF is expressed in this form because x is either 1 or 0: n (cid:40) θ1(1−θ)1−1 =θ, if x =1, p (x ; θ)= n Xn n θ0(1−θ)1−0 =1−θ, if x =0. n Example 8.2. (Parameter of a Gaussian) If X is a Gaussian random variable, the n PDF is 1 (cid:26) (x −µ)2(cid:27) f (x ; θ )= √ exp − n , Xn n (cid:124)(cid:123)(cid:122)(cid:125) 2πσ2 2σ2 =(µ,σ) where θ = [µ,σ] consists of both the mean and the variance. We can also designate the parameter θ to be the mean only. For example, if we know that σ = 1, then the PDF is 1 (cid:26) (x −µ)2(cid:27) f (x ; θ )= √ exp − n , Xn n (cid:124)(cid:123)(cid:122)(cid:125) 2π 2 =µ where θ is the mean. Since all probability density functions have parameters, estimating them from the observed random variables is a well-defined inverse problem. Of course, there are better estimates and there are worse estimates. Let us look at the following example to develop our intuitions about estimation. Figure 8.2showsadatasetcontaining1000datapointsgeneratedfroma2DGaussian distribution with an unknown mean vector µ and an unknown covariance matrix Σ. We duplicate this dataset in the four subfigures. The estimation problem is to recover the unknown mean vector µ and the covariance matrix Σ. In the subfigures we propose four candidates, each with a different mean vector and a different covariance matrix. We draw thecontourlinesofthecorrespondingGaussians.ItcanbeseenthatsomeGaussiansfitthe data better than others. The goal of this chapter is to develop a systematic way of finding the best fit for the data. Plan for this chapter The discussions in this chapter concern the three elementary distributions: 466
5 5 5 5 4 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1 0 0 0 0 -1 -1 -1 -1 -2 -2 -2 -2 -3 -3 -3 -3 -4 -4 -4 -4 -5 -5 -5 -5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 Bad estimate Bad estimate Bad estimate Good estimate (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 2 0 −0.5 0 µ= µ= µ= µ= −0.5 −1.5 −0.7 0 (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 0.25 0.2 1 −0.2 1 0 0.25 0.3 Σ= Σ= Σ= Σ= 0.2 1 −0.2 0.1 0 1 0.3 1 Figure8.2:Anestimationproblem.Givenasetof1000datapointsdrawnfromaGaussiandistribution with unknown mean µ and covariance Σ, we propose several candidate Gaussians and see which one would be the best fit to the data. Visually, we observe that the right-most Gaussian has the best fit. The goal of this chapter is to develop a systematic way of solving estimation problems of this type. • Likelihood: f (x|θ), which is the conditional PDF of X given that the parameter X|Θ is Θ. • Prior: f (θ), which is the PDF of Θ. Θ • Posterior: f (θ|x), which is the conditional PDF of Θ given the data X. Θ|X Eachofthesedensityfunctionshasitsrespectivemeaning,andconsequentlyasetofdifferent estimationtechniques.InSection8.1weintroducetheconceptof maximum-likelihood(ML) estimation. As the name suggests, the estimate is constructed by maximizing the likelihood function. We will discuss a few examples of ML estimation and draw connections between ML estimation and regression. In Section 8.2 we will discuss several basic properties of an ML estimate. Specifically, we will introduce the ideas of unbiasedness, consistency, and the invariance principle. The second topic discussed in this chapter is the maximum-a-posteriori (MAP) esti- mation,detailedinSection8.3.InMAP,theparameterΘisarandomvariable.SinceΘisa random variable, it has its own probability density function f (θ), which we call the prior. Θ Given the likelihood and the prior, we can define the posterior. The MAP estimation finds the peak of the posterior distribution as a way to “explain” the data. Several important topics will be covered in Section 8.3. For example, we will discuss the choice of the prior via the concept of conjugate prior. We will also discuss how MAP is related to regularized regressions such as the ridge and LASSO regressions. The third topic is the minimum mean-square estimation (MMSE), outlined in Sec- tion8.4.TheMMSEisaBayesianapproach.Animportantresultthatwillbedemonstrated is that the MMSE estimate is the conditional expectation of the posterior distribution. In otherwords,itisthemeanoftheposterior.AnMMSEestimatehasanimportantdifference compared to a MAP estimate, namely that while an MMSE estimate is the mean of the posterior, a MAP estimate is the mode of the posterior. We discuss the formulation of the estimation problem and ways of solving the problem. We also discuss how the MMSE can be performed for multidimensional Gaussian distributions. 467
CHAPTER 8. ESTIMATION 8.1 Maximum-Likelihood Estimation Maximum-likelihood (ML) estimation, as the name suggests, is an estimation method that “maximizes”the“likelihood”.Therefore,tounderstandtheMLestimation,wefirstneedto understand the meaning of likelihood, and why maximizing the likelihood would be useful. 8.1.1 Likelihood function ConsiderasetofN datapointsD ={x ,x ,...,x }.Wewanttodescribethesedatapoints 1 2 N using a probability distribution. What would be the most general way of defining such a distribution? SincewehaveN datapoints,andwedonotknowanythingaboutthem,themostgen- eralwaytodefineadistributionisasahigh-dimensionalprobabilitydensityfunction(PDF) f (x). This is a PDF of a random vector X = [X ,...,X ]T. A particular realization of X 1 N this random vector is x=[x ,...,x ]T. 1 N f (x) is the most general description for the N data points because f (x) is the X X joint PDF of all variables. It provides the complete statistical description of the vector X. For example, we can compute the mean vector E[X], the covariance matrix Cov(X), the marginal distributions, the conditional distribution, the conditional expectations, etc. In short, if we know f (x), we know everything about X. X ThejointPDFf (x)isalwaysparameterizedbyacertainparameterθ.Forexample,if X weassumethatX isdrawnfromajointGaussiandistribution,thenf (x)isparameterized X by the mean vector µ and the covariance matrix Σ. So we say that the parameter θ is θ =(µ,Σ). To state the dependency on the parameter explicitly, we write f (x; θ)=PDF of the random vector X with a parameter θ. X When you express the joint PDF as a function of x and θ, you have two variables to play with. The first variable is the observation x, which is given by the measured data. We usually think about the probability density function f (x) in terms of x, because the PDF X is evaluated at X = x. In estimation, however, x is something that you cannot control. Whenyourbosshandsadatasettoyou,xisalreadyfixed.Youcanconsidertheprobability of getting this particular x, but you cannot change x. The second variable stated in f (x; θ) is the parameter θ. This parameter is what X we want to find out, and it is the subject of interest in an estimation problem. Our goal is to find the optimal θ that can offer the “best explanation” to data x, in the sense that it can maximize f (x; θ). X The likelihood function is the PDF that shifts the emphasis to θ: Definition 8.1. Let X =[X ,...,X ]T be a random vector drawn from a joint PDF 1 N f (x;θ), and let x = [x ,...,x ]T be the realizations. The likelihood function is a X 1 N function of the parameter θ given the realizations x: def L(θ|x) = f (x; θ). (8.1) X 468
8.1. MAXIMUM-LIKELIHOOD ESTIMATION A word of caution: L(θ|x) is not a conditional PDF because θ is not a random variable. The correct way to interpret L(θ|x) is to view it as a function of θ. This function changes its shape according the observed data x. We will return to this point shortly. Independent observations Whilef (x)providesuswithacompletepictureoftheX,usingf (x)istedious.Weneed X X to describe how each X is generated and describe how X is related to X for all pairs of n n m n and m. If the vector X contains N entries, then there are N2/2 pairs of correlations we need to compute. When N is large, finding f (x) would be very difficult if not impossible. X In practice, f (x) may sometimes be overkill. For example, if we measure the inter- X arrival time of a bus for several days, it is quite likely that the measurements will not be correlated. In this case, instead of using the full f (x), we can make assumptions about X the data points. The assumption we will make is that all the data points are independent and that they are drawn from an identical distribution f (x). The assumption that the X datapointsareindependently and identically distributed(i.i.d.)significantlysimplifiesthe problem so that the joint PDF f can be written as a product of single PDFs f : X Xn N (cid:89) f (x)=f (x ,...,x )= f (x ). X X1,...,XN 1 N Xn n n=1 If you prefer a visualization, we can take a look at the covariance matrix, which goes from a full covariance matrix to a diagonal matrix and then to an identity matrix:     Var[X1] Cov(X1,X2) ··· Cov(X1,XN) Var[X1] 0 ··· 0 Cov[X2,X1] Var[X2] ··· Cov(X2,XN) 0 Var[X2] ··· 0   . . . . . . ... . . .   inde= pe⇒ ndent  . . . . . . ... . . .   Cov(XN,X1) Cov(XN,X2) ··· Var[XN] 0 0 ··· Var[XN]  σ2 0 ··· 0 0 σ2 ··· 0 ide= n⇒ tical  . . . . . . ... . . .  . 0 0 ··· σ2 The assumption of i.i.d. is strong. Not all data can be modeled as i.i.d. (For example, photons passing through a scattering medium have correlated statistics.) However, if the i.i.d. assumption is valid, we can simplify the model significantly. If the data points are i.i.d., then we can write the joint PDF as N (cid:89) f (x; θ)= f (x ; θ). X Xn n n=1 This simplifies the likelihood function as a product of the individual PDFs. Definition8.2. Giveni.i.d.randomvariablesX ,...,X thatallhavethesamePDF 1 N f (x ), the likelihood function is Xn n N def (cid:89) L(θ|x) = f (x ; θ). (8.2) Xn n n=1 Incomputationweoftentakethelogofthelikelihoodfunction.Wecalltheresultingfunction the log-likelihood. 469
CHAPTER 8. ESTIMATION Definition8.3. Givenasetofi.i.d.randomvariablesX ,...,X withPDFf (x;;θ), 1 N Xn the log-likelihood is defined as N (cid:88) logL(θ|x)=logf (x; θ)= logf (x ; θ). (8.3) X Xn n n=1 Example8.3.Findthelog-likelihoodofasequenceofi.i.d.Gaussianrandomvariables X ,...,X with mean µ and variance σ2. 1 N Solution. Since the random variables X ,...,X are i.i.d. Gaussian, the PDF is 1 N N (cid:26) (cid:27) f X(x; µ,σ2)= (cid:89) √ 1 e−(xn 2σ− 2µ)2 . (8.4) 2πσ2 n=1 Taking the log on both sides yields the log-likelihood function: logL(µ,σ2|x)=logf (x; µ,σ2) X (cid:40) N (cid:26) (cid:27)(cid:41) =log (cid:89) √ 1 e−(xn 2σ− 2µ)2 2πσ2 n=1 N (cid:26) (cid:27) = (cid:88) log √ 1 e−(xn 2σ− 2µ)2 2πσ2 n=1 = (cid:88)N (cid:26) −1 log(2πσ2)− (x n−µ)2(cid:27) 2 2σ2 n=1 N N 1 (cid:88) =− log(2πσ2)− (x −µ)2. 2 2σ2 n n=1 PracticeExercise8.1.Findthelog-likelihoodofasequenceofi.i.d.Bernoullirandom variables X ,...,X with parameter θ. 1 N Solution. If X ,...,X are i.i.d. Bernoulli random variables, we have 1 N N (cid:26) (cid:27) (cid:89) f (x; θ)= θxn(1−θ)1−xn . X n=1 Taking the log on both sides of the equation yields the log-likelihood function: (cid:40) N (cid:26) (cid:27)(cid:41) (cid:89) logL(θ|x)=log θxn(1−θ)1−xn . n=1 470
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Hence, N (cid:26) (cid:27) (cid:88) logL(θ|x)= log θxn(1−θ)1−xn n=1 N (cid:88) = x logθ+(1−x )log(1−θ) n n n=1 (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) = x ·logθ+ N − x ·log(1−θ). n n n=1 n=1 Visualizing the likelihood function The likelihood function L(θ|x) is a function of θ, but its value also depends on the under- lying measurements x. It is extremely important to keep in mind the presence of both. Tohelpyouvisualizetheeffectofθ andx,weconsiderasetofi.i.d.Bernoullirandom variables. As we have just shown in the practice exercise, the likelihood function of these i.i.d. random variables is (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) logL(θ|x)= x ·logθ+ N − x ·log(1−θ), (8.5) n n n=1 n=1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) S N−S where we define S =(cid:80)N x as the sum of the (binary) measurements. n=1 n To make the dependency on S and θ explicit, we write L(θ|x) as logL(θ |S)=Slogθ+(N −S)log(1−θ), (8.6) which emphasizes the role of S in defining the log-likelihood function. We plot the surface of L(θ|S) as a function of S and θ, assuming that N =50. As shown on the left-hand side of Figure 8.3, the surface L(θ|S) has a saddle shape. Along one direction the function goes up, whereas along another direction the function goes down. In the middle of Figure 8.3, we show a bird’s-eye view of the surface, with the color-coding matched with the surface plot. As you can see, when plotted as a function of θ and x (in our case, we use a summary statistic S = (cid:80)N x ), the two-dimensional plot tells us how the log-likelihood function n=1 n changes when S changes. On the right-hand side of Figure 8.3, we show two particular cross sections of the two-dimensional plot. One cross section is taken from S = 25 and the other cross section is taken from S =12. Since the total number of heads in this numerical experiment is assumed to be N = 50, the first cross section at S = 25 is obtained when half of the Bernoulli measurements are “1”, whereas the second cross section at S = 12 is obtained when a quarter of the Bernoulli measurements are “1”. The cross sections tell us the log-likelihood function logL(θ|S) is a function defined specifically for a given measurement x. As you can see from Figure 8.3, the log-likelihood function changes when S changes. Therefore, if our goal is to “find a θ that maximizes the log-likelihood function”, then for a different x we will have a different answer. For example, according to Figure 8.3, the maximum for logL(θ|S = 25) occurs when θ ≈ 0.5, and the maximum for logL(θ|S = 12) occurs when θ ≈ 0.24. These are the maximum-likelihood estimates for the respective measurements. 471
CHAPTER 8. ESTIMATION Figure 8.3: We plot the log-likelihood function as a function of S =(cid:80)N x and θ. [Left] We show n=1 n the surface plot of L(θ|S) = Slogθ+(N −S)log(1−θ). Note that the surface has a saddle shape. [Middle] By taking a bird’s-eye view of the surface plot, we obtain a 2-dimensional contour plot of the surface, where the color code matches the height of the log-likelihood function. [Right] We take two cross sections along S =25 and S =12. Observe how the shape changes. We use the following MATLAB code to generate the surface plot: % MATLAB code to generate the surface plot N = 50; S = 1:N; theta = linspace(0.1,0.9,100); [S_grid, theta_grid] = meshgrid(S, theta); L = S_grid.*log(theta_grid) + (N-S_grid).*log(1-theta_grid); s = surf(S,theta,L); s.LineStyle = ’-’; colormap jet view(65,15) For the bird’s-eye view plot, we replace surf with imagesc(S,theta,L). For the cross section plots, we call the commands plot(theta, L(:,12)) and plot(theta, L(:,25)). 8.1.2 Maximum-likelihood estimate The likelihood is the PDF of X but viewed as a function of θ. The optimization problem of maximizing L(θ|x) is called the maximum-likelihood (ML) estimation: Definition 8.4. Let L(θ) be the likelihood function of the parameter θ given the measurements x=[x ,...,x ]T. The maximum-likelihood estimate of the parameter 1 N θ is a parameter that maximizes the likelihood: def θ(cid:98)ML = argmax L(θ|x). (8.7) θ 472
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Example 8.4. Find the ML estimate for a set of i.i.d. Bernoulli random variables {X ,...,X } with X ∼Bernoulli(θ) for n=1,...,N. 1 N n Solution. We know that the log-likelihood function of a set of i.i.d. Bernoulli random variables is given by (cid:32) N (cid:33) (cid:32) N (cid:33) (cid:88) (cid:88) logL(θ|x)= x ·logθ+ N − x ·log(1−θ). (8.8) n n n=1 n=1 Thus, to find the ML estimate, we need to solve the optimization problem (cid:40)(cid:32) N (cid:33) (cid:32) N (cid:33) (cid:41) (cid:88) (cid:88) θ(cid:98)ML =argmax x n ·logθ+ N − x n ·log(1−θ) . θ n=1 n=1 Taking the derivative with respect to θ and setting it to zero, we obtain (cid:40)(cid:32) N (cid:33) (cid:32) N (cid:33) (cid:41) d (cid:88) (cid:88) x ·logθ+ N − x ·log(1−θ) =0. dθ n n n=1 n=1 This gives us (cid:16) (cid:17) (cid:80)N n=1x n − N −(cid:80)N n=1x n =0. θ 1−θ Rearranging the terms yields N 1 (cid:88) θ(cid:98)ML = N x n. n=1 Let’s do a sanity check to see if this result makes sense. The solution to this problem says that θ(cid:98)ML is the empirical average of the measurements. Assume that N = 50. Let us consider two particular scenarios as illustrated in Figure 8.4. • Scenario 1: x is a vector of measurements such that S d =ef (cid:80)N x = 25. Since n=1 n N =50, the formula tells us that θ(cid:98)ML = 52 05 =0.5. This is the best guess based on the 50 measurements where 25 are heads. If you look at Figure 8.3 and Figure 8.4, when S = 25, we are looking at a particular cross section in the 2D plot. The likelihood function we are inspecting is L(θ|S =25). For this likelihood function, the maximum occurs at θ =0.5. • Scenario 2:xisavectorofmeasurementssuchthatS d =ef(cid:80)N x =12.Theformula n=1 n tells us that θ(cid:98)ML = 1 52 0 = 0.24. This is again the best guess based on the 50 measure- ments where 12 are heads. Referring to Figure 8.3 and Figure 8.4, we can see that the likelihood function corresponds to another cross section L(θ|S = 12) where the maximum occurs at θ =0.24. Atthispoint,youmaywonderwhytheshapeofthelikelihoodfunctionL(θ|x)changes so radically as x changes? The answer can be found in Figure 8.5. Imagine that we have N = 50 measurements of which S = 40 give us heads. If these i.i.d. Bernoulli random 473
CHAPTER 8. ESTIMATION Figure 8.4: Illustration of how the maximum-likelihood estimate of a set of i.i.d. Bernoulli random variables is determined. The subfigures above show two particular scenarios at S = 25 and S = 12, assuming that N = 50. When S = 25, the likelihood function has a quadratic shape centered at θ=0.5.ThispointisalsothepeakofthelikelihoodfunctionwhenS =25.Therefore,theMLestimate is θ(cid:98)ML =0.5. The second case is when S =12. The quadratic likelihood is shifted toward the left. The ML estimate is θ(cid:98)ML =0.24. variables have a parameter θ = 0.5, it is quite unlikely that we will get 40 out of 50 measurements to be heads. (If it were θ = 0.5, we should get more or less 25 out of 50 heads.) When S = 40, and without any additional information about the experiment, the most logical guess is that the Bernoulli random variables have a parameter θ = 0.8. Since the measurement S can be as extreme as 0 out of 50 or 50 out of 50, the likelihood function L(θ|x) has to reflect these extreme cases. Therefore, as we change x, we observe a big change in the shape of the likelihood function. As you can see from Figure 8.5, S = 40 corresponds to the marked vertical cross section. As we determine the maximum-likelihood estimate, we search among all the possi- bilities,suchasθ =0.2,θ =0.5,θ =0.8,etc.Thesepossibilitiescorrespondtothehorizontal lines we drew in the figure. Among those horizontal lines, it is clear that the best estimate occurs when θ =0.8, which is also the ML estimate. Visualizing ML estimation as N grows Maximum-likelihood estimation can also be understood directly from the PDF instead of the likelihood function. To explain this perspective, let’s do a quick exercise. Practice Exercise 8.2. Suppose that X is a Gaussian random variable. Assume n that σ =1 is known but the mean θ is unknown. Find the ML estimate of the mean. Solution. The ML estimate θ(cid:98)ML is θ(cid:98)ML =argmax logL(θ|x). θ 474
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Figure 8.5: Suppose that we have a set of measurements such that S = 40. To determine the ML estimate, we look at the vertical cross section at S = 40. Among the different candidate parameters, e.g., θ =0.2, θ =0.5 and θ =0.8, we pick the one that has the maximum response to the likelihood function.ForS =40,itismorelikelythattheunderlyingparameterisθ=0.8thanθ=0.2orθ=0.5. With some calculation, we can show that θ(cid:98)ML =argm θ ax log(cid:40) (cid:89)N √1 2π exp(cid:26) −(x n− 2 θ)2(cid:27)(cid:41) n=1 N N 1 (cid:88) =argmax − log(2π)− (x −θ)2. 2 2 n θ n=1 Taking the derivative with respect to θ, we obtain (cid:40) N (cid:41) d N 1 (cid:88) − log(2π)− (x −θ)2 =0. dθ 2 2 n n=1 This gives us (cid:80)N (x −θ)=0. Therefore, the ML estimate is n=1 n N 1 (cid:88) θ(cid:98)ML = N x n. n=1 Now we will draw the PDF and compare it with the measured data points. Our focus is to analyze how the ML estimate changes as N grows. WhenN =1.Thereisonlyoneobservationx .ThebestGaussianthatfitsthissample 1 must be the one that is centered at x . In fact, the optimization is1 1 (cid:26) 1 (cid:26) (x −θ)2(cid:27)(cid:27) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 =argm θ ax −(x 1−θ)2 =x 1. 1Weskipthestepofcheckingwhetherthestationarypointisamaximumoraminimum,whichcanbe done by evaluating the second-order derivative. In fact, since the function −(x1−θ)2 is concave in θ, a stationarypointmustbeamaximum. 475
CHAPTER 8. ESTIMATION Therefore, the ML estimate is θ(cid:98)ML =x 1. Figure 8.6 illustrates this case. As we conduct the ML estimation, we imagine that there are a few candidate PDFs. The ML estimation says thatamongallthesecandidatePDFsweneedtofindonethatcanmaximizetheprobability of obtaining the observation x . Since we only have one observation, we have no choice but 1 to pick a Gaussian centered at x . Certainly the sample X =x could be bad, and we may 1 1 1 findawrongGaussian.However,withonlyonesamplethereisnowayforustomakebetter decisions. 0.5 0.45 Data Point 0.4 Candidate PDF 0.35 Estimated PDF 0.3 0.25 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 x Figure 8.6: N =1. Suppose that we are given one observed data point located around x=−2.1. To conducttheMLestimationweproposeafewcandidatePDFs,eachbeingaGaussianwithunitvariance but a different mean θ. The ML estimate is a parameter θ such that the corresponding PDF matches thebestwiththeobserveddata.InthisexamplethebestmatchhappenswhentheestimatedGaussian PDF is centered at x . 1 When N = 2. In this case we need to find a Gaussian that fits both x and x . The 1 2 probability of simultaneously observing x and x is determined by the joint distribution. 1 2 By independence we then have (cid:40)(cid:18) 1 (cid:19)2 (cid:26) (x −θ)2+(x −θ)2)(cid:27)(cid:41) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 2 (cid:26) (x −θ)2+(x −θ)2(cid:27) x +x =argmax − 1 2 = 1 2, 2σ2 2 θ where the last step is obtained by taking the derivative: d (cid:8) (x −θ)2+(x −θ)2(cid:9) =2(x −θ)+2(x −θ). dθ 1 2 1 2 Equating this with zero yields the solution θ = x1+x2. Therefore, the best Gaussian that 2 fits the observations is Gaussian(x1+x2,σ2). 2 Does this result make sense? When you have two data points x and x , the ML 1 2 estimation is trying to find a Gaussian that can best fit both of these two data points. Your best bet here is θ(cid:98)ML =(x 1+x 2)/2, because there are no other choices. If you choose θ(cid:98)ML = x 1 or θ(cid:98)ML = x 2, it cannot be a good estimate because you are not using both data points. As shown in Figure 8.7, for these two observed data points x and x , the PDF 1 2 marked in red (which is a Gaussian centered at (x +x )/2) is indeed the best fit. 1 2 476
8.1. MAXIMUM-LIKELIHOOD ESTIMATION 0.5 0.45 Data Point 0.4 Candidate PDF 0.35 Estimated PDF 0.3 0.25 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 x Figure 8.7: N = 2. Suppose that we are given two observed data points located around x = −0.98 1 and x = −1.15. To conduct the ML estimation we propose a few candidate PDFs, each being a 2 Gaussian with unit variance but a different mean θ. The ML estimate is a parameter θ such that the correspondingPDFbestmatchestheobserveddata.Inthisexamplethebestmatchhappenswhenthe estimated Gaussian PDF is centered at (x +x )/2≈−1.07. 1 2 When N = 10 and N = 100. We can continue the above calculation for N = 10 and N =100. In this case the MLE is (cid:40)(cid:18) 1 (cid:19)N (cid:26) (x −θ)2+···+(x −θ)2(cid:27)(cid:41) θ(cid:98)ML =argm θ ax log √ 2πσ2 exp − 1 2σ2 N =argmax −(cid:88)N (x n−θ)2 = 1 (cid:88)N x . 2σ2 N n θ n=1 n=1 where the optimization is solved by taking the derivative: N N d (cid:88) (cid:88) (x −θ)2 =−2 (x −θ) dθ n n n=1 n=1 Equating this with zero yields the solution θ = 1 (cid:80)N x . N n=1 n The result suggests that for an arbitrary number of training samples the ML estimate is the sample average. These cases are illustrated in Figure 8.8. As you can see, the red curves (the estimated PDF) are always trying to fit as many data points as possible. The above experiment tells us something about the ML estimation: How does ML estimation work, intuitively? • The likelihood function L(θ|x) measures how “likely” it is that we will get x if the underlying parameter is θ. • InthecaseofaGaussianwithanunknownmean,youmovearoundtheGaussian until you find a good fit. 477
CHAPTER 8. ESTIMATION 0.5 0.5 0.45 Data Point 0.45 Data Point 0.4 Candidate PDF 0.4 Candidate PDF 0.35 Estimated PDF 0.35 Estimated PDF 0.3 0.3 0.25 0.25 0.2 0.2 0.15 0.15 0.1 0.1 0.05 0.05 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (c) N =10 (d) N =100 Figure 8.8: When N = 10 and N = 100, the ML estimation continues to evaluate the different candidate PDFs. For a given set of data points, the ML estimation picks the best PDF to fit the data points. In this Gaussian example it was shown that the optimal parameter is θ(cid:98)ML = (1/N)(cid:80)N n=1x n, which is the sample average. 8.1.3 Application 1: Social network analysis MLestimationhasextremelybroadapplicability.Inthissubsectionandthenextwediscuss two real examples. We start with an example in social network analysis. In Chapter 3, when we discussed the Bernoulli random variables, we introduced the Erd˝os-R´enyigraph—oneofthesimplestmodelsforsocialnetworks.TheErd˝os-R´enyigraph isasingle-membershipnetworkthatassumesthatallusersbelongtothesamecluster.Thus theconnectivitybetweenusersisspecifiedbyasingleparameter,whichisalsotheprobability of the Bernoulli random variable. In our discussions in Chapter 3 we defined an adjacency matrix to represent a graph. Theadjacencymatrixisabinarymatrix,withthe(i,j)thentryindicatinganedgeconnect- ing nodes i and j. Since the presence and absence of an edge is binary and random, we may model each element of the adjacency matrix as a Bernoulli random variable X ∼Bernoulli(p). ij In other words, the edge X linking user i and user j in the network is either X =1 with ij ij probability p, or X =0 with probability 1−p. In terms of notation, we define the matrix ij X ∈RN×N as the adjacency matrix, with the (i,j)th element being X . ij Afewexamplesofasingle-membershipErd˝os-R´enyigraphareshowninFigure8.9.As the figure shows, the network connectivity increases as the Bernoulli parameter p increases. This happens because p defines the “density” of the edges. If p is large, we have a greater chance of getting X = 1, and so there is a higher probability that an edge is present ij between node i and node j. If p is small, the probability is lower. Supposethatwearegivenone snapshotofthenetwork,i.e.,onerealizationx∈RN×N of the adjacency matrix X ∈RN×N. The problem of recovering the latent parameter p can be formulated as an ML estimation. 478
8.1. MAXIMUM-LIKELIHOOD ESTIMATION p = 0.3 p = 0.5 p = 0.7 p = 0.9 4 --012 21 2 5 32 4 8 3 1 96 1 1 1 337 6 8 0 1 4 612 8 307 7 2 1 3 4 8 23 73 9 3 1 45 2 2 0 0 61 3 1 29 13 2 23 3 2 113 5 29 5 2 4 21 ---0123 321 114 33 54 3 256 2 6 1 1 9 2 9 34 03 3 2 4 1 3 93 7 20 3 1 4 28 3 6 1 2 2 1 5 1 88 1 2 1 7 23 79 2 1 3 1 262 5 8 0 10 3 3 72 ---0123 321 3 2 2 1 1 41 1 2 3 4 5 1 0 56 2 1 39 34 8 1 3 4 11 2 5 2 7 5 3 8 73 0 33 32 9 1 2 1 2 91 2 2 8 73 236 8 932 6 2 11 0 60 3 47 -02 2 7 32 220 6 2 6 3 4 15 9 1 2 5 2 2 1 3 3 6 6 1 7 1 91 345 3 1 1 3 23 1 1 2 2 338 9 74 38 9 11 3 18 38 4 2 31 0 0 2 5 4 02 72 -3 -4 -4 -4 -2 0 2 -2 0 2 -4 -2 0 2 4 -4 -2 0 2 4 (a) Graph representations of Erd˝os-R´enyi graphs at different p. (b) Adjacent matrices of the corresponding graphs. Figure 8.9: A single-membership Erdo˝s-R´enyi graph is a graph structure in which the edge between nodeiandnodej isdefinedasaBernoullirandomvariablewithparameterp.Aspincreases,thegraph has a higher probability of having more edges. The adjacent matrices shown in the bottom row are the mathematical representations of the graphs. Example8.5.Writedownthelog-likelihoodfunctionofthesingle-membershipErd˝os- R´enyi graph ML estimation problem. Solution. Based on the definition of the graph model, we know that X ∼Bernoulli(p). ij Therefore, the probability mass function of X is ij P[X =1]=p and P[X =0]=1−p. ij ij This can be compactly expressed as N N (cid:89)(cid:89) f (x; p)= pxij(1−p)1−xij. X i=1j=1 Hence, the log-likelihood is N N (cid:88)(cid:88) logL(p|x)= {x logp+(1−x )log(1−p)}. ij ij i=1j=1 Now that we have the log-likelihood function, we can proceed to estimate the param- eter p. The solution to this is the ML estimate. 479
CHAPTER 8. ESTIMATION Practice Exercise 8.3. Solve the ML estimation problem: p =argmax logL(p|x). (cid:98)ML p Solution. Using the log-likelihood we just derived, we have that N N (cid:88)(cid:88) p = {x logp+(1−x )log(1−p)}. (cid:98)ML ij ij i=1j=1 Taking the derivative and setting it to zero,   d d (cid:88)N (cid:88)N  logL(p|x)= {x logp+(1−x )log(1−p)} dp dp ij ij   i=1j=1 N N (cid:26) (cid:27) =(cid:88)(cid:88) x ij − 1−x ij =0. p 1−p i=1j=1 Let S =(cid:80)N (cid:80)N x . The equation above then becomes i=1 j=1 ij S N2−S − =0. p 1−p Rearranging the terms yields (1−p)S =p(N2−S), which gives us N N S 1 (cid:88)(cid:88) p = = x . (8.9) (cid:98)ML N2 N2 ij i=1j=1 Oncomputers,visualizingthegraphsandcomputingtheMLestimatesarereasonably straightforward. In MATLAB, you can call the command graph to build a graph from the adjacencymatrixA.Thiswillallowyoutoplotthegraph.Thecomputation,however,isdone directlybytheadjacencymatrix.Inthecodebelow,youcanseethatwecallrandtogenerate the Bernoulli random variables. The command triu extracts the upper triangular matrix from the matrix A. This ensures that we do not pick the diagonals. The symmetrization of A+A’ ensures that the graph is indirectional, meaning that i to j is the same as j to i. % MATLAB code to visualize a graph n = 40; # Number of nodes p = 0.3 # probability A = rand(n,n)<p; A = triu(A,1); A = A+A’; # Adj matrix G = graph(A); # Graph plot(G); # Drawing p_ML = mean(A(:)); # ML estimate 480
8.1. MAXIMUM-LIKELIHOOD ESTIMATION In Python, the computation is done similarly with the help of the networkx library. Thenumberofedgesmisdefinedasm=pn2.Thisisbecauseforagraphwithnnodes,there 2 are at most n2 unique pairs ofindirected edges. Multiplying this number by the probability 2 p will give us the number of edges m. # Python code to visualize a graph import networkx as nx import numpy as np n = 40 # Number of nodes p = 0.3 # probability m = np.round(((n ** 2)/2)*p) # Number of edges G = nx.gnm_random_graph(n,m) # Graph A = nx.adjacency_matrix(G) # Adj matrix nx.draw(G) # Drawing p_ML = np.mean(A) # ML estimate AsyoucanseeinboththeMATLABandthePythoncode,theMLestimatep isde- (cid:98)ML terminedbytakingthesampleaverage.ThustheMLestimate,accordingtoourcalculation, is p = 1 (cid:80)N (cid:80)N x . (cid:98)ML N2 i=1 j=1 ij 8.1.4 Application 2: Reconstructing images Beingabletoseeinthedarkistheholygrailofimaging.Manyadvancedsensingtechnologies havebeendevelopedoverthepastdecade.Inthisexample,weconsiderasingle-photonimage sensor. This is a counting device that counts the number of photons arriving at the sensor. Physicists have shown that a Poisson process can model the arrival of the photons. For simplicity we assume a homogeneous pattern of N pixels. The underlying intensity of the homogeneous pattern is a constant λ. Suppose that we have a sensor with N pixels X ,...,X . According to the Poisson 1 N statistics,theprobabilityofobservingapixelvalueisdeterminedbythePoissonprobability: X ∼Poisson(λ), n=1,...,N, n or more explicitly, λxn P[X =x ]= e−λ, n n x ! n where x is the nth observed pixel value, and is an integer. n A single-photon image sensor is slightly more complicated in the sense that it does not report X but instead reports a truncated version of X . Depending on the number of n n incoming photons, the sensor reports (cid:40) 1, X ≥1, Y = n (8.10) n 0, X =0. n We call this type of sensors a one-bit single-photon image sensor (see Figure 8.10). Our question is: If we are given the measurements X ,...,X , can we estimate the underlying 1 N parameter λ? 481
CHAPTER 8. ESTIMATION Figure 8.10: A one-bit single-photon image sensor captures an image with binary bits: It reports a “1” when the number of photons exceeds certain threshold, and “0” otherwise. The recovery problem here is to estimate the underlying image from the measurements. Example 8.6. Derive the log-likelihood function of the estimation problem for the single-photon image sensors. Solution.SinceY isabinaryrandomvariable,itsprobabilityiscompletelyspecified n by the two states it takes: P[Y =0]=P[X =0]=e−λ n n P[Y =1]=P[X (cid:54)=0]=1−e−λ. n n Thus, Y is a Bernoulli random variable with probability 1−e−λ of getting a value n of 1, and probability e−λ of getting a value of 0. By defining y as a binary number n taking values of either 0 or 1, it follows that the log-likelihood is (cid:26) N (cid:27) logL(λ|y)=log (cid:89)(cid:0) 1−e−λ(cid:1)yn(cid:0) e−λ(cid:1)1−yn n=1 N (cid:26) (cid:27) (cid:88) = y log(1−e−λ)−λ(1−y ) . n n n=1 Practice Exercise 8.4. Solve the ML estimation problem λ(cid:98)ML =argmax logL(λ|y). (8.11) λ Solution. First, we define S =(cid:80)N y . This simplifies the log-likelihood function to n=1 n N (cid:26) (cid:27) (cid:88) logL(λ|y)= y log(1−e−λ)−λ(1−y ) n n n=1 =Slog(1−e−λ)−λ(N −S). 482
8.1. MAXIMUM-LIKELIHOOD ESTIMATION The ML estimation is λ(cid:98)ML =argmax Slog(1−e−λ)−λ(N −S). λ Taking the derivative w.r.t. λ yields (cid:26) (cid:27) d S Slog(1−e−λ)−λ(N −S) = e−λ−(N −S). dλ 1−e−λ Moving around the terms, it follows that (cid:18) (cid:19) S S e−λ−(N −S)=0 =⇒ λ=−log 1− . 1−e−λ N Therefore, the ML estimate is (cid:32) N (cid:33) 1 (cid:88) λ(cid:98)ML =−log 1− N y n . (8.12) n=1 Forrealimages,youcanextrapolatetheideafromy toy ,whichdenotesthe(i,j)th n i,j,t pixel located at time t. Defining y ∈RN×N as the tth frame of the observed data, we can t use T frames to recover one image λ(cid:98)ML ∈RN×N. It follows from the above derivation that the ML estimate is (cid:32) T (cid:33) 1 (cid:88) λ(cid:98)ML =−log 1− T y t . (8.13) t=1 Figure 8.11 shows a pair of input-output images of a 256×256 image. (a) Observed data (1-frame) (b) ML estimate (using 100 frames) Figure 8.11: ML estimation for a single-photon image sensor problem. The observed data consists of 100 frames of binary measurements y ,...,y , where T = 100. The ML estimate is constructed by 1 T λ=−log(1− 1 (cid:80)T y ). T t=1 t On a computer the ML estimation can be done in a few lines of MATLAB code. The code in Python requires more work, as it needs to read images using the openCV library. 483
CHAPTER 8. ESTIMATION % MATLAB code to recover an image from binary measurements lambda = im2double(imread(’cameraman.tif’)); T = 100; % 100 frames x = poissrnd( repmat(lambda, [1,1,T]) ); % generate Poisson r.v. y = (x>=1); % binary truncation lambdahat = -log(1-mean(y,3)); % ML estimation figure(1); imshow(x(:,:,1)); figure(2); imshow(lambdahat); # Python code to recover an image from binary measurements import cv2 import numpy as np import scipy.stats as stats import matplotlib.pyplot as plt lambd = cv2.imread(’./cameraman.tif’) # read image lambd = cv2.cvtColor(lambd, cv2.COLOR_BGR2GRAY)/255 # gray scale T = 100 lambdT = np.repeat(lambd[:, :, np.newaxis], T, axis=2) # repeat image x = stats.poisson.rvs(lambdT) # Poisson statistics y = (x>=1).astype(float) # binary truncation lambdhat = -np.log(1-np.mean(y,axis=2)) # ML estimation plt.imshow(lambdhat,cmap=’gray’) 8.1.5 More examples of ML estimation By now you should be familiar with the procedure for solving the ML estimation problem. We summarize the two steps as follows. How to solve an ML estimation problem • Write down the likelihood L(θ|x). • Maximize the likelihood by solving θ(cid:98)ML =argmax logL(θ|x). θ Practice Exercise 8.5 (Gaussian). Suppose that we are given a set of i.i.d. Gaus- sian random variables X ,...,X , where both the mean µ and the variance σ2 are 1 N unknown. Let θ =[µ,σ2]T be the parameter. Find the ML estimate of θ. Solution. We first write down the likelihood. The likelihood of these i.i.d. Gaussian random variables is (cid:18) 1 (cid:19)N (cid:40) 1 (cid:88)N (cid:41) L(θ|x)= √ exp − (x −µ)2 . 2πσ2 2σ2 n n=1 484
8.1. MAXIMUM-LIKELIHOOD ESTIMATION To solve the ML estimation problem, we maximize the log-likelihood: def θ(cid:98)ML = argmax L(θ|x) θ (cid:40) N (cid:41) N 1 (cid:88) =argmax − log(2πσ2)− (x −µ)2 . 2 2σ2 n µ,σ2 n=1 Since we have two parameters, we need to take the derivatives for both. (cid:40) N (cid:41) d N 1 (cid:88) − log(2πσ2)− (x −µ)2 =0, dµ 2 2σ2 n n=1 (cid:40) N (cid:41) d N 1 (cid:88) − log(2πσ2)− (x −µ)2 =0. dσ2 2 2σ2 n n=1 (Note that the derivative of the second equation is taken w.r.t. to σ2 and not σ.) This pair of equations gives us N N 1 (cid:88) N 1 1 (cid:88) (x −µ)=0, and − · ·(2π)+ (x −µ)2 =0. σ2 n 2 2πσ2 2σ4 n n=1 n=1 Rearranging the equations, we find that N N 1 (cid:88) 1 (cid:88) µ = x and σ2 = (x −µ )2. (8.14) (cid:98)ML N n (cid:98)ML N n (cid:98)ML n=1 n=1 Practice Exercise 8.6. (Poisson) Given a set of i.i.d. Poisson random variables X ,...,X with an unknown parameter λ, find the ML estimate of λ. 1 N Solution. For a Poisson random variable, the likelihood function is (cid:89)N (cid:26) λxn (cid:27) L(λ|x)= e−λ . (8.15) x ! n n=1 To solve the ML estimation problem, we note that (cid:40) (cid:89)N λxn (cid:41) λ(cid:98)ML =argmax L(λ|x)=argmax log x !e−λ λ λ n n=1 =argmax log(cid:26) λ(cid:80) nxn e−Nλ(cid:27) . (cid:81) x ! λ n n (cid:81) Since x !isindependentofλ,itspresenceorabsencewillnotaffecttheoptimization n n 485
CHAPTER 8. ESTIMATION problem. Consequently we can drop the term. It follows that λ(cid:98)ML =argmax log(cid:110) λ(cid:80) nxne−Nλ(cid:111) λ (cid:32) (cid:33) (cid:88) =argmax x logλ−Nλ. n λ n Taking the derivative and setting it to zero yields (cid:40)(cid:32) (cid:33) (cid:41) (cid:80) d (cid:88) x logλ−Nλ = nx n −N =0. dλ n λ n Rearranging the terms yields N 1 (cid:88) λ(cid:98)ML = N x n. (8.16) n=1 The idea of ML estimation can also be extended to vector observations. Example 8.7. (High-dimensional Gaussian) Suppose that we are given a set of i.i.d. d-dimensional Gaussian random vectors X ,...,X such that 1 N X ∼Gaussian(µ,Σ). n We assume that Σ is fixed and known, but µ is unknown. Find the ML estimate of µ. Solution. The likelihood function is N (cid:89) L(µ|{x }N )= f (x ; µ) n n=1 Xn n n=1 N (cid:40) (cid:26) (cid:27)(cid:41) (cid:89) 1 1 = exp − (x −µ)TΣ−1(x −µ) (cid:112) (2π)d|Σ| 2 n n n=1 (cid:32) (cid:33)N (cid:40) N (cid:41) 1 1 (cid:88) = exp − (x −µ)TΣ−1(x −µ) . (cid:112) (2π)d|Σ| 2 n n n=1 Thus the log-likelihood function is N (cid:26) (cid:27) N N (cid:88) 1 logL(µ|{x }N )= log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) . n n=1 2 2 2 n n n=1 The ML estimate is found by maximizing this log-likelihood function: µ =argmax logL(µ|{x }N ). (cid:98)ML n n=1 µ 486
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Taking the gradient of the function and setting it to zero, we have that (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0. dµ 2 2 2 n n n=1 Thederivativesofthefirsttwotermsarezerobecausetheydonotdependonµ).Thus we have that: N (cid:26) (cid:27) (cid:88) Σ−1(x −µ) =0. n n=1 Rearranging the terms yields the ML estimate N 1 (cid:88) µ = x . (cid:98)ML N n n=1 Example 8.8. (High-dimensional Gaussian) Assume the same problem setting as in Example 8.7, except that this time we assume that both the mean vector µ and the covariance matrix Σ are unknown. Find the ML estimate for θ =(µ,Σ). Solution. The log-likelihood follows from Example 8.7: N N logL(µ|{x }N )= log|Σ|+ log(2π)d n n=1 2 2 N (cid:26) (cid:27) (cid:88) 1 + (x −µ)TΣ−1(x −µ) . 2 n n n=1 FindingtheMLestimaterequirestakingthederivativewithrespecttobothµandΣ: (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0, dµ 2 2 2 n n n=1 (cid:40) N (cid:26) (cid:27)(cid:41) d N N (cid:88) 1 log|Σ|+ log(2π)d+ (x −µ)TΣ−1(x −µ) =0. dΣ 2 2 2 n n n=1 After some tedious algebraic steps (see Duda et al., Pattern Classification, Problem 3.14), we have that N 1 (cid:88) µ = x , (8.17) (cid:98)ML N n n=1 N 1 (cid:88) Σ(cid:98)ML = N (x n−µ (cid:98)ML)(x n−µ (cid:98)ML)T. (8.18) n=1 487
CHAPTER 8. ESTIMATION 8.1.6 Regression versus ML estimation ML estimation is closely related to regression. To understand the connection, we consider a linear model that we studied in Chapter 7. This model describes the relationship between the inputs x ,...,x and the observed outputs y ,...,y , via the equation 1 N 1 N d−1 (cid:88) y = θ φ (x )+e , n=1,...,N. (8.19) n p p n n p=0 In this expression, φ (·) is a transformation that extracts the “features” of the input vector p x to produce a scalar. The coefficient θ defines the relative weight of the feature φ (x ) in p p n constructing the observed variable y . The error e defines the modeling error between the n n observation y and the prediction (cid:80)d−1θ φ (x ). We call this equation a linear model. n p=0 p p n Expressed in matrix form, the linear model is        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . ,  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X =θ =e or more compactly as y =Xθ+e. Rearranging the terms, it is easy to show that N N (cid:32) d−1 (cid:33)2 (cid:88) (cid:88) (cid:88) e2 = y − θ φ (x ) n n p p n n=1 n=1 p=0 N = (cid:88) (y −[Xθ] )2 =(cid:107)y−Xθ(cid:107)2. n n n=1 Nowwemakeanassumption:thateachnoisee isani.i.d.copyofaGaussianrandom n variable with zero mean and variance σ2. In other words, the error vector e is distributed according to e ∼ Gaussian(0,σ2I). This assumption is not always true because there are many situations in which the error is not Gaussian. However, this assumption is necessary for us to make the connection between ML estimation and regression. With this assumption, we ask, given the observations y ,...,y , what would be the 1 N ML estimate of the unknown parameter θ? We answer this question in two steps. Example 8.9. Find the likelihood function of θ, given y =[y ,...,y ]T. 1 N Solution. The PDF of y is given by a Gaussian: f (y; θ)= (cid:89)N (cid:26) √ 1 exp(cid:26) −(y n−[Xθ] n)2(cid:27)(cid:27) Y 2πσ2 2σ2 n=1 (cid:40) N (cid:41) 1 1 (cid:88) = exp − (y −[Xθ] )2 (cid:112) (2πσ2)N 2σ2 n n n=1 (cid:26) (cid:27) 1 1 = exp − (cid:107)y−Xθ(cid:107)2 . (8.20) (cid:112) (2πσ2)N 2σ2 488
8.1. MAXIMUM-LIKELIHOOD ESTIMATION Therefore, the log-likelihood function is (cid:40) (cid:26) (cid:27)(cid:41) 1 1 logL(θ|y)=log exp − (cid:107)y−Xθ(cid:107)2 (cid:112) (2πσ2)N 2σ2 N 1 =− log(2πσ2)− (cid:107)y−Xθ(cid:107)2. 2 2σ2 The next step is to solve the ML estimation by maximizing the log-likelihood. Example 8.10.SolvetheMLestimationproblemstatedinExample8.9.Assumethat XTX is invertible. Solution. θ(cid:98)ML =argmax logL(θ|y) θ (cid:26) (cid:27) N 1 =argmax − log(2πσ2)− (cid:107)y−Xθ(cid:107)2 . 2 2σ2 θ Taking the derivative w.r.t. θ yields (cid:26) (cid:27) d N 1 − log(2πσ2)− (cid:107)y−Xθ(cid:107)2 =0. dθ 2 2σ2 Since d θTAθ =A+AT, it follows from the chain rule that dθ (cid:26) (cid:27) (cid:26) (cid:27) d 1 d 1 − (cid:107)y−Xθ(cid:107)2 = − (y−Xθ)T(y−Xθ) dθ 2σ2 dθ 2σ2 1 = XT(Xθ−y). σ2 Substituting this result into the equation, 1 XT(Xθ−y)=0. σ2 Rearranging terms we obtain XTXθ =XTy, of which the solution is θ(cid:98)ML =(XTX)−1XTy. (8.21) Since the ML estimate in Equation (8.21) is the same as the regression solution (see Chapter7),weconcludethattheregressionproblemofalinearmodelisequivalenttosolving an ML estimation problem. ThemaindifferencebetweenalinearregressionproblemandanMLestimationproblem is the underlying statistical model, as illustrated in Figure 8.12. In linear regression, you do not care about the statistics of the noise term e . We choose (·)2 as the error because it n is differentiable and convenient. In ML estimation, we choose (·)2 as the error because the noise is Gaussian. If the noise is not Gaussian, e.g., the noise follows a Laplace distribution, we need to choose |·| as the error. Therefore, you can always get a result by solving the linearregression.However,thisresultwillonlybecomemeaningfulifyouprovideadditional 489
CHAPTER 8. ESTIMATION Figure 8.12: ML estimation is equivalent to a linear regression when the underlying statistical model forMLestimationisaGaussian.Specifically,iftheerrorterme=y−Xθ isanindependentGaussian vectorwithzeromeanandcovariancematrixσ2I,thentheresultingMLestimationisthesameaslinear regression. If the underlying statistical model is not Gaussian, then solving the regression is equivalent to applying a Gaussian ML estimation to a non-Gaussian problem. This will still give us a result, but that result will not maximize the likelihood, and thus it will not have any statistical guarantee. information about the problem. For example, if you know that the noise is Gaussian, then the regression solution is also the ML solution. This is a statistical guarantee. In practice, of course, we do not know whether the noise is Gaussian or not. At this point we have two courses of action: (i) Use your prior knowledge/domain expertise to determine whether a Gaussian assumption makes sense, or (ii) select an alternative model and see if the alternative model fits the data better. In practice, we should also question whether maximizing the likelihood is what we want. We may have some knowledge and therefore prefer the parameter θ, e.g., we want a sparse solution so that θ only contains a few non-zeros. In that case, maximizing the likelihood without any constraint may not be the solution we want. ML estimation versus regression • ML estimation requires a statistical assumption, whereas regression does not. • Suppose that you use a linear model y = (cid:80)d−1θ φ (x ) + e where e ∼ n p=0 p p n n n Gaussian(0,σ2), for n=1,...,N. • Then the likelihood function in the ML estimation is (cid:26) (cid:27) 1 1 L(θ|y)= exp − (cid:107)y−Xθ(cid:107)2 , (cid:112) (2πσ2)N 2σ2 • The ML estimate θ(cid:98)ML is θ(cid:98)ML = (XTX)−1XTy, which is exactly the same as theregressionsolution.Iftheabovestatisticalassumptionsdonothold,thenthe regression solution will not maximize the likelihood. 490
8.2. PROPERTIES OF ML ESTIMATES 8.2 Properties of ML Estimates MLestimationisaveryspecialtypeofestimation.NotallestimationsareML.Ifanestimate is ML, are there any theoretical properties we can analyze? For example, will ML estimates guarantee the recovery of the true parameter? If so, when will this happen? In this section we investigate these theoretical questions so that you will acquire a better understanding of the statistical nature of ML estimates.2 8.2.1 Estimators We know that an ML estimate is defined as θ(cid:98)ML(x)=argmax L(θ|x). (8.22) θ We write θ(cid:98)ML(x) to emphasize that θ(cid:98)ML is a function of x. The dependency of θ(cid:98)ML(x) on x should not be a surprise. For example, if the ML estimate is the sample average, we have that N 1 (cid:88) θ(cid:98)ML(x 1,...,x N)= N x n, n=1 where x=[x ,...,x ]T. 1 N However, in this setting we should always remember that x ,...,x are realizations 1 N of the i.i.d. random variables X ,...,X . Therefore, if we want to analzye the randomness 1 N ofthevariables,itismorereasonabletowriteθ(cid:98)ML asarandomvariableΘ(cid:98)ML.Forexample, in the case of sample average, we have that N 1 (cid:88) Θ(cid:98)ML(X 1,...,X N)= N X n. (8.23) n=1 We call Θ(cid:98)ML the ML estimator of the true parameter θ. Estimate versus estimator N 1 (cid:88) • An estimate is a number, e.g., θ(cid:98)ML = N x n. It is the random realization of n=1 a random variable. N 1 (cid:88) • An estimator is a random variable, e.g., Θ(cid:98)ML = N X n. It takes a set of n=1 random variables and generates another random variable. 2For notational simplicity, in this section we will focus on a scalar parameter θ instead of a vector parameterθ. 491
CHAPTER 8. ESTIMATION The ML estimators are one type of estimator, namely those that maximize the likeli- hoodfunctions.Ifwedonotwanttomaximizethelikelihoodwecanstilldefineanestimator. An estimator is any function that takes the data points X ,...,X and maps them to a 1 N number (or a vector of numbers). That is, an estimator is Θ(cid:98)(X 1,...,X N). We call Θ(cid:98) the estimator of the true parameter θ. Example 8.11. Let X ,...,X be Gaussian i.i.d. random variables with unknown 1 N mean θ and known variance σ2. Construct two possible estimators. Solution. We define two estimators: N 1 (cid:88) Θ(cid:98)1(X 1,...,X N)= N X n, n=1 Θ(cid:98)2(X 1,...,X N)=X 1, Inthefirstcase,theestimatortakesallthesamplesandconstructsthesampleaverage. The second estimator takes all the samples and returns on the first element. Both are legitimate estimators. However, Θ(cid:98)1 is the ML estimator, whereas Θ(cid:98)2 is not. 8.2.2 Unbiased estimators Whileyoucandefineestimatorsinanywayyoulike,certainestimatorsaregoodandothers arebad.By“good”wemeanthattheestimatorcanprovideyouwiththeinformationabout thetrueparameterθ;otherwise,whywouldyouevenconstructsuchanestimator?However, thedifficultyhereisthatΘ(cid:98) isarandom variablebecauseitisconstructedfromX 1,...,X N. Therefore, we need to define different metrics to quantify the usefulness of the estimators. Definition 8.5. An estimator Θ(cid:98) is unbiased if E[Θ(cid:98)]=θ. (8.24) Unbiasedness means that the average of the random variable Θ(cid:98) matches the true parameter θ. In other words, while we allow Θ(cid:98) to fluctuate, we expect the average to match the true θ. If this is not the case, using more measurements will not help us get closer to θ. Example 8.12. Let X ,...,X be i.i.d. Gaussian random variables with a unknown 1 N mean θ. It has been shown that the ML estimator is N 1 (cid:88) Θ(cid:98)ML = N X n. (8.25) n=1 Is the ML estimator Θ(cid:98)ML unbiased? 492
8.2. PROPERTIES OF ML ESTIMATES Solution: To check the unbiasedness, we look at the expectation: N N 1 (cid:88) 1 (cid:88) E[Θ(cid:98)ML]= N E[X n]= N θ =θ. n=1 n=1 Thus, Θ(cid:98)ML = N1 (cid:80)N n=1X n is an unbiased estimator of θ. Example 8.13. Same as the example before, but this time we consider an estimator Θ(cid:98) =X 1+X 2+5. (8.26) Is this estimator unbiased? Solution: In this case, E[Θ(cid:98)]=E[X 1+X 2+5]=E[X 1]+E[X 2]+5=2θ+5(cid:54)=θ. Therefore, the estimator is biased. Example 8.14. Let X ,...,X be i.i.d. Gaussian random variables with unknown 1 N mean µ and unknown variance σ2. We have shown that the ML estimators are N N 1 (cid:88) 1 (cid:88) µ = X and σ2 = (X −µ )2. (cid:98)ML N n (cid:98)ML N n (cid:98)ML n=1 n=1 It is easy to show that E[µ ]=µ. How about σ2 ? Is it an unbiased estimator? (cid:98)ML (cid:98)ML Solution: For simplicity we assume µ=0 so that E[X2]=E[(X −0)2]=σ2. n n Note that N (cid:26) (cid:27) 1 (cid:88) E[σ2 ]= E[X2]−2E[µ X ]+E[µ2 ] (cid:98)ML N n (cid:98)ML n (cid:98)ML n=1 1 (cid:88)N    1 (cid:88)N  (cid:32) 1 (cid:88)N (cid:33)2  = N σ2−2E  N X jX n+E  N X n  .   n=1 j=1 n=1 By independence, we observe that E[X X ]=E[X ]E[X ]=0, for any j (cid:54)=n. There- j n j n fore,   N (cid:20) (cid:21) 1 (cid:88) 1 E  N X jX n= NE X 1X n+···+X NX n j=1 1 σ2 = (0+···+σ2+···+0)= . N N 493
CHAPTER 8. ESTIMATION Similarly, we have that (cid:32) 1 (cid:88)N (cid:33)2 1 (cid:88)N   (cid:88)   E  N X n = N2 E[X n2]+ E[X jX n]   n=1 n=1 j(cid:54)=n 1 (cid:88)N (cid:110) (cid:111) σ2 = σ2+0 = . N2 N n=1 Combining everything, we arrive at the result: 1 (cid:88)N    1 (cid:88)N  (cid:32) 1 (cid:88)N (cid:33)2  E[σ (cid:98)M2 L]= N σ2−2E  N X jX n+E  N X n    n=1 j=1 n=1 1 (cid:88)N (cid:26) 2σ2 σ2(cid:27) = σ2− + N N N n=1 N −1 = σ2, N which is not equal to σ2. Therefore, σ2 is a biased estimator of σ2. (cid:98)ML In the previous example, it is possible to construct an unbiased estimator for the variance. To do so, we can use N 1 (cid:88) σ2 = (X −µ )2, (8.27) (cid:98)unbias N −1 n (cid:98)ML n=1 sothatE[σ2 ]=σ2.However,notethatσ2 doesnotmaximizethelikelihood,sowhile (cid:98)unbias (cid:98)unbias youcangetunbiasedness,youcannotmaximizethelikelihood.Ifyouwanttomaximizethe likelihood, you cannot get unbiasedness. What is an unbiased estimator? • An estimator Θ(cid:98) is unbiased if E[Θ(cid:98)]=θ. • Unbiased means that the statistical average of Θ(cid:98) is the true parameter θ. • If X n ∼ Gaussian(θ,σ2), then Θ(cid:98) = (1/N)(cid:80)N n=1X n is unbiased, but Θ(cid:98) = X 1 is biased. 8.2.3 Consistent estimators Bydefinition,anestimatorΘ(cid:98)(X 1,...,X N)isafunctionofN randomvariablesX 1,...,X N. Therefore,Θ(cid:98)(X 1,...,X N)changesasN grows.InthissubsectionweanalyzehowΘ(cid:98) behaves when N changes. For notational simplicity we use the following notation: Θ(cid:98)N =Θ(cid:98)(X 1,...,X N). (8.28) Thus, as N increases, we use more random variables in defining Θ(cid:98)(X 1,...,X N). 494
8.2. PROPERTIES OF ML ESTIMATES p Definition 8.6. An estimator Θ(cid:98)N is consistent if Θ(cid:98)N −→θ, i.e., (cid:20)(cid:12) (cid:12) (cid:21) lim P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) =0. (8.29) N→∞ The definition here follows from our discussions of the law of large numbers in Chapter 6. The specific type of convergence is known as the convergence in probability. It says that as N grows, the estimator Θ(cid:98) will be close enough to θ so that the probability of getting a large deviation will diminish, as illustrated in Figure 8.13. 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (a) N =1 (b) N =2 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 -5 -4 -3 -2 -1 0 1 2 3 4 5 (c) N =4 (d) N =8 Figure 8.13: The four subfigures here illustrate the probability of error P(cid:2) |Θ(cid:98)N −θ| ≥ (cid:15)(cid:3) , which is represented by the areas shaded in blue. We assume that the estimator Θ(cid:98)N is a Gaussian random variable following a distribution Gaussian(0,σ2), where we set σ = 1. The threshold we use in this N figure is (cid:15) = 1. As N grows, we see that the probability of error diminishes. If the probability of error goes to zero, we say that the estimator is consistent. The examples in Figure 8.13 are typical situations for an estimator based on the sample average. For example, if we assume that X ,...,X are i.i.d. Gaussian copies of 1 N Gaussian(0,σ2), then the estimator N 1 (cid:88) Θ(cid:98)(X 1,...,X N)= N X n n=1 willfollowaGaussiandistributionGaussian(0,σ2).(PleaserefertoChapter6forthederiva- N tion.) Then, as N grows, the PDF of Θ(cid:98)N becomes narrower and narrower. For a fixed (cid:15), it followsthattheprobabilityoferrorwilldiminishtozero.Infact,wecanprovethat,forthis 495
CHAPTER 8. ESTIMATION example, (cid:20)(cid:12) (cid:12) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) =P Θ(cid:98)N −θ ≥(cid:15) +P Θ(cid:98)N −θ ≤−(cid:15) (cid:90) ∞ (cid:18) (cid:12) σ2(cid:19) (cid:90) θ−(cid:15) (cid:18) (cid:12) σ2(cid:19) = Gaussian z(cid:12)θ, dz+ Gaussian z(cid:12)θ, dz N N θ+(cid:15) −∞ (cid:90) ∞ 1 −(z−θ)2 (cid:90) θ−(cid:15) 1 −(z−θ)2 = (cid:112) e 2σ2/N dz+ (cid:112) e 2σ2/N dz 2πσ2/N 2πσ2/N θ+(cid:15) −∞ =(cid:90) ∞ √1 e−z 22 dz+(cid:90) − σ/√(cid:15) N √1 e−z 22 dz √(cid:15) 2π −∞ 2π σ/ N (cid:18) (cid:19) (cid:18) (cid:19) (cid:15) −(cid:15) =1−Φ √ +Φ √ σ/ N σ/ N (cid:18) (cid:19) −(cid:15) =2Φ √ . σ/ N Therefore, as N →∞, it holds that −√(cid:15) →−∞. Hence, σ/ N (cid:20)(cid:12) (cid:12) (cid:21) (cid:18) −(cid:15) (cid:19) lim P (cid:12) (cid:12)Θ(cid:98)N −θ(cid:12) (cid:12)≥(cid:15) = lim 2Φ √ =0. N→∞ N→∞ σ/ N This explains why in Figure 8.13 the probability of error diminishes to zero as N grows. Therefore, we say that Θ(cid:98)N is consistent. In general, there are two ways to check whether an estimator is consistent: • Prove convergence in probability. This is based on the definition of a consistent estimator. If we can prove that lim P(cid:2) |Θ(cid:98)N −θ|≥(cid:15)(cid:3) =0, (8.30) N→∞ then we say that the estimator is consistent. • Prove convergence in mean squared error: lim E[(Θ(cid:98)N −θ)2]=0. (8.31) N→∞ Toseewhyconvergenceinthemeansquarederrorissufficienttoguaranteeconsistency, we recall Chebyshev’s inequality in Chapter 6, which says that P(cid:2) |Θ(cid:98)N −θ|≥(cid:15)(cid:3) ≤ E[(Θ(cid:98)N (cid:15)2−θ)2] . Thus, if lim N→∞E[(Θ(cid:98)N −θ)2] = 0, convergence in probability will also hold. How- ever,sincemeansquareconvergenceisstrongerthanconvergenceinprobability,being unable to show mean square convergence does not imply that an estimator is incon- sistent. Be careful not to confuse a consistent estimator and an unbiased estimator. The two are different concepts; one does not imply the other. 496
8.2. PROPERTIES OF ML ESTIMATES Consistent versus unbiased • Consistent = If you have enough samples, then the estimator Θ(cid:98) will converge to the true parameter. • Unbiasedness does not imply consistency. For example (Gaussian), if Θ(cid:98) =X 1, then E[X 1] = µ. But P[|Θ(cid:98) −µ| > (cid:15)] does not converge to 0 as N grows. So this estimator is inconsistent. (See Example 8.16 below.) • Consistency does not imply unbiasedness. For example (Gaussian), if N 1 (cid:88) Θ(cid:98) = N (X n−µ)2 n=1 is a biased estimate for variance, but it is consistent. (See Example 8.17 below.) Example 8.15.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and known variance σ2. We know that the ML estimator for the mean is µ =(1/N)(cid:80)N X . Is µ consistent? (cid:98)ML n=1 n (cid:98)ML Solution. We have shown that the ML estimator is N 1 (cid:88) µ = X . (cid:98)ML N n n=1 Since E[µ ]=µ, and E[(µ −µ)2]=Var[µ ]= σ2, it follows that (cid:98)ML (cid:98)ML (cid:98)ML N P(cid:2) |µ −µ|≥(cid:15)(cid:3) ≤ E[(µ (cid:98)ML−µ)2] = σ2 . (cid:98)ML (cid:15)2 N(cid:15)2 Thus, when N goes to infinity, the probability converges to zero, and hence the esti- mator is consistent. Example 8.16.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and known variance σ2. Define an estimator µ=X . Show that the estimator (cid:98) 1 is unbiased but inconsistent. Solution. We know that E[µ] = E[X ] = µ. So µ is an unbiased estimator. However, (cid:98) 1 (cid:98) we can show that E[(µ−µ)2]=E[(X −µ)2]=σ2. (cid:98) 1 SincethisvarianceE[(µ−µ)2]doesnotshrinkasN increases,itfollowsthatnomatter (cid:98) 497
CHAPTER 8. ESTIMATION how many samples we use we cannot make E[(µ−µ)2] go to zero. To be more precise, (cid:98) (cid:20) (cid:21) (cid:20) (cid:21) P |µ−µ|≥(cid:15) =P |X −µ|≥(cid:15) (cid:98) 1 (cid:20) (cid:21) (cid:20) (cid:21) =P X ≤µ−(cid:15) +P X ≥µ+(cid:15) 1 1 =(cid:90) µ−(cid:15) √ 1 e−(x 2− σµ 2)2 dx+(cid:90) ∞ √ 1 e−(x 2− σµ 2)2 dx 2πσ2 2πσ2 −∞ µ+(cid:15) (cid:18) (cid:19) −(cid:15) =2Φ , σ which does not converge to zero as N →∞. So the estimator is inconsistent. Example 8.17.LetX ,...,X bei.i.d.Gaussianrandomvariableswithanunknown 1 N mean µ and an unknown variance σ2. Is the ML estimate of the variance, i.e., σ2 , (cid:98)ML consistent? Solution. We know that the ML estimator for the mean is N 1 (cid:88) µ = X , (cid:98)ML N n n=1 and we have shown that it is an unbiased and consistent estimator of the mean. For the variance, N N σ2 = 1 (cid:88) (X −µ )2 = 1 (cid:88)(cid:2) X2−2µ X +µ2 (cid:3) (cid:98)ML N n (cid:98)ML N n (cid:98)ML n (cid:98)ML n=1 n=1 N N 1 (cid:88) 1 (cid:88) = X2−2µ · X +µ2 N n (cid:98)ML N n (cid:98)ML n=1 n=1 N 1 (cid:88) = X2−µ2 . N n (cid:98)ML n=1 Note that 1 (cid:80)N X2 is the sample average of the second moment, and so by the N n=1 n weak law of large numbers it should converge in probability to E[X2]. Similarly, µ n (cid:98)ML will converge in probability to µ. Therefore, we have N σ2 = 1 (cid:88) X2−µ2 −p →(σ2+µ2)−µ2 =σ2. (cid:98)ML N n (cid:98)ML n=1 Thus, we have shown that the ML estimator of the variance is biased but consistent. 498
8.2. PROPERTIES OF ML ESTIMATES The following discussions about the consistency of ML estimators can be skipped. As we have said, there are many estimators. Some estimators are consistent and some arenot.TheMLestimatorsarespecial.Itturnsoutthatundercertainregularityconditions the ML estimators of i.i.d. observations are consistent. Without proving this result formally, we highlight a few steps to illustrate the idea. Suppose that we have a set of i.i.d. data points x ,...,x drawn from some distribution 1 N f(x,|θ ). To formulate the ML estimation, we consider the log-likelihood function (di- true vided by N): N 1 1 (cid:88) logL(θ|x)= logf(x ; θ). (8.32) N N n n=1 Here, the variable θ is unknown. We need to find it by maximizing the log-likelihood. By the weak law of large numbers, we can show that the log-likelihood based on the N samples will converge in probability to N 1 (cid:88) logf(x ; θ)−p →E[logf(x; θ)]. (8.33) N n n=1 (cid:124) (cid:123)(cid:122) (cid:125) gN(θ) The expectation can be evaluated by integrating over the true distribution: (cid:90) E[logf(x; θ)]= logf(x; θ)·f(x; θ )dx. true (cid:124) (cid:123)(cid:122) (cid:125) g(θ) where f(x; θ ) denotes the true distribution of the samples x ’s. From these two results true n we define two functions: def 1 (cid:88)N def(cid:90) g (θ) = logf(x ; θ), and g(θ) = logf(x; θ)·f(x; θ )dx, N N n true n=1 p and we know that g (θ)−→g(θ). N We also know that θ(cid:98)ML is the ML estimator, and so θ(cid:98)ML =argmax g N(θ). θ Let θ∗ be the maximizer of the limiting function, i.e., θ∗ =argmax g(θ). θ Because g N(θ) →p g(θ), we can (loosely3) argue that θ(cid:98)ML →p θ∗. If we can show that θ∗ =θ true, then we have shown that θ(cid:98)ML →p θ true, implying that θ(cid:98)ML is consistent. 3To rigorously prove this statement we need some kind of regularity conditions on gN and g. A more formalproofcanbefoundinH.VincentPoor,AnIntroductionSignalDetectionandEstimation,Springer, 1998,SectionIV.D. 499
CHAPTER 8. ESTIMATION To show that θ∗ =θ , we note that true d (cid:90) (cid:90) d logf(x; θ)·f(x; θ )dx= logf(x; θ)·f(x; θ )dx dθ true dθ true (cid:90) f(cid:48)(x; θ) = ·f(x; θ )dx. f(x; θ) true We ask whether this is equal to zero. Putting θ =θ , we have that true (cid:90) f(cid:48)(x; θ ) (cid:90) true ·f(x; θ )dx= f(cid:48)(x; θ )dx. f(x; θ ) true true true However, this integral can be simplified to (cid:90) f(cid:48)(x; θ true)dx= dd θ(cid:90) f(x; θ)dx(cid:12) (cid:12) (cid:12) (cid:12) =0. (cid:124) (cid:123)(cid:122) (cid:125) θ=θtrue =1 Therefore, θ is the maximizer for g(θ), and so θ =θ∗. true true End of the discussion. Please join us again. 8.2.4 Invariance principle Another useful property satisfied by the ML estimate is the invariance principle. The in- variance principle says that a monotonic transformation of the true parameter is preserved for the ML estimates. What is the invariance principle? • There is a monotonic function h. • There is an ML estimate θ(cid:98)ML for θ. • The monotonic function h maps the true parameter θ (cid:55)−→h(θ). • Then the same function will map the ML estimate θ(cid:98)ML (cid:55)−→h(θ(cid:98)ML). The formal statement of the invariance principle is given by the theorem below. Theorem 8.1. If θ(cid:98)ML is the ML estimate of θ, then for any one-to-one function h of θ, the ML estimate of h(θ) is h(θ(cid:98)ML). Proof. Define the likelihood function L(θ) (we have dropped x to simplify the notation). Then, for any monotonic function h, we have that L(θ)=L(h−1(h(θ))). 500
8.2. PROPERTIES OF ML ESTIMATES Let θ(cid:98)ML be the ML estimate: θ(cid:98)ML =argmax L(θ)=argmax L(h−1(h(θ))). θ θ By the definition of ML, θ(cid:98)ML must maximize the likelihood. Therefore, L(h−1(h(θ))) is maximizedwhenh−1(h(θ))=θ(cid:98)ML.Thisimpliesthath(θ)=h(θ(cid:98)ML)becausehismonotonic. Since h(θ) is the parameter we try to estimate, the equality h(θ) = h(θ(cid:98)ML) implies that h(θ(cid:98)ML) is the ML estimate of h(θ). (cid:3) Example 8.18. Consider the single-photon image sensor example we discussed in Section 8.1. We consider a set of i.i.d. Bernoulli random variables with PMF p (1)=1−e−η and p (0)=e−η. (8.34) Xn Xn Find the ML estimate through (a) direct calculation and (b) the invariance principle. Solution. (a) Following the example in Equation (8.12), the ML estimate of η is N (cid:89) η =argmax (1−e−η)xn(e−η)1−xn (cid:98)ML η n=1 (cid:32) N (cid:33) 1 (cid:88) =−log 1− x . N n n=1 (b) We can obtain the same result using the invariance principle. Since X is a n binary random variable, we assume that it is a Bernoulli with parameter θ. Then the ML estimate of θ is N (cid:89) θ(cid:98)ML =argmax θxn(1−θ)1−xn θ n=1 N 1 (cid:88) = x . N n n=1 The relationship between θ and η is that θ = 1−e−η, or η = −log(1−θ). So we let h(θ)=−log(1−θ). The invariance principle says that the ML estimate of h(θ) is def η = h(cid:100)(θ) (cid:98)ML ML (i) = h(θ(cid:98)ML) (cid:32) N (cid:33) 1 (cid:88) =−log 1− x , N n n=1 where (i) follows from the invariance principle. The invariance principle can be very convenient, especially when the transformation h is complicated, so that a direct evaluation of the ML estimate is difficult. 501
CHAPTER 8. ESTIMATION Figure 8.14: The invariance principle is a transformation of the ML estimate. In this example, we consideraBernoullilog-likelihoodfunctionshowninthelowermostplot.Forthislog-likelihood,theML estimate is θ(cid:98)ML = 0.4. On the left-hand side we show another log-likelihood, derived for a truncated Poisson random variable. Note that the ML estimate is η =0.5108. The invariance principle asserts (cid:98)ML that, instead of computing these ML estimates directly, we can first derive the relationship between η and θ for any θ. Since we know that θ = 1−e−η, it follows that η = −log(1−θ). We define this transformationasη=h(θ)=−log(1−θ).ThentheMLestimateisη (cid:98)ML =h(θ(cid:98)ML)=h(0.4)=0.5108. The invariance principle saves us the trouble of computing the maximization of the more truncated Poisson likelihood. The invariance principle is portrayed in Figure 8.14. We start with the Bernoulli log- likelihood logL(θ|S)=Slogθ+(1−S)log(1−θ). In this particular example we let S =20, where S denotes the sum of the N =50 Bernoulli random variables. The other log-likelihood is the truncated Poisson, which is given by logL(η|S)=Slog(1−e−η)+(1−S)log(e−η). The transformation between the two is the function η =h(θ)=−log(1−θ). Putting everything into the figure, we see that the ML estimate (θ = 0.4) is translated to η = 0.5108. The invariance principle asserts that this calculation can be done by η = (cid:98)ML h(θ(cid:98)ML)=h(0.4)=−0.5108. 502
8.3. MAXIMUM A POSTERIORI ESTIMATION 8.3 Maximum A Posteriori Estimation InMLestimation,theparameterθistreatedasadeterministicquantity.Thereare,however, many situations where we have some prior knowledge about θ. For example, we may not know exactly the speed of a car, but we may know that the speed is roughly 65 mph with a standard deviation of 5 mph. How do we incorporate such prior knowledge into the estimation problem? Inthissection,weintroducethesecondestimationtechnique,knownasthemaximum a posteriori(MAP)estimation.MAPestimationlinksthelikelihoodandtheprior.Thekey idea is to treat the parameter θ as a random variable (vector) Θ with a PDF f (θ). Θ 8.3.1 The trio of likelihood, prior, and posterior To understand how the MAP estimation works, it is important first to understand the role of the parameter θ, which changes from a deterministic quantity to a random quantity. Recall the likelihood function we defined in the ML estimation; it is L(θ|x)=f (x; θ), X ifweassumethatwehaveasetofi.i.d.observationsx=[x ,...,x ]T.BywritingthePDF 1 N of X as f (x; θ), we emphasize that θ is a deterministic but unknown parameter. There X is nothing random about θ. In MAP, we change the nature of θ from deterministic to random. We replace θ by Θ and write becomes f (x; θ) =⇒ f (x|θ). (8.35) X X|Θ The difference between the left-hand side and the right-hand side is subtle but important. On the left-hand side, f (x; θ) is the PDF of X. This PDF is parameterized by θ. On the X right-hand side, f (x|θ) is a conditional PDF of X given Θ. The values they provide X|Θ are exactly the same. However, in f (x|θ), θ is a realization of a random variable Θ. X|Θ Because Θ is now a random variable (vector), we can define its PDF (yes, the PDF of Θ), and denote it by f (θ), (8.36) Θ which is called the prior distribution. The prior distribution of Θ is unique in MAP estima- tion. There is nothing called a prior in ML estimation. Multiplying f (x|θ) with the prior PDF f (θ), and using Bayes’ Theorem, we X|Θ Θ obtain the posterior distribution: f (x|θ)f (θ) X|Θ Θ f (θ|x)= . (8.37) Θ|X f (x) X The posterior distribution is the PDF of Θ given the measurements X. Thelikelihood,theprior,andtheposteriorcanbeconfusing.Letusclarifytheirmean- ings. 503
CHAPTER 8. ESTIMATION • Likelihood f (x|θ): This is the conditional probability density of X given the pa- X|Θ rameter Θ. Do not confuse the likelihood f (x|θ) defined in the MAP context X|Θ and the likelihood f (x;|θ) defined in the ML context. The former assumes that Θ X is random whereas the latter assumes that θ is deterministic. They have the same values. • Prior f (θ): This is the prior distribution of Θ. It does not come from the data X Θ but from our prior knowledge. For example, if we see a bike on the road, even before we take any measurement we will have a rough idea of its speed. This is the prior distribution. • Posteriorf (θ|x):ThisistheposteriordensityofΘgiventhatwehaveobservedX. Θ|X Donotconfusef (θ|x)andL(θ|x).Theposteriordistributionf (θ|x)isaPDF Θ|X Θ|X of Θ given X = x. The likelihood L(θ|x) is not a PDF. If you integrate f (θ|x) Θ|X with respect to θ, you get 1, but if you integrate L(θ|x) with respect to θ, you do not get 1. What is the difference between ML and MAP? Likelihood ML f (x; θ) The parameter θ is deterministic. X MAP f (x|θ) The parameter Θ is random. X|Θ Prior ML There is no prior, because θ is deterministic. MAP f (θ) This is the PDF of Θ. Θ Optimization ML Find the peak of the likelihood f (x; θ). X MAP Find the peak of the posterior f (θ |x). Θ|X Maximum a posteriori (MAP) estimation is a form of Bayesian estimation. Bayesian methods emphasize our prior knowledge or beliefs about the parameters. As we will see shortly, the prior has something valuable to offer, especially when we have very few data points. 8.3.2 Understanding the priors Since the biggest difference between MAP and ML is the addition of the prior f (θ), we Θ need to take a closer look at what they mean. In Figure 8.15 below, we show a set of six different priors. We ask two questions: (1) What do they mean? (2) Which one should we use? What does the shape of a prior tell us? It tells us your belief as to how the underlying parameter Θ should be distributed. 504
8.3. MAXIMUM A POSTERIORI ESTIMATION Figure8.15:Thisfigureillustratessixdifferentexamplesofthepriordistributionf (θ),whentheprior Θ is a 1D parameter θ. The prior distribution f (θ) is the PDF of Θ. (a) f (θ)=δ(θ), which is a delta Θ Θ function. (b) f (θ) = 1 for a ≤ θ ≤ b. This is a uniform distribution. (c) This is also a uniform Θ b−a distribution, but the spread is very wide. (d) f (θ)=Gaussian(0,σ2), which is a zero-mean Gaussian. Θ (e)ThesameGaussian,butwithadifferentmean.(f)AGaussianwithzeromean,butalargevariance. The meaning of this statement can be best understood from the examples shown in Fig- ure 8.15: • Figure8.15(a).Thisisadeltapriorf (θ)=δ(θ)(orf (θ)=δ(θ−θ )).Ifyouusethis Θ Θ 0 prior, you are absolutely sure that the parameter Θ takes a specific value. There is no uncertainty about your belief. Since you are so confident about your prior knowledge, you will ignore the likelihood that is constructed from the data. No one will use a delta prior in practice. • Figure 8.15(b). f (θ) = 1 for a ≤ θ ≤ b, and is zero otherwise. This is a bounded Θ b−a uniform prior. You do not have any preference for the parameter Θ, but you do know from your prior experience that a≤Θ≤b. • Figure 8.15(c). This prior is the same as (b) but is short and very wide. If you use this prior, it means that you know nothing about the parameter. So you give up the prior and let the likelihood dominate the MAP estimate. • Figure8.15(d).f (θ)=Gaussian(0,σ2).Youusethispriorwhenyouknowsomething Θ about the parameter, e.g., that it is centered at certain location and you have some uncertainty. • Figure 8.15(e). Same as (d), but the parameter is centered at some other location. • Figure 8.15(f). Same as (d), but you have less confidence about the parameter. As you can see from these examples, the shape of the prior tells us how you want Θ to be distributed. The choice you make will directly influence the MAP optimization, and hence the MAP estimate. Since the prior is a subjective quantity in the MAP framework, you as the user have the freedom to choose whatever you like. For instance, if you have conducted a similar experiment before, you can use the results of the previous experiments as the current prior. Another strategy is to go with physics. For instance, we can argue that θ should be sparse so that it contains as few non-zeros as possible. In this case, a sparsity-driven prior, such as f (θ) = exp{−(cid:107)θ(cid:107) }, could be a choice. The third strategy is to choose a prior that is Θ 1 computationally “friendlier”, e.g., in quadratic form so that the MAP is differentiable. One such choice is the conjugate prior. We will discuss this later in Section 8.3.6. 505
CHAPTER 8. ESTIMATION Which prior should we choose? • Basedonyourpreference,e.g.,youknowfromhistoricaldatathattheparameter should behave in certain ways. • Based on physics, e.g., the parameter has a physical interpretation, so you need to abide by the physical laws. • Choose a prior that is computationally “friendlier”. This is the topic of the conjugate prior, which is a prior that does not change the form of the posterior distribution. (We will discuss this later in Section 8.3.6.) 8.3.3 MAP formulation and solution Our next task is to study how to formulate the MAP problem and how to solve it. Definition 8.7. Let X = [X ,...,X ]T be i.i.d. observations. Let Θ be a random 1 N parameter. The maximum-a-posteriori estimate of Θ is θ(cid:98)MAP =argmax f Θ|X(θ|x). (8.38) θ Philosophicallyspeaking,MLandMAPhavetwodifferentgoals.MLconsidersapara- metricmodelwithadeterministicparameter.Itsgoalistofindtheparameterthatmaximizes the likelihood for the data we have observed. MAP also considers a parametric model but the parameter Θ is random. Because Θ is random, we are finding one particular state θ of the parameter Θ that offers the best explanation conditioned on the data X we observe. In a sense, the two optimization problems are θ(cid:98)ML =argmax f X|Θ(x|θ), θ θ(cid:98)MAP =argmax f Θ|X(θ|x). θ This pair of equations is interesting, as the pair tells us that the difference between the ML estimation and the MAP estimation is the flipped order of X and Θ. There are two reasons we care about the posterior. First, in MAP the posterior allows us to incorporate the prior. ML does not allow a prior. A prior can be useful when the number of samples is small. Second, maximizing the posterior does have some physical interpretations. MAP asks for the probability of Θ=θ after observing N training samples X = x. ML asks for the probability of observing X = x given a parameter θ. Both are correct and legitimate criteria, but sometimes we might prefer one over the other. To solve the MAP problem, we notice that θ(cid:98)MAP =argmax f Θ|X(θ|x) θ f (x|θ)f (θ) X|Θ Θ =argmax f (x) θ X =argmax f (x|θ)f (θ), f (x) does not contain θ X|Θ Θ X θ =argmax logf (x|θ)+logf (θ). X|Θ Θ θ 506
8.3. MAXIMUM A POSTERIORI ESTIMATION Therefore, what MAP adds is the prior logf (θ). If you use an uninformative prior, e.g., a Θ prior with extremely wide support, then the MAP estimation will return more or less the same result as the ML estimation. When does MAP = ML? • The relation “=” does not make sense here, because θ is random in MAP but deterministic in ML. • Solution of MAP optimization = solution of ML optimization, when f (θ) is Θ uniform over the parameter space. • In this case, f (θ)=constant and so it can be dropped from the optimization. Θ Example 8.19. Let X ,...,X be i.i.d. random variables with a PDF f (x |θ) 1 N Xn|Θ n for all n, and Θ be a random parameter with PDF f (θ): Θ 1 (cid:26) (x −θ)2(cid:27) f (x |θ)= √ exp − n , Xn|Θ n 2πσ2 2σ2 1 (cid:26) (θ−µ )2(cid:27) f (θ)= exp − 0 . Θ (cid:112) 2πσ2 2σ2 0 0 Find the MAP estimate. Solution. The MAP estimate is θ(cid:98)MAP =argm θ ax (cid:34) n(cid:89)N =1√ 21 πσ2 exp(cid:26) −(x n 2− σ2θ)2(cid:27)(cid:35) ×(cid:34) (cid:112) 21 πσ 02 exp(cid:26) −(θ− 2σµ 020)2(cid:27)(cid:35) =argmax (cid:18) √ 1 (cid:19)N × 1 exp(cid:40) −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:41) . θ 2πσ2 (cid:112) 2πσ 02 n=1 2σ2 2σ 02 Since the maximizer is not changed by any monotonic function, we apply logarithm to the above equations. This yields (cid:26) θ(cid:98)MAP =argmax − N 2 log(cid:0) 2πσ2(cid:1) − 1 2log(2πσ 02) θ −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:27) . 2σ2 2σ2 n=1 0 Constants in the maximization do not matter. So by dropping the constant terms we obtain θ(cid:98)MAP =argmax (cid:40) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2(cid:41) . (8.39) θ n=1 0 It now remains to solve the maximization. To this end we take the derivative w.r.t. θ 507
CHAPTER 8. ESTIMATION and show that d (cid:40) −(cid:88)N (x n−θ)2 − (θ−µ 0)2(cid:41) =0. dθ 2σ2 2σ2 n=1 0 This yields N (cid:88) (x n−θ) − θ−µ 0 =0. σ2 σ2 n=1 0 Rearranging the terms gives us the final result: (cid:16) (cid:17) σ2 1 (cid:80)N x + σ2µ 0 N n=1 n N 0 θ(cid:98)MAP = σ2+ σ2 . (8.40) 0 N PracticeExercise8.7.Provethatiff Θ(θ)=δ(θ−θ 0),theMAPestimateisθ(cid:98)MAP = θ . 0 Solution. If f (θ)=δ(θ−θ ), then Θ 0 θ(cid:98)MAP =argmax logf X|Θ(x|θ)+logf Θ(θ) θ =argmax logf (x|θ)+logδ(θ−θ ) X|Θ 0 θ  argmax logf (x|θ)−∞, θ (cid:54)=θ .  X|Θ 0 = θ argmax logf X|Θ(x|θ)+0, θ =θ 0. θ Thus, if θ(cid:98)MAP (cid:54)= θ 0, the first case says that there is no solution, so we must go with the second case θ(cid:98)MAP = θ 0. But if θ(cid:98)MAP = θ 0, there is no optimization because we have already chosen θ(cid:98)MAP =θ 0. This proves the result. 8.3.4 Analyzing the MAP solution As we said earlier, MAP offers something that ML does not. To see this, we will use the result of the Gaussian random variables as an example and analyze the MAP solution as we change the parameters N and σ . Recall that if X ,...,X are i.i.d. Gaussian random 0 1 N variables with unknown mean θ and known variance σ, the ML estimate is N 1 (cid:88) θ(cid:98)ML = N x n. n=1 508
8.3. MAXIMUM A POSTERIORI ESTIMATION Assuming that the parameter Θ is distributed according to a PDF Gaussian(µ ,σ2), we 0 0 have shown in the previous subsection that (cid:16) (cid:17) θ(cid:98)MAP = σ 02 N1 (cid:80) σN n 2= +1x σn 2 + σ N2µ 0 = σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 . 0 N 0 N In what follows, we will take a look at the behavior of the MAP estimate θ(cid:98)MAP as N and σ change. The results of our discussion are summarized in Figure 8.16. 0 (a) Effect of N (b) Effect of σ 0 Figure8.16:TheMAPestimateθ(cid:98)MAP swingsbetweentheMLestimateθ(cid:98)ML andthepriorµ 0.(a)When N increases, the likelihood is more reliable and so we lean towards the ML estimate. If N is small, we should trust the prior more than the ML estimate. (b) When σ decreases, we become more confident 0 about the prior and so we will use it. If σ is large, we use more information from the ML estimate. 0 First, let’s look at the effect of N. How does N change θ(cid:98)MAP? • As N → ∞, the MAP estimate θ(cid:98)MAP → θ(cid:98)ML: If we have enough samples, we trust the data. • As N → 0, the MAP estimate θ(cid:98)MAP → θ 0. If we do not have any samples, we trust the prior. Thesetworesultscanbedemonstratedbytakingthelimits.AsN →∞,theMAPestimate converges to Nl →im ∞θ(cid:98)MAP = Nl →im ∞σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 =θ(cid:98)ML. (8.41) 0 N This result is not surprising. When we have infinitely many samples, we will completely rely on the data and make our estimate. Thus, the MAP estimate is the same as the ML estimate. When N →0, the MAP estimate converges to Nli →m 0θ(cid:98)MAP = Nli →m 0σ 02θ(cid:98) σM 2L ++ σσ N 22µ 0 =µ 0. (8.42) 0 N Thismeansthat,whenwedonothaveanysamples,theMAPestimateθ(cid:98)MAP willcompletely use the prior distribution, which has a mean µ . 0 509
CHAPTER 8. ESTIMATION The implication of this result is that MAP offers a natural swing between θ(cid:98)ML and θ(cid:98)0, controlled by N. Where does this N come from? If we recall the derivation of the result, we note that the N affects the likelihood term through the number of samples: θ(cid:98)MAP =argmax (cid:26) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2(cid:27) . θ n=1 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) 1term N termshere Thus, as N increases, the influence of the data term grows, and so the result will gradually shift towards θ(cid:98)ML. Figure 8.17 illustrates a numerical experiment in which we draw N random samples x ,...,x according to a Gaussian distribution Gaussian(θ,σ2), with σ = 1. We assume 1 N thatthepriordistributionisGaussian(µ ,σ2),withµ =0andσ =0.25.TheMLestimate 0 0 0 0 ofthisproblemisθ(cid:98)ML = N1 (cid:80)N n=1x n,whereastheMAPestimateisgivenbyEquation(8.40). The figure shows the resulting PDFs. A helpful analogy is that the prior and the likelihood arepullingaropeintwooppositedirections.AsN grows,theforceofthelikelihoodincreases and so the influence becomes stronger. (a) N =1 (b) N =50 Figure 8.17: The subfigures show the prior distribution f (θ) and the likelihood function f (x|θ), Θ X|Θ given the observed data. (a) When N = 1, the estimated posterior distribution f (θ|x) is pulled Θ|X towards the prior. (b) When N =50, the posterior is pulled towards the ML estimate. The analogy for the situation is that each data point is acting as a small force against the big force of the prior. As N grows, the small forces of the data points accumulate and eventually dominate. We next look at the effect of σ . 0 How does σ 0 change θ(cid:98)MAP? • Asσ 0 →∞,theMAPestimateθ(cid:98)MAP →θ(cid:98)ML:Ifwehavedoubtsabouttheprior, we trust the data. • As σ 0 → 0, the MAP estimate θ(cid:98)MAP → θ 0. If we are absolutely sure about the prior, we ignore the data. 510
8.3. MAXIMUM A POSTERIORI ESTIMATION When σ 0 →∞, the limit of θ(cid:98)MAP is σ0li →m ∞θ(cid:98)MAP = σ0li →m ∞σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 =θ(cid:98)ML. (8.43) The reason why this happens is that σ is the uncertainty level of the prior. If σ is high, 0 0 we are not certain about the prior. In this case, MAP chooses to follow the ML estimate. When σ 0 →0, the limit of θ(cid:98)MAP is σl 0i →m 0θ(cid:98)MAP = σl 0i →m 0σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 =µ 0. (8.44) Note that when σ → 0, we are essentially saying that we are absolutely sure about the 0 prior. If we are so sure about the prior, there is no need to look at the data. In that case the MAP estimate is µ . 0 The way to understand the influence of σ is to inspect the equation: 0 θ(cid:98)MAP =argmax (cid:26) −(cid:88)N (x n 2− σ2θ)2 − (θ− 2σµ 20)2 (cid:27) . θ n=1 0 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) fixedw.r.t.σ0 changeswithσ0 Since σ is purely a preference you decide, you can control how much trust to put onto the 0 prior. (a) σ =0.1 (b) σ =1 0 0 Figure 8.18: The subfigures show the prior distribution f (θ) and the likelihood function f (x|θ), Θ X|Θ given the observed data. (a) When σ = 0.1, the estimated posterior distribution f (θ|x) is pulled 0 Θ|X towards the prior. (b) When σ = 1, the posterior is pulled towards the ML estimate. An analogy for 0 the situation is that the strength of the prior depends on the magnitude of σ . If σ is small the prior 0 0 is strong, and so the influence is large. If σ is large the prior is weak, and so the ML estimate will 0 dominate. Figure 8.18 illustrates a numerical experiment in which we compare σ = 0.1 and 0 σ =1. If σ is small, the prior distribution f (θ) becomes similar to a delta function. We 0 0 Θ can interpret it as a very confident prior, so confident that we wish to align with the prior. Thesituationcanbeimaginedasagameoftug-of-warbetweenapowerfulbullandahorse, 511
CHAPTER 8. ESTIMATION which the bull will naturally win. If σ is large the prior distribution will become flat. It 0 means that we are not very confident about the prior so that we will trust the data. In this case the MAP estimate will shift towards the ML estimate. 8.3.5 Analysis of the posterior distribution Whenthelikelihoodismultipliedwiththepriortoformtheposterior,whatdoestheposte- rior distribution look like? To answer this question we continue our Gaussian example with a fixed variance σ and an unknown mean θ. The posterior distribution is proportional to f (x|θ)f (θ) X|Θ Θ f (θ|x)= ∝f (x|θ)f (θ) Θ|X f (x) X|Θ Θ X =(cid:34) (cid:89)N √ 1 exp(cid:26) −(x n−θ)2(cid:27)(cid:35) ·(cid:34) 1 exp(cid:26) −(θ−µ 0)2(cid:27)(cid:35) . (8.45) 2πσ2 2σ2 (cid:112) 2πσ2 2σ2 n=1 0 0 Performing the multiplication and completing the squares, (cid:88)N (x n−θ)2 + (θ−µ 0)2 = (θ−θ(cid:98)MAP)2 , 2σ2 2σ2 2σ2 n=1 0 MAP where θ(cid:98)MAP = σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 , and σ (cid:98)M21 AP = σ1 02 + σN 2. (8.46) In other words, the posterior distribution f (θ|x) is also a Gaussian with Θ|X f Θ|X(θ|x)=Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP). If f (x|θ) = Gaussian(x; θ,σ), and f (θ) = Gaussian(θ; µ ,σ2), what is the X|Θ Θ 0 0 posterior f (θ|x)? Θ|X The posterior f Θ|X(θ|x) is Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP), where θ(cid:98)MAP = σ 02θ(cid:98) σM 02L ++ σ Nσ N 22µ 0 , and σ (cid:98)M21 AP = σ1 02 + σN 2. (8.47) The posterior tells us how N and σ will influence the MAP estimate. As N grows, 0 the posterior mean and variance becomes lim θ(cid:98)MAP =θ(cid:98)ML =θ, and lim σ (cid:98)MAP =0. N→∞ N→∞ As a result, the posterior distribution f (θ|x) will converge to a delta function centered Θ|X attheMLestimateθ(cid:98)ML.Therefore,aswetrytosolvetheMAPproblembymaximizingthe posterior, the MAP estimate has to improve because σ →0. (cid:98)MAP We can plot the posterior distribution Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP) as a function of the numberofsamplesN.Figure8.19illustratesthisexampleusingthefollowingconfigurations. 512
8.3. MAXIMUM A POSTERIORI ESTIMATION The likelihood is Gaussian with µ = 1, σ = 0.25. The prior is Gaussian with µ = 0 and 0 σ = 0.25. We construct the Gaussian according to Gaussian(θ(cid:98)MAP, σ (cid:98)M2 AP) by varying N. TheresultshowninFigure8.19confirmsourprediction:AsN grows,theposteriorbecomes more like a delta function whose mean is the true mean µ. The posterior estimator θ(cid:98)MAP, for each N, is the peak of the respective Gaussian. 8 N = 0 N = 1 6 N = 2 N = 5 N = 8 4 N = 12 N = 20 2 0 -1 -0.5 0 0.5 1 1.5 Figure 8.19: Posterior distribution f Θ|X(θ|x)=Gaussian(θ(cid:98)MAP, σ M2 AP) as N grows. When N is small, the posterior distribution is dominated by the prior. As N increases, the posterior distribution changes its mean and its variance. What is the pictorial interpretation of the MAP estimate? • For every N, MAP has a posterior distribution f (θ|x). Θ|X • As N grows, f Θ|X(θ|x) converges to a delta function centered at θ(cid:98)ML. • MAP tries to find the peak of f Θ|X(θ|x). For large N, it returns θ(cid:98)ML. 8.3.6 Conjugate prior Choosing the prior is an important topic in a MAP estimation. We have elaborated two “engineering” solutions: Use your prior experience or follow the physics. In this subsection, we discuss the third option: to choose something computationally friendly. To explain what we mean by “computationally friendly”, let us consider the following example, thanks to Avinash Kak.4 Consider a Bernoulli distribution with a PDF N (cid:89) f (x|θ)= θxn(1−θ)1−xn. (8.48) X|Θ n=1 TocomputetheMAPestimate,weassumethatwehaveapriorf (θ).Therefore,theMAP Θ 4Avinash Kak “ML, MAP, and Bayesian — The Holy Trinity of Parameter Estimation and Data Pre- diction”,https://engineering.purdue.edu/kak/Tutorials/Trinity.pdf 513
CHAPTER 8. ESTIMATION estimate is given by θ(cid:98)MAP =argmax f X|Θ(x|θ)f Θ(θ) θ (cid:34) N (cid:35) (cid:89) =argmax θxn(1−θ)1−xn ·f (θ) Θ θ n=1 N (cid:88) =argmax x logθ+(1−x )log(1−θ)+logf (θ). n n Θ θ n=1 Let us consider three options for the prior. Which one would you use? (cid:110) (cid:111) • Candidate 1: f Θ(θ) = √ 21 πσ2 exp −(θ 2− σµ 2)2 , a Gaussian prior. If you choose this prior, the optimization problem will become (cid:88)N (cid:26) (cid:27) (θ−µ)2 θ(cid:98)MAP =argmax x nlogθ+(1−x n)log(1−θ) − 2σ2 . θ n=1 We can still take the derivative and set it to zero. This gives (cid:80)N x N −(cid:80)N x θ−µ n=1 n − n=1 n = . θ 1−θ σ2 Defining S =(cid:80)N x and moving the terms around, we have n=1 n (1−θ)σ2S−θσ2(N −S)=θ(1−θ)(θ−µ). Thisisacubicpolynomialproblemthathasaclosed-formsolutionandisalsosolvable by a computer. But it’s also tedious, at least to lazy engineers like ourselves. • Candidate2:f (θ)= λe−λ|θ|,aLaplaceprior.Inthiscase,theoptimizationproblem Θ 2 becomes N (cid:26) (cid:27) (cid:88) θ(cid:98)MAP =argmax x nlogθ+(1−x n)log(1−θ) −λ|θ|. θ n=1 Welcometoconvexoptimization!Thereisnoclosed-formsolution.Ifyouwanttosolve this problem, you need to call a convex solver. • Candidate 3: f (θ)= 1θα−1(1−θ)β−1, a beta prior. This prior looks very compli- Θ C cated, but let’s plug it into our optimization problem: N (cid:26) (cid:88) θ(cid:98)MAP =argmax x nlogθ θ n=1 (cid:27) +(1−x )log(1−θ) +(α−1)logθ+(β−1)log(1−θ) n =argmax (S+α−1)logθ+(N −S+β−1)log(1−θ), θ 514
8.3. MAXIMUM A POSTERIORI ESTIMATION where S =(cid:80)N x . Taking the derivative and setting it to zero, we have n=1 n S+α−1 N −S+β−1 = . θ 1−θ Rearranging the terms we obtain the final estimate: S+α−1 θ(cid:98)MAP = N +β+α−2. (8.49) There are a number of intuitions that we can draw from this beta prior, but most importantly, we have obtained a very simple solution. That is because the posterior distri- bution remains in the same form as the prior, after multiplying by the prior. Specifically, if we use the beta prior, the posterior distribution is f (θ|x)∝f (x|θ)f (θ) Θ|X X|Θ Θ (cid:34) N (cid:35) (cid:89) 1 = θxn(1−θ)1−xn · θα−1(1−θ)β−1 C n=1 =θS+α−1(1−θ)N−S+β−1. This is still in the form of θ(cid:70)−1(1 − θ)(cid:4)−1, which is the same as the prior. When this happens, we call the prior a conjugate prior. In this example, the beta prior is a conjugate before the Bernoulli likelihood. What is a conjugate prior? • It is a prior such that when multiplied by the likelihood to form the posterior, the posterior f (θ|x) takes the same form as the prior f (θ). Θ|X Θ • Every likelihood has its conjugate prior. • Conjugate priors are not necessarily good priors. They are just computationally friendly. Some of them have good physical interpretations. We can make a few interpretations of the beta prior, in the context of Bernoulli likeli- hood. First, the beta distribution takes the form 1 f (θ)= θα−1(1−θ)β−1, (8.50) Θ B(α,β) with B(α,β) is the beta function5. The shape of the beta distribution is shown in Fig- ure 8.20. For different choices of α and β, the distribution has a peak located towards either side of the interval [0,1]. For example, if α is large but β is small, the distribution f (θ) leans towards 1 (the yellow curve). Θ As a user, you have the freedom to pick f (θ). Even if you are restricted to the beta Θ distribution, you still have plenty of degrees of freedom in choosing α and β so that your choice matches your belief. For example, if you know ahead of time that the Bernoulli experiment is biased towards 1 (e.g., the coin is more likely to come up heads), you can choose a large α and a small β. By contrast, if you believe that the coin is fair, you choose α=β.Theparametersαandβ areknownasthehyperparametersofthepriordistribution. Hyperparameters are parameters for f (θ). Θ 5The beta function is defined as B(α,β) = Γ(α)Γ(β), where Γ is the gamma function. For integer n, Γ(α+β) Γ(n)=(n−1)! 515
CHAPTER 8. ESTIMATION 4 = 2, = 8 = 3, = 7 = 8, = 2 3 2 1 0 0 0.2 0.4 0.6 0.8 1 Figure 8.20: Beta distribution f (θ) for various choices of α and β. When (α,β) = (2,8), the beta Θ distribution favors small θ. When (α,β) = (8,2), the beta distribution favors large θ. By swinging between the (α,β) pairs, we obtain a prior that has a preference over θ. Example 8.20. (Prior for Gaussian mean) Consider a Gaussian likelihood for a fixed variance σ2 and unknown mean θ: f (x|θ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−θ)2(cid:41) . X|Θ 2πσ2 2σ2 n=1 Show that the conjugate prior is given by 1 (cid:26) (θ−µ )2(cid:27) f (θ)= exp − 0 . (8.51) Θ (cid:112) 2πσ2 2σ2 0 0 Solution.Wehaveshownthisresultpreviously.Bysome(tedious)completingsquares, we show that 1 (cid:26) (θ−µ )2(cid:27) f (θ|x)= exp − N , Θ|X (cid:112) 2πσ2 2σ2 N N where σ2 Nσ2 µ N = Nσ2+σ2µ 0+ Nσ2+0 σ2θ(cid:98)ML, 0 0 σ2σ2 σ2 = 0 . N σ2+Nσ2 0 Sincef (θ|x)isinthesameformasf (θ),weknowthatf (θ)isaconjugateprior. Θ|X Θ Θ 516
8.3. MAXIMUM A POSTERIORI ESTIMATION Example 8.21. (Prior for Gaussian variance) Consider a Gaussian likelihood for a mean µ and unknown variance σ2: f (x|σ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−µ)2(cid:41) . X|σ 2πσ2 2σ2 n=1 Find the conjugate prior. Solution. We first define the precision θ = 1 . The likelihood is σ2 f (x|θ)=(cid:18) √ 1 (cid:19)N exp(cid:40) −(cid:88)N (x n−µ)2(cid:41) X|Θ 2πσ2 2σ2 n=1 (cid:40) N (cid:41) 1 θ (cid:88) = θN/2exp − (x −µ)2 . (2π)N/2 2 n n=1 We propose to choose the prior f (θ) as Θ 1 f (θ)= baθa−1exp{−bθ}, Θ Γ(a) forsomeaandb.Thisf (θ)iscalledtheGammadistributionGamma(θ|a,b).Wecan Θ show that E[Θ] = a and Var[Θ] = a. With some (tedious) completing squares, we b b2 show that the posterior is (cid:40) (cid:32) N (cid:33) (cid:41) 1 (cid:88) f (θ|x)∝θ(a0+N/2)−1exp − b + (x −µ)2 θ , Θ|X 0 2 n n=1 which is in the same form as the prior. So we know that our proposed f (θ) is a Θ conjugate prior. Thestoryofconjugatepriorsisendlessbecauseeverylikelihoodhasitsconjugateprior. Table 8.1 summarizes a few commonly used conjugate priors, their likelihoods, and their posteriors. The list can be expanded further to distributions with multiple parameters. For example, if a Gaussian has both unknown mean and variance, then there exists a conjugate priorconsistingofaGaussianmultipliedbyaGamma.Conjugatepriorsalsoapplytomulti- dimensional distributions. For example, the prior for the mean vector of a high-dimensional Gaussian is another high-dimensional Gaussian. The prior for the covariance matrix of a high-dimensionalGaussianistheWishartprior.Thepriorforboththemeanvectorandthe covariance matrix is the normal Wishart. 8.3.7 Linking MAP with regression MLandregressionrepresentthestatisticsandtheoptimizationaspectsofthesameproblem. Withtheparallelargument,MAPislinkedtotheregularizedregression.Thereasonfollows 517
CHAPTER 8. ESTIMATION Table of Conjugate Priors Likelihood Conjugate Prior Posterior f (x|θ) f (θ) f (θ|x) X|Θ Θ Θ|X Bernoulli(θ) Beta(α,β) Beta(α+S,β+N −S) (cid:16) (cid:17) Poisson(θ) Gamma(α,β) Gamma α+S, β 1+N (cid:16) (cid:17) Exponential(θ) Gamma(α,β) Gamma α+N, β 1+βS Gaussian(θ,σ2) Gaussian(µ 0,σ 02) Gaussian(cid:16) µ 10 // σσ 202 ++ NS // σσ 22(cid:17) 0 (cid:16) (cid:17) Gaussian(µ,θ2) Inv. Gamma(α,β) Gamma α+ N,β+ 1(cid:80)N (x −µ)2 2 2 n=1 n Table 8.1: Commonly used conjugate priors. immediately from the definition of MAP: θ(cid:98)MAP =argmax logf X|Θ(x|θ)+ logf Θ(θ) . θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) datafidelity regularization To make this more explicit, we consider following linear regression problem:        y φ (x ) φ (x ) ··· φ (x ) θ e 1 0 1 1 1 d−1 1 0 1 y 2 φ 0(x 2) φ 1(x 2) ··· φ d−1(x 2) θ 1  e 2  . = . . .  . + . .  .   . . .  .   .   .   . ··· . .  .   .  y φ (x ) φ (x ) ··· φ (x ) θ e N 0 N 1 N d−1 N d−1 N (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) =y =X =θ =e If we assume that e∼Gaussian(0,σ2I), the likelihood is defined as (cid:26) (cid:27) 1 1 f (y|θ)= exp − (cid:107)y−Xθ(cid:107)2 . (8.52) Y|Θ (cid:112) (2πσ2)N 2σ2 In the ML setting, the ML estimate is the maximizer of the likelihood: θ(cid:98)ML =argmax logf Y|Θ(y|θ) θ 1 =argmax − (cid:107)y−Xθ(cid:107)2. 2σ2 θ For MAP, we add a prior term so that the optimization becomes θ(cid:98)MAP =argmax logf Y|Θ(y|θ)+logf Θ(θ) θ 1 =argmin (cid:107)y−Xθ(cid:107)2−logf (θ). 2σ2 Θ θ 518
8.3. MAXIMUM A POSTERIORI ESTIMATION Therefore,theregularizationoftheregressionisexactly−logf (θ).Wecanperformreverse Θ engineeringtofindoutthecorrespondingpriorforourfavoritechoicesoftheregularization. Ridge regression. Suppose that (cid:26) (cid:107)θ(cid:107)2(cid:27) f (θ)=exp − . Θ 2σ2 0 Taking the negative log on both sides yields (cid:107)θ(cid:107)2 −logf (θ)= . Θ 2σ2 0 Putting this into the MAP estimate, 1 1 θ(cid:98)MAP =argmin 2σ2(cid:107)y−Xθ(cid:107)2+ 2σ2(cid:107)θ(cid:107)2 θ 0 σ2 =argmin (cid:107)y−Xθ(cid:107)2+ (cid:107)θ(cid:107)2, σ2 θ 0 (cid:124) (cid:123)(cid:122) (cid:125) =λ whereλisthecorrespondingridgeregularizationparameter.Therefore,theridgeregression is equivalent to a MAP estimation using a Gaussian prior. How is MAP related to ridge regression? • In MAP, define the prior as a Gaussian: (cid:26) (cid:107)θ(cid:107)2(cid:27) f (θ)=exp − . (8.53) Θ 2σ2 0 • ThepriorsaysthatthesolutionθisnaturallydistributedaccordingtoaGaussian with mean zero and variance σ2. 0 LASSO regression. Suppose that (cid:26) (cid:27) (cid:107)θ(cid:107) f (θ)=exp − 1 . Θ α Taking the negative log on both sides yields (cid:107)θ(cid:107) −logf (θ)= 1. Θ α Putting this into the MAP estimate we can show that 1 1 θ(cid:98)MAP =argmin 2σ2(cid:107)y−Xθ(cid:107)2+ α(cid:107)θ(cid:107) 1 θ 1 σ2 =argmin (cid:107)y−Xθ(cid:107)2+ (cid:107)θ(cid:107) . 2 α 1 θ (cid:124) (cid:123)(cid:122) (cid:125) =λ 519
CHAPTER 8. ESTIMATION To summarize: How is MAP related to LASSO regression? • LASSO is a MAP using the prior (cid:26) (cid:27) (cid:107)θ(cid:107) f (θ)=exp − 1 . (8.54) Θ α At this point, you may be wondering what MAP buys us when regularized regression canalreadydothejob.Theanswerisabouttheinterpretation.Whileregularizedregression canalwaysreturnusaresult,thatisjustaresult.However,ifyouknowthattheparameterθ isdistributedaccordingtosomedistributionsf (θ),MAPoffersastatisticalperspectiveof Θ thesolutioninthesensethatitreturnsthepeakoftheposteriorf (θ|x).Forexample,if Θ|X weknowthatthedataisgeneratedfromalinearmodelwithGaussiannoise,andifweknow that the true regression coefficients are drawn from a Gaussian, then the ridge regression is guaranteedtobeoptimalintheposteriorsense.Similarly,ifweknowthatthereareoutliers and have some ideas about the outlier statistics, perhaps the LASSO regression is a better choice. ItisalsoimportanttonotethedifferentoptimalitiesofferedbyMAPversusMLversus regression. The optimality offered by regression is the training loss, which can always give us a result even if the underlying statistics do not match the optimization formulation, e.g.,thereareoutliers,andyouuseunregularizedleast-squaresminimization.Youcangeta result, but the outliers will heavily influence your solution. On the other hand, if you know thedatastatisticsandchoosetofollowtheML,thentheMLsolutionisoptimalinthesense of optimizing the likelihood f (x|θ). If you further know the prior statistics, the MAP X|Θ solution will be optimal, but this time it is optimal w.r.t. to the posterior f (θ|x). Since Θ|X eachoftheseisoptimizingforadifferentgoal,theyareonlygoodfortheirchosenobjectives. For example, θ(cid:98)MAP can be a biased estimate if our goal is to maximize the likelihood. The θ(cid:98)ML is optimal for the likelihood but can be a bad choice for the posterior. Both θ(cid:98)MAP and θ(cid:98)ML can possibly achieve a reasonable mean-squared error, but their results may not make sense (e.g., if θ is an image then θ(cid:98)MAP may over-smooth the image whereas θ(cid:98)ML amplifies noise). So it’s incorrect to think that θ(cid:98)MAP is superior to θ(cid:98)ML because it is more general. Here are some rules of thumb for MAP, ML, and regression: When should I use regression, ML and MAP? • Regression: If you are lazy and you know nothing about the statistics, do the regression with whatever regularization you prefer. It will give you a result. See if it makes sense with your data. • MAP:Ifyouknowthestatisticsofthedata,andifyouhavesomepreferencefor the prior distribution, go with MAP. It will offer you the optimal solution w.r.t. finding the peak of the posterior. • ML: If you are interested in some simple-form solution, and you want those nice properties such as consistency and unbiasedness, then go with ML. It usually possessesthe“friendly”propertiessothatyoucanderivetheperformancelimit. 520
8.4. MINIMUM MEAN-SQUARE ESTIMATION 8.4 Minimum Mean-Square Estimation First-time readers are often tempted to think that the maximum-likelihood estimation or themaximumaposterioriestimationarethe bestmethodstoestimateparameters.Insome sense,thisistruebecausebothestimationproceduresoffersomeformofoptimalexplanation for the observed variables. However, as we said above, being optimal with respect to the likelihoodortheposterioronlymeansoptimalundertherespectivecriteria.AnMLestimate is not necessarily optimal for the posterior, whereas a MAP estimate is not necessarily optimal for the likelihood. Therefore, as we proceed to the third commonly used estimation strategy, we need to remind ourselves of the specific type of optimality we seek. 8.4.1 Positioning the minimum mean-square estimation Mean-square error estimation, as it is termed, uses the mean-square error as the optimality criterion. The corresponding estimation process is known as the minimum mean-square estimation (MMSE). MMSE is a Bayesian approach, meaning that it uses the prior f (θ) Θ as well as the likelihood f (x|θ). As we will show shortly, the MMSE estimate of a set X|Θ of i.i.d. observation X =[X ,...,X ]T is 1 N θ(cid:98)MMSE(x)( =a)E Θ|X[Θ|X =x] (a):We will discuss this. (cid:90) = θ·f (θ|x)dθ. (8.55) Θ|X You may find this equation very surprising, because it says that the MMSE estimate is the mean of the posterior distribution f (θ|x). Let’s compare this result with the ML Θ|X estimate and the MAP estimate: θ(cid:98)ML =peak of f X|Θ(x|θ), θ(cid:98)MAP =peak of f ΘX|(θ |x), θ(cid:98)MMSE =average of f Θ|X(θ |x). Therefore,anMMSEestimateisnotbyanymeansuniversallysuperiororinferiortoaMAP estimate or an ML estimate. It is just a different estimate with a different goal. Sohowexactlyaretheseestimatesdifferent?Figure 8.21illustratesatypicalsituation of asymmetric distribution. Here, we plot both the likelihood function f (x|θ) and the X|Θ posterior function f (θ |x). ΘX| Asshowninthefigure,theMLestimateisthepeakofthelikelihood,whereastheMAP estimate is the peak of the posterior. The third estimate is the MMSE estimate, which is the average of the posterior distribution. It is easy to see that if the posterior distribution is symmetric and has a single peak, the peak is always the mean. Therefore, for single-peak symmetric distributions, MMSE and MAP estimates are identical. 521
CHAPTER 8. ESTIMATION Figure 8.21: A typical example of an ML estimate, a MAP estimate and an MMSE estimate. What is so special about the MMSE estimate? • MMSE is a Bayesian estimation, so it requires a prior. • An MMSE estimate is the mean of the posterior distribution. • MMSE estimate = MAP estimate if the posterior distribution is symmetric and has a single peak. 8.4.2 Mean squared error The MMSE is based on minimizing the mean squared error (MSE). In this subsection we discuss the mean squared error in the Bayesian setting. In the deterministic setting, given an estimate θ(cid:98)and a ground truth θ, the MSE is defined as MSE( θ , θ(cid:98) )=(θ−θ(cid:98))2. (8.56) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) groundtruth estimate In any estimation problem, the estimate θ(cid:98)is always a function of the observed variables. Thus, we have θ(cid:98)(X)=g(X), where X =[X 1,...,X N]T, forsomefunctiong(·).SubstitutingthisintothedefinitionofMSE,andrecognizingthatX is drawn from a distribution f (x), we take the expectation to define the MSE as X MSE(θ,θ(cid:98))=(θ−θ(cid:98))2 ⇓replaceθ(cid:98)byg(X) MSE(θ,θ(cid:98))=(θ−g(X))2 ⇓takeexpectationoverX MSE(θ,θ(cid:98))=E X(cid:2) (θ−g(X))2(cid:3) . ThuswehavearrivedatthedefinitionofMSE.Wecallthisthefrequentistversion,because the parameter θ is deterministic. 522
8.4. MINIMUM MEAN-SQUARE ESTIMATION Definition 8.8 (Mean squared error, frequentist). The mean squared error of an estimate g(X) w.r.t. the true parameter θ is MSE (θ,g(·))=E (cid:2) (θ−g(X))2(cid:3) . (8.57) freq X If the parameter θ is high-dimensional, so is the estimate g(X), and the MSE is MSE (θ,g(·))=E (cid:2) (cid:107)θ−g(X)(cid:107)2(cid:3) . (8.58) freq X Note that in the above definition the MSE is measured between the true parameter θ and the estimator g(·). We use the function g(·) here because we have taken the expectation of all the possible inputs X. So we are not comparing θ with a value g(X) but with the function g(·). If we take a Bayesian approach such as the MAP, then θ itself is a random variable Θ. To compute the MSE, we then need to take the average across all the possible choices of ground truth Θ. This leads to MSE(θ,θ(cid:98))=E X(cid:2) (θ−g(X))2(cid:3) ⇓replaceθ byΘ MSE(θ,θ(cid:98))=E X(cid:2) (Θ−g(X))2(cid:3) ⇓takeexpectationoverΘ MSE(θ,θ(cid:98))=E X,Θ(cid:2) (Θ−g(X))2(cid:3) . Therefore, we have arrived at our definition of the MSE, in the Bayesian setting. Definition 8.9 (Mean squared error, Bayesian). The mean squared error of an es- timate g(X) w.r.t. the true parameter Θ is MSE (Θ,g(·))=E (cid:2) (Θ−g(X))2(cid:3) . (8.59) Bayes Θ,X If the parameter Θ is high-dimensional, so is the estimate g(X), and the MSE is MSE (Θ,g(·))=E (cid:2) (cid:107)Θ−g(X)(cid:107)2(cid:3) . (8.60) Bayes Θ,X ThedifferencebetweentheBayesianMSEandthefrequentistMSEistheexpectationoverΘ. Practicallyspeaking,thefrequentistMSEismoreofanevaluationmetricthananobjective function for solving an inverse problem. The reason is that in an inverse problem, we never have access to the true parameter θ. (If we knew θ, there would be no problem to solve.) Bayesian MSE is more meaningful. It says that we do not know the true parameter θ, but we know its statistics. We are trying to find the best g(·) that minimizes the error. Our solution will depend on the statistics of Θ but not on the unknown true parameter θ. When we say minimum mean squared error estimation, we typically refer to the Bayesian MMSE. In this case, the problem we solve is g(·)=argmin E (cid:2) (Θ−g(X))2(cid:3) . (8.61) Θ,X g(·) 523
CHAPTER 8. ESTIMATION As you can see from Definition 8.9, the goal of the Bayesian MMSE is to find a function g : RN → R such that the joint expectation E (cid:2) (Θ−g(X))2(cid:3) is minimized. In the case Θ,X where Θ is a vector, the problem becomes g(·)=argmin E (cid:2) (cid:107)Θ−g(X)(cid:107)2(cid:3) , (8.62) Θ,X g(·) where g(·):RN×d →Rd if Θ is a d-dimensional vector. The function g will take a sequence of N observed numbers and estimate the parameter Θ. What is the Bayesian MMSE estimate? The Bayesian MMSE estimate is obtained by minimizing the MSE: g(·)=argmin E (cid:2) (Θ−g(X))2(cid:3) . (8.63) Θ,X g(·) 8.4.3 MMSE estimate = conditional expectation Theorem 8.2. The Bayesian MMSE estimate is θ(cid:98)MMSE =argmin E Θ,X(cid:2) (Θ−g(X))2(cid:3) g(·) =E [Θ|X =x]. (8.64) Θ|X Proof. First of all, we decompose the joint expectation: (cid:90) E (cid:2) (Θ−g(X))2(cid:3) = E (cid:2) (Θ−g(X))2 |X =x(cid:3) f (x)dx. Θ,X Θ|X X Since f (x) ≥ 0 for all x, and E (cid:2) (Θ−g(X))2 |X =x(cid:3) ≥ 0 because it is a square, it X Θ|X follows that the integral is minimized when E (cid:2) (Θ−g(X))2 |X =x(cid:3) is minimized. Θ|X The conditional expectation can be evaluated as E [(Θ−g(X))2 |X =x] Θ|X (cid:20) (cid:12) (cid:21) =E Θ|X Θ2−2Θg(X)+g(X)2 (cid:12) (cid:12)X =x (cid:12) (cid:20) (cid:12) (cid:21) (cid:20) (cid:12) (cid:21) =E Θ2 (cid:12)X =x −2E Θ(cid:12)X =x g(x)+g(x)2 Θ|X (cid:12) Θ|X (cid:12) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) d=efV(x) d=efu(x) =V(x)−2u(x)g(x)+g(x)2+u(x)2−u(x)2 =V(x)−u(x)2+(u(x)−g(x))2 ≥V(x)−u(x)2, ∀g(·), where the last inequality holds because no matter what g(·) we choose, the square term (u(x)−g(x))2 is non-negative. Therefore, E [(Θ−g(X))2 |X =x] is lower-bounded by Θ|X 524
8.4. MINIMUM MEAN-SQUARE ESTIMATION V(x)−u(x)2, which is a bound that is independent of g(·). If we can find a g(·) such that this lower bound can be met, the corresponding g(·) is the minimizer. To this end we only need to make E [(Θ−g(X))2 | X = x] equal V(x)−u(x)2, Θ|X but this is easy: the equality holds if and only if (u(x)−g(x))2 = 0. In other words, if we choose g(·) such that g(x) = u(x), the corresponding g(·) is the minimizer. This g(·), by substituting the definition of u(x), is (cid:20) (cid:12) (cid:21) g(x)=E Θ(cid:12)X =x . (8.65) Θ|X (cid:12) This completes the proof. (cid:3) What is the MMSE estimate? The MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x]. (8.66) We emphasize that θ(cid:98)MMSE(x) is a function of x, because for a different set of observations x we will have a different estimated value. Since x is a random realization of the random vector X, we can also define the MMSE estimator as Θ(cid:98)MMSE(X)=E Θ|X[Θ|X]. (8.67) Inthisnotation,weemphasizethattheestimatorΘ(cid:98)MMSE returnsarandomparameter.The input to the estimator is the random vector X. Because we are not looking at a particular realization X =x but the general X, Θ(cid:98)MMSE is a function of X and not x. Conditional expectation of what? An MMSE estimator is the conditional expectation of Θ given X =x: (cid:20) (cid:12) (cid:21) (cid:90) E Θ(cid:12)X =x = θ f (θ|x)dθ. (8.68) Θ|X (cid:12) Θ|X This is the expectation using the posterior distribution f (θ|x). It should be compared Θ|X tothepeakoftheposterior,whichreturnsustheMAPestimate.Theposteriordistribution is constructed through Bayes’ theorem: f (x|θ)f (θ) X|Θ Θ f (θ|x)= . (8.69) Θ|X f (x) X Therefore, to evaluate the expectation of the condition distribution, we need to include the normalization constant f (x), which was omitted in MAP. X 525
CHAPTER 8. ESTIMATION The discussion about the mean squared error and the vector estimates can be skipped if this is your first time reading the book. What is the mean squared error when using the MMSE estimator? • The mean squared error conditioned on the observation is MSE(Θ,Θ(cid:98)MMSE(X))d =efE Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X] =Var [Θ|X], Θ|X which is the conditional variance. • The overall mean squared error, unconditioned, is MSE(Θ,Θ(cid:98)MMSE(·))=E X(cid:2) Var Θ|X[Θ|X](cid:3) =Var [Θ]. Θ Proof. Let us prove these two statements. The resulting MSE is obtained by substituting Θ(cid:98)MMSE(x)=E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3) into the MSE(Θ,Θ(cid:98)MMSE(X)). To this end, we have that E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X]=V(X)−u(X)2 + (u(X)−Θ(cid:98)MMSE(X))2 . (cid:124) (cid:123)(cid:122) (cid:125) =0,becauseΘ(cid:98)MMSE(X)=E Θ|X[Θ|X]=u(X) The variables V and u are defined as (cid:12) V(X)=E (cid:2) Θ2 (cid:12)X(cid:3) =2nd moment of Θ using f (θ|x), Θ|X (cid:12) Θ|X u(X)=E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3) =1st moment of Θ using f Θ|X(θ|x). Since Var[Z]=E[Z2]−E[Z]2 for any random variable Z, it follows that E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X]=V(X)−u(X)2 =E Θ|X(cid:2) Θ2 (cid:12) (cid:12) (cid:12)X(cid:3) −(cid:0)E Θ|X(cid:2) Θ(cid:12) (cid:12)X(cid:3)(cid:1)2 =variance of Θ using f (θ|x) Θ|X def = Var [Θ|X]. Θ|X Substituting this conditional variance into the MSE definition, (cid:90) MSE(Θ,Θ(cid:98)MMSE(·))= E Θ|X[(Θ−Θ(cid:98)MMSE(X))2 |X =x]f X(x)dx (cid:90) = Var [Θ|X =x]f (x)dx Θ|X X =Var [Θ]. Θ (cid:3) 526
8.4. MINIMUM MEAN-SQUARE ESTIMATION What happens if the parameter is a vector? • The MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x]. • The MSE is (cid:26) (cid:110) (cid:111)(cid:27) MSE(Θ,Θ(cid:98)MMSE(·))=Tr E X Cov(Θ|X) . (8.70) Proof. The first statement, that the MMSE estimate is θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x], iseasytounderstandsinceitjustfollowsfromthescalarcase.TheestimatorisΘ(cid:98)MMSE(X)= E [Θ|X]. The corresponding MSE is Θ|X MSE(Θ,Θ(cid:98)MMSE(·))=E Θ,X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2] (cid:26) (cid:27) =E X E Θ|X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2 |X] , where we have used the law of total expectation to decompose the joint expectation. Using the matrix identity below, we have that (cid:26) (cid:27) E X E Θ|X[(cid:107)Θ−Θ(cid:98)MMSE(X)(cid:107)2 |X] (cid:26) (cid:104) (cid:110) (cid:111) (cid:105)(cid:27) =E X E Θ|X Tr (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X (cid:26) (cid:26) (cid:104) (cid:105)(cid:27)(cid:27) =Tr E X E Θ|X (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X . However, since the MMSE estimator is the condition expectation of the posterior, it follows that the inner expectation is the conditional covariance. Therefore, we arrive at the second statement: (cid:26) (cid:26) (cid:104) (cid:105)(cid:27)(cid:27) MSE(Θ,Θ(cid:98)MMSE(·))=Tr E X E Θ|X (Θ−Θ(cid:98)MMSE(X))(Θ−Θ(cid:98)MMSE(X))T |X (cid:26) (cid:110) (cid:111)(cid:27) =Tr E Cov(Θ|X) . X (cid:3) To prove the two statements above, we need some tools from linear algebra. The two specific matrix identities are given by the following lemma: Lemma 8.1. The following are matrix identities: • For any random vector Θ∈Rd, (cid:107)Θ(cid:107)2 =Tr(ΘTΘ)=Tr(ΘΘT). 527
CHAPTER 8. ESTIMATION • For any random vector Θ∈Rd, E [Tr(ΘΘT)]=Tr(E [ΘΘT]). Θ Θ The proof of these two results is straightforward. The first is due to the cyclic property of the trace operator. The second statement is true because the trace is a linear operator that sums the diagonal of a matrix. The end of the discussion. Please join us again. Example 8.22. Let (cid:40) (cid:40) θe−θx, x≥0, αe−αθ, θ ≥0, f (x|θ)= and f (θ)= X|Θ Θ 0, x<0, 0, θ <0. Find the ML, MAP, and MMSE estimates for a single observation X =x. Solution. We first find the posterior distribution: f (x|θ)f (θ) X|Θ Θ f (θ|x)= Θ|X f (x) X αθe−(α+x)θ = (cid:82)∞ αθe−(α+x)θ dθ 0 αθe−(α+x)θ = α (α+x)2 =(α+x)2θe−(α+x)θ. The MMSE estimate is the conditional expectation of the posterior: θ(cid:98)MMSE(x)=E Θ|X[Θ|X =x] (cid:90) ∞ = θf (θ|x)dθ Θ|X 0 (cid:90) ∞ = θ(α+x)2θe−(α+x)θ dθ 0 (cid:90) ∞ =(α+x) θ2·(α+x)e−(α+x)θ dθ 0 (cid:124) (cid:123)(cid:122) (cid:125) 2ndmomentofexponentialdistribution 2 2 =(α+x)· = . (α+x)2 α+x 528
8.4. MINIMUM MEAN-SQUARE ESTIMATION The MAP estimate is the peak of the posterior: θ(cid:98)MAP(x)=argmax logf X|Θ(x|θ)+logf Θ(θ) θ =argmax −θx+logθ−αθ+logα. θ Taking the derivative and setting it to zero yields −x+ 1 −α=0. This implies that θ 1 θ(cid:98)MAP(x)= α+x. Finally, the ML estimate is 1 θ(cid:98)ML(x)=argmax logf X|Θ(x|θ)= x. θ Practice Exercise 8.8. Following the previous example, derive the estimates for multiple observations X =x. Solution. The posterior is f (x|θ)f (θ) X|Θ Θ f (θ|x)= Θ|X f (x) X ((cid:81)N f (x |θ))f (θ) = n=1 X|Θ n Θ f (x) X αθe−(α+(cid:80)N n=1xn)θ = (cid:82)∞ αθe−(α+(cid:80)N n=1xn)θ dθ 0 (cid:32) N (cid:33)2 = α+(cid:88) x n θe−(α+(cid:80)N n=1xn)θ. n=1 Therefore, we are only replacing x by the sum (cid:80)N x in the posterior. Hence, the n=1 n estimates are: 2 θ(cid:98)MMSE(x)= α+(cid:80)N x , n=1 n 1 θ(cid:98)MAP(x)= α+(cid:80)N x , n=1 n 1 θ(cid:98)ML(x)= (cid:80)N x . n=1 n This example shows that as N →∞, the ML estimate θ(cid:98)ML(x)→0. The reason is that the likelihoodisanexponentialdistribution.Therefore,thepeakisalwaysat0.Theposterioris an Erlang distribution, and therefore the peak is offset by α in the denominator. However, as N →∞ the posterior distribution is dominated by the likelihood, so the peak is shifted 529
CHAPTER 8. ESTIMATION towards 0. Finally, since the Erlang distribution is asymmetric, the mean is different from the peak. Hence, the MMSE estimate is different from the MAP estimate. 8.4.4 MMSE estimator for multidimensional Gaussian The multidimensional Gaussian has some very important uses in data science. Accordingly, we devote this subsection to the discussion of the MMSE estimate of a Gaussian. The main result is stated as follows. What is the MMSE estimator for a multi-dimensional Gaussian? Theorem 8.3. Suppose Θ∈Rd and X ∈RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX The MMSE estimator is Θ(cid:98)MMSE(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.71) The proof of this result is not difficult but it is tedious. The flow of the argument is: • Step 1: Show that the posterior distribution f (θ|x) is a Gaussian. Θ|X • Step 2: To do so we need to complete the squares for matrices. • Step 3: Once we have the f (θ|x), the posterior mean is the MMSE estimator. Θ|X The proof below can be skipped if this is your first time reading the book. Proof. The posterior PDF is f (θ,x) f (θ|x)= Θ,X Θ|X f (x) X (cid:40) (cid:20) (cid:21)T (cid:20) (cid:21)−1(cid:20) (cid:21)(cid:41) θ−µ Σ Σ θ−µ √ 1 exp −1 Θ ΘΘ ΘX Θ (2π)d+N|Σ| 2 x−µ X Σ XΘ Σ XX x−µ X = . √ 1 exp(cid:110) −1(cid:2) x−µ (cid:3)T Σ−1 (cid:2) x−µ (cid:3)(cid:111) (2π)N|ΣXX| 2 X XX X Without loss of generality, we assume that µ =µ =0. Then the posterior becomes X Θ 1 f (θ|x)= Θ|X (cid:112) (2π)d|Σ|/|Σ | XX (cid:40) 1(cid:20) θ(cid:21)T (cid:20) Σ Σ (cid:21)−1(cid:20) θ(cid:21) 1 (cid:41) ×exp − ΘΘ ΘX + xTΣ−1 x . 2 x Σ XΘ Σ XX x 2 XX (cid:124) (cid:123)(cid:122) (cid:125) H(θ,x) The tedious task here is to simplify H(θ,x). 530
8.4. MINIMUM MEAN-SQUARE ESTIMATION Regardless of what the 2-by-2 matrix inverse is, the matrix will take the form (cid:20) (cid:21)−1 (cid:20) (cid:21) Σ Σ A B ΘΘ ΘX = , Σ Σ C D XΘ XX forsomechoicesofmatricesA,B,C andD.Therefore,thefunctionH(θ,x)canbewritten as 1(cid:110) (cid:111) H(θ,x)=− θTAθ+θTBx+xTCθ+xTDx−xTΣ−1 x . (8.72) 2 XX Our goal is to complete the square for H(θ,x). To this end, we propose to write 1(cid:110) (cid:111) H(θ,x)=− (θ−Gx)TA(θ−Gx)+Q(x) , (8.73) 2 for some matrix G and function Q(·) of x only. If we compare Equation (8.72) and Equa- tion (8.73), we observe that G must satisfy G=−A−1B. Therefore,ifwecandetermineAandB,wewillknowG.IfweknowG,wehavecompleted the square for H(θ,x). If we can complete the square for H(θ,x), we can write (cid:26) (cid:27) exp{−Q(x)/2} 1 f (θ|x)= ×exp − (θ−Gx)TA(θ−Gx) . Θ|X (cid:112) (2π)d|Σ|/|Σ | 2 XX (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) constantinθ aGaussian Hence, the MMSE estimate, which is the posterior mean E[Θ|X =x], is simply Gx: θ(cid:98)MMSE(x)=E[Θ|X =x] =Gx =−A−1Bx. So it remains to determine A and B by solving the tedious matrix inversion problem. The result is:6 A=(Σ −Σ Σ−1 Σ )−1, ΘΘ ΘX XX XΘ B =−(Σ −Σ Σ−1 Σ )−1Σ Σ−1 , ΘΘ ΘX XX XΘ ΘX XX C =(Σ −Σ Σ−1Σ )−1Σ Σ−1, XX XΘ ΘΘ ΘX XΘ ΘΘ D =(Σ −Σ Σ−1Σ )−1. XX XΘ ΘΘ ΘX Therefore, plugging everything into the equation, θ(cid:98)MMSE(x)=−A−1Bx =Σ Σ−1 x. Θ,X XX For non-zero means, we can repeat the same arguments above and show that θ(cid:98)MMSE(x)=µ Θ+Σ Θ,XΣ− X1 X(x−µ X). (cid:3) 6See Matrix Cookbook https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf Section 9.1.5 ontheSchurcomplement. 531
CHAPTER 8. ESTIMATION End of the proof. Please join us again. Practice Exercise 8.9. Suppose Θ ∈ Rd and X ∈ RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX We know that the MMSE estimator is Θ(cid:98)MMSE(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.74) Find the mean squared error when using the MMSE estimator. Solution. Conditioned on X =x, according to Equation (8.70), the MMSE is MSE(Θ,Θ(cid:98)(X))=Tr{Cov[Θ|X]}. The conditional covariance Cov[Θ|X] is the covariance of the posterior distribution f (θ|x), which is Θ|X Tr{Cov[Θ|X]}=Tr{A} =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) . ΘΘ ΘX XX XΘ The overall mean squared error is (cid:16) (cid:17) (cid:20) (cid:21) MSE Θ,Θ(cid:98)(·) =E X MSE(Θ,Θ(cid:98)(X)) (cid:90) = MSE(Θ,Θ(cid:98)(x))f X(x)dx (cid:90) = Tr{Cov[Θ|X]}f (x)dx X (cid:90) = Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) f (x)dx ΘΘ ΘX XX XΘ X (cid:90) =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) f (x)dx ΘΘ ΘX XX XΘ X =Tr(cid:8) (Σ −Σ Σ−1 Σ )−1(cid:9) . ΘΘ ΘX XX XΘ For multidimensional Gaussian, does MMSE = MAP? The answer is YES. Theorem 8.4. Suppose Θ∈Rd and X ∈RN are jointly Gaussian with a joint PDF (cid:20) (cid:21) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19) Θ µ Σ Σ ∼Gaussian Θ , ΘΘ ΘX . X µ Σ Σ X XΘ XX 532
8.4. MINIMUM MEAN-SQUARE ESTIMATION The MAP estimate is Θ(cid:98)MAP(X)=µ Θ+Σ ΘXΣ− X1 X(X−µ X). (8.75) Proof. The proof of this result is straightforward. If we return to the proof of the MMSE result, we note that (cid:26) (cid:27) exp{−Q(x)/2} 1 f (θ|x)= ×exp − (θ−Gx)TA(θ−Gx) . Θ|X (cid:112) (2π)d|Σ|/|Σ | 2 XX (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) constantinθ aGaussian Therefore, the maximizer of this posterior distribution, which is the MAP estimate, is θ(cid:98)MAP(x)=argmax f Θ|X(θ|x) θ 1 =argmax − (θ−Gx)TA(θ−Gx). 2 θ Taking the derivative w.r.t. θ and setting it zero, we have θ(cid:98)MAP(x)=Gx=Σ Θ,XΣ− X1 Xx. If the mean vectors are non-zero, we have θ(cid:98)MAP(x)=µ Θ+Σ ΘXΣ− X1 X(x−µ X). (cid:3) 8.4.5 Linking MMSE and neural networks The blossoming of deep neural networks since 2010 has created a substantial impact on modern data science. The basic idea of a neural network is to train a stack of matrices and nonlinear functions (known as the network weights and the neuron activation functions, respectively), among other innovative ideas, so that a certain training loss is minimized. Expressingthisbyequations,thegoalofthelearningisequivalenttosolvingtheoptimization problem (cid:20) (cid:21) g(·)=argmin E (cid:107)Θ−g(X)(cid:107)2 , (8.76) (cid:98) X,Θ g(·) where X ∈ RM is the input data and Θ ∈ Rd is the ground truth prediction. We want to find g(·) such that the error is minimized. The error we choose here is the (cid:96) -norm error (cid:107)·(cid:107)2. It is only one of many possi- 2 ble choices. You may recognize that this is exactly the same as the MMSE optimization. Therefore,theneuralnetworkwearefindinghereistheMMSEestimator.SincetheMMSE estimator is the conditional expectation of the posterior distribution, the neural network approximates the mean of the posterior distribution. Often the struggle we have with deep neural networks is whether we can find the optimal network parameters via optimization algorithms such as the stochastic gradient descent algorithms. However, if we think about this problem more deeply, the equivalence between the MMSE estimator and the posterior mean tells us that the hard part is related to the posterior distribution. In the high-dimensional landscape, it is close to impossible to determine the posterior and its mean. If we add to these difficulties and the nonconvexity of the function g, training a network is very challenging. 533
CHAPTER 8. ESTIMATION Onemisconceptionaboutneuralnetworksisthatifwecanachievealowtrainingerror, andifthemodelcanalsoachievealowtestingerror,thenthenetworkisgood.Thisisafalse sense of satisfaction. If a model can achieve very good training and testing errors, then the model is only good with respect to the error you choose. For example, if we choose the (cid:96) - 2 normerror(cid:107)·(cid:107)2 andifourmodelachievesgoodtrainingandtestingerrors(intermsof(cid:107)·(cid:107)2), we can conclude that the model does well with respect to (cid:107)·(cid:107)2. The more serious problem here, unfortunately, is that (cid:107)·(cid:107)2 is not necessarily a good metric of performance (for both trainingandtesting)becausetrainingwith(cid:107)·(cid:107)2 isequivalenttoapproximatingtheposterior mean. There is absolutely no reason to believe that in the high-dimensional landscape, the posterior mean is the optimal. If we choose the posterior mode or the posterior median, we will also obtain a result. Why are the modes and medians “worse” than the mean? In practice, it has been observed that training deep neural networks for image-processing tasksgenerallyleadstoover-smoothedimages.Thisdemonstrateshowminimizingthemean squared error (cid:107)·(cid:107)2 can be a fundamental mismatch with the problem. Is minimizing the MSE the best option? • No.MinimizingtheMSEisequivalenttofindingthemeanoftheposterior.There is no reason why the mean is the “best”. • You can find the mode of the posterior, in which case you will get a MAP estimator. • You can also find the median of the posterior, in which case you will get the minimum absolute error estimator. • Ultimately, you need to define what is “good” and what is “bad”. • The same principle applies to deep neural networks. Especially in the regression setting, why is (cid:107)·(cid:107)2 a good evaluation metric for testing (not just training)? 8.5 Summary In this chapter, we have discussed the basic principles of parameter estimation. The three building blocks are: • Likelihood f (x|θ): the PDF that we observe samples X conditioned on the un- X|Θ known parameter Θ. In the frequentist world, Θ is a deterministic quantity. In the Bayesian world, Θ is random and so it has a PDF. • Prior f (θ): the PDF of Θ. The prior f (θ) is used by all Bayesian computation. Θ Θ • Posterior f (θ|x): the PDF that the underlying parameter is Θ=θ given that we Θ|X have observed X =x. The three building blocks give us several strategies to estimate the parameters: • Maximum likelihood (ML) estimation: Maximize f (x|θ). X|Θ 534
8.6. REFERENCES • Maximum a posteriori (MAP) estimation: Maximize f (θ|x). Θ|X • Minimummean-squareestimation(MMSE):Minimizethemeansquarederror,which is equivalent to finding the mean of f (θ|x). Θ|X As discussed in this chapter, no single estimation strategy is universally “better” because one needs to specify the optimality criterion. If the goal is to minimize the mean squared error, then the MMSE estimator is the optimal strategy. If the goal is to maximize the likelihood without assuming any prior knowledge, the ML estimator would be the optimal strategy. It may appear that if we knew the ground truth parameter θ∗ we could minimize the distance between the estimated parameter θ and the true value θ∗. If the parameter is a scalar, this will work. However, if the parameter is a vector, the noise of the distance becomes an issue. For example, if one cares about the mean absolute error (MAE), the optimal estimator would be the median of the posterior distribution instead of the mean of the posterior in the MMSE case. Therefore, it is the end user’s responsibility to specify the optimality criterion. Wheneverweconsiderparameterestimation,wetendtothinkthatitisaboutestimat- ing the model parameters, such as the mean of a Gaussian PDF. While in many statistics problems this is indeed the case, parameter estimation can be much broader if we link it with regression. Specifically, a regularized linear regression problem can be formulated as a MAP estimation θ∗ =argmax (cid:107)Xθ−y(cid:107)2 + λR(θ) , (8.77) θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) −logfX|Θ(x|θ) −logfΘ(θ) for some regularization R(θ), which is also the negative log of the prior. Expressed in this way,werecognizethattheMAPestimationcanbeusedtorecoversignals.Forexample,we can model X as a linear degradation process of certain imaging systems. Then solving the MAPestimationisequivalenttofindingthebestsignalexplainingthedegradedobservation using the posterior as the criterion. There is rich literature dealing with solving MAP esti- mationproblemssimilartotheseinsubjectssuchascomputationalimaging,communication systems, remote sensing, radar engineering, and recommendation systems, to name a few. 8.6 References Basic 8-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 8 and Chapter 9. 8-2 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 6 and Chapter 8. 8-3 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 8. 8-4 HenryStarkandJohnW.Woods,ProbabilityandRandomProcesseswithApplications to Signal Processing, Prentice Hall, 3rd Edition, 2002. Chapter 5. 535
CHAPTER 8. ESTIMATION 8-5 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Chapter 12. Theoretical analysis 8-6 H. Vincent Poor, An Introduction Signal Detection and Estimation, Springer, 1998. 8-7 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory, Prentice-Hall, 1993. 8-8 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation, Springer, 2008. 8-9 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 2001. Chapter 8. 8-10 LarryWasserman,AllofStatistics:AConciseCourseinStatisticalInference,Springer, 2010. 8-11 Erich L. Lehmann, Elements of Large-Sample Theory, Springer, 1999. Chapter 7. 8-12 George Casella and Roger L. Berger Statistical Inference, Duxbury, 2002. Chapter 7. Machine-learning 8-13 ChristopherBishop,PatternRecognitionandMachineLearning,Springer,2006.Chap- ter 2 and Chapter 3. 8-14 Richard O. Duda, Peter E. Hart and David G. Stork, Pattern Classification, Wiley 2001. Chapter 3. 8.7 Problems Exercise 1. Let X ,...,X be a sequence of i.i.d. Bernoulli random variables with P[X = 1] = θ. 1 N n Suppose that we have observed x ,...,x . 1 N (a) Show that the PMF of X n is p Xn(x n |θ)=θxn(1−θ)1−xn. Find the joint PMF p (x ,...,x ). X1,...,XN 1 N (b) Find the maximum likelihood estimate θ(cid:98), i.e., θ(cid:98)ML =argmax log p X1,...,XN(x 1,...,x N). θ Express your answer in terms of x ,...,x . 1 N 536
8.7. PROBLEMS (c) Letθ =1/2.UseChebyshev’sinequalitytofindanupperboundforP[|Θ(cid:98)ML−θ|>0.1]. Exercise 2. Let Y = θ + W be the output of a noisy channel where the input is a scalar θ and n n W ∼N(0,1) is an i.i.d. Gaussian noise. Suppose that we have observed y ,...,y . n 1 N (a) Express the PDF of Y in terms of θ and y . Find the joint PDF of Y ,...,Y . n n 1 N (b) Findthemaximumlikelihoodestimateθ(cid:98)ML.Expressyouranswerintermsofy 1,...,y N. (c) Find E[Θ(cid:98)ML]. Exercise 3. Let X ,...,X be a sequence of i.i.d. Gaussian random variables with unknown mean θ 1 N 1 and variance θ . Suppose that we have observations x ,...,x . 2 1 N (a) Express the PDF of X in terms of x , θ and θ . Find the joint PDF of X ,...,X . n n 1 2 1 N (b) Find the maximum likelihood estimates of θ and θ . 1 2 Exercise 4. In this problem we study a single-photon image sensor. First, recall that photons arrive according to a Poisson distribution, i.e., the probability of observing k photons is λke−λ P[Y =k]= , k! whereλisthe(unknown)underlyingphotonarrivalrate.Whenphotonsarriveatthesingle- photon detector, the detector generates a binary response “1” when one or more photons are detected, and “0” when no photon is detected. (a) Let B be the random variable denoting the response of the single-photon detector. That is, (cid:40) 1, Y ≥1, B = 0, Y =0. Find the PMF of B. (b) Suppose we have obtained T independent measurements with realizations B = b , 1 1 B =b ,...,B =b .Showthattheunderlyingphotonarrivalrateλcanbeestimated 2 2 T T by (cid:32) (cid:80)T b (cid:33) λ=−log 1− t=1 t . T (c) Get a random image from the internet and turn it into a grayscale array with values between 0 and 1. Write a MATLAB or Python program to synthetically generate a sequence of T =1000 binary images. Then use the previous result to reconstruct the grayscale image. 537
CHAPTER 8. ESTIMATION Exercise 5. Consider a deterministic vector s∈Rd and random vectors f (y|θ)=Gaussian(sθ,Σ), Y|Θ f (θ)=Gaussian(µ,σ2). Θ (a) Show that the posterior distribution is given by f (θ|y)=Gaussian(m,q2), (8.78) Θ|Y where d2 =sTΣ−1s, (cid:18) 1 (cid:19)−1(cid:16) µ (cid:17) m= d2+ sTΣ−1y+ , σ2 σ2 1 q2 = . d2+ 1 σ2 (b) Show that the MMSE estimate θ(cid:98)MMSE(y) is given by σ2sTΣ−1y+µ θ(cid:98)MMSE(y)= σ2d2+1 . (8.79) (c) Show that the MSE is given by 1 MSE(Θ,Θ(cid:98)MMSE(Y))= d2+ 1 . (8.80) σ2 What happens when σ →0? (d) Give an interpretation of d2. What happens when d2 →0 and when d2 →∞? Exercise 6. Prove the following identity: (cid:20) (cid:21)−1 Σ Σ ΘΘ ΘX Σ Σ XΘ XX (cid:20) (Σ −Σ Σ−1 Σ )−1 −(Σ −Σ Σ−1 Σ )−1Σ Σ−1 (cid:21) = ΘΘ ΘX XX XΘ ΘΘ ΘX XX XΘ ΘX XX . (Σ −Σ Σ−1Σ )−1Σ Σ−1 (Σ −Σ Σ−1Σ )−1 XX XΘ ΘΘ ΘX XΘ ΘΘ XX XΘ ΘΘ ΘX Hint:Youcanperformreverseengineeringbycheckingwhethertheproductoftheleft-hand side and the right-hand side would give you the identity matrix. Exercise 7. Let X , X , X and X be four i.i.d. Poisson random variables with mean θ =4. Find the 1 2 3 4 mean and variance of the following estimators Θ(cid:98)(X) for θ and determine whether they are biased or unbiased. 538
8.7. PROBLEMS • Θ(cid:98)(X)=(X 1+X 2)/2 • Θ(cid:98)(X)=(X 3+X 4)/2 • Θ(cid:98)(X)=(X 1+2X 2)/3 • Θ(cid:98)(X)=(X 1+X 2+X 3+X 4)/4 Exercise 8. LetX ,...,X bei.i.d.randomvariableswithauniformdistributionof[0,θ].Considerthe 1 N following estimator: Θ(cid:98)(X)=max(X 1,...,X N). (8.81) (a) Show that the PDF of Θ(cid:98) is f Θ(cid:98)(θ) = N[F X(x)]N−1f X(x), where f X and F X are re- spectively the PDF and CDF of X . n (b) Show that Θ(cid:98) is a biased estimator. (c) Find the variance of Θ(cid:98). Is it a consistent estimator? (d) Find a constant c so that cΘ(cid:98) is unbiased. Exercise 9. Let X ,...,X be i.i.d. Gaussian random variables with unknown mean θ and known 1 N variance σ =1. (a) Show that the log-likelihood function is N N 1 (cid:88) logL(θ|x)=− log(2π)− (x −θ)2. (8.82) 2 2 n n=1 (b) Let X2 = 1 (cid:80)N x2 and X = 1 (cid:80)N x . Show that X2 > (X)2 if and only if N n=1 n N n=1 n (cid:80)N (x −θ)2 ≥0 for all θ. n=1 n (c) Use Python to plot the function logL(θ|x), when X =2 and X2 =1. Exercise 10. Let X ,...,X be i.i.d. uniform random variables over the interval [0,θ]. 1 N Let T =max(X ,...,X ). 1 N (a) Consider the estimator h(X)= 1 (cid:80)N X . Is h(·) an unbiased estimator? N n=1 n (b) Consider the estimator g(X)= 1 (cid:80)N X . Is g(·) an unbiased estimator? N n=1 n (c) Show that (cid:18) (cid:19) N +1 E[g(X)|T =t]= t. N 539
CHAPTER 8. ESTIMATION (d) Let g(X)=E[g(X)|T]=(cid:0)N+1(cid:1) T. Show that (cid:98) N (cid:18) (N +1)2 (cid:19) E[g(X)2]= θ2. (cid:98) N(N +2) (e) Show that (cid:18) (cid:19) 1 E[(g(X)−θ)2]= θ2. (cid:98) N(N +2) Exercise 11. The Kullback-Leibler divergence between two distributions p (x) and p (x) is defined as 1 2 (cid:90) p (x) KL(p (cid:107)p )= p (x)log 1 dx. (8.83) 1 2 1 p (x) 2 Suppose we approximate p using a distribution p . Let us choose p = Gaussian(µ,Σ). 1 2 2 Show that µ and Σ, which minimize the KL divergence, are such that µ=E [x] and Σ=E [(x−µ)(x−µ)T]. x∼p1(x) x∼p1(x) Exercise 12. (a) Recall that the trace operator is defined as tr[A] = (cid:80)d [A] . Prove the matrix i=1 i,i identity xTAx=tr[AxxT], (8.84) where A∈Rd×d. (b) Show that the likelihood function (cid:89)N (cid:26) 1 (cid:110) 1 (cid:111)(cid:27) p(D|Σ)= exp − (x −µ)TΣ−1(x −µ) (8.85) (2π)d/2|Σ|1/2 2 n n n=1 can be written as (cid:40) (cid:34) N (cid:35)(cid:41) 1 1 (cid:88) p(D|Σ)= |Σ−1|N/2exp − tr Σ−1 (x −µ)(x −µ)T . (8.86) (2π)Nd/2 2 n n n=1 (c) Let A = Σ−1Σ(cid:98)ML, and λ 1,...,λ d be the eigenvalues of A. Show that the result from part (b) leads to (cid:32) d (cid:33)N/2 (cid:40) d (cid:41) 1 (cid:89) N (cid:88) p(D|Σ)= λ exp − λ . (8.87) (2π)Nd/2|Σ(cid:98)ML|N/2 i=1 i 2 i=1 i Hint: For matrix A with eigenvalues λ ,...,λ , tr[A]=(cid:80)d λ . 1 d i=1 i (d) Find λ ,...,λ such that Equation (8.87) is maximized. 1 d 540
8.7. PROBLEMS (e) With the choice of λ i given in (d), derive the ML estimate Σ(cid:98)ML. (f) WhatwouldbethealternativewayoffindingΣ(cid:98)ML?Youdonotneedtoproveit.Just briefly describe the idea. (g) Σ(cid:98)ML is a biased estimate of the covariance matrix because E[Σ(cid:98)ML](cid:54)=Σ. Can you suggest an unbiased estimate Σ(cid:98)unbias such that E[Σ(cid:98)unbias] = Σ? You don’t need to prove it. Just state the result. 541
CHAPTER 8. ESTIMATION 542
Chapter 9 Confidence and Hypothesis InChapters7and8welearnedaboutregressionandestimation,whichallowustodetermine theunderlyingparametersofourstatisticalmodels.Afterobtainingtheestimates,wewould liketoquantifytheaccuracyoftheestimatesanddrawstatisticalconclusions.Additionally, we would like to understand the confidence of these estimates along with their statistical significance. This chapter presents a few principles that involve analyzing the confidence of theestimatesandconductinghypothesistesting.Therearetwomainquestionsthatwewill address: • How good is our estimate? This is a fundamental question about the estimator Θ(cid:98), a random variable with a PDF, a mean, and a variance.1 The estimator we construct todaymaybedifferentfromtheestimatorweconstructtomorrowduetovariationsin theobserveddata.Therefore,thequalityoftheestimatordependsontherandomness andthenumberofsamplesusedtoconstructit.Tomeasurethequalityoftheestimator we need to introduce an important concept known as the confidence. • Is there statistical significance? Suppose that we ran a campaign and observed that there is a change in the statistics. On what basis do we claim that the change is statistically significant? How should the cutoff be determined? If we claim that a result is statistically significant but there is no significance in reality, how much error will we suffer? These questions are the subjects of hypothesis testing. These two principal questions are critical for modern data science. If they are not properly answered, our statistical conclusions could potentially be flawed. A toy example: Imagine thatyou aredeveloping aCOVID-19vaccine.Youtested thevaccineon three patients, and all of them show positive responses to the vaccine. You felt excited because your vaccine has a 100% success rate. You submit your vaccine application to FDA. Within 1 second your application is rejected. Why? The answer is obvious. You only have three testing samples. How reliable can these three samples be? While you are laughing at this toy example, it raises deep statistical questions. First, why are three samples not enough? Well, it is because the variance of the estimator can potentiallybehuge.Moresamplesarebetterbecauseiftheestimatoristhesampleaverageof theindividualresponses,theestimatorwillbehavelikeaGaussianaccordingtotheCentral 1Not all random variables have a well-defined PDF, mean, and variance. E.g., a Cauchy variable does nothaveamean. 543
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Limit Theorem. The variance of this Gaussian will diminish as we have more samples. Therefore,ifwewanttocontrolthevarianceoftheestimator,weneedmoresamples.Second, even if we have many samples, how confident is this estimator with respect to the unknown population parameter? Note that the population parameter is unknown, and so we cannot measurethingssuchasthemeansquarederror.Weneedatooltoreportconfidence.Third, for simple estimators such as the sample average, we can approximate it by a Gaussian. However,iftheestimatorismorecomplicated,e.g.,thesamplemedian,howdoweestimate the variance and the confidence? Fourth, suppose that we have expanded the vaccine test to, say, 951 patients, and we have obtained some statistics. To what extent can we declare that the vaccine is effective? We need a decision rule that turns the statistics into a binary decision. Finally, even if we declare that the vaccine is effective with a confidence of 95%, what about the remaining 5%? What if we want to push the confidence to 99%? What is the trade-off? As you can see, these questions are the recurring themes of all data science problems. No matter if you are developing a medical diagnostic system, a computer vision algorithm, a speech recognition system, a recommendation system, a search engine, stock forecast, fraud detection, or robotics controls, you need to answer these questions. This chapter will introduce useful concepts related to data analysis in the form of five basic principles: 1. Confidence interval (Section 9.1). A confidence interval is a random interval that includes the true parameter. We will discuss how a confidence interval is constructed and the correct way to interpret the confidence interval. 2. Bootstrapping (Section 9.2). When constructing the confidence interval, we need the variance of the estimator. However, since we do not know the true distribution, we need an alternative way to estimate the variance. Bootstrapping is designed for this purpose. 3. Hypothesis testing (Section 9.3). Many statistical tasks require a binary decision at the end, e.g., there is a disease versus there is no disease. Hypothesis testing is a principle for making a systematic decision with statistical guarantees. 4. Neyman-Pearson decision(Section9.4).Thesimplehypothesistestingprocedurehas manylimitationsthatcanonlyberesolvedifweunderstandamoregeneralframework. We will study such a framework, called the Neyman-Pearson decision rule. 5. ROC and PR curves (Section 9.5). No decision rule is perfect. There is always a trade-off between how much we can detect and how much we will miss. The receiver operating characteristic (ROC) curve and the precision-recall (PR) curve can give us more insight into this trade-off. We will establish the equivalence between the ROC and the PR curve and correct any misconceptions about them. After reading this chapter, we hope that you will be able to apply these principles to your favorite data analysis problems correctly. With these principles, you can tell your customers or bosses the statistical significance of your conclusions. You will also be able to help your friends understand the many misconceptions that they may find on the internet. 544
9.1. CONFIDENCE INTERVAL 9.1 Confidence Interval The first topic we discuss in this chapter is the confidence interval. At a high level, the confidence interval tells us the quality of our estimator with respect to the number of sam- ples. We begin this section by reviewing the randomness of an estimator. Then we develop the concept of the confidence interval. We discuss several methods for constructing and interpreting these confidence intervals. 9.1.1 The randomness of an estimator Imagine that we have a dataset X = {X ,...,X }, where we assume that X are i.i.d. 1 N n copies drawn from a distribution f X(x;θ). We want to construct an estimator Θ(cid:98) of θ from the dataset X. For example, if f is a Gaussian distribution with an unknown mean θ, we X would like to estimate θ using the sample average Θ(cid:98). In statistics, an estimator Θ(cid:98) is also known as a statistic, which is constructed from the samples. In this book we use the terms “estimator”and“statistic”interchangeably.Writtenasequations,anestimatorisafunction of the samples: Θ(cid:98) = g(X 1,...,X N), (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) estimator functionofX where g is a function that takes the samples X 1,...,X N and returns a random variable Θ(cid:98). For example, the sample average N 1 (cid:88) Θ(cid:98) = N X n n=1 (cid:124) (cid:123)(cid:122) (cid:125) g(X1,...,XN) is an estimator because it is computed by summing the samples X ,...,X and dividing it 1 N by N. What is an estimator? • An estimator Θ(cid:98) is a function of the samples X 1,...,X N: Θ(cid:98) =g(X 1,...,X N). (9.1) • Θ(cid:98) is a random variable. It has a PDF, CDF, mean, variance, etc. Byconstruction,Θ(cid:98) isarandomvariablebecauseitisafunctionoftherandomsamples. Therefore, Θ(cid:98) has its own PDF, CDF, mean, variance, etc. Since Θ(cid:98) is a random variable, we should report both the estimator’s value and the estimator’s confidence when reporting its performance. The confidence measures the quality of Θ(cid:98) when compared to the true parameter θ. It provides a measure of the reliability of the estimator Θ(cid:98). If Θ(cid:98) fluctuates a greatdealwemaynotbeconfidentofourestimates.Let’sconsiderthefollowingexample. 545
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Example 9.1. A class of 1000 students took a test. The distribution of the score is roughly a Gaussian with mean 50 and standard deviation 20. A teaching assistant was too lazy to calculate the true population mean. Instead, he sampled a subset of 5 scores listed as follows: Student ID 1 2 3 4 5 Scores 11 97 1 78 82 He calculated the average, which is 53.8. This is a very good estimate of the class average (which is 50). What is wrong with his procedure? Solution. He was just lucky. It quite possible that if he sampled another 5 scores, he would get something very different. For example, if he looks at the 11 to 15 student scores, he could get: Student ID 11 12 13 14 15 Scores 44 29 19 27 15 In this case the average is 26.8. Both53.8and26.8arelegitimateestimates,buttheyaretherandomrealizations of a random variable Θ(cid:98). This Θ(cid:98) has a PDF, CDF, mean, variance, etc. It may be misleading to simply report the estimated value from a particular instant, so the confidence of the estimator must be specified. DistributionsofΘ(cid:98).WenextdiscussthedistributionofΘ(cid:98).Figure9.1illustratesseveral key ideas. Suppose that the population distribution f (x) is a mixture of two Gaussians. X Let θ be the mean of this distribution (somewhere between the two peak locations). We sample N =50 data points X ,...,X from this distribution. However, the 50 data points 1 N we sample today could differ from the 50 data points we sample tomorrow. If we compute the sample average from each of these finite-sample distributions, we will obtain a set of sampleaveragesΘ(cid:98).Notably,wehaveaset ofΘ(cid:98) becausetodaywehaveoneΘ(cid:98) andtomorrow we have another Θ(cid:98). By plotting the histogram of the sample averages Θ(cid:98), we will have a distribution. The histogram of Θ(cid:98) depends on several factors. According to Central Limit Theorem, the shape of f (θ) is a Gaussian because Θ(cid:98) is the average of N i.i.d. random variables. Θ(cid:98) If Θ(cid:98) is not the average of i.i.d. random variables, the shape is not necessarily a Gaussian. This results in additional complications, so we will discuss some tools for dealing with this problem. The spread of the sample distribution is mainly driven by the number of samples we have in each subdataset. As you can imagine, the more samples we have in a subdataset themoreaccuratethedistribution.Thusyouwillhaveamoreaccuratesampleaverage.The fluctuation of the sample average will also be smaller. Before we continue, let’s summarize the randomness of Θ(cid:98): What is the randomness of Θ(cid:98)? • Θ(cid:98) is generated from a finite-sample dataset. Each time we draw a finite-sample dataset, we introduce randomness. 546
9.1. CONFIDENCE INTERVAL Figure9.1:PictorialillustrationoftherandomnessoftheestimatorΘ(cid:98).Givenapopulation,ourdatasets areusuallyasubsetofthepopulation.Computingthesampleaveragefromthesefinite-sampledistribu- tions introduces the randomness to Θ(cid:98). If we plot the histogram of the sample averages, we will obtain adistribution.Themeanofthisdistributionisthepopulationmean,butthereisanontrivialamountof fluctuation. The purpose of the concept of confidence interval is to quantify this fluctuation. • IfΘ(cid:98) isthesampleaverage,thePDFis(roughly)aGaussian.IfΘ(cid:98) isnotasample average, the PDF is not necessarily a Gaussian. • The spread of the fluctuation depends on the number of samples in each sub- dataset. 9.1.2 Understanding confidence intervals The confidence interval is a probabilistic statement about Θ(cid:98). Instead of studying Θ(cid:98) as a point, we construct an interval (cid:104) (cid:105) I = Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15) , (9.2) for some (cid:15) to be determined. Note that this interval is a random interval: If we have a different realization of Θ(cid:98), we will have a different I. We call I the confidence interval for the estimator Θ(cid:98). Given this random interval, we ask: What is the probability that I includes θ? That means that we want to evaluate the probability (cid:104) (cid:105) P[θ ∈I]=P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) . WeemphasizethattherandomnessinthisprobabilityiscausedbyΘ(cid:98),notθ.Thisisbecause the interval I changes when we conduct a different experiment to obtain a different Θ(cid:98). The 547
CHAPTER 9. CONFIDENCE AND HYPOTHESIS situation is similar to that illustrated on the left-hand side of Figure 9.2. The confidence interval I changes but the true parameter θ is fixed. Figure 9.2: ConfidenceintervalistherandomintervalI =[Θ(cid:98)−(cid:15),Θ(cid:98)+(cid:15)],notthedeterministicinterval [θ−(cid:15),θ+(cid:15)]. The random interval in the former case does not require any knowledge about the true parameter θ, whereas the latter requires θ. By claiming a 95% confidence interval, we say that there is 95% chance that the random interval will include the true parameter. So if you have 100 random realizations of the confidence intervals, then 95 on average will include the true parameter. Confidence intervals can be confusing. Often the confusion arises because of the fol- lowing identity: (cid:104) (cid:105) (cid:104) (cid:105) P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) =P −(cid:15)≤θ−Θ(cid:98) ≤(cid:15) (cid:104) (cid:105) =P −(cid:15)−θ ≤−Θ(cid:98) ≤(cid:15)−θ (cid:104) (cid:105) =P θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15) . (9.3) Although the values of the two probabilities are the same, the two events are interpreted differently. The right-hand side of Figure 9.2 illustrates P[θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15)]. The interval [θ−(cid:15),θ+(cid:15)]isfixed.WhatistheprobabilitythattheestimatorΘ(cid:98) lieswithinthisdeterministic interval? To find this probability, we need to know the true parameter θ, which is not available. By contrast, the other probability P[Θ(cid:98) −(cid:15) ≤ θ ≤ Θ(cid:98) +(cid:15)] does not require any knowledge about the true parameter θ. What is the probability that the true parameter is included inside the random interval? If the probability is high, we say that there is a good chance that our confidence interval will contain the true parameter. This is observed in the left-hand side of Figure 9.2. In practice we often set P[Θ(cid:98) −(cid:15) ≤ θ ≤ Θ(cid:98) +(cid:15)] to be greater than a certain confidence level, say 95%, and then we determine (cid:15). Once we have determined (cid:15), we can claim that 548
9.1. CONFIDENCE INTERVAL with 95% probability the interval [Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15)] will include the unknown parameter θ. We do not need to know θ at any point in this process. To make this more general, we define 1−α as the confidence level for some parame- ter α. For example, if we would like to have a 95% confidence level, we set α=0.05. Then the probability inequality (cid:104) (cid:105) P Θ(cid:98) −(cid:15)≤θ ≤Θ(cid:98) +(cid:15) ≥1−α (9.4) tells us that there is at least a 95% chance that the random interval I =[Θ(cid:98)−(cid:15), Θ(cid:98)+(cid:15)] will include the true parameter θ. In this case we say that I is a “95% confidence interval”. What is a 95% confidence interval? • It is a random interval [Θ(cid:98) −(cid:15),Θ(cid:98) +(cid:15)] such that there is 95% probability for it to include the true parameter θ. • It is not the deterministic interval [θ−(cid:15),θ+(cid:15)], because we never know θ. Example 9.2. After analyzing the life expectancy of people in the United States, it wasconcludedthatthe95%confidenceintervalis(77.8,79.1)yearsold.Isthefollowing claim valid? About 95% of the people in the United States have a life expectancy between 77.8 years old and 79.1 years old. Solution. No. The confidence interval tells us that with 95% probability the random interval (77.8,79.1) will include the true average. We emphasize that (77.8,79.1) is random because it is constructed from a small set of data points. If we survey another set of people we will have another interval. Since we do not know the true average, we do not know the percentage of people whoselifeexpectancyisbetween77.8yearsoldand79.1yearsold.Itcouldbethatthe true average is 80 years old, which is out of the range. It could also be that the true average is 77.9 years old, which is within the range, but only 10% of the population may have life expectancy in (77.8,79.1). Example 9.3. After studying the SAT scores of 1000 high school students, it was concluded that the 95% confidence interval is (1134, 1250) points. Is the following claim valid? There is a 95% probability that the average SAT score in the population is in the range 1134 and 1250. Solution. Yes, but it can be made clearer. The average SAT score in the population remains unknown. It is a constant and it is deterministic, so there is no probability associated with it. A better way to say this is: “There is 95% probability that the random interval 1134 and 1250 will include the average SAT score.” We emphasize that the 95% probability is about the random interval, not the unknown parameter. 549
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 9.1.3 Constructing a confidence interval Let’s consider an example. Suppose that we have a set of i.i.d. observations X ,...,X 1 N that are Gaussians with an unknown mean θ and a known variance σ2. We consider the maximum-likelihood estimator, which is the sample average: N 1 (cid:88) Θ(cid:98) = N X n. n=1 Our goal is to construct a confidence interval. Figure 9.3: Conceptual illustration of how to construct a confidence interval. Starting with the pop- ulation, we draw random subsets. Each random subset gives us an estimator, and correspondingly an interval. Before we consider the equations, let’s look at a graph illustrating what we want to achieve. Figure 9.3 shows a population distribution, which is a Gaussian in this example. WedrawN samplesfromtheGaussiantoconstructarandomsubset.Basedonthisrandom subsetweconstructtheestimatorΘ(cid:98).Sincethisestimatorisbasedontheparticularrandom subset we have, we can follow the same approach by drawing another random subset. To differentiate the estimators constructed by the different random subsets, let’s call the esti- matorsΘ(cid:98)(1) andΘ(cid:98)(2),respectively.Foreachestimatorweconstructaninterval[Θ(cid:98)−(cid:15), Θ(cid:98)+(cid:15)] to obtain two different intervals: I1 =[Θ(cid:98)(1)−(cid:15), Θ(cid:98)(1)+(cid:15)] and I2 =[Θ(cid:98)(2)−(cid:15), Θ(cid:98)(2)+(cid:15)]. If we can determine (cid:15), we have found the confidence interval. We can determine the confidence interval by observing the histogram of Θ(cid:98), which in our case is the histogram of the sample average, since the histogram of Θ(cid:98) is well-defined, 550
9.1. CONFIDENCE INTERVAL especially if we are looking at the sample average. The histogram of the sample average is a Gaussianbecausetheaverage ofN i.i.d.GaussianrandomvariablesisGaussian.Therefore, the width of this Gaussian is determined by the answer to this question: For what (cid:15) can we cover 95% of the histogram of Θ(cid:98)? To find the answer, we set up the following probability inequality:   |Θ(cid:98) −E[Θ(cid:98)]| P  (cid:113) ≤(cid:15)≥1−α. Var[Θ(cid:98)] This probability says that we want to find an (cid:15) such that the majority of Θ(cid:98) is living close to its mean. The level 1−α is our confidence level, which is typically 95%. Equivalently, we let α=0.05. In the above equation, we can define the quotient as def Θ(cid:98) −E[Θ(cid:98)] Z(cid:98) = (cid:113) . Var[Θ(cid:98)] We know that Z(cid:98) is a zero-mean unit-variance Gaussian because it is the standardized vari- able.[Note:NotallnormalizedvariablesareGaussian,butifΘ(cid:98) isaGaussianthenormalized variable will remain a Gaussian.] Thus, the probability inequality we are looking at is (cid:104) (cid:105) P |Z(cid:98)|≤(cid:15) ≥ 1−α. (cid:124) (cid:123)(cid:122) (cid:125) twotailsofastandardGaussian The PDF of Z(cid:98) is shown in Figure 9.4. As you can see, to achieve 95% confidence we need to pick an appropriate (cid:15) such that the shaded area is less than 5%. 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 (cid:113) Figure 9.4: PDF of the random variable Z(cid:98) = (Θ(cid:98) −E[Θ(cid:98)])/ Var[Θ(cid:98)]. The shaded area denotes the α=0.05 confidence level. Since P[Z(cid:98)≤(cid:15)] is the CDF of a Gaussian, it follows that P[|Z(cid:98)|≤(cid:15)]=P[−(cid:15)≤Z(cid:98)≤(cid:15)] =P[Z(cid:98)≤(cid:15)]−P[Z(cid:98)≤−(cid:15)] =Φ((cid:15))−Φ(−(cid:15)). 551
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Using the symmetry of the Gaussian, it follows that Φ(−(cid:15))=1−Φ((cid:15)) and hence P[|Z(cid:98)|≤(cid:15)]=2Φ((cid:15))−1. Equating this result with the probability inequality P[|Z(cid:98)|≤(cid:15)]≥1−α, we have that (cid:16) α(cid:17) (cid:15)≥Φ−1 1− . 2 The remainder of this problem is solvable on a computer. On MATLAB, we can call icdf to compute the inverse CDF of a standard Gaussian. On Python, the command is stats.norm.ppf. The commands are as shown below. % MATLAB code to compute the width of the confidence interval alpha = 0.05; mu = 0; sigma = 1; % Standard Gaussian epsilon = icdf(’norm’,1-alpha/2,mu,sigma) # Python code to compute the width of the confidence interval import scipy.stats as stats alph = 0.05; mu = 0; sigma = 1; # Standard Gaussian epsilon = stats.norm.ppf(1-alph/2, mu, sigma) print(epsilon) If everything is done properly, we see that for a 95% confidence level (α =0.05) the corre- sponding (cid:15) is (cid:15)=1.96. Afterdetermining(cid:15),itremainstodetermineE[Θ(cid:98)]andVar[Θ(cid:98)]inordertocompletethe probability inequality. To this end, we note that (cid:34) N (cid:35) 1 (cid:88) E[Θ(cid:98)]=E N X n =θ, n=1 (cid:34) 1 (cid:88)N (cid:35) σ2 Var[Θ(cid:98)]=Var N X n = N, n=1 if we assume that the population distribution is Gaussian(θ,σ2), where θ is unknown but σ is known. Substituting these into the probability inequality, we have that   |Θ(cid:98) −E[Θ(cid:98)]| (cid:20) σ σ (cid:21) P  (cid:113) ≤(cid:15)=P Θ(cid:98) −(cid:15)√ ≤θ ≤Θ(cid:98) +(cid:15)√ N N Var[Θ(cid:98)] (cid:20) (cid:21) σ σ =P Θ(cid:98) −1.96√ ≤θ ≤Θ(cid:98) +1.96√ , N N where we let (cid:15)=1.96 for a 95% confidence level. Therefore, the 95% confidence interval is (cid:20) (cid:21) σ σ Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ . (9.5) N N Asyoucansee,wedonotneedtoknowthevalueofθ atanypointofthederivationbecause the confidence interval in Equation (9.5) does not involve θ. This is an important difference with the other probability P[θ−(cid:15)≤Θ(cid:98) ≤θ+(cid:15)], which requires θ. 552
9.1. CONFIDENCE INTERVAL How to construct a confidence interval • Compute the estimator Θ(cid:98). • Determine the width of the confidence interval (cid:15) by inspecting the confidence level 1−α. If Θ(cid:98) is Gaussian, then (cid:15)=Φ−1(1− α). 2 • If Θ(cid:98) is not a Gaussian, replace the Gaussian CDF by the CDF of Θ(cid:98). • The confidence interval is [Θ(cid:98) −(cid:15), Θ(cid:98) +(cid:15)]. 9.1.4 Properties of the confidence interval Some important properties of the confidence interval are listed below. • Probability of Θ(cid:98) is the same as probability of Z(cid:98). First, the two random variables Θ(cid:98) and Z(cid:98) have a one-to-one correspondence. We proved the following in Chapter 6: If Θ(cid:98) ∼Gaussian(θ,σ2), then N Z(cid:98)d =ef Θ(cid:98) √−θ ∼Gaussian(0,1). (9.6) σ/ N For example, if Θ(cid:98) ∼ Gaussian(θ,σ2) with N = 1, θ = 1 and σ = 2, then a 95% N confidence level is 0.95≈P[−1.96≤Z(cid:98)≤1.96], (Z(cid:98) is within 1.96 std from Z(cid:98)’s mean) Θ(cid:98) −θ =P[−1.96≤ √ ≤1.96] σ/ N (cid:20) (cid:21) σ σ =P θ−1.96√ ≤Θ(cid:98) ≤θ+1.96√ N N =P[−2.92≤Θ(cid:98) ≤4.92]. (Θ(cid:98) is within 1.96 std from Θ(cid:98)’s mean) NotethatwhiletherangeforZ(cid:98) isdifferentfromtherangeforΘ(cid:98),theybothreturnthe sameprobability.TheonlydifferenceisthatΘ(cid:98) isconstructedbeforethenormalization and Z(cid:98) is constructed after the normalization. • Standard error.InthisestimationproblemweknowthatΘ(cid:98) isthesampleaverage.We assume that the mean θ is unknown but the variance Var[Θ(cid:98)] is known. The standard deviation of Θ(cid:98) is called the standard error: (cid:113) σ se= Var[Θ(cid:98)]= √ . (9.7) N • Critical value. The value 1.96 in our example is often known as the critical value. It is defined as (cid:16) α(cid:17) z =Φ−1 1− . (9.8) α 2 553
CHAPTER 9. CONFIDENCE AND HYPOTHESIS The z value gives us a multiplier applied to the standard error that will result in a α valuewithintheconfidenceinterval.Thisisbecause,bythedefinitionoftheconfidence interval, the interval is (cid:20) σ σ (cid:21) (cid:104) (cid:105) Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ = Θ(cid:98) −z αse, Θ(cid:98) +z αse N N • Margin of error. The margin of error is defined as σ margin of error=z √ . (9.9) α N The margin of error is also the width of the confidence interval. As the name implies, the margin of error tells us how much error the confidence interval includes when predicting the population parameter. Practice Exercise 9.1. Suppose that the number of photos a Facebook user uploads per day is a random variable with σ = 2. In a set of 341 users, the sample average is 2.9. Find the 90% confidence interval of the population mean. Solution. We set α=0.1. The z -value is α (cid:16) α(cid:17) z =Φ−1 1− =1.6449. α 2 The 90% confidence interval is then (cid:20) (cid:21) 2 2 Θ(cid:98) −1.64√ , Θ(cid:98) +1.64√ =[2.72,3.08]. 341 341 Therefore,with90%probability,theinterval[2.72,3.08]includesthepopulationmean. Example 9.4. Professional cyber-athletes have a standard deviation of σ = 73.4 actions per minute. If we want to estimate the average actions per minute of the population, how many samples are needed to obtain a margin of error < 20 at 90% confidence? Solution. With a 90% confidence level, the z -value is α (cid:16) α(cid:17) z =Φ−1 1− =Φ−1(0.95)=1.645. α 2 The margin of error is 20. So we have z α√σ =20. Moving around the terms gives us N (cid:16) σ (cid:17)2 N ≥ z =36.45. α20 Therefore, we need at least N = 37 samples to ensure a margin of error of < 20 at a 90% confidence level. 554
9.1. CONFIDENCE INTERVAL Figure 9.5: Relationships between the standard error se, the z value, and the margin of error. The α confidence level α is the area under the curve for the tails of each PDF. Theconceptsofstandarderrorse,thez value,andthemarginoferroraresummarized α in Figure 9.5. The left-hand side is the PDF of Z(cid:98). It is the normalized random variable, whichisalsothestandardGaussian.Theright-handsideisthePDFofΘ(cid:98),theunnormalized random variable. The z α value is located in the Z(cid:98)-space. It defines the range of Z(cid:98) in the PDF within which we are confident about the true parameter. The corresponding value in the Θ(cid:98)-space is the margin of error. This is found by multiplying z α with the standard deviation of Θ(cid:98), known as the standard error. Correspondingly, in the Z(cid:98)-space the standard deviation is the unity. Two further points about the confidence interval should be mentioned: • Number of Samples N.TheconfidenceintervalisafunctionofN.Asweincreasethe numberofsamples,thedistributionoftheestimatorΘ(cid:98) becomesnarrower.Specifically, if Θ(cid:98) follows a Gaussian distribution (cid:18) σ2(cid:19) Θ(cid:98) ∼Gaussian θ, , N p then Θ(cid:98) →θ as N →∞. Figure 9.6 illustrates a few examples of Θ(cid:98) as N grows. In the limit when N →∞, we observe that the interval becomes (cid:20) σ σ (cid:21) (cid:104) (cid:105) Θ(cid:98) −1.96√ , Θ(cid:98) +1.96√ −→ Θ(cid:98), Θ(cid:98) =Θ(cid:98). N N (cid:104) (cid:105) In this case, the statement θ ∈ Θ(cid:98) −1.96√σ , Θ(cid:98) +1.96√σ becomes θ = Θ(cid:98). That N N means the estimator Θ(cid:98) returns the correct true parameter θ. Of course, it is possible that E[Θ(cid:98)] (cid:54)= θ, i.e., the estimator is biased. In that case, having more samples will approach another estimate that is not θ. • Distribution of Z(cid:98). When defining the confidence interval we constructed an interme- diate variable Θ(cid:98) −θ Z(cid:98)= √ . σ/ N Since X ’s are i.i.d. Gaussian, it follows that Z is also Gaussian. This gives us a way n to calculate the probability using the standard Gaussian table. What happens when X ’s are not Gaussian? The good news is that even if X ’s are not Gaussian, for n n 555
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 4 3.5 N = 10 N = 25 3 N = 100 2.5 2 1.5 1 0.5 0 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 Figure 9.6: The PDF of Θ(cid:98) as the number of samples N grows. Here, we assume that X n are i.i.d. Gaussian random variables with mean θ=0 and variance σ2 =1. sufficiently large N, the random variable Θ(cid:98) is more or less Gaussian, because of the Central Limit Theorem. Therefore, even if X ’s are not Gaussian we can still use the n Gaussian probability table to construct α and (cid:15). 9.1.5 Student’s t-distribution In the discussions above, we estimate the population mean θ using the estimator Θ(cid:98). The assumption was that the variance σ2 was known a priori and hence is fixed. In practice, however, there are many situations where σ2 is not known. Thus we not only need to use the mean estimator Θ(cid:98) but also the variance estimator S(cid:98), which can be defined as N S(cid:98)2 d =ef N1 −1 (cid:88) (X n−Θ(cid:98))2, n=1 where Θ(cid:98) is the estimator of the mean. What is the confidence interval for Θ(cid:98)? For a confidence interval to be valid, we expect it to take the form of (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −z α√ , Θ(cid:98) +z α√ , N N which is essentially the confidence interval we have just derived but with σ replaced by S(cid:98). However, there is a problem with this. When we derive the confidence interval assuming a known σ, the z value is determined by checking the standard Gaussian α Θ(cid:98) −θ Z(cid:98)= √ , σ/ N which gives us z α = Φ−1(1−α/2). The whole derivation is based on the fact that Z(cid:98) is a standard Gaussian. Now that we have replaced σ by S(cid:98), the new random variable def Θ(cid:98) −θ T = √ (9.10) S(cid:98)/ N 556
9.1. CONFIDENCE INTERVAL is not a standard Gaussian. It turns out that the distribution of T is Student’s t-distribution with N −1 degrees of freedom. The PDF of Student’s t-distribution is given as follows. Definition 9.1. If X is a random variable following Student’s t-distribution of ν degrees of freedom, then the PDF of X is Γ(cid:0)ν+1(cid:1) (cid:18) x2(cid:19)−ν+ 21 f (x)= √ 2 1+ . (9.11) X νπΓ(cid:0)ν(cid:1) ν 2 We may compare Student’s t-distribution with the Gaussian distribution. Figure 9.7 shows the standard Gaussian and several t distributions with ν =N −1 degrees of freedom. Note that Student’s t-distribution has a similar shape to the Gaussian but it has a heavier tail. 0.4 0.35 Gaussian(0,1) t-dist, N = 11 0.3 t-dist, N = 3 0.25 t-dist, N = 2 0.2 0.15 0.1 0.05 0 -5 -4 -3 -2 -1 0 1 2 3 4 5 Figure 9.7: The PDF of Student’s t-distribution with ν =N −1 degrees of freedom. Since T = Θ(cid:98)√−θ is a t-random variable, to determine the z α value we can follow the S(cid:98)/ N same procedure by considering the CDF of T. Let the CDF of the Student’s t-distribution with ν degrees of freedom be Ψ (z)=CDF of X at z. ν If we want P[|T|≤z ]=1−α, it follows that α (cid:16) α(cid:17) z =Ψ−1 1− . (9.12) α ν 2 Therefore, the new confidence interval, assuming an unknown S(cid:98), is (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −z α√ , Θ(cid:98) +z α√ , N N with z defined in Equation (9.12), using ν =N −1. α 557
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Practice Exercise 9.2.AsurveyaskedN =14peoplefortheirratingofamovie.As- sumethatthemeanestimatorisΘ(cid:98) andthevarianceestimatorisS(cid:98).Findtheconfidence interval. Solution. If we use Student’s t-distribution, it follows that (cid:16) α(cid:17) z =Ψ−1 1− =2.16, α 13 2 where the degrees of freedom are ν =14−1=13. Thus the confidence interval is (cid:34) (cid:35) S(cid:98) S(cid:98) I = Θ(cid:98) −2.16√ , Θ(cid:98) +2.16√ . N N The MATLAB and Python codes to report the z value of a Student’s t-distribution α are shown below. They are both called through the inverse CDF function. In MATLAB it is icdf, and in Python it is stats.t.ppf. % MATLAB code to compute the z_alpha value of t distribution alpha = 0.05; nu = 13; z = icdf(norm,1-alpha/2,nu) # Python code to compute the z_alpha value of t distribution import scipy.stats as stats alph = 0.05 nu = 13 z = stats.t.ppf(1-alph/2, nu) print(z) Example 9.5. A class of 10 students took a midterm exam. Their scores are given in the following table. Student 1 2 3 4 5 6 7 8 9 10 Score 72 69 75 58 67 70 60 71 59 65 Find the 95% confidence interval. Solution.ThemeanandstandarddeviationofthedatasetsarerespectivelyΘ(cid:98) =66.6 and S(cid:98)=5.61. The critical z α value is determined by Student’s t-distribution: (cid:16) α(cid:17) z =Ψ−1 1− =2.26. α 9 2 558
9.1. CONFIDENCE INTERVAL The confidence interval is (cid:34) (cid:35) S(cid:98) S(cid:98) Θ(cid:98) −z α√ , Θ(cid:98) +z α√ =[62.59,70.61]. N N Therefore, with 95% probability, the interval [62.59,70.61] will include the true popu- lation mean. Remark 1. Make sure you understand the meaning of “population mean” in this example.Sincewehavetenstudents,isn’tthepopulationmeanjusttheaverageofthe ten scores? This is incorrect. In statistics, we assume that these ten students are the realizationsofsomeunderlying(unknown)randomvariableX withsomePDFf (x). X The population mean θ is therefore the expectation E[X], where the expectation is taken w.r.t. f X. The sample average Θ(cid:98), which is the average of the ten numbers, is an estimator of the population mean θ. Remark 2. You may be wondering why we are using Student’s t-distribution here when we do not even know the PDF of X. The answer is that it is an approximation. When X is Gaussian, the sample average Θ(cid:98) is a Student’s t-distribution, assuming that the variance is approximated by the sample variance S(cid:98). This result is attributed to the original paper of William Gosset, who developed Student’s t-distribution. Theaboveexamplecanbesolvedcomputationally.AnimplementationthroughPython is given below, and the MATLAB implementation is straightforward. # Python code to generate a confidence interval import numpy as np import scipy.stats as stats x = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65]) N = x.size Theta_hat = np.mean(x) # Sample mean S_hat = np.std(x) # Sample standard deviation nu = x.size-1 # degrees of freedom alpha = 0.05 # confidence level z = stats.t.ppf(1-alph/2, nu) CI_L = Theta_hat-z*S_hat/np.sqrt(N) CI_U = Theta_hat+z*S_hat/np.sqrt(N) print(CI_L, CI_U) What is Student’s t-distribution? • It was developed by William Gosset in 1908. When he published the paper he used the pseudonym Student. • We use Student’s t-distribution to model the estimator Θ(cid:98)’s PDF when the vari- ance σ2 is replaced by the sample variance S(cid:98)2. • Student’s t-distribution has a heavier tail than a Gaussian. 559
CHAPTER 9. CONFIDENCE AND HYPOTHESIS 9.1.6 Comparing Student’s t-distribution and Gaussian We now discuss an important theoretical result regarding the relationship between a Stu- dent’s t-distribution and Gaussian distribution. The main result is that the standard Gaus- sian is a limiting distribution of the t distribution as the degrees of freedom ν →∞. Theorem 9.1. Asν →∞,theStudent’st-distributionapproachesthestandardGaus- sian distribution: νl →im ∞(cid:40) √Γ ν(cid:0) πν Γ+ 2 (cid:0)1 ν(cid:1) (cid:1)(cid:18) 1+ y ν2(cid:19)−ν+ 21(cid:41) = √1 2πe−t 22 . (9.13) 2 The proof of the theorem requires Stirling’s approximation, which is not essential for this book. Feel free to skip it if needed. Proof. There are two results we need to use: (cid:113) • Stirling’s approximation:2 Γ(z)≈ 2π (cid:0)z(cid:1)z . z e • Exponential approximation: (1+ x)−k →e−x, as k →∞. k We have that Γ(cid:0)ν+1(cid:1) (cid:113) ν2 +π 1 (cid:0)ν 2+ e1(cid:1)ν+ 21 √ νπΓ2 (cid:0)ν 2(cid:1) ≈ √ νπ2 (cid:113) 2 νπ (cid:0) 2ν e(cid:1)ν 2 2 √ (cid:114) (cid:18) (cid:19)ν 1 ν 1 ν+1 2 ν+1 = √ √ √ νπ ν+1 e ν ν √ (cid:18) (cid:19)ν 1 ν ν+1 2 = √ √ νπ 2e ν (cid:18) (cid:19)ν 1 1 2 = √ 1+ . 2πe ν Putting a limit of ν →∞, we have that (cid:18) (cid:19)ν lim √1 1+ 1 2 = √1 e1 2 = √1 . ν→∞ 2πe ν 2πe 2π The other limit follows from the fact that lim (cid:18) 1+ t2(cid:19)−ν+ 21 =e−t 22 . ν→∞ ν Combining the two limits proves the theorem. (cid:3) 2K.G.Binmore,Mathematicalanalysis:Astraightforwardapproach.CambridgeUniversityPress,1977. Section17.7.2. 560
9.2. BOOTSTRAPPING End of the proof. Please join us again. This theorem has several implications: • When N is large, S2 → σ2. The Gaussian approximation kicks in, and so Student’s t-distribution is more or less the same as the Gaussian. • Student’s t-distribution is better for small N, usually N ≤ 30. If N ≥ 30, using the Gaussian approximation suffices. • If X is Gaussian, Student’s t-distribution is an excellent model. If X is not Gaussian, Student’s t-distribution will have some issues unless N increases. 9.2 Bootstrapping When estimating the confidence interval, we focus exclusively on the sample average Θ(cid:98) = (1/N)(cid:80)N X . There are, however, many estimators that are not sample averages. For n=1 n example, we might be interested in an estimator that estimates the sample median: Θ(cid:98) = median{X ,...,X }. In such cases, the Gaussian-based analysis or the Student’s t-based 1 N analysis we just derived would not work. Steppingbackalittlefurther,itisimportanttounderstandthehierarchyofestimation. Figure 9.8 illustrates a rough breakdown of the various techniques. On the left-hand side of the tree, we have three point estimation methods: MLE, MAP, and MMSE. They are so-called point estimation methods because they are reporting a point — a single value. Thisstandsincontrasttotheright-handsideofthetree,inwhichwereporttheconfidence interval. Note that point estimates and confidence intervals do not conflict with each other. Thepointestimatesareusedfortheactualengineeringsolutionandtheconfidenceintervals areusedtoreporttheconfidenceaboutthepointestimates.Underthebranchofconfidence intervals we discussed sample average. However, if we want to study an estimator that is not the sample average, we need the technique known as the bootstrapping — a method forestimatingtheconfidenceinterval.Notably,itdoesnotgiveyouabetterpointestimate. As we have frequently emphasized, since Θ(cid:98) is a random variable, it has its own PDF, CDF, mean, variance, etc. The confidence interval introduced in the previous section pro- videsonewaytoquantifytherandomnessofΘ(cid:98).Throughoutthederivationoftheconfidence interval we need to estimate the variance Var(Θ(cid:98)). For simple problems such as the sample average,analyzingVar(Θ(cid:98))isnotdifficult.However,ifΘ(cid:98) isamorecomplicatedstatistic,e.g., the median, analyzing Var(Θ(cid:98)) may not be as straightforward. Bootstrapping is a technique that is suitable for this purpose. Why is it difficult to provide a confidence interval for estimators such as the median? A couple of difficulties arise: • Many estimators do not have a simple expression for the variance. For simple esti- mators such as the sample average Θ(cid:98) =(1/N)(cid:80)N n=1X n, the variance is σ2/N. If the 561
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.8: Hierarchy of estimation. Bootstrapping belongs to the category of confidence interval. It is used to report the confidence intervals for estimators that are not the sample averages. estimator is the median Θ(cid:98) = median{X 1,...,X N}, the variance of Θ(cid:98) will depend on theunderlyingdistributionoftheX ’s.Iftheestimatorissomethingbeyondthesam- n ple median, the variance of Θ(cid:98) can be even more complicated to determine. Therefore, techniques such as Central Limit Theorem do not apply here. • Wetypicallyhaveonlyone setofdatapoints.Wecannotre-collectmorei.i.d.samples to estimate the variance of the estimator. Therefore, our only option is to squeeze the information from the data we have been given. When do we use bootstrapping? • Bootstrapping is a technique to estimate the confidence interval. • We use bootstrapping when the estimator does not have a simple expression for the variance. • Bootstrappingallowsustoestimatethevariancewithoutre-collectingmoredata. • Bootstrapping does not improve your point estimates. 9.2.1 A brute force approach Before we discuss the idea of bootstrapping, we need to elaborate on the difficulty of esti- matingthevarianceusingrepeatedmeasurements.Supposethatwesomehowhaveaccessto the population distribution. Let us denote the CDF of this population distribution by F , X and the PDF by f . By having access to the population distribution we can synthetically X generate as many samples X ’s as we want. This is certainly hypothetical, but let’s assume n that it is possible for now. If we have full access to the population distribution, then we are able to draw K replicate datasets X1,...,XK from F : X 562
9.2. BOOTSTRAPPING X(1) ={X(1),...,X(1)}∼F , 1 N X X(2) ={X(2),...,X(2)}∼F , (9.14) 1 N X . . . X(K) ={X(K),...,X(K)}∼F . 1 N X Each dataset X(K) contains N data points, and by virtue of i.i.d. all the samples have the same underlying distribution F . X For each dataset we construct an estimator Θ(cid:98) = g(·) for some function g(·). The estimator takes the data points of the dataset X and returns a value. Since we have K datasets, correspondingly we will have K estimators: Θ(cid:98)(1) =g(X(1))=g(X(1),...,X(1)), 1 N Θ(cid:98)(2) =g(X(2))=g(X(2),...,X(2)), (9.15) 1 N . . . Θ(cid:98)(K) =g(X(K))=g(X(K),...,X(K)). 1 N Note that these estimators g(·) can be anything. It can be the sample average or it can be the sample median. There is no restriction. SinceweareinterestedinconstructingtheconfidenceintervalforΘ(cid:98),weneedtoanalyze the mean and variance of Θ(cid:98). The true mean and the estimated mean of Θ(cid:98) are E[Θ(cid:98)]=true mean of Θ(cid:98), (9.16) M(Θ(cid:98))=estimated mean based on Θ(cid:98)(1),...,Θ(cid:98)(K) K K d =ef 1 (cid:88) Θ(cid:98)(k) = 1 (cid:88) g(X(k)), (9.17) K K k=1 k=1 respectively. Similarly, the true variance and the estimated variance of Θ(cid:98) are Var[Θ(cid:98)]=true variance of Θ(cid:98), (9.18) V(Θ(cid:98))=estimated variance based on Θ(cid:98)(1),...,Θ(cid:98)(K) K d =ef 1 (cid:88)(cid:16) Θ(cid:98)(k)−M(Θ(cid:98))(cid:17)2 K k=1 K 1 (cid:88)(cid:16) (cid:17)2 = g(X(k))−M(Θ(cid:98)) . (9.19) K k=1 These two equations should be familiar: Since Θ(cid:98) is a random variable, and {Θ(cid:98)(k)} are i.i.d. copies of Θ(cid:98), we can compute the average of Θ(cid:98)(1),...,Θ(cid:98)(K) and the corresponding variance. AsthenumberofrepeatedtrialsK approaches∞,theestimatedvarianceV(Θ(cid:98))willconverge to Var(Θ(cid:98)) according to the law of large numbers. Wecansummarizetheprocedurewehavejustoutlined.Toproduceanestimateofthe variance, we run the algorithm below. 563
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Algorithm 1: Brute force method to generate an estimated variance • Assume: We have access to F . X • Step 1: Generate datasets X(1),...,X(K) from F . X • Step 2: Compute M(Θ(cid:98)) and V(Θ(cid:98)) based on the samples. • Output: The estimated variance is V(Θ(cid:98)). The problem, however, is that we only have one dataset X(1). We do not have access to X(2),...,X(K), and we do not have access to F . Therefore, we are not able to approxi- X matethevarianceusingtheabovebruteforcesimulation.Bootstrappingisacomputational technique to mimic the above simulation process by using the available data in X(1). 9.2.2 Bootstrapping The idea of bootstrapping is illustrated in Figure 9.9. Imagine that we have a population CDFF andPDFf .Thedatasetwehaveinhand,X,isacollectionoftherandomrealiza- X X tionsoftherandomvariableX.ThisdatasetX containsN datapointsX ={X ,...,X }. 1 N Figure 9.9: A conceptual illustration of bootstrapping. Given the observed dataset X, we synthetically construct K bootstrapped datasets (colored in yellow) by sampling with replacement from X. We then compute the estimators, e.g., computing the median, for every bootstrapped dataset. Finally, we construct the estimator’s histogram (in blue) to compute the bootstrapped mean and variance. In bootstrapping, we synthesize K bootstrapped datasets Y(1),...,Y(K), where each bootstrappeddatasetY(k) consistsofN samplesredrawnfromX.Essentially,wedrawwith replacement N samples from the observed dataset X: Y(1) ={Y(1),...,Y(1)}=N random samples from X, 1 N . . . Y(K) ={Y(K),...,Y(K)}=N random samples from X. 1 N 564
9.2. BOOTSTRAPPING Afterward,weconstructourestimatorΘ(cid:98) accordingtoourdesiredfunctiong(·).Forexample, if g(·)= median, we have Θ(cid:98)(1) =g(Y(1))=median(Y(1)), boot . . . Θ(cid:98)(K) =g(Y(K))=median(Y(K)). boot Then, we define the bootstrapped mean and the bootstrapped variance as K M boot(Θ(cid:98))= K1 (cid:88) Θ(cid:98)( bk o) ot, (9.20) k=1 K V boot(Θ(cid:98))= K1 (cid:88)(cid:16) Θ(cid:98)( bk o) ot−M boot(Θ(cid:98))(cid:17)2 . (9.21) k=1 The procedure we have just outlined can be summarized as follows. Algorithm 2: Bootstrapping to generate an estimated variance • Assume: We do NOT have access to F , but we have one dataset X. X • Step 1: Generate datasets Y(1),...,Y(K) from X, by sampling with replacement from X. • Step 2: Compute M boot(Θ(cid:98)) and V boot(Θ(cid:98)) based on the samples. • Output: The bootstrapped variance is V boot(Θ(cid:98)). The only difference between this algorithm and the previous one is that we are not synthe- sizing data from the population but rather from the observed dataset X. What makes bootstrapping work? The basic principle of bootstrapping is based on three approximations: (a) Var F(Θ(cid:98)) ≈ V full(Θ(cid:98)) )b( ≈ (c) Var F(cid:98)(Θ(cid:98)) ≈ V boot(Θ(cid:98)) In this set of equations, the ultimate quantity we want to know is Var F(Θ(cid:98)), which is the variance of Θ(cid:98) under F. (By “under F” we mean that the variance was found by integrating with respect to the distribution F .) However, since we do not have access to F, we have X to approximate Var F(Θ(cid:98)) by V full(Θ(cid:98)). V full(Θ(cid:98)) is the sample variance computed from the K hypothetical datasets X(1),...,X(K). We call it “full” because we can generate as many hypothetical datasets as we want. It is marked as the approximation (a) above. In the bootstrapping world, we approximate the underlying distribution F by some other distribution F(cid:98). For example, if F is the CDF of a Gaussian distribution, we can chooseF(cid:98) tobethefinite-samplestaircasefunctionapproximatingF.Inourcase,weusethe observeddatasetX toserveasaproxyF(cid:98)toF.Thisisthesecondapproximation,markedby 565
CHAPTER 9. CONFIDENCE AND HYPOTHESIS (b). Normally, if you have a reasonably large X, it is safe to assume that this finite-sample dataset X has a CDF F(cid:98) that is close to the true CDF F. The third approximation is to find a numerical estimate Var (Θ(cid:98)) via the simulation F(cid:98) procedure we have just outlined. This is essentially the same line of argument for (a) but now applied to the bootstrapping world. We mark this approximation by (c). Its goal is to approximate Var F(cid:98)(Θ(cid:98)) via V boot(Θ(cid:98)). The three approximations have their respective influence on the accuracy of the boot- strapped variance: How does bootstrapping work? • It is based on three approximations: • (a): A hypothetical approximation. The best we can do is that we have access to F. It is practically impossible to achieve, but it gives us intuition. • (b): Approximate F by F(cid:98), where F(cid:98) is the empirical CDF of the observed data. This is usually the source of error. The approximation error reduces when you use more samples to approximate F. • (c): Approximate the theoretical bootstrapped variance by a finite approxima- tion.Thisapproximationerrorisusuallysmallbecauseyoucangenerateasmany bootstrapped datasets as you want. One “mysterious” property of bootstrapping is the sampling with replacement scheme used to synthesize the bootstrapped samples. The typical questions are: • (1) Why does sampling from the observed dataset X lead to meaningful boot- strapped datasets Y(1),...,Y(K)? To answer this question we consider the following toy example. Suppose we have a dataset X containing N = 20 samples, as shown below. X = [0 0 0 0 0 0 1 1 1 1 2 2 2 2 2 2 2 2 2 2] ThisdatasetisgeneratedfromarandomvariableX withaPDFf(cid:98)havingthreestates: 0 (30%), 1 (20%), 2 (50%). As we draw samples from X, the percentage of the states will determine the likelihood of one state being drawn. For example, if we randomly pick a sample Y from X, we have a 30% chance of having Y to be 0, 20% chance n n of having it to be 1, and 50% chance of having it to be 2. Therefore, the PDF of Y n (the randomly drawn sample from X) will be 0 (30%), 1 (20%), 2 (50%), the same as the original PDF. If you think about this problem more deeply, by “sampling with replacement” we essentially assign each X with an equal probability of 1/N. If one n of the states is more popular, the individual probabilities will add to form a higher probability mass. • (2) Why can’t we do sampling without replacement, aka permutation? We need to understandthatsamplingwithoutreplacementisthesameaspermutingthedatainX. BypermutingthedatainX,thesimpleprobabilityassignmentssuchasP[X =0]= 6 , 20 P[X =1]= 4 and P[X =2]= 10 will be destroyed. Moreover, permuting the data 20 20 does not change the mean and variance of the data because we are only shuffling the order. As far as constructing the confidence interval is concerned, shuffling the order is not useful. 566
9.2. BOOTSTRAPPING On computers it is easy to generate the bootstrapped dataset, along with their mean and variance. In MATLAB the key step is to call a for loop. Inside the for loop, we draw N randomindicesrandifrom1toN andpickthesamples.TheestimatorThetahatisthen constructed by calling your target estimator function g(·). In this example the estimator is the median. After the for loop, we compute the mean and variance of Θ(cid:98). These are the bootstrapped mean and variance, respectively. % MATLAB code to estimate a bootstrapped variance X = [72, 69, 75, 58, 67, 70, 60, 71, 59, 65]; N = size(X,2); K = 1000; Thetahat = zeros(1,K); for i=1:K % repeat K times idx = randi(N,[1, N]); % sampling w/ replacement Y = X(idx); Thetahat(i) = median(Y); % estimator end M = mean(Thetahat) % bootstrapped mean V = var(Thetahat) % bootstrapped variance The Python commands are similar. We call np.random.randint to generate random integers and we pick samples according to Y = X[idx]. After generating the bootstrapped dataset, we compute the bootstrap estimators Thetahat. # Python code to estimate a bootstrapped variance import numpy as np X = np.array([72, 69, 75, 58, 67, 70, 60, 71, 59, 65]) N = X.size K = 1000 Thetahat = np.zeros(K) for i in range(K): idx = np.random.randint(N, size=N) Y = X[idx] Thetahat[i] = np.median(Y) M = np.mean(Thetahat) V = np.var(Thetahat) After we have constructed the bootstrapped variance, we can define the bootstrapped standard error as (cid:113) s (cid:98)e boot = V boot(Θ(cid:98)). (9.22) Accordingly we define the bootstrapped confidence interval as (cid:2) (cid:3) I = Θ(cid:98) −z αs (cid:98)e boot, Θ(cid:98) +z αs (cid:98)e boot , (9.23) where z is the critical value of the Gaussian. α The validity of the confidence intervals constructed by bootstrapping is subject to the validity of z α. If Θ(cid:98) is roughly a Gaussian, the bootstrapped confidence interval will be 567
CHAPTER 9. CONFIDENCE AND HYPOTHESIS reasonablygood.IfΘ(cid:98) isnotGaussian,thereareadvancedmethodstoreplacez α withbetter estimates. This topic is beyond the scope of this book; we refer interested readers to Larry Wasserman, All of Statistics, Springer 2003, Chapter 8. 9.3 Hypothesis Testing Imagine that you are a vaccine company developing COVID-19 vaccines. You gave the vaccine to 934 patients, and 928 patients have developed antigens. How confident can you be that your vaccine is effective? Questions like this are becoming more common nowadays insituationsinwhichweneedtomakestatisticallyinformedchoicesbetweenYESandNO. The subject of this section is hypothesis testing — a principled statistical procedure used to evaluate statements that should be accepted or rejected. 9.3.1 What is a hypothesis? A hypothesis is a statement that requires testing by observation to determine whether it is true or false. A few examples: • The coin is unbiased. • Students entering the graduate program have GPA ≥3. • More people like orange juice than lemonade. • Algorithm A performs better than Algorithm B. As you can see from these examples, a hypothesis is something we can test based on the data.Therefore,being“correct”or“wrong”dependsonthestatisticswehaveandthecutoff threshold. Accepting or rejecting a hypothesis does not mean that the statement is correct or wrong, since the truth is unknown. If we accept a hypothesis, we have made a better decisionsolelybasedonthestatisticalevidence.Itispossiblethattomorrowwhenyouhave collected more data we may reject a previously accepted hypothesis. Theprocedurefortestingwhetherahypothesisshouldbeacceptedorrejectedisknown as hypothesis testing. In hypothesis testing, we often have two opposite hypotheses: • H : Null hypothesis. It is the “status quo”, or the current status. 0 • H : Alternative hypothesis. It is the alternative to the null hypothesis. 1 To better understand hypothesis testing, consider a courthouse. By default, any person being prosecuted is assumed to be innocent. The police need to show sufficient evidence in order to prove the person guilty. The null hypothesis is the default assumption. Hypothesis testing asks whether we have strong enough evidence to reject the null hypothesis. If our evidence is not strong enough, we must assume that the null hypothesis is possibly true. Example 9.6. Suggest a null hypothesis and an alternative hypothesis regarding whether a coin is unbiased. Solution: Let θ be the probability of getting a head. 568
9.3. HYPOTHESIS TESTING • H : θ =0.5, and H : θ >0.5. This is a one-sided alternative. 0 1 • H : θ =0.5, and H : θ <0.5. This is another one-sided alternative. 0 1 • H : θ =0.5, and H : θ (cid:54)=0.5. This is a two-sided alternative. 0 1 PracticeExercise9.3.Suggestanullandanalternativehypothesisregardingwhether more than 62% of people in the United States use Microsoft Windows. Solution:LetθbetheproportionofpeopleusingMicrosoftWindowsinUnitedStates. • H : θ ≥0.62, and H : θ <0.62. This is a one-sided alternative. 0 1 PracticeExercise9.4.Suggestanullandanalternativehypothesisregardingwhether self-checkout at Walmart is faster than using a cashier. Solution: Let θ be the proportion of people that check out faster with self-checkout.. • H : θ ≥0.5, and H : θ <0.5. This is a one-sided alternative. 0 1 9.3.2 Critical-value test In hypothesis testing, there are two major approaches: the critical-value test, and the p-valuetest.Thetwotestsaremoreorlessequivalent.Ifyourejectthenullhypothesisusing the critical-value test, you will reject the hypothesis using the p-value. In this subsection, we will discuss the critical-value test. Let us consider a toy problem: Suppose that we have a 4-sided die and our goal is to test whether the die is unbiased. To do so, we define the null and the alternative hypotheses as • H : θ =0.25, which is our default belief. 0 • H : θ >0.25, which is a one-sided alternative. 1 There is no particular reason for considering the one-sided alternative other than the fact thatthecalculationisslightlyeasier.Youarewelcometoconsiderthetwo-sidedalternative. Wemustobtaindatapriortoconductinganyhypothesistesting.Let’sassumethatwe have thrown the die N =1000 times. We find that “3” appears 290 times (we could just as wellhavechosen1,2,or4).WeletX ,...,X betheN =1000binaryrandomvariables 1 1000 representingwhetherwehaveobtaineda“3”ornot.Ifthetrueprobabilityisθ =0.25,then we will have P[X =3]=θ =0.25 and P[X (cid:54)=3]=1−θ =0.75. We know that we cannot n n access the true probability, so we can only construct an estimator of the probability: N 1 (cid:88) Θ(cid:98) = N X n. n=1 In this experiment, we can show that Θ(cid:98) =290/1000=0.29. To make our problem slightly easier, we pretend that we know the variance Var[X ]. n In practice, we certainly do not know Var[X ], and so we need to estimate the variance. If n 569
CHAPTER 9. CONFIDENCE AND HYPOTHESIS we knew the variance, it should be Var[X ] = θ(1−θ) = 0.25(1−0.25) = 0.1875, because n X is a Bernoulli random variable with a mean θ. n The question asked by hypothesis testing is: How far is “Θ(cid:98) = 0.29” from “θ = 0.25”? If the statistic generated by our data, Θ(cid:98) = 0.29, is “far” from the hypothesized θ = 0.25, then we need to reject H because H says that θ = 0.25. However, if there is no strong 0 0 evidence that θ > 0.25, we will need to assume that H may possibly be true. So the key 0 question is what is meant by “far”. Formanyproblemslikethisone,itispossibletoanalyzethePDFofΘ(cid:98).SinceΘ(cid:98) isthe sample average of a sequence of Bernoulli random variables, it follows that Θ(cid:98) is a binomial (withascalingconstant1/N).IfN islargeenough,e.g.,N ≥30,theCentralLimitTheorem tells us that Θ(cid:98) is also very close to a Gaussian. Therefore, we can more or less claim that (cid:18) σ2(cid:19) Θ(cid:98) ∼Gaussian θ, . N With a simple translation and scaling, we can normalize Θ(cid:98) to obtain Z(cid:98): Θ(cid:98) −θ Z(cid:98)= √ ∼Gaussian(0,1). σ/ N Figure 9.10 illustrates the range of values for this problem. There are two axes: the Θ(cid:98)- axis (which is the estimator) and the Z(cid:98)-axis (which is the normalized variable). The values corresponding to each axis are shown in the figure. For example. Θ(cid:98) = 0.29 is equivalent to Z(cid:98) = 2.92, and Θ(cid:98) = 0.25 is equivalent to Z(cid:98) = 0, etc. Therefore, when we ask how far “Θ(cid:98) = 0.29” is from “θ = 0.25”, we can map this question from the Θ(cid:98)-axis to the Z(cid:98)-axis, and ask the relative position of Z(cid:98) from the origin. Figure 9.10: The mapping between Θ(cid:98) and Z(cid:98). To decide whether we want to reject or keep H 0, the critical-value approach compares Z(cid:98) relative to the critical value z α. Onacomputer,obtainingthesevaluesisquitestraightforward.UsingMATLAB,find- ing Z(cid:98) can be done by calling the following commands. The Python code is analogous. % MATLAB command to estimate the Z_hat value. Theta_hat = 0.29; % Your estimate 570
9.3. HYPOTHESIS TESTING theta = 0.25; % Your hypothesis sigma = sqrt(theta*(1-theta)); % Known standard deviation N = 1000; % Number of samples Z_hat = (Theta_hat - theta)/(sigma/sqrt(N)); # Python command to estimate the Z_hat value import numpy as np Theta_hat = 0.29 # Your estimate theta = 0.25 # Your hypothesis N = 1000 # Number of samples sigma = np.sqrt(theta*(1-theta)) # Known standard deviation Z_hat = (Theta_hat - theta)/(sigma / np.sqrt(N)) print(Z_hat) One essential element of hypothesis testing is the cutoff threshold, which is defined through the critical level α. It is the area under the curve of the PDF of Z(cid:98). Typically, α is chosen to be a small value, such as α = 0.05 (corresponding to a 5% margin). The corresponding cutoff is known as the critical value. It is defined as z =cutoff location where area under the curve is α. α If Z(cid:98) is Gaussian(0,1) and if we are looking at the right-hand tail, it follows that z =Φ−1(1−α). (9.24) α In our example, we find that z =1.65, which is marked in Figure 9.10. 0.05 On computers, determining the critical value z is straightforward. In MATLAB the α command is icdf, and in Python the command is stats.norm.ppf. % MATLAB code to compute the critical value alpha = 0.05; z_alpha = icdf(’norm’, 1-alpha, 0, 1); # Python code to compute the critical value import scipy.stats as stats alpha = 0.05 z_alpha = stats.norm.ppf(1-alpha, 0, 1) Do we have enough evidence to reject H in this example? Of course! The estimated 0 value Θ(cid:98) = 0.29 is equivalent to Z(cid:98) = 2.92, which is much too far from the cutoff z α = 1.65. In other words, we conclude that at a 5% critical level we have strong evidence to believe that the die is biased. Therefore, we need to reject H . 0 This conclusion makes a lot of sense if you think about it carefully. The estimator Θ(cid:98) =0.29 is obtained from N =1000 independent experiments. If we were only conducting N = 20 experiments, it might be consistent with the null hypothesis to have Θ(cid:98) = 0.29. However, if we have N = 1000 experiments, having Θ(cid:98) = 0.29 does not seem likely when there is no systematic bias. If there is no systematic bias, the estimator Θ(cid:98) should slightly jitter around Θ(cid:98) = 0.25, but it is quite unlikely to vary wildly to Θ(cid:98) = 0.29. Thus, based on the available statistics, we decide to reject the null hypothesis. 571
CHAPTER 9. CONFIDENCE AND HYPOTHESIS The decision based on comparing the critical value is known as the critical-value test. The idea (for testing a right-hand tail of a Gaussian random variable) is summarized in three steps: How to conduct a critical-value test √ • Set a critical value z α. Compute Z(cid:98)=(Θ(cid:98) −θ)/(σ/ N). • If Z(cid:98)≥z α, then reject H 0. • If Z(cid:98)<z α, then keep H 0. If you are testing a left-hand tail, you can switch the order of the inequalities. The critical-value test belongs to a larger family of testing procedures based on deci- sion theory. To give you a preview of the general theory of hypothesis testing, we define a decision rule,afunctionthatmapsarealizationoftheestimatortoabinarydecisionspace. In our problem the estimator is Z(cid:98) (or equivalently Θ(cid:98)). We denote its realization by z (cid:98). The binary decision space is {H , H }, corresponding to whether we want to claim H or H . 0 1 0 1 Claiming H is equivalent to keeping H , and claiming H is equivalent to rejecting H . 0 0 1 0 For the critical-value test, the decision rule δ(·) : R → {0,1} is given by the equation (for testing a right-hand tail): (cid:40) 1, if z ≥z , (claim H ), δ(z)= (cid:98) α 1 (9.25) (cid:98) 0, if z <z , (claim H ). (cid:98) α 0 Example 9.7. It was found that only 35% of the children in a kindergarten eat broccoli. The teachers conducted a campaign to get more kids to eat broccoli, after whichitwasfoundthat390kidsoutof1009kidsreportedthattheyhadeatenbroccoli. Has the campaign successfully increased the number of kids eating broccoli? Assume that the standard deviation is known. Solution. We setup the null and the alternative hypothesis. H : θ =0.35, H : θ >0.35. 0 1 We construct an estimator Θ(cid:98) = (1/N)(cid:80)N n=1X n, where X n is Bernoulli with proba- bility θ. Based on θ, σ2 = θ(1−θ) = 0.227. (Again, in practice we do not know the true variance, but in this problem we pretend that we know it.) By the Central Limit Theorem, Θ(cid:98) is roughly a Gaussian. We compute the test statistics Θ(cid:98) = 390 = 0.387. Standardization gives Z(cid:98) = Θ(cid:98)√−θ = 2.432. At a 5% 1009 σ/ N critical level, we have that z α = 1.65. So Z(cid:98) = 2.432 > 1.65 = z α, and hence we need to reject the null hypothesis. Even if we choose a 1% critical level so that z = 2.32, α our estimator Z(cid:98)=2.432>2.32=z α will still reject the null hypothesis. A graphical illustration of this problem is shown in Figure 9.11. It can be seen that Θ(cid:98) =0.387 is actually quite far away from the cutoff 1.65. Thus, we need to reject the null hypothesis. 572
9.3. HYPOTHESIS TESTING Figure 9.11: Example of a critical-value test. In this example, the test statistic Θ(cid:98) = 0.387 is equivalent to Z(cid:98) = 2.432, which is significantly larger than the cutoff z α = 1.65. Therefore, we havestrongevidencetorejectthenullhypothesis,becausetheprobabilityofobtainingΘ(cid:98) =0.387 is very low if H is true. 0 9.3.3 p-value test An alternative to the critical-value test is the p-value test. Instead of looking at the cutoff valuez ,weinspecttheprobabilityofobtainingourobservationifH istrue.Tounderstand α 0 how the p-value test works, we consider another toy problem. Suppose that we have two hypotheses about flipping a coin: • H : θ =0.9, which is our default belief. 0 • H : θ <0.9, which is a one-sided alternative. 1 It was found that with N = 150 coin flips, the coin landed on heads 128 times. Thus the estimator is Θ(cid:98) = 128 =0.853. Then, by following our previous procedures, we have that 150 Θ(cid:98) −θ 0.853−0.9 Z(cid:98)= √ = (cid:113) =−1.92. σ/ N 0.9(1−0.9) 150 At this point we can follow the previous subsection by computing the critical value z α and make the decision. However, let’s take a different route. We want to know what is the probability under the curve if we integrate the PDF of Z(cid:98) from −∞ to −1.92. This is easy. Since Z(cid:98) is Gaussian(0,1), it follows from the CDF of a Gaussian that P[Z(cid:98)≤−1.92]=0.0274. (cid:124) (cid:123)(cid:122) (cid:125) p-value Referring to Figure 9.12, the value 0.0274 is the pink area under the curve, which is the PDFofZ(cid:98).Sincetheareaunderthecurveislessthanthecriticallevelα(say5%),wereject the null hypothesis. On computers, computing the p-value is done using the CDF commands. % MATLAB code to compute the p-value p = cdf(’norm’, -1.92, 0, 1); 573
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure9.12:Thep-valuetestasksustolookattheprobabilityofZ(cid:98)≤z (cid:98).Ifthisprobability(thep-value) is less than the critical level α, we have significant evidence to reject the null hypothesis. # Python code to compute the p-value import scipy.stats as stats p = stats.norm.cdf(-1.92,0,1) In this example, the probability P[Z(cid:98) ≤ −1.92] is known as the p-value. It is the probability of Z(cid:98) ≤ z, under the distribution mandated by the null hypothesis, where z is the (normalized) estimated value based on data. Using our example, z is −1.92. By “distributionmandatedbythenullhypothesis”wemeanthatthePDFofZ(cid:98)isthePDFthat the null hypothesis wants. In the above example the PDF is Gaussian(0,1), corresponding √ to Gaussian(θ,σ/ N) for Θ(cid:98). More formally, the p-value for a left-hand tail test is defined as p-value(z (cid:98))=P[Z(cid:98)≤z (cid:98)], where z (cid:98) is the random realization of Z(cid:98) estimated from the data. The decision rule based on the p-value is (for the left-hand tail): (cid:40) δ(z)= 1, P[Z(cid:98)≤z (cid:98)]<α (claim H 1), (9.26) (cid:98) 0, P[Z(cid:98)≤z (cid:98)]≥α (claim H 0). Ifthealternativehypothesisisright-handed,thentheprobabilitybecomesP[Z(cid:98)≥z (cid:98)]instead. Relationship between critical-value and p-value tests. There is a one-to-one corre- spondence between the p-value and the critical value. In the p-value test, if Z(cid:98) is Gaussian, it follows that p-value=P[Z(cid:98)≤z (cid:98)]=Φ(z (cid:98)), where Φ is CDF of the standard Gaussian. Taking the inverse, the corresponding z is (cid:98) z =Φ−1(p-value). (cid:98) 574
9.3. HYPOTHESIS TESTING In practice, we do not need to take any inverse of the p-value to obtain z because it is (cid:98) directly available from the data. To test the p-value, we compare it with the critical level α by checking p-value<α. Taking the inverse of both sides, it follows that the decision rule is equivalent to Φ−1(p-value)<Φ−1(α), (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) z(cid:98) zα where the quantity on the right-hand side is the critical value z . Therefore, if the test α statistic fails in the p-value test it will also fail in the critical-value test, and vice versa. What is the difference between the critical-value test and p-value test? • Critical-value test: Compare w.r.t. critical value, which is the cutoff on the Z- axis. • p-value test: Compare w.r.t. α, which is the probability. • Both will give you the same statistical conclusion. So it does not matter which one you use. Example 9.8.WeflipacoinforN =150timesandfindthat128areheads.Consider two hypotheses • H : θ =0.9, which is our default belief. 0 • H : θ (cid:54)=0.9, which is a two-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject H ? 0 Solution. We know that Θ(cid:98) =128/150=0.853. The normalized statistic is Θ(cid:98) −θ 0.853−0.9 Z(cid:98)= √ = (cid:113) =−1.92. σ/ N 0.9(1−0.9) 150 To compute the p-value, we observe that the two-sided test means that we consider the two tails. Thus, we have p-value=P[|Z(cid:98)|>1.92] =2×P[Z(cid:98)>1.92] =2×0.0274=0.055. For a critical level of α = 0.05, the p-value is larger. This means that the probability of obtaining |Z| > 1.92 is not extreme enough. Therefore, we do not have sufficient evidence to reject the null hypothesis. 575
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If we take the critical-value test, we will reach the same conclusion. The critical value for α=0.05 is determined by taking the inverse CDF at 1−0.025, giving (cid:16) α(cid:17) z =Φ−1 1− =1.96. α 2 Since Z(cid:98) = 1.92 has not passed this threshold, we conclude that there is not enough evidence to reject the null hypothesis. Figure 9.13: Example of a two-sided test using the p-value and the z -value. α 9.3.4 Z-test and T-test The critical-value test and the p-value tests are generic tools for hypothesis testing. In this subsection we introduce the Z-test and the T-test. It is important to understand that the Z-test and the T-test refer to the distributional assumptions we make about the variance. They define the distribution we use to conduct the test but not the tools. In fact, both the Z-test and the T-test can be implemented using the critical-value test or the p-value test. Figure 9.14 illustrates the hierarchy of the tests. Figure 9.14: When conducting a hypothesis testing of the sample average, we may or may not know thevariance.Ifweknowthevariance,weusetheGaussiandistributiontoconducteitherap-valuetest or a critical-value test. If we do not know the variance, we use Student’s t-distribution. The difference between the Gaussian distribution and the T distribution is mainly attributable to the knowledge about the population variance. If the variance is known, the distribution of the estimator (which in our case is the sample average) is Gaussian. If 576
9.3. HYPOTHESIS TESTING the variance is estimated from the sample, the distribution of the estimator will follow a Student’s t-distribution. To introduce the Z-test and the T-test we consider the following two examples. The first example is a Z-test. Example 9.9 (Z-test). Suppose we have a Gaussian random variable with unknown mean θ and a known variance σ = 11.6. We draw N = 25 samples and construct an estimator Θ(cid:98) =80.94. We propose two hypotheses: • H : θ =85, which is our default belief. 0 • H : θ <85, which is a one-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject the null hypothesis? Solution. The test statistic is Θ(cid:98) −θ Z(cid:98)= √ =−1.75. σ/ N Since the individual samples are assumed to follow a Gaussian, the sample average Θ(cid:98) is also a Gaussian. Hence, Z(cid:98) is distributed according to Gaussian(0,1). Figure 9.15: A one-sided Z-test using the p-value and the z -value. α For a critical level of 0.05, a one-sided critical value is z =Φ−1(1−α)=−1.645. α Since Z(cid:98) =−1.75, which is more extreme than the critical value, we conclude that we need to reject H . 0 If we use the p-value test, we have that the p-value is P[Z(cid:98)≤−1.75]=Φ(−1.75)=0.0401. Since the p-value is smaller than the critical level α=0.05, it implies that Z(cid:98)=−1.75 is more extreme. Hence, we reject H . 0 577
CHAPTER 9. CONFIDENCE AND HYPOTHESIS ThefollowingexampleisaT-test.InaT-testwedonotknowthepopulationvariance butonlyknowthesamplevarianceS(cid:98).ThustheteststatisticweuseisaT randomvariable. Example 9.10 (T-test). Suppose we have a Gaussian random variable with unknown mean θ and an unknown variance σ. We draw N = 100 samples and construct an estimator Θ(cid:98) =130.1, with a sample variance S(cid:98)=21.21. We propose two hypotheses: • H : θ =120, which is our default belief. 0 • H : θ (cid:54)=120, which is a two-sided alternative. 1 For a critical level of α=0.05, shall we keep or reject the null hypothesis? Solution. The test statistic is Θ(cid:98) −θ T(cid:98)= √ =4.762. S(cid:98)/ N Note that while the sample average Θ(cid:98) is a Gaussian, the test statistic T(cid:98) is distributed according to a T distribution with N −1 degrees of freedom. For a critical level of 0.05, a two-sided critical value is (cid:16) α(cid:17) t =Ψ−1 1− =1.984. α 99 2 Since T(cid:98) = 4.762, which is more extreme than the critical value, we conclude that we need to reject H . 0 If we use the p-value test, we have that the p-value is P[|T(cid:98)|≥4.762]=2×P[T(cid:98)≥4.762] =3.28×10−6. Since the p-value is (much) smaller than the critical level α = 0.05, it implies that |T(cid:98)|≥4.762 is quite extreme. Hence, we reject H 0. Figure 9.16: A two-sided T-test using the p-value and the z -value. α 578
9.4. NEYMAN-PEARSON TEST Forthisexample,theMATLABandPythoncommandstocomputet andthep-value α are % MATLAB code to compute critical-value and p-value t_alpha = icdf(’t’, 1-0.025, 99); p = 1-cdf(’t’, 4.762, 99); # Python code to compute critical value and p-value import scipy.stats as stats t_alpha = stats.t.ppf(1-0.025,99) p = 1-stats.t.cdf(4.762,99) What are the Z-test and the T-test? • Both are hypothesis testings for the sample averages. • Z-test: Assume known variance. Hence, use the Gaussian distribution. • T-test: Assume unknown variance. Hence, use the Student’s t-distribution. Remark. We are exclusively analyzing the sample average in this section. There are other types of estimators we can analyze. For example, we can discuss the difference between the twomeans,theratiooftworandomvariables,etc.Ifyouneedtoolsforthesemoreadvanced problems, please refer to the reference section at the end of this chapter. 9.4 Neyman-Pearson Test The hypothesis testing procedures we discussed in the previous section are elementary in the sense that we have not discussed much theory. This section aims to fill the gap so that you can understand hypothesis testing from a broader perspective. This generalization will also help to bridge statistics to other disciplines such as classification in machine learning and detection in signal processing. We call this theoretical analysis the Neyman-Pearson framework. 9.4.1 Null and alternative distributions Whenwediscussedhypothesistestingintheprevioussection,wefocusedexclusivelyonthe null hypothesis H . Regardless of whether we are studying the Z-test or the T-test, using 0 the critical value or the p-value, all the distributions are associated with the distribution under H . 0 What do we mean by “distribution under H 0”? Using Θ(cid:98) as an example, the PDF of Θ(cid:98) is assumed to be Gaussian(θ,σ2/N). This Gaussian, centered at θ, is the distribution assumed under H . As we decide whether to keep or rejectH , we look at the critical value 0 0 and the p-value of the test statistic under Gaussian(θ,σ2/N). 579
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Importantly, the analysis of hypothesis testing is not just about H — it is also about 0 the alternative hypothesis H , which uses a different PDF. For example, H could use 1 1 Gaussian(θ(cid:48),σ2/N)forθ(cid:48) >θ.Therefore,forthesametestingstatisticΘ(cid:98),wecancheckhow close it is to H . 1 To capture both distributions, we define f (y)=f (y |H ), 0 Y 0 f (y)=f (y |H ). 1 Y 1 The first PDF defines the distribution when the true model is H . The second PDF is the 0 distribution when the true model is H . 1 Example9.11.ConsideranestimatorY ∼Gaussian(θ,σ2/N).Definetwohypotheses H :θ =120 and H :θ >120. The two PDFs are then 0 1 f (y)=f (y|H )=Gaussian(120,σ2/N), 0 Y 0 f (y)=f (y|H )=Gaussian(θ(cid:48),σ2/N), θ(cid:48) >120. 1 Y 1 A graph of the two distributions is shown in Figure 9.17. In this figure we plot the PDF under the null hypothesis and the PDF under an alternative hypothesis. The decision is based on the null, where we marked the critical value. Figure 9.17: The PDF of the estimator under hypotheses H and H . The yellow region defines the 0 1 rejection zone R . If the estimator has a realization Y = y that falls into the rejection zone R , we α α need to reject H . 0 Students are frequently confused about the exact equation of the PDF under H . If 1 the alternative hypothesis is defined as θ > 120, shall we define the PDF as a Gaussian centered at 130 or 151.4? They are both valid alternative hypotheses. The answer is that we are going to express all equations based on θ(cid:48). For example, if we want to analyze the prediction error (this term will be explained later), the prediction error will be a function of θ(cid:48). If θ(cid:48) is close to θ, we will expect a larger prediction error. However, if θ(cid:48) is far away from θ, the prediction error may be small. Whenever we discuss hypothesis testing, a decision rule is always implied. A decision ruleisamappingδ(·)fromsamplespaceY oftheteststatisticY (orΘ(cid:98) ifyouprefer)tothe 580
9.4. NEYMAN-PEARSON TEST binary space of {0,1}: (cid:40) 1, if y ∈R , (we will reject H ), δ(y)= α 0 (9.27) 0, if y (cid:54)∈R , (we will keep H ). α 0 Here R is the rejection zone. For example, in a one-sided testing at a critical level α, the α rejection zone is R = {y ≥ Φ−1(1−α)}. Therefore, as long as y ≥ Φ−1(1−α), we will α reject the null hypothesis. Otherwise, we will keep the null hypothesis. A rejection zone can be one-sided, two-sided, or even more complicated. Example 9.12. Consider H : θ = 0.35 and H : θ > 0.35. It was found that the 0 1 sample average o √ver 1009 samples is Θ(cid:98) =0.387, with σ2 =0.227. The normalized test statistic is Z(cid:98) = N(Θ(cid:98) −θ)/σ =2.432. At a 5% critical level, define the decision rule based on the critical-value approach. Solution. If α = 0.05, it follows that z = Φ−1(1 − 0.05) = 1.65. Therefore, the α decision rule is (cid:40) 1, if z ≥1.65, (we will reject H ), δ(z)= (cid:98) 0 (cid:98) 0, if z <1.65, (we will keep H ), (cid:98) 0 where z (cid:98)is the realization of Z(cid:98). In this particular problem, we have z (cid:98)= 2.432. Thus, according to the decision rule, we need to reject H . 0 A decision rule is something you create. You do not need to follow the critical-value or the p-value procedure — you can create your own decision rule. For example, you can say “reject H when |y|>0.000001”. There is nothing wrong with this decision rule except 0 that you will almost always reject the null hypothesis (so it is a bad decision rule). See Figure 9.18 for a graph of a similar example. If you follow the critical-value or the p-value procedures,itturnsoutthattheresultingdecisionruleisequivalenttosomeformofoptimal decisionrule.ThisconceptistheNeyman-Pearsonframework,whichwewillexplainshortly. 9.4.2 Type 1 and type 2 errors Since hypothesis testing is about applying a decision rule to the test statistics, and since no decision rule is perfect, it is natural to ask about the error expected from a particular decisionrule.Inthissubsectionwedefinethedecisionerror.However,theterminologyvaries from discipline to discipline. We will explain the decision error first through the statistics perspective and then through the signal processing perspective. Twotablesofthecasesthatcanbegeneratedbyabinarydecision-makingprocessare shown in Figure 9.19. The columns of the tables are the true statements, i.e., whether the test statistic has a population distribution under H or H . The rows of the tables are the 0 1 statements predicted by the decision rule, i.e., whether we should declare the statistics are from H or H . Each combination of the truth and prediction has a label: 0 1 • True positive: The truth is H , and you declare H . 1 1 • True negative: The truth is H , and you declare H . 0 0 • False positive: The truth is H , and you declare H . 0 1 581
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.18: Two possible decision rules δ (y) and δ (y). In this example, δ (y) is designed according 1 2 1 tothecritical-valueapproachatα=0.025,whereasδ (y)isarbitrarilydesigned.Botharevaliddecision 2 rules, although δ should not be used because it tends to reject the null hypothesis more often than 2 desired. • False negative: The truth is H , and you declare H . 1 0 Differentcommunitieshavedifferentwaysoflabelingthesequantities.Inthestatisticscom- munity the false negative rate (i.e., the number of false negative cases divided by the total number of cases) is called the type 2 error, and the false positive rate is called the type 1 error. The true positive rate is called the power of the decision rule. In the engineering community (e.g., radar engineering and signal processing) the ob- jective is to detect whether a target (e.g., a missile or an enemy aircraft) is present. In this context, the false positive rate is known as the probability of false alarm, since personnel willbealertedwhennotargetispresent.Thefalsenegativerateisknownastheprobability of miss because you miss a target. If the truth is H and the prediction is also H , we call 1 1 this the probability of detection. Figure 9.19: Terminologies used in labeling the prediction error. The terms “Type 1 error” and “Type 2 error” are commonly used by the statistics community, whereas the terms “false alarm”, “miss” and “detection” are more often used in the engineering community. ThediagraminFigure 9.20willhelptoclarifythesedefinitions.Giventwohypotheses H andH ,thereexiststhecorrespondingdistributionsf (y)andf (y),whicharethePDFs 0 1 0 1 582
9.4. NEYMAN-PEARSON TEST of the test statistics Y (or Θ(cid:98) if you prefer). Supposing that our decision rule is to declare H whenY ≥η forsomeη,forexample,η =1.65fora5%criticallevel,therearetwoareas 1 under the curve that we need to consider. • Type 1 / False alarm. The blue region under the curve represents the probability of declaring H (i.e., we choose to reject the null) while the truth is actually H (i.e., we 1 0 should have not rejected the null). Mathematically, this probability is (cid:90) p =P[Y ≥η |H ]= f (y)dy. (9.28) F 0 0 y≥η • Type2/Miss.Thepinkregionunderthecurverepresentstheprobabilityofdeclaring H (i.e., we choose to keep the null) while the truth is actually H (i.e., we should 0 1 have rejected the null). Mathematically, this probability is (cid:90) p =P[Y <η |H ]= f (y)dy. (9.29) M 1 1 y<η Figure 9.20: Definition of type 1 and type 2 errors. The power of the decision rule is also known as the detection. It is defined as p =P[Y ≥η |H ]. (9.30) D 1 A plot illustrating the power of the decision rule is shown in Figure 9.21. Since p is the D conditional probability of Y ≥η given H , it is the complement of p , and so we have the 1 M identity p =1−p . D M Some communities refer to the above quantities in terms of the counts instead of the probabilities. The difference is that the probabilities are normalized to [0,1] whereas the counts are just the raw integers obtained from running an experiment. We prefer to use the probabilities because they are the theoretical values. If you tell us the distributions f and 0 f ,wecanreporttheprobabilities.Thecounts,bycontrast,arejustanotherformofsample 1 statistics.Thenumberofcountstodaymaybedifferentfromthenumberofcountstomorrow 583
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Figure 9.21: The power of the decision rule is the area under the curve of f , integrated for y inside 1 the rejection zone. because they are obtained from the experiments. The difference between probabilities and counts is analogous to the difference between PMFs and histograms. Since the probability of errors changes as the decision rule changes, it is necessary to definep ,p andp asfunctionsofδ.Inaddition,hypothesistestingisnotlimitedtoone- F D M sidedtests.WecandefinetherejectionzoneasR ={y |reject H using a critical level α}. α 0 The probabilities p and p are defined as F M (cid:90) (cid:90) p (δ)= δ(y)f (y)dy = f (y)dy, (9.31) F 0 0 y∈Rα (cid:90) (cid:90) p (δ)= δ(y)f (y)dy = f (y)dy. (9.32) M 1 1 y(cid:54)∈Rα Using the property that p =1−p , we have that D M (cid:90) p (δ)=1−p (δ)= f (y)dy. (9.33) D M 1 y∈Rα Note that the rejection zone does not need to depend on α. You can arbitrarily define the rejection zone, and the probabilities p , p , and p can still be defined. F M D Example 9.13. Find p (δ ) and p (δ ) for the decision rule in Figure 9.18. F 1 F 2 Solution. Since f is a Gaussian with zero mean and unit variance, it follows that 0 p F(δ 1)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(1.92)=0.025, 2π 1.96 p F(δ 2)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(0.5)=0.3085. 2π 0.5 9.4.3 Neyman-Pearson decision At this point you have probably observed something about the critical-value test and the p-value test. Among the four types of decision combinations, we are looking at the false 584
9.4. NEYMAN-PEARSON TEST positive rate, or the probability of false alarm p (δ). The critical-value test requires us to F findδ suchthatp (δ)isequaltoα.Thatis,ifyoutellusthecriticallevelα(e.g.,α=0.05), F we will find a decision rule (by telling you the cutoff) such that the false alarm rate is α. Consider an example: Example 9.14. Let α = 0.05. Assume that f is a Gaussian with zero-mean and 0 unit-variance. Let us do a one-sided test for H :θ =0 versus H :θ >0. Find δ such 0 1 that p (δ)=α. F Solution. Let the decision rule δ be (cid:40) 1, y ≥η, δ(y)= 0, y <η. Our goal is to find η. The probability of false alarm is p F(δ)=(cid:90) ∞ √1 e−y 22 dy =1−Φ(η). 2π η Equatingthisto α,itfollows that 1−Φ(η)=α implies η =Φ−1(1−α)=1.65.Sothe decision rule becomes (cid:40) 1, y ≥1.65, δ(y)= 0, y <1.65. Ifyouapplythisdecisionrule,youareguaranteedthatthefalsealarmrateisα=0.05. But why should we aim for p (δ) equal to α? Isn’t a lower false alarm rate better? F Indeed, we would not mind having a lower false alarm, so we are happy to have any δ that satisfies p (δ) ≤ α. However, changing the equality to an inequality means that we F now have a set of δ instead of a unique δ. More important, we need to pay attention to the trade-off between p (δ) and p (δ). The smaller the p (δ) a decision rule δ provides, F D F the smaller the p (δ) you can achieve. This is immediately apparent from Figure 9.20 and D Figure 9.21. (If you move the cutoff to the right, the gray area and the blue area will both shrink.) Therefore, the desired optimization should be formulated as: From all the decision rules δ that have a false alarm rate of no larger than α, we pick the one that maximizes the detection rate. The resulting decision rule is known as the Neyman-Pearson decision rule. Definition 9.2. The Neyman-Pearson decision rule is defined as the solution to the optimization δ∗ =argmax p (δ), D δ subject to p (δ)≤α. (9.34) F Figure 9.22illustratestwodecisionrulesδ∗(y)andδ(y).Thefirstdecisionruleδ∗(y)is obtained according to the critical-value approach, with α=0.025. As we will prove shortly, this is also the optimal Neyman-Pearson decision rule for a one-sided hypothesis testing at α = 0.025. The second decision rule δ(y) has a harsher cutoff, meaning that you need an extreme test statistic to reject the null hypothesis. Clearly, the p-value obtained by δ(y) is 585
CHAPTER 9. CONFIDENCE AND HYPOTHESIS less than α = 0.025. Thus, δ(y) is a valid decision rule according to the Neyman-Pearson formulation. However, δ(y) is not optimal because the detection rate is not maximized. Figure 9.22: Twodecisionrulesδ(y)andδ∗(y).Assumethatα=0.025.Thenδ(y)isoneofthemany feasible choices in the Neyman-Pearson optimization, but δ∗(y) is the optimal solution. Because of the complementary behavior of p and p , it follows that p is maximized F D D whenp hitstheupperbound.Ifwewanttomaximizethedetectionrateweneedtostretch F the false alarm rate as much as possible. As a result, the Neyman-Pearson solution occurs when p (δ)=α, i.e., when the equality is met. F TheNeyman-Pearsonframeworkisageneralframeworkforalldistributionsf andf , 0 1 asopposedtothecritical-valueandp-valueexamples,whichareeitherGaussianorStudent’s t-distribution. The solution to the Neyman-Pearson optimization is a decision rule known as the likelihood ratio test. The likelihood ratio is defined as follows. Definition 9.3. The likelihood ratio for two distributions f (y) and f (y) is 1 0 f (y) L(y)= 1 . (9.35) f (y) 0 It turns out that the solution to the Neyman-Pearson optimization takes the form of the likelihood ratio. Theorem 9.2. The solution to the Neyman-Pearson optimization is a decision rule that checks the likelihood ratio (cid:40) 1, L(y)≥η, δ∗(y)= (9.36) 0, L(y)<η, for some decision boundary η which is a function of the critical level α. 586
9.4. NEYMAN-PEARSON TEST What is so special about Neyman-Pearson decision rule? • Itistheoptimaldecision.Itsoptimalityisdefinedw.r.t.maximizingthedetection rate while keeping a reasonable false alarm rate: δ∗ =argmax p (δ), D δ subject to p (δ)≤α. F • If your goal is to maximize the detection rate while maintaining the false alarm rate, you cannot do better than Neyman-Pearson. • Its solution is the likelihood ratio test: (cid:40) 1, L(y)≥η, δ∗(y)= 0, L(y)<η, where L(y)=f (y)/f (y) is the likelihood ratio. 1 0 • The critical-value test and the p-value test are special cases of the Neyman- Pearson test. Deriving the solution to the Neyman-Pearson optimization can be skipped if this is your first time reading the book. Proof. Given α, choose δ∗ such that the false alarm rate is maximized: p (δ∗)=α. Then, F by substituting the definition of δ∗ into the false alarm rate, (cid:90) ∞ α=p (δ∗)= δ∗(y)f (y)dy F 0 −∞ (cid:90) (cid:90) = 1·f (y)dy+ 0·f (y)dy. (9.37) 0 0 L(y)≥η L(y)<η Now, consider another decision rule δ that is not optimal but is feasible. That means that δ satisfies p (δ)≤α. Therefore, F (cid:90) ∞ α≥p (δ)= δ(y)f (y)dy F 0 −∞ (cid:90) (cid:90) = δ(y)·f (y)dy+ δ(y)·f (y)dy. (9.38) 0 0 L(y)≥η L(y)<η Our goal is to show that p (δ∗)≥p (δ), because by proving this result we can claim that D D δ∗ maximizes the detection rate. By combining Equation (9.37) and Equation (9.38), we have 0≤p (δ∗)−p (δ) F F (cid:90) (cid:90) = (1−δ(y))f (y)dy− δ(y)f (y)dy. (9.39) 0 0 L(x)≥η L(y)<η 587
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Define L(y)= f1(y). Then L(y)≥η if and only if f (y)≥ηf (y). So, f0(y) 1 0 (cid:90) (cid:90) p (δ∗)−p (δ)= (1−δ(y))f (y)dy− δ(y)f (y)dy D D 1 1 L(y)≥η L(y)<η (cid:90) (cid:90) = (1−δ(y))ηf (y)dy− δ(y)ηf (y)dy 0 0 L(y)≥η L(y)<η (cid:34) (cid:35) (cid:90) (cid:90) =η (1−δ(y))f (y)dy− δ(y)f (y)dy ≥0, 0 0 L(y)≥η L(y)<η where the last inequality holds because of Equation (9.39). Therefore, we conclude that δ∗ maximizes p . (cid:3) D End of the proof. Please join us again. At this point, you may object that the likelihood ratio test (i.e., the Neyman-Pearson decision rule) is very different from the hypothesis testing examples we have seen in the previous chapter because now we need to handle the likelihood ratio L(y). Rest assured that they are the same, as illustrated by the following example. Example 9.15. Consider two hypotheses: H : Y ∼ Gaussian(0,σ2), and H : Y ∼ 0 1 Gaussian(µ,σ2), with µ > 0. Construct the Neyman-Pearson decision rule (i.e., the likelihood ratio test). Solution. Let us first define the likelihood functions. It is clear from the description that 1 (cid:26) y2 (cid:27) 1 (cid:26) (y−µ)2(cid:27) f (y)= √ exp − and f (y)= √ exp − . 0 2πσ2 2σ2 1 2πσ2 2σ2 Therefore, the likelihood ratio is (cid:26) (cid:27) f (y) 1 L(y)= 1 =exp − (µ2−2µy) . f (y) 2σ2 0 The likelihood ratio test states that the decision rule is (cid:40) 1, L(y)≥η, δ∗(y)= 0, L(y)<η. So it remains to simplify the condition L(y)(cid:82)η. To this end, we observe that 1 L(y)≥η ⇐⇒ − (µ2−2µy)≥logη 2σ2 µ σ2 ⇐⇒ y ≥ − logη. 2 µ (cid:124) (cid:123)(cid:122) (cid:125) d=efτ 588
9.4. NEYMAN-PEARSON TEST Therefore,insteadofdeterminingη,wejustneedtodefineτ becausethedecisionrules based on η and τ are equivalent. Todetermineτ,Neyman-Pearsonstatesthatp (δ)≤α(andattheoptimalpoint F the equality has to hold). Substituting this criterion into the decision rule, (cid:90) α=p (δ)= f (y)dy F 0 L(y)≥η (cid:90) = f (y)dy 0 y≥τ =(cid:90) √ 1 e− 2y σ2 2 dy 2πσ2 y≥τ (cid:16)τ(cid:17) =1−Φ . σ Taking the inverse of the CDF, we obtain τ: τ =σΦ−1(1−α). Putting everything together, the final decision rule is (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). √ So if α = 0.05 we will reject H when y ≥ 1.65σ. We can also replace σ by σ/ N if 0 the estimator is constructed from multiple measurements. The above example tells us that even though the likelihood ratio test may appear complicatedatfirstglance,thedecisionisthesameasthegoodoldhypothesistestingrules we have derived. The flexibility we have gained with the likelihood ratio test is the variety of distributions we can handle. Instead of restricting ourselves to Gaussians or Student’s t-distribution (which exclusively focuses on the sample averages), the likelihood ratio test allows us to consider any distributions. The exact decision rule could be less obvious, but the method is generalizable to a broad range of problems. Practice Exercise 9.5. In a telephone system, the waiting time is defined as the inter-arrival time between two consecutive calls. However, it is known that sometimes the waiting time can be mistakenly recorded as the time between three consecutive calls (i.e., by skipping the second one). Since the interarrival time of an independent PoissonprocessiseitheranexponentialrandomvariableoranErlangrandomvariable, depending on how many occurrences we are counting, we define the hypotheses (cid:40) (cid:40) e−y, y ≥0, ye−y, y ≥0, f (y)= and f (y)= 0 1 0, y <0, 0, y <0. Suppose we are given one measurement Y = y. Find the Neyman-Pearson decision rule for α=0.05. 589
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Solution. The likelihood ratio is f (y) ye−y L(y)= 1 = =y, y ≥0. f (y) e−y 0 Substituting this into the decision rule, we have (cid:40) 1, L(y)≥η ⇐⇒y ≥η, δ∗(y)= 0, L(y)<η ⇐⇒y <η. It remains to determine η. Inspecting p (δ), we have that F (cid:90) α=p (δ∗)= f (y)dy F 0 L(y)≥η (cid:90) = e−y dy =e−η. y≥η Setting e−η =α, we have that α=−logα. Hence, the decision rule is (cid:40) 1, L(y)≥η ⇐⇒y ≥−logα, δ∗(y)= 0, L(y)<η ⇐⇒y <−logα. For α = 0.05, we reject the null hypothesis when y ≥ 2.9957. Figure 9.23 illustrates the hypothesis testing rule. Figure 9.23: Neyman-Pearson decision rule at α=0.05. Remark. This example is instructive in that we have only one measurement Y =y. If we have repeated measurements and take the average, then the Central Limit The- orem will kick in. In that case, we can resort to our favorite Gaussian distribution or Student’s t-distribution instead of dealing with the exponential and the Erlang distributions. However, the example demonstrates the usefulness of Neyman-Pearson, especially when the distributions are complicated. 590
9.5. ROC AND PRECISION-RECALL CURVE 9.5 ROC and Precision-Recall Curve Beingabinary decisionrule,thehypothesistestingproceduresharesmanysimilaritieswith a two-class classification algorithm.3 Given a testing statistic or a testing sample, both the hypothesis testing and a classification algorithm will report YES or NO. Therefore, any performance evaluation metric developed for hypothesis testing is equally applicable to classification and vice versa. Thetopicwestudyinthissectionisthereceiveroperatingcharacteristic(ROC)curve and the precision-recall (PR) curve. The ROC curve and the PR curve are arguably the mostpopularmetricsinmodernmachinelearning,inparticularforclassification,detection, and segmentation tasks in computer vision. There are many unresolved questions about thesetwocurvesandtherearemanydebatesabouthowtousethem.Ourgoalisnottoadd another voice to the debate; rather, we would like to fill in the gap between the hypothesis testing theory (particularly the Neyman-Pearson framework) and these two sets of curves. We will establish the equivalence between the two curves and leave the open-ended debates to you. 9.5.1 Receiver Operating Characteristic (ROC) Our approach to understanding the ROC curve and the PR curve is based on the Neyman- Pearson framework. Under this framework, we know that the optimal decision rule w.r.t to the Neyman-Pearson criterion is the solution to the optimization δ∗(α)=argmax p (δ) D δ subject to p (δ)≤α. F As a result of this optimization, the decision rule δ∗ will achieve a certain false alarm rate p (δ∗) and detection rate p (δ∗). Clearly, the decision rule δ∗ changes as we change the F D critical level α. Accordingly we write δ∗ as δ∗(α) to reflect this dependency. Whatthisobservationimpliesisthataswesweepthroughtherangeofα’s,weconstruct different decision rules, each one with a different p and p . If we denote the decision rules F D by δ ,δ ,...,δ , we have M pairs of false alarm rate p and detection rate p : 1 2 M F D • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). 1 F 1 D 1 • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). 2 F 2 D 2 . . • . • Decision rule δ : False alarm rate p (δ ) and detection rate p (δ ). M F M D M 3Inaclassificationalgorithm,thegoalistolookatthetestingsampleyandcomputecertainthresholding (cid:40) 1, wTφ(y)≥τ criteria.Forexample,atypicaldecisionruleofaclassificationalgorithmisδ(y)= .Here, 0, wTφ(y)<τ youcanthinkofthevectorw astheregressioncoefficient,andφ(·)issomekindoffeaturetransform.The equation says that class 1 will be reported if the inner product is larger than a threshold τ, and class 0 will be reported otherwise. Therefore, a binary classification, when written in this form, is the same as a hypothesistestingprocedure. 591
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If we plot p (δ) on the y-axis as a function of p (δ) on the x-axis, we obtain a curve shown D F in Figure 9.24 (see the example below for the problem setting). The black curve shown on the right is known as the receiver operating characteristic (ROC) curve. Figure9.24:AnexampleofanROCcurve,whereweconsidertwohypotheses:H :Y ∼Gaussian(0,2), 0 and H : Y ∼ Gaussian(3,2). We construct the Neyman-Pearson decision rule for a range of critical 1 levels α. For each α we compute the theoretical p (α) and p (α), shown on the left-hand side of the F D figure. The pair of (p ,p ) is then plotted as the right-hand side curve by sweeping the α’s. D F The setup of the figure follows the example below. Example 9.16. We consider two hypotheses: H :Y ∼Gaussian(0,2), and H :Y ∼ 0 1 Gaussian(3,2). Derive the Neyman-Pearson decision rule and plot the ROC curve. Solution. We construct a Neyman-Pearson decision rule: (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). whereτ isatunablethreshold.Forexample,ifα=0.05,thenσΦ−1(1−0.05)=3.2897, and if α = 0.1, then σΦ−1(1−0.1) = 2.5631. Therefore, the false alarm rate and the detection rate are functions of the critical level α. For this particular example, we have the false alarm rate and detection rate in closed form, as functions of α: p F(α)=(cid:90) ∞ √ 1 e− 2y σ2 2 dy σΦ−1(1−α) 2πσ2 (cid:18) σΦ−1(1−α)(cid:19) =1−Φ =α, σ 592
9.5. ROC AND PRECISION-RECALL CURVE p D(α)=(cid:90) ∞ √ 1 e−(y 2− σµ 2)2 dy σΦ−1(1−α) 2πσ2 (cid:16) µ(cid:17) =1−Φ Φ−1(1−α)− . σ These give us the two curves on the left-hand side of Figure 9.24. What is an ROC curve? • It is a plot showing p on the y-axis and p on the x-axis. D F • p = detection rate (also known as the power of the test). D • p = false alarm rate (also known as the type 1 error of the test). F TheROCcurvetellsusthebehaviorofthedecisionruleaswechangethethresholdα. A graphical illustration is shown in Figure 9.25. There are a few key observations we need to pay attention to: Figure 9.25: Interpreting the ROC curve. • The ROC curve must go through (0,0). This happens when you always keep the null hypothesis or always declare class 0, no matter what observations. If you always keep H , certainly you will not make any false positive (or false alarm), because you 0 will never say H is wrong. Therefore, the detection rate (or the power of the test) is 0 also 0. This is a useless decision rule for both classification and hypothesis testing. • TheROCcurvemustgothrough(1,1).Thishappenswhenyoualwaysrejectthenull hypothesis, no matter what observations we have. If you always reject H , you will 0 always say that “there is a target”. As far as detection is concerned, you are perfect 593
CHAPTER 9. CONFIDENCE AND HYPOTHESIS because you have not missed any targets. However, the false positive rate is also high because you will falsely declare a target when there is nothing. Therefore, this is also a useless decision rule. • TheROCcurvetellsustheoperatingpointofthedecisionruleaswechangethethresh- old. A threshold is a universal concept for both hypothesis testing and classification. In hypothesis testing, we have the critical level α, say 0.05 or 0.1. In classification, we also have a threshold for judging whether a sample should be classified as class 1 or class0.Ofteninclassification,theintermediateestimatesareprobabilitiesordistances to decision boundaries. These real numbers need to be binarized to generate a binary decision. The ROC curve tells us that if you pick a threshold, your decision rule will have a certain p and p as predicted by the curve. If you want to tolerate a higher F D p , you can move along the curve to find your operating point. F • The ideal operating point on a ROC curve is when p =0 and p =1. However, this F D is a hypothetical situation that does not happen in any real decision rule. 9.5.2 Comparing ROC curves BecauseofhowtheROCcurvesareconstructed,everybinarydecisionrulehasitsownROC curve. Typically, when one tries to compare classification algorithms, the area under the curve(AUC)occupiedbytheROCcurveiscompared.AdecisionrulehavingalargerAUC is often a “better” decision rule. Toillustratetheideaofcomparingestimators,weconsideratrivialdecisionrulebased on a blind guess. Example 9.17. (A blind guess decision) Consider a decision rule that we reject H 0 withprobabilityαandkeepH withprobability1−α.Wecallthisablindguess,since 0 the decision rule ignores observation y. Mathematically, this trivial decision rule is (cid:40) 1, with probability α, δ(y)= 0, with probability 1−α. Find p , p , and AUC. F D Solution.Forthisdecisionrulewecomputeitsfalsepositiverate(orfalsealarmrate) and its true positive rate (or detection rate). However, since δ(y) is now random, we needtotaketheexpectationoverthetworandomstatesthatδ(y)cantake.Thisgives us (cid:20)(cid:90) (cid:21) p (α)=E δ(y)f (y)dy F 0 (cid:90) (cid:90) = 1·f (y)dyP[δ(y)=1]+ 0·f (y)dyP[δ(y)=0] 0 0 (cid:90) =α f (y)dy =α. 0 594
9.5. ROC AND PRECISION-RECALL CURVE Similarly, the detection rate is (cid:20)(cid:90) (cid:21) (cid:90) p (α)=E δ(y)f (y)dy =α f (y)dy =α. D 1 1 If we plot p as a function of p , we notice that the function is a straight line going D F from (0,0) to (1,1). This decision rule is useless. Comparing this with the Neyman- Pearson decision rule, it is clear that Neyman-Pearson has a larger AUC. The AUC for this trivial decision rule is the area of the triangle, which is 0.5. Figure 9.26: The ROC curve of the blind guess decision rule is a straight line. The AUC is 0.5. If you set α=0.5, then the decision rule becomes (cid:40) 1, with probability 1, δ(y)= 2 0, with probability 1. 2 This is equivalent to flipping a fair coin with probability 1/2 of declaring H and 1/2 0 declaring H . Its operating point is the yellow circle. 1 Computing the AUC can be done by calling special library functions. However, to spell out the details we demonstrate something more elementary. The program below is a piece of MATLAB code plotting two ROC curves corresponding to two different decision rules. The first decision rule is the trivial decision rule, where we have just shown that p (α) = p (α) = α. The second decision rule is the Neyman-Pearson decision rule, for F D which we showed in Figure 9.24 that p (α) = α and p (α) = 1−Φ(cid:0) Φ−1(1−α)− µ(cid:1) . F D σ Using the MATLAB code below, we can plot the two ROC curves shown in Figure 9.26. % MATLAB code to plot ROC curve sigma = 2; mu = 3; alphaset = linspace(0,1,1000); PF1 = zeros(1,1000); PD1 = zeros(1,1000); PF2 = zeros(1,1000); PD2 = zeros(1,1000); for i=1:1000 alpha = alphaset(i); 595
CHAPTER 9. CONFIDENCE AND HYPOTHESIS PF1(i) = alpha; PD1(i) = alpha; PF2(i) = alpha; PD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma); end figure; plot(PF1, PD1,’LineWidth’, 4, ’Color’, [0.8, 0, 0]); hold on; plot(PF2, PD2,’LineWidth’, 4, ’Color’, [0, 0, 0]); hold off; To compute the AUC we perform a numerical integration: (cid:90) (cid:88) AUC= p (α)·dp (α)≈ p (α )·∆p (α ) D F D i F i i (cid:88) (cid:2) (cid:3) = p (α )· p (α )−p (α ) , D i F i F i−1 i where α is the ith critical level we use to plot the ROC curve. (We assume that the α’s are i sorted in ascending order.) In MATLAB, the commands are auc1 = sum(PD1.*[0 diff(PF1)]) auc2 = sum(PD2.*[0 diff(PF2)]) The AUC of the two decision rules computed by MATLAB are 0.8561 and 0.5005, respec- tively.Thesmallslackof0.0005iscausedbythenumericalapproximationatthetail,which can be ignored as long as you are consistent for all the ROC curves. The commands for Python are analogous to the commands for MATLAB. # Python code to plot ROC curve import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats sigma = 2; mu = 3; alphaset = np.linspace(0,1,1000) PF1 = np.zeros(1000); PD1 = np.zeros(1000) PF2 = np.zeros(1000); PD2 = np.zeros(1000) for i in range(1000): alpha = alphaset[i] PF1[i] = alpha PD1[i] = alpha PF2[i] = alpha PD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) plt.plot(PF1,PD1) plt.plot(PF2,PD2) To compute the AUC, the Python code is (continuing from the previous code): 596
9.5. ROC AND PRECISION-RECALL CURVE auc1 = np.sum(PD1 * np.append(0, np.diff(PF1))) auc2 = np.sum(PD2 * np.append(0, np.diff(PF2))) It is possible to get a decision rule that is worse than a blind guess. The following example illustrates a trivial setup. Practice Exercise 9.6. (Flipped Neyman-Pearson). Consider two hypotheses H =Gaussian(0,σ2), 0 H =Gaussian(µ,σ2), µ>0. 1 Let α be the critical level. The Neyman-Pearson decision rule is (cid:40) 1, y ≥σΦ−1(1−α), δ∗(y)= 0, y <σΦ−1(1−α). Now, consider a flipped Neyman-Pearson decision rule (cid:40) 1, y <σΦ−1(1−α), δ+(y)= 0, y ≥σΦ−1(1−α). Find p , p , and AUC for the new decision rule δ+. F D Solution. Since we flip the rejection zone, the probability of false alarm is (cid:90) p (α)= δ+(y)f (y)dy F 0 =(cid:90) σΦ−1(1−α) √ 1 e− 2y σ2 2 dy 2πσ2 −∞ (cid:18) σΦ−1(1−α)(cid:19) =Φ σ =1−α. Similarly, the probability of detection is (cid:90) p (α)= δ+(y)f (y)dy D 1 =(cid:90) σΦ−1(1−α) √ 1 e−(y 2− σµ 2)2 dy 2πσ2 −∞ (cid:18) σΦ−1(1−α)−µ(cid:19) =Φ σ (cid:16) µ(cid:17) =Φ Φ−1(1−α)− . σ 597
CHAPTER 9. CONFIDENCE AND HYPOTHESIS If you plot p as a function of p , you will obtain a curve shown in Figure 9.27. D F The AUC for this flipped decision rule is 0.1439, whereas that for Neyman-Pearson is 0.8561. The two numbers are complements of each other, meaning that their sum is unity. Figure 9.27: The ROC curve of a flipped Neyman-Pearson decision rule. What if we arbitrarily construct a decision rule that is neither Neyman-Pearson nor the blind guess? The following example demonstrates one possible choice. Practice Exercise 9.7. Consider two hypotheses H =Gaussian(0,σ2), 0 H =Gaussian(µ,σ2), µ>0. 1 Let α be the critical level. Consider the following decision rule: (cid:40) 1, |y|≥σΦ−1(1−α), δ♣(y)= 0, |y|<σΦ−1(1−α). Find p , p , and AUC for the new decision rule δ♣. F D Solution. The probability of false alarm is (cid:90) p (α)= δ♣(y)f (y)dy F 0 =1−(cid:90) σΦ−1(1−α) √ 1 e− 2y σ2 2 dy −σΦ−1(1−α) 2πσ2 =1−Φ(cid:0) Φ−1(1−α)(cid:1) +Φ(cid:0) −Φ−1(1−α)(cid:1) =2α. 598
9.5. ROC AND PRECISION-RECALL CURVE Similarly, the probability of detection is (cid:90) p (α)= δ♣(y)f (y)dy D 1 =1−(cid:90) σΦ−1(1−α) √ 1 e−(y 2− σµ 2)2 dy −σΦ−1(1−α) 2πσ2 (cid:18) σΦ−1(1−α)−µ(cid:19) (cid:18) −σΦ−1(1−α)−µ(cid:19) =1−Φ +Φ σ σ (cid:16) µ(cid:17) (cid:16) µ(cid:17) =1−Φ Φ−1(1−α)− +Φ −Φ−1(1−α)− . σ σ If you plot p as a function of p , you will obtain a curve shown in Figure 9.28. D F The AUC for this proposed decision rule is 0.7534, whereas that of Neyman-Pearson is 0.8561. Therefore, the Neyman-Pearson decision rule is better. Figure 9.28: The ROC curve of a proposed decision rule. The MATLAB code we used to generate Figure 9.28 is shown below. Note that we need to separate the calculations of the two curves, because the proposed curve can only take 0<α<0.5. The Python code is implemented analogously. % MATLAB code to generate the ROC curve. sigma = 2; mu = 3; PF1 = zeros(1,1000); PD1 = zeros(1,1000); PF2 = zeros(1,1000); PD2 = zeros(1,1000); alphaset = linspace(0,0.5,1000); for i=1:1000 alpha = alphaset(i); PF1(i) = 2*alpha; PD1(i) = 1-(normcdf(norminv(1-alpha)-mu/sigma)-... normcdf(-norminv(1-alpha)-mu/sigma)); end alphaset = linspace(0,1,1000); 599
CHAPTER 9. CONFIDENCE AND HYPOTHESIS for i=1:1000 alpha = alphaset(i); PF2(i) = alpha; PD2(i) = 1-normcdf(norminv(1-alpha)-mu/sigma); end figure; plot(PF1, PD1,’LineWidth’, 4, ’Color’, [0.8, 0, 0]); hold on; plot(PF2, PD2,’LineWidth’, 4, ’Color’, [0, 0, 0]); hold off; import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats sigma = 2; mu = 3; PF1 = np.zeros(1000); PD1 = np.zeros(1000) PF2 = np.zeros(1000); PD2 = np.zeros(1000) alphaset = np.linspace(0,0.5,1000) for i in range(1000): alpha = alphaset[i] PF1[i] = 2*alpha PD1[i] = 1-(stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) \ -stats.norm.cdf(-stats.norm.ppf(1-alpha)-mu/sigma)) alphaset = np.linspace(0,1,1000) for i in range(1000): alpha = alphaset[i] PF2[i] = alpha PD2[i] = 1-stats.norm.cdf(stats.norm.ppf(1-alpha)-mu/sigma) plt.plot(PF1, PD1) plt.plot(PF2, PD2) 9.5.3 The ROC curve in practice If the Neyman-Pearson decision rule is the optimal rule, why don’t we always use it? The problem is that in practice we may not have access to the distributions. For example, if we classify images, how do we know that the data follows a Gaussian distribution or a mixture of distributions? Consequently, the ROC curves we discussed in the subsections above are the theoretical ROC curves. In practice, we plot the empirical ROC curves. Plotting an empirical ROC curve for a binary classification method (and hypothesis testing) is intuitive. The ingredients we need are a set of scores and a set of labels. The scores are the probability values determining the likelihood of a sample belonging to one class. Generally speaking, for empirical data this requires looking at the training data, building a model, and computing the likelihood. We will not go into the details of how a binary classifier is built. Instead, we assume that you have already built a binary classifier and have obtained the scores. Our goal is to show you how to plot the ROC curve. 600
9.5. ROC AND PRECISION-RECALL CURVE The following MATLAB code uses a dataset fisheriris. The code builds a binary classifier and returns the scores. % MATLAB code to train a classification algorithm. % Do not worry if you cannot understand this code. % It is not the focus on this book. load fisheriris pred = meas(51:end,1:2); resp = (1:100)’>50; mdl = fitglm(pred,resp,’Distribution’,’binomial’,’Link’,’logit’); scores = mdl.Fitted.Probability; labels = [ones(1,50), zeros(1,50)]; save(’ch9_ROC_example_data’,’scores’,’labels’); To give you an idea of how the scores of the classifier look, we plot the histogram of thescoresinFigure 9.29.Asyoucansee,thereisnocleardivisionbetweenthetwoclasses. No matter what threshold τ we use, some cases will be misclassified. Figure 9.29: The distribution of probability scores obtained from a binary classifier for the dataset fisheriris.Thegreenverticallinesrepresentthethresholdforturningthescoresintobinarydecisions. Anyscoregreaterthanτ willbeclassifiedasClass1,andanyscorethatislessthanτ willbeclassified as Class 0. These predicted labels would then be compared to the true labels to plot the ROC curve. Recall that the ROC curve is a function of p versus p . Using terminology from D F statistics, p is the true positive rate and p is the false positive rate. By sweeping a range D F of decision thresholds (over the scores), we can compute the corresponding p ’s and p ’s. F D On a computer this can be done by setting up two columns of labels: the true label labels and the predicted labels prediction. For any threshold τ, we binarize the scores to turn them into a decision vector. Then we count the number of true positives, true negatives, false positives, and false negatives. The total of these numbers will give us p and p . F D In MATLAB, the above description can be easily implemented by sweeping through the range of τ. % MATLAB code to generate an empirical ROC curve load ch9_ROC_example_data 601
CHAPTER 9. CONFIDENCE AND HYPOTHESIS tau = linspace(0,1,1000); for i=1:1000 idx = (scores <= tau(i)); predict = zeros(1,100); predict(idx) = 1; true_positive = 0; true_negative = 0; false_positive = 0; false_negative = 0; for j=1:100 if (predict(j)==1) && (labels(j)==1) true_positive = true_positive + 1; end if (predict(j)==1) && (labels(j)==0) false_positive = false_positive + 1; end if (predict(j)==0) && (labels(j)==1) false_negative = false_negative + 1; end if (predict(j)==0) && (labels(j)==0) true_negative = true_negative + 1; end end PF(i) = false_positive/50; PD(i) = true_positive/50; end plot(PF, PD, ’LineWidth’, 4, ’Color’, [0, 0, 0]); The Python codes of this problem are similar. We give them here for completeness. # Python code to generate an empirical ROC curve import numpy as np import matplotlib.pyplot as plt import scipy.stats as stats scores = np.loadtxt(’ch9_ROC_example_data.txt’) labels = np.append(np.ones(50), np.zeros(50)) tau = np.linspace(0,1,1000) PF = np.zeros(1000) PD = np.zeros(1000) for i in range(1000): idx = scores<= tau[i] predict = np.zeros(100) predict[idx] = 1 true_positive = 0; true_negative = 0 false_positive = 0; false_negative = 0 for j in range(100): if (predict[j]==1) and (labels[j]==1): true_positive += 1 if (predict[j]==1) and (labels[j]==0): false_positive += 1 if (predict[j]==0) and (labels[j]==1): false_negative += 1 if (predict[j]==0) and (labels[j]==0): true_negative += 1 PF[i] = false_positive/50 PD[i] = true_positive/50 plt.plot(PF, PD) 602
9.5. ROC AND PRECISION-RECALL CURVE The empirical ROC curve for this problem is shown in Figure 9.30. Each point on the curve is a coordinate (p ,p ), evaluated at a particular threshold τ. Mathematically, the F D decision rule we used was (cid:40) 1, score(y)≥τ, δ(y)= 0, score(y)<τ. For every τ, we have a false alarm rate and a detection rate. Since this is an empirical dataset with only 100 samples, there are many occasions where p does not change but F p increases, or p stays constant but p increases. For this particular example, we can D D F compute the AUC, which is 0.7948. Figure 9.30: The empirical ROC curve for the dataset fisheriris, using a classifier based on the logistic regression. Note that the empirical ROC is rough. It does not have the smooth concave shape of the theoretical ROC curve. One can prove that if the decision rule is Neyman-Pearson, i.e., if we conduct a likelihood ratio test, then the resulting ROC curve is concave. Otherwise, you can still obtain an empirical ROC curve for real datasets and classifiers. However, the shape is not necessarily concave. 9.5.4 The Precision-Recall (PR) curve Inmoderndatascience,analternativeperformancemetrictotheROCcurveistheprecision- recall (PR) curve. The precision and recall are defined as follows. Definition 9.4. Let TP = true positive, FP = false positive, FN = false negative. The precision is defined as TP p precision= = D , (9.40) TP+FP p +p D F and the recall is defined as TP p recall= = D =p . (9.41) TP+FN p +p D D M 603
CHAPTER 9. CONFIDENCE AND HYPOTHESIS In this definition, TP, FP, and FN are the numbers of samples that are classified as true positive,falsepositive,andfalsenegative,respectively.However,bothprecisionandrecallare definedasratiosofnumbers.Theratioscanbeequivalentlydefinedthroughtherates.Using ourterminology,thisgivesusthedefinitionsintermsofp ,p andp .Sincep =1−p , D F M D M it also holds that the recall is p . D Let us take a moment to consider the meanings of precision and recall. Precision is defined as TP # true positives precision= = . (9.42) TP+FP total # positives you claim The numerator of the precision is the number of true positive samples and the denominator is the total number of positives that you claim. This includes the true positives and the false positives. Therefore, precision measures how trustworthy your claim is. There are two scenarios to consider: • High precision: This means that among all the positives you claim, many of them are the true positives. Therefore, whatever you claim is trustworthy. One possibility for obtaining a high precision is that the critical level α of the Neyman-Pearson decision ruleapproaches1.Inotherwords,youareveryacceptingofthenullhypotheses.Thus, whenever you reject, it will be a reliable reject. • Low precision: This means that you are overclaiming the positives, and so there are many false positives. Thus, even though you claim many positives, not all are trust- worthy. One reason why low precision occurs is that you are too eager to reject the null. Thus you tend to overkill the unnecessary cases. A similar analysis can be applied to the recall. The recall is defined as TP # true positives recall= = . (9.43) TP+FN total # positives in the distribution The difference between the recall and the precision is the denominator. For recall, the denominator is the total number of positives in the distribution. We are not interested in knowing what you have claimed but in knowing how many of them are there in the distribution.Ifyouexaminethedefinitionusingp ,youcanseethatrecallistheprobability D of detection — how successfully you can detect a target. A high recall and a low recall can occur in two situations: • High recall: This means that you are very good at detecting the target or rejecting thenullappropriately.Ahighrecallcanhappenwhenthecriticallevelαislowsothat you never miss a target. However, if the critical level α is low, you will suffer from a low precision. • Low recall: This means that you are too accepting of the null hypotheses, and so you never claim that there is a target. As a result the number of successful detections is low.However,havingalowrecallcanbuyyouhighprecisionbecauseyoudonotreject the null unless it has extreme evidence (hence there is no false alarm.) As you can see from the discussions above, the precision-recall has a trade-off, just as theROCcurvedoes.SincethePRcurveandROCcurvearederivedfromp andp ,there F D isaone-to-onecorrespondence.Thiscanbeprovedbyrearrangingthetermsintheprevious theorem. 604
9.5. ROC AND PRECISION-RECALL CURVE Theorem 9.3. The false alarm rate p and the detection rate p can be expressed F D in terms of the precision and recall as recall(1−precision) p = , (9.44) F precision p =recall. D This result implies that whenever we have an ROC curve we can convert it to a PR curve. Moreover, whenever we have a PR curve we can convert it to an ROC curve. Therefore, there is no additional information one can squeeze out by converting the curves. What we can claim, at most, is that the two curves offer different ways of interpreting the decision rule. To illustrate the equivalence between an ROC curve and a PR curve, we plot two differentdecisionrulesinFigure9.31.AnypointontheROCcurvewillhaveacorresponding point on the PR curve, and vice versa. Figure 9.31: There is a one-to-one correspondence between the ROC curve and the PR curve. The MATLAB and Python codes for generating the PR curve are straightforward. Assuming that we have run the code used to generate Figure 9.28, we plot the PR curve as follows (this will give us Figure 9.31). % MATLAB code to generate a PR curve precision1 = PD1./(PD1+PF1); precision2 = PD2./(PD2+PF2); recall1 = PD1; recall2 = PD2; plot(recall1, precision1, ’LineWidth’, 4); hold on; plot(recall2, precision2, ’LineWidth’, 4); hold off; 605
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Practice Exercise 9.8. Suppose that the decision rule is a blind guess: (cid:40) 1, with probability α, δ(y)= 0, with probability 1−α, Plot the ROC curve and the PR curve. Solution:Aswehaveshownearlier,p (α)andp (α)forthisdecisionrulearep (α)= F D F α and p (α)=α. Therefore, D p α 1 precision= D = = , and recall=p =α. p +p α+α 2 D D F Thus the PR curve is a straight line with a level of 0.5. Figure 9.32: The PR curve of a blind-guess decision rule is a straight line. Practice Exercise 9.9. Convert the ROC curve in Figure 9.30 to a PR curve. Solution:Theconversionisdonebyfirstcomputingp andp .Definingtheprecision F D and recall in terms of p and p , we plot the PR curves below. F D Figure 9.33: The PR curve of a real dataset. 606
9.6. SUMMARY As you can see from the figure, the PR curve behaves very differently from the ROC curve. It is sometimes argued that the two curves can be interpreted differently, even though they describe the same decision rule for the same dataset. 9.6 Summary In this chapter, we have discussed five principles for quantifying the confidence of an esti- matorandmakingstatisticaldecisions.Tosummarizethechapter,weclarifyafewcommon misconceptions about these topics. • Confidence interval. Students frequently become confused about the meaning of a confidence interval. It is not the interval that 95% of the samples will fall inside. It is also not the interval within which the estimator has a 95% chance to show up. A confidence interval is a random interval that has a 95% chance of including the populationparameter.Abetterwaytothinkaboutaconfidenceintervalistothinkof it as an alternative to a point estimate. A point estimate only gives a point, whereas aconfidenceintervalextendsthepointtoaninterval.Alltherandomnessofthepoint estimate is also there in the confidence interval. However, if the confidence interval is narrow, there is a good chance for the point estimate to be accurate. • Bootstrapping. The most common misconception about bootstrapping is that it can createsomethingfromnothing.Anothermisconceptionisthatbootstrappingcanmake your estimates better. Both beliefs are wrong. Bootstrapping is a technique for esti- mating the estimator’s variance, and consequently it provides a confidence interval. Bootstrapping does not improve the point estimate, no matter how many bootstrap- ping samples you synthesize. Bootstrapping works because the sampling with the re- placement step is equivalent to drawing samples from the empirical distribution. The whole process relies on the proximity between the empirical distribution and the true population.Ifyoudonothaveenoughsamplesandtheempiricaldistributiondoesnot approximate the population, bootstrapping will not work. Therefore, bootstrapping does not create something from nothing; it uses whatever you have and tells you how reliable the estimate is. • Hypothesis testing. Students are often overwhelmed at first by the great number of tests one can use for hypothesis testing, e.g., p-value, critical value, Z-test, T-test, χ2 test, F-test, etc. Our advice is to forget about them and remember that hypothesis testing is a court trial. Your job is to decide whether you have enough evidence to declare that the defendant is guilty. To reach a guilty verdict, you need to make sure that the test statistic is unlikely to happen. Therefore, the best practice is to draw the distributions of the test statistic and ask yourself how likely is it that the test statistic has such a value. When you draw the pictures of the distributions, you will know whether you should use a Gaussian Z, a Student’s t, a χ2, a F-statistic, etc. Whenyouexaminethelikelihoodoftheteststatistic,youwillknowwhetheryouwant to use the p-value or the critical value. If you follow this principle, you will never be confused by the oceans of tests you find in the textbooks. 607
CHAPTER 9. CONFIDENCE AND HYPOTHESIS • Neyman-Pearson. Beginners often find Neyman-Pearson abstract and do not under- stand why it is useful. In this chapter, however, we have explained why we need to understand Neyman-Pearson. It is a very general framework for many kinds of hy- pothesistestingproblems.Allitsaysisthatifwewanttomaximizethedetectionrate while maintaining the false alarm rate, then the optimal testing procedure boils down tothecritical-valuetestandthep-valuetest.Thisgivesusacertificatethatourusual hypothesis testing is optimal according to the Neyman-Pearson framework. • ROC and PR curves. On the internet nowadays there is a huge quantity of articles, blogs, and tutorials about how to plot the ROC curve and the PR curve. Often these curvesareexplainedthroughprogrammingexamplessuchasPython,R,orMATLAB. OuradviceforstudyingtheROCcurveandthePRcurveistogobacktotheNeyman- Pearson framework. These two curves do not come out of the blue. The ROC curve is the natural figure explaining the objective and the constraint in the Neyman-Pearson framework. By changing the coordinates, we obtain the PR curve. Therefore, the two curves are the same in terms of the amount of information, but they offer different interpretations. 9.7 Reference Confidence Interval 9-1 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 9.1. 9-2 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.3. 9-3 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.2. 9-4 Larry Wasserman, All of Statistics, Springer 2003. Chapter 6. 9-5 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 8.4. Bootstrapping 9-6 TrevorHastie,RobertTibshirani,andJeromeFriedman,ElementsofStatisticalLearn- ing, Springer, 2nd Edition. Chapter 8.2. 9-7 Larry Wasserman, All of Statistics, Springer 2003. Chapter 8. 9-8 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.4. 9-9 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.9. 608
9.8. PROBLEMS Hypothesis Testing 9-10 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 4.5. 9-11 Athanasios Papoulis and S. Unnikrishna Pillai, Probability, Random Variables and Stochastic Processes, McGraw-Hill, 4th Edition, 2001. Chapter 8. 9-12 Alberto Leon-Garcia, Probability, Statistics, and Random Processes for Electrical En- gineering, Prentice Hall, 3rd Edition, 2008. Chapter 8.5. 9-13 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. Chapter 9. 9-14 Michael J Evans and Jeffrey S. Rosenthal, Probability and Statistics, W. H. Freeman, 2nd Edition, 2009. Chapter 6.3. 9-15 Larry Wasserman, All of Statistics, Springer 2003. Chapter 10. 9-16 Laura Simon, Introduction to Mathematical Statistics, Penn State University STAT 415Textbook,Online materials.Accessed12/2020. https://online.stat.psu.edu/ stat415/ Neyman-Pearson and ROC curves 9-17 RobertV.Hogg,JosephW.McKean,andAllenT.Craig,IntroductiontoMathematical Statistics, Pearson, 7th Edition, 2013. Chapter 8. 9-18 H. Vincent Poor, An Introduction Signal Detection and Estimation, Springer, 1998. 9-19 Bernard C. Levy, Principles of Signal Detection and Parameter Estimation, Springer, 2008. 9-20 Steven M. Kay, Fundamentals of Statistical Signal Processing: Estimation Theory, Prentice-Hall, 1993. 9-21 StevenM.Kay,FundamentalsofStatisticalSignalProcessing:DetectionTheory,Prentice- Hall, 1998. 9.8 Problems Exercise 1. Consideri.i.d.GaussianrandomvariablesX ,...,X withanunknownmeanθandaknown 1 N varianceσ2 =1.SupposeN =30.Findtheconfidencelevel1−αfortheconfidenceintervals of the mean Θ(cid:98): (a) I =[Θ(cid:98) − 2√.14σ,Θ(cid:98) + 2√.14σ] N N (b) I =[Θ(cid:98) − 1√.85σ,Θ(cid:98) + 1√.85σ] N N 609
CHAPTER 9. CONFIDENCE AND HYPOTHESIS Exercise 2. Suppose that we have conducted an experiment with N = 100 samples. A 95% confidence interval of the mean was 0.45≤µ≤0.82. (a) Woulda99%confidenceintervalcalculatedfromthesampledatabewiderornarrower? (b) Is it correct to interpret the confidence interval as saying that there is a 95% chance thatµisbetween0.49and0.82?Youmayansweryes,no,orpartiallycorrect.Explain. (c) Is it correct to say that if we conduct the experiment 1000 times, there will be 950 confidence intervals that will contain µ? You may answer yes, no, or partially correct. Explain. Exercise 3. Supposethatwehaveconductedanexperiment.Weknowthatσ =25.WeobtainedN =20 samples and found that the sample mean is Θ(cid:98) =1014. (a) Construct a 95% two-sided confidence interval of Θ(cid:98). (b) Construct a 95% one-sided confidence interval (the lower tail) of Θ(cid:98). Exercise 4. Let X 1,...,X N be i.i.d. Gaussian with X n ∼ Gaussian(0,1). Let Y n = eXn, and suppose we have N =100 samples. We want to compute a 95% confidence interval for skewness. (a) Randomly subsample the dataset with B = 30 samples. Repeat the exercise 5 times. Plot the resulting histograms using MATLAB or Python. (b) Repeat(a)forM =500timesandcomputethe95%bootstrappedconfidenceinterval of the skewness. (c) Try using a larger B = 70 and a smaller B = 10. Report the 95% bootstrapped confidence interval of the skewness. Exercise 5. Let X 1,...,X N be i.i.d. uniform with X n ∼ Uniform(0,θ). Let Θ(cid:98) = max{X 1,...,X N}. Generate a dataset of N =50 with θ =1. (a) Find the distribution of the estimator Θ(cid:98). (b) Show that P[Θ(cid:98) =θ]=1−(1−(1/n))N. Thus, as N →∞, we have P[Θ(cid:98) =θ]=0. (c) Use Python or MATLAB to generate the histogram of Θ(cid:98) from bootstrapping. How does the bootstrapped histogram look as N grows? Why? Exercise 6. Let X be a Gaussian random variable with unknown mean and unknown variance. It was found that with N =15, N N (cid:88) (cid:88) X =250, X2 =10000. n n n=1 n=1 610
9.8. PROBLEMS Find a 95% confidence interval of the mean of X. Exercise 7. Let Θ(cid:98) be the sample mean of a dataset containing N samples. It is known that the samples are drawn from Gaussian(θ,32). Find N such that P[Θ(cid:98) −1≤θ ≤Θ(cid:98) +1]=0.95. Exercise 8. Which of the following statements are valid hypothesis testing problems? (a) H : µ=25 and H : µ(cid:54)=25. 0 1 (b) H : σ >10 and H : σ =10. 0 1 (c) H : X =50 and H : X (cid:54)=50. 0 1 (d) H : p-value = 0.1, H : p-value = 0.5. 0 1 Exercise 9. It is claimed that the mean is θ = 12 with a standard deviation 0.5. Consider H : θ = 12 0 and H 1: θ < 12. Ten samples are obtained, and it is found that Θ(cid:98) = 13.5. With a 95% confidence level, should we accept or reject the null hypothesis? Exercise 10. Consider a hypothesis testing problem: H : θ = 175 versus an alternative hypothesis H : 0 1 θ >175. Assume N =10 and σ =20. (a) Find the type 1 error if the critical region is Θ(cid:98) >185. (b) Find the type 2 error if the true mean is 195. Exercise 11. Consider H : θ = 30000 versus an alternative hypothesis H : θ > 30000. Suppose N = 16, 0 1 and let σ =1500. (a) If we want α=0.01, what is z ? α (b) What is the type 2 error when θ =31000? Exercise 12. Let W ∼Gaussian(0,σ2), and consider two hypotheses: n H : X =θ +W , n=1,...,N, 0 n 0 n H : X =θ +W , n=1,...,N. 1 n 1 n Let X =(1/N)(cid:80)N X . n=1 n 611
CHAPTER 9. CONFIDENCE AND HYPOTHESIS (a) Show that the likelihood of observing X ,...,X given H is 1 N 0 (cid:40) N (cid:41) 1 1 (cid:88) f (x|H )= exp − (X −θ )2 . X 0 (2πσ2)N/2 2σ2 n 0 n=1 (b) Find the likelihood f (x|H ) of observing X ,...,X given H . X 1 1 N 1 (c) The likelihood ratio test states that f (x|H ) X 1 ≷H1 τ. f X(x|H 0) H0 Show that the likelihood ratio test is given by θ +θ σ2logτ X ≷H1 0 1 + . H0 2 N(θ 1−θ 0) 612
Chapter 10 Random Processes In modern data science, many problems involve time. The stock market changes every minute;aspeechsignalchangeseverymillisecond;acarchangesitssteeringangleconstantly; the examples are endless. A common theme among all these examples is randomness. We do not know whether a stock will go up or down tomorrow, although we may be able to make some predictions based on previous observations. We do not know the next word of a sentence, but we can guess based on the context. Random processes are tools that can be appliedtothesesituations.Wetreatarandomprocessasaninfinitelylongvectorofrandom variables wherethe correlationsbetween theindividualvariables definethe statisticalprop- erties of the process. If we can determine these correlations, we will be able to summarize the past and predict the future. The objective of this chapter is to introduce the basic concepts of random processes. Given the breadth of the subject, we can only cover the most elementary results, but they are sufficient for many engineering and data science problems. However, there are complex situations for which these elementary results will be insufficient. The references at the end of this chapter contain more in-depth discussions of random processes. Plan of this chapter We begin by outlining the definition of random processes and ways to characterize their randomnessinSection10.1.InSection10.2wediscussthemeanfunction,theautocorrelation function, and the autocovariance function of a random process. In Section 10.3 we look at a special subclass of random processes known as the wide-sense stationary processes. Wide- sense stationary processes allow us to use tools in the Fourier domain to make statistical statements. Based on wide-sense stationary processes, we discuss power spectral density in Section10.4.Withthisconcept,wecanaskwhatwillhappentotherandomprocesswhenwe passitthroughalineartransformation.InSection10.5wediscusssuchinteractionsbetween therandomprocessandalineartime-invariantsystem.Finally,wediscussapracticalusage of random processes in the subject of optimal linear filters in Section 10.6. 613
CHAPTER 10. RANDOM PROCESSES 10.1 Basic Concepts 10.1.1 Everything you need to know about a random process Here is the single most important thing you need to remember about random processes: What is a random process? A random process is a function indexed by a random key. That’s it. Now you may be wondering what exactly a “function indexed by a random key” means. To help you see the picture, we consider two examples. Example 10.1. We consider a set of straight lines. We define two random variables a and b that are uniformly distributed in a certain range. We then define a function: f(t)=at+b, −2≤t≤2. (10.1) Clearly,f(t)isafunctionoftimet.Butsinceaandbarerandom,f(t)isalsorandom. The randomness iscausedby a and b. To emphasize this dependency, we write f(t) as f(t,ξ)=a(ξ)t+b(ξ), −2≤t≤2, where ξ ∈ Ω denotes the random index of the constants (a,b) and Ω is the sample space of ξ. Therefore, by picking a different pair of constants (a(ξ),b(ξ)), we will have a different function f(t,ξ), which in our case is a straight line of different slope and y-intercept. 1 0.5 0 -0.5 -1 -2 -1 0 1 2 t )t(f Figure 10.1: The set of straight lines f(x)=ax+b where a,b∈R. As a special case of the example, suppose that the sample space contains only two pairs of constants: (a,b) = (1.2,0.6) and (a,b) = (−0.75,1.8). The probability of 614
10.1. BASIC CONCEPTS getting either pair is 1. Then the function f(t,ξ) will take two forms: 2 (cid:40) 1.2t+0.6, with probability 1, f(t,ξ)= 2 −0.75t+1.8, with probability 1. 2 Every time you pick a sample you pick one of the two functions, either f(t,ξ ) or 1 f(t,ξ ).Sowesaythatf(t,ξ)isarandomprocessbecauseitisafunctionf(t)indexed 2 by a random key ξ. Example 10.2. This example studies the function f(t)=cos(ω t+Θ), −1≤t≤1, 0 whereΘisarandomphasedistributeduniformlyovertherange[0,2π].Dependingon therandomnessofΘ,thefunctionf(t)willtakeadifferentphaseoffset.Toemphasize this dependency, we write f(t,ξ)=cos(ω t+Θ(ξ)), −1≤t≤1. (10.2) 0 2 1 0 -1 -2 -1 -0.5 0 0.5 1 t )t(f Figure 10.2: The set of phase-shifted cosines f(t)=cos(ω t+θ) where θ∈[0,2π]. 0 Again,ξ denotestheindexoftherandomvariableΘ.SinceΘisdrawnuniformly from the interval [0,2π], the following functions are two possible realizations: (cid:18) (cid:19) 3π f(t,ξ )=cos ω t+ , −1≤t≤1, 1 0 4 (cid:18) (cid:19) 7π f(t,ξ )=cos ω t− , −1≤t≤1. 2 0 3 Just as with the previous example, f(t) is a function indexed by a random key ξ. Thesetwoexamplesshouldgiveyouafeelingforwhattoexpectfromarandomprocess. A random process is quite similar to a random variable because they are both contained in a certain sample space. For (discrete) random variables, the sample space is a collection of outcomes {ξ ,ξ ,...,ξ }. The random variable X : F → R is a mapping that maps 1 2 N ξ to X(ξ ), where X(ξ ) is a number. For random processes, the sample space is also n n n 615
CHAPTER 10. RANDOM PROCESSES {ξ ,ξ ,...,ξ }. However, the mapping X does not map ξ to a number X(ξ ) but to a 1 2 N n n functionX(t,ξ ).Afunctionhasthetimeindext,whichisabsentinthenumber.Therefore, n for the same ξ , X(t ,ξ ) can take one value and X(t ,ξ ) can take another value. n 1 n 2 n Figure 10.3: The sample space of a random process X(t,ξ) contains many functions. Therefore, each random realization is a function. Figure 10.3 shows the sample space of a random process. Each outcome in the sample space is a function. The probability of getting a function is specified by the probability mass or the probability density of the associated random key ξ. If you put your hand into the sample space, the sample you pick will be a function that will change with time and is indexed by the random key. From our discussions of joint random variables in Chapter 5, you can think of the function as a vector. When you pull a sample from the sample space, you pull the entire vector and not just an element. 10.1.2 Statistical and temporal perspectives Sincearandomprocessisafunctionindexedbyarandomkey,itisatwo-dimensionalobject. It is a function both of time t and of the random key ξ. That’s why we use the notation X(t,ξ) to denote a random process. These two axes play different roles, as illustrated in Figure 10.4. Temporal perspective: Let us fix the random key at ξ = ξ . This gives us a function 0 X(t,ξ ). Since ξ is already fixed at ξ , we are looking at a particular realization drawn 0 0 from the sample space. This realization is expressed as a function X(t,ξ ), which is just 0 a deterministic function that evolves over time. There is no randomness associated with it. This is analogous to a random variable. While X itself is a random variable, by fixing the random key ξ = ξ , X(ξ ) is just a real number. For random processes, X(t,ξ ) now 0 0 0 becomes a function. SinceX(t,ξ )isafunctionthatevolvesovertime,weviewitalongthehorizontalaxis. 0 For example, we can study the sequence X(t ,ξ ),X(t ,ξ ),...,X(t ,ξ ), 1 0 2 0 K 0 where t ,...,t are the time indices of the function. This sequence is deterministic and is 1 K just a sequence of numbers, although the numbers evolve as t changes. Statistical perspective: The other perspective, which could be slightly more abstract, is the statistical perspective. Let us fix the time at t=t . The random key ξ can take any 0 616
10.1. BASIC CONCEPTS (a) Temporal perspective (b) Statistical perspective Figure 10.4: Temporal and statistical perspectives of a random process. For the temporal perspective (which we call the horizontal perspective), we fix the random key ξ and look at the function in time. For the statistical perspective (which we call the vertical perspective), we fix the time and look at the function at different random keys. statedefinedinthesamplespace.Soifthesamplespacecontains{ξ ,...,ξ },thesequence 1 N {X(t ,ξ ),...,X(t ,ξ )} is a sequence of random variables, because the ξ’s can go from 0 1 0 N one state to another state. A good way to visualize the statistical perspective is the vertical perspective in which we write the sequence as a vertical column of random variables: X(t ,ξ ) 0 1 X(t ,ξ ) 0 2 . . . X(t ,ξ ) 0 N That is, if you fix the time at t = t , you are getting a sequence of random variables. The 0 probability of getting a particular value X(t ) depends on which random state you land on. 0 Why do we bother to differentiate the temporal perspective and the statistical per- spective? The reason is that the operations associated with the two are different, even if sometimes they give you the same result. For example, if we take the temporal average of the random process, we get a number: 1 (cid:90) T X(ξ)= X(t,ξ)dt. (10.3) T 0 Wecallthisthe“temporalaverage”becausewehaveintegratedthefunctionovertime.The resulting value will not change with time. However, X(ξ) depends on the random key you provide. If you pick a different random realization, X(ξ) will take a different value. So the temporal average is a random variable. On the other hand, if we take the statistical average of the random process, we get (cid:90) E[X(t)]= X(t,ξ)p(ξ)dξ, (10.4) Ω 617
CHAPTER 10. RANDOM PROCESSES where p(ξ) is the PDF of the random key ξ. We call this the statistical average because we have taken the expectation over all possible random keys. The resulting object E[X(t)] is deterministic but a function of time. No matter how you look at the temporal average or the statistical average, they are different with the following exception: that X(ξ)=const and E[X(t)]=const, for example, X(ξ) = E[X(t)] = 0. This happens only for some special (and useful) random processes known as ergodic processes that allow us to approximate the statistical average using the temporal average, with some guarantees derived from the law of large numbers. We will return to this point later. Example 10.3. Let A∼Uniform[0,1]. Define X(t,ξ)=A(ξ)cos(2πt). In this example, the magnitude A(ξ) is a random variable depending on the random key ξ. For example if we draw ξ , perhaps we will get a value A(ξ ) = 0.5. 1 1 Then X(t,ξ ) = 0.5cos(2πt). To take another example, if we draw ξ , we may get 1 2 A(ξ ) = 1. Then X(t,ξ ) = 1cos(2πt). Figure 10.5 shows a few random realizations 2 2 of the cosines. We can look at X(t,ξ) from the statistical and the temporal views. 1 X (t) 1 X (t) 2 0.5 X (t) 3 X (t) 4 X (t) 5 0 -0.5 -1 -2 -1 0 1 2 Figure 10.5: Five different realizations of the random process X(t)=Acos(2πt). • Statistical View: Fix t (for example t=10). In this case, we have X(t,ξ)=A(ξ)cos(2π(10)) =A(ξ)cos(20π), which is a random variable because cos(20π) is a constant. The randomness of X comes from the fact that A(ξ)∼ Uniform[0,1]. • Temporal View: Fix ξ (for example A(ξ)=0.7). In this case, we have X(t,ξ)=0.7cos(2πt), which is a deterministic function of t. 618
10.1. BASIC CONCEPTS Example 10.4. Let A be a discrete random variable with a PMF 1 1 P(A=+1)= and P(A=−1)= . 2 2 We define the function X[n,ξ] = A(ξ)(−1)n. In this example, A can only take two states. If A=+1, then X[n,ξ]=(−1)n. If A=−1, then X[n,ξ]=(−1)n+1. 1.5 1.5 X(n) X(n) 1 2 1 1 0.5 0.5 0 0 -0.5 -0.5 -1 -1 -1.5 -1.5 0 1 2 3 4 5 0 1 2 3 4 5 Figure 10.6: Realizations of the random process X[n]=A(−1)n. ThegraphicalillustrationofthisexampleisshowninFigure 10.6.Again,wecan look at X[n,ξ] from two views. • Statistical View: Fix n, say n=10. Then, (cid:40) (−1)10 =1, with prob1/2, X(ξ)= (−1)11 =−1, with prob1/2, which is a Bernoulli random variable. • Temporal View: Fix ξ. Then, (cid:40) (−1)n, if A=+1, X[n]= (−1)n+1, if A=−1, which is a time series. Inthisexample,weseethatthesamplespaceofX(n,ξ)consistsofonlytwofunctions with probabilities 1 P(X[n]=(−1)n)= , 2 1 P(X[n]=(−1)n+1)= , 2 Therefore, if there is a sequence outside the sample space, e.g., P(cid:0) X[n]=(cid:2) 1 1 1 −1 1 −1 ··· (cid:3)(cid:1) =0 then the probability of obtaining that sequence is 0. 619
CHAPTER 10. RANDOM PROCESSES What do we mean by statistical average and temporal average? • Statistical average: Take the expectation of X(t,ξ) over ξ. This is the vertical average. • Temporal average: Take the expectation of X(t,ξ) over t. This is the horizontal average. • In general, statistical average (cid:54)= temporal average. 10.2 Mean and Correlation Functions Given a random variable, we often want to know the expectation and variance, and often we also want to know the expectation and variance for the random processes. Nevertheless, we need to consider the time axis. In this section, we discuss the mean function and the autocorrelation function. 10.2.1 Mean function Definition 10.1. The mean function µ (t) of a random process X(t) is X µ (t)=E[X(t)]. (10.5) X Let’s consider the “expectation” of X(t). Recall that a random process is actually X(t,ξ) whereξ istherandomkey.Therefore,theexpectationistakenwithrespecttoξ,ortostate it more explicitly, (cid:90) µ (t)=E[X(t)]= X(t,ξ)p(ξ)dξ, X Ω wherep(ξ)isthePDFoftherandomkey.Thisisanabstractdefinition,butitisnotdifficult to understand if you follow the example below. Example 10.5. Let A∼Uniform[0,1], and let X(t)=Acos(2πt). Find µ (t). X Solution. The solution to this problem is actually very simple: µ (t)=E[X(t)]=E[Acos(2πt)] X 1 =cos(2πt)E[A]= cos(2πt). 2 So the answer is µ (t)= 1cos(2πt). X 2 We can link the equations to the definition more explicitly. To do so, we rewrite X(t) as X(t,ξ)=A(ξ)cos(2πt). 620
10.2. MEAN AND CORRELATION FUNCTIONS Then we take the expectation over A: (cid:90) (cid:90) 1 µ (t)= X(t,a)p (a)da= acos(2πt)·1da X A Ω 0 (cid:20) a2(cid:21)1 1 =cos(2πt) = cos(2πt). 2 2 0 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.7: The mean function of X(t)=Acos(2πt). An illustration is provided in Figure 10.7, in which we observe many random realizations of the random process X(t,ξ). On top of these, we also see the mean function. The way to visualize the mean function is to use the statistical perspective. That is, fix a time t and look at all the possible values that the function can take. For example,ifwefixt=t ,thenwewillhaveasetofrealizationsofonerandomvariable: 0 (cid:26) (cid:27) 0.71cos(2πt ), 0.58cos(2πt ), ..., 0.93cos(2πt ) → take expectation 0 0 0 Therefore,whenwetaketheexpectation,itisthatoftheunderlyingrandomvariable. If we move to another timestamp t=t , we will have a different expectation because 1 cos(2πt ) now becomes cos(2πt ). 0 1 The MATLAB/Python codes used to generate Figure 10.7 are shown below. You can also replace the line 0.5*cos(2*pi*t) by the mean function mean(X) (in MATLAB). % MATLAB code for Example 10.5 x = zeros(1000,20); t = linspace(-2,2,1000); for i=1:20 X(:,i) = rand(1)*cos(2*pi*t); end plot(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; plot(t, 0.5*cos(2*pi*t), ’LineWidth’, 4, ’Color’, [0.6 0 0]); # Python code for Example 10.5 x = np.zeros((1000,20)) 621
CHAPTER 10. RANDOM PROCESSES t = np.linspace(-2,2,1000) for i in range(20): x[:,i] = np.random.rand(1)*np.cos(2*np.pi*t) plt.plot(t,x,color=’gray’) plt.plot(t,0.5*np.cos(2*np.pi*t),color=’red’) plt.show() Example 10.6. Let Θ∼Uniform[−π,π], and let X(t)=cos(ωt+Θ). Find µ (t). X Solution. (cid:90) π 1 µ (t)=E[cos(ωt+Θ)]= cos(ωt+θ)· dθ =0. X 2π −π Again, as in the previous example, we can try to map this simple calculation with the definition. Write X(t) as X(t,ξ)=cos(ωt+Θ(ξ)). Then the expectation is (cid:90) µ (t)= cos(ωt+θ)p (θ)dθ X Θ Ω (cid:90) π 1 = cos(ωt+θ)· dθ =0. 2π −π 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.8: The mean function of X(t)=cos(ωt+Θ). Figure 10.8 illustrates the random realizations for X(t) = cos(ωt+Θ) and the meanfunction.Thezeromeanshouldnotbeasurprisebecauseifwetakethestatistical average (the vertical average) across all the possible values at any time instant, the positive and negative values of the realizations will make the mean zero. Weshouldemphasizethatthestatisticalaverageisnotthesameasthetemporal average, even if they give you the same value. Why do we say that? If we calculate the temporal average of the function cos(ωt+θ ) for a specific value Θ=θ , then we 0 0 622
10.2. MEAN AND CORRELATION FUNCTIONS have 1 (cid:90) T X = cos(ωt+θ )dt=0, T 0 0 assuming that T is a multiple of the cosine period. This implies that the temporal average is zero, which is the same as the statistical average. This gives us an example inwhichthestatisticalaverageandthetemporalaveragehavethesamevalue,although we know they are two completely different things. The MATLAB/Python codes used to generate Figure 10.8 are shown below. % MATLAB code for Example 10.6 x = zeros(1000,20); t = linspace(-2,2,1000); for i=1:20 X(:,i) = cos(2*pi*t+2*pi*rand(1)); end plot(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; plot(t, 0*cos(2*pi*t), ’LineWidth’, 4, ’Color’, [0.6 0 0]); # Python code for Example 10.6 x = np.zeros((1000,20)) t = np.linspace(-2,2,1000) for i in range(20): Theta = 2*np.pi*(np.random.rand(1)) x[:,i] = np.cos(2*np.pi*t+Theta) plt.plot(t,x,color=’gray’) plt.plot(t,np.zeros((1000,1)),color=’red’) plt.show() Example10.7.Letusconsideradiscrete-timerandomprocess.LetX[n]=Sn,where S ∼Uniform[0,1]. Find µ [n]. X (cid:90) 1 1 µ [n]=E[sn]= sn ds= . X n+1 0 In this example the randomness goes with the constant s. Thus, if we write X[n] as X[n,ξ]=[S(ξ)]n, the expectation is (cid:90) (cid:90) 1 1 E[X[n]]= snp (s)ds= sn·1ds= . S n+1 Ω 0 The graphical illustration is provided in Figure 10.9. 623
CHAPTER 10. RANDOM PROCESSES 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 0 5 10 15 20 Figure 10.9: The mean function of X[n]=Sn, where S ∼Uniform[0,1]. TheMATLABcodeusedtogenerateFigure 10.9isshownbelow.WeskipthePython implementation because it is straightforward. % MATLAB code for Example 10.7 t = 0:20; for i=1:20 X(:,i) = rand(1).^t; end stem(t, X, ’LineWidth’, 2, ’Color’, [0.8 0.8 0.8]); hold on; stem(t, 1./(t+1), ’LineWidth’, 2, ’MarkerSize’, 8); 10.2.2 Autocorrelation function Inrandomprocesses,thenotionsof“variance”and“covariance”aretrickierthanforrandom variables. Let us first define the concept of an autocorrelation function. Definition 10.2. The autocorrelation function of a random process X(t) is R (t ,t )=E[X(t )X(t )]. (10.6) X 1 2 1 2 R (t ,t ) is not difficult to calculate — just integrate X(t )X(t ) using the appropriate X 1 2 1 2 PDFs. Example 10.8. Let A∼Uniform[0,1], X(t)=Acos(2πt). Find R (t ,t ). X 1 2 Solution. R (t ,t )=E[Acos(2πt )Acos(2πt )] X 1 2 1 2 1 =E[A2]cos(2πt )cos(2πt )= cos(2πt )cos(2πt ). 1 2 3 1 2 624
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.9. Let Θ∼Uniform[−π,π], X(t)=cos(ωt+Θ). Find R (t ,t ). X 1 2 Solution. R (t ,t )=E[cos(ωt +Θ)cos(ωt +Θ)] X 1 2 1 2 1 (cid:90) π = cos(ωt +θ)cos(ωt +θ)dθ 2π 1 2 −π (a) 1 (cid:90) π 1(cid:20) (cid:21) = cos(ω(t +t )+2θ)+cos(ω(t −t )) dθ 2π 2 1 2 1 2 −π 1 (cid:16) (cid:17) = cos ω(t −t ) , 2 1 2 where in (a) we applied the trigonometric formula: 1 cosAcosB = [cos(A+B)+cos(A−B)], 2 Asyoucansee,thecalculationsarenotdifficult.Thetrickythingistheinterpretation of R (t ,t ). X 1 2 How do we understand the meaning of E[X(t )X(t )]? 1 2 E[X(t )X(t )] is analogous to the correlation E[XY] between two random variables 1 2 X and Y. The autocorrelation function E[X(t )X(t )] is analogous to the correlation E[XY] in rela- 1 2 tion to a pair of random variables. In our discussions of E[XY], we mentioned that E[XY] couldberegardedastheinnerproductoftwovectors,andsoitisameasureofthecloseness between X and Y. Now, if we substitute X and Y with X(t ) and X(t ) respectively, then 1 2 weareeffectivelyaskingabouttheclosenessbetweenX(t )andX(t ).So,inanutshell,the 1 2 autocorrelation function tells us the correlation between the function at two different time stamps. What do we mean by the correlation between two timestamps? Remember that X(t ) 1 and X(t ) are two random variables. Consider the following example. 2 Example10.10.LetX(t)=Acos(2πt),whereA∼Uniform[0,1].FindE[X(0)X(0.5)]. Solution. If X(t)=Acos(2πt), then X(0)=Acos(0)=A, X(0.5)=Acos(π)=−A. When you have two random variables, you consider their correlations. Using this ex- 625
CHAPTER 10. RANDOM PROCESSES ample, we have that E[X(0)X(0.5)]=−E[A·A] 1 =−E[A2]=− . 3 A picture will reveal what is happening. Figure 10.10 presents the realizations of the random process X(t) = Acos(2πt). If we consider X(0) and X(0.5), each of them is a random variable, and thus we can ask about their PDFs. It is obvious from the illustration that the random variable X(0) has a PDF that is a uniform distribution from 0 to 1, whereastherandomvariableX(0.5)hasaPDFthatisauniformdistributionfrom−1to0. Mathematically, the PDFs are (cid:40) (cid:40) 1, 0≤x≤1, 1, −1≤x≤0, f (x)= and f (x)= X(0) X(0.5) 0, otherwise 0, otherwise. Since X(0) and X(0.5) have their own PDFs, we can calculate their correlation. This will give us E[X(0)X(0.5)] which after some calculations is E[X(0)X(0.5)]=−1. 3 1 0.5 0 -0.5 -1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 Figure 10.10: The autocorrelation between X(0) and X(0.5) should be regarded as the correlation between two random variables. Each random variable has its own PDF. We can now consider the autocorrelation for any t and t . When you are evaluating 1 2 the autocorrelation function, you are not just evaluating at t = 0 and t = 0.5, you are also evaluating the correlation for all pairs of t and t . Now you want to know what the 1 2 correlation is between t = 0 and t = 0.5, t = 2 and t = 3.1, etc. Of course, there are infinitely many pairs of time instants. The point of the autocorrelation function is to tell youthecorrelationof allthepairs.Inotherwords,ifwetellyouR (t ,t ),youwillbeable X 1 2 to plug in a value of t and a value of t and tell us the correlation at (t ,t ). How is this 1 2 1 2 possible? To find out, let’s consider the following example. 626
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.11.LetA∼Uniform[0,1],X(t)=Acos(2πt).FindR (0,0.5),anddraw X R (t ,t ). X 1 2 Solution. From the previous example, we know that 1 R (t ,t )= cos(2πt )cos(2πt ). X 1 2 3 1 2 Therefore, R (0,0.5) = 1cos(2π0)cos(2π0.5) = −1, which is the same as if we had X 3 3 computed it from the first principle. Theautocorrelationfunctiontellsyouhowonepointofatimeseriesiscorrelated withanotherpointofthetimeseries.IfR (t ,t )givesahighvalue,thenitmeansthe X 1 2 random variables at t and t have a strong correlation. To understand this, suppose 1 2 we let t =0, and let us vary t . Then 1 2 1 R (0,t )= cos(2π0)cos(2πt ) X 2 3 2 1 = cos(2πt ). 3 2 This is a periodic function that cycles through itself whenever t is an integer. As 2 we recall from Figure 10.10, if t = 0.5, the random variable X(t ) will take only 2 2 the negative values, but otherwise it is correlated with X(0). On the other hand, if t =0.25, then Figure 10.10 says that the random variable X(t ) is a constant 0, and 2 2 so the correlation with X(0) is zero. Clearly, R (t ,t ) is a 2-dimensional function of t and t . You need to tell R X 1 2 1 2 X which of the two time instants you want to compare, and then R will tell you the X correlation. So no matter what happens, you must specify two time instants. Because R (t ,t )isa2-dimensionalfunction,wecanvisualizeitbycalculatingallthepossible X 1 2 values it takes. For example, if R (t ,t )= 1cos(2πt )cos(2πt ), we can plot R as X 1 2 3 1 2 X a function of t and t . Figure 10.11 shows the plot. 1 2 -1 1 -0.75 -0.5 0.5 -0.25 0 0 0.25 -0.5 0.5 0.75 -1 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 t 1 t 2 Figure 10.11: The autocorrelation function R (t ,t )= 1cos(2πt )cos(2πt ). X 1 2 3 1 2 627
CHAPTER 10. RANDOM PROCESSES The MATLAB/Python code for Figure 10.11 is shown below. % MATLAB code for Example 10.11 t = linspace(-1,1,1000); R = (1/3)*cos(2*pi*t(:)).*cos(2*pi*t); imagesc(t,t,R); # Python code for Example 10.11 import numpy as np import matplotlib.pyplot as plt t = np.linspace(-1,1,1000) R = (1/3)*np.outer(np.cos(2*np.pi*t), np.cos(2*np.pi*t)) plt.imshow(R, extent=[-1, 1, -1, 1]) plt.show() To understand the 2D function shown on the right hand side of Figure 10.11, we can take a closer look by drawing Figure 10.12. For any two time instants t and t , we have 1 2 two random variables X(t ) and X(t ). The joint expectation E[X(t )X(t )] will return us 1 2 1 2 some value, and this is a point in the 2D plot R (t ,t ). The value tells us the correlation X 1 2 between X(t ) and X(t ). In the example in which t =0 and t =0.5, the correlation is 1 2 1 2 −1. Interestingly, if we pick another pair of time instants t = −0.5 and t = 0, the joint 3 1 2 expectation is E[X(−0.5)X(0)]=−1, which is the same value. However, this −1 is located 3 3 at a different valley than E[X(0)X(0.5)] in the 2D plot. Figure 10.12: To understand the autocorrelation function, pick two time instants t and t , and then 1 2 evaluate the joint expectation E[X(t )X(t )]. 1 2 The above example shows a periodic autocorrelation function. The fact that it is peri- odic is coincidental because the random process X(t) is a periodic function. In general, an arbitraryrandomprocesscanhaveanarbitraryautocorrelationfunctionthatisnotperiodic. There are, of course, various properties of the autocorrelation functions and special types of autocorrelation functions. We will study one of them, called the wide-sense stationary processes, later. 628
10.2. MEAN AND CORRELATION FUNCTIONS Example 10.12. Let Θ∼Uniform[−π,π], X(t)=cos(ωt+Θ). Draw the autocorrela- tion function R (t ,t ). X 1 2 Solution. From the previous example we know that 1 (cid:16) (cid:17) R (t ,t )= cos ω(t −t ) . X 1 2 2 1 2 Figure 10.13 shows the realizations, and the mean and autocorrelation functions. Note that the autocorrelation function has a structure: Every row is a shifted versionofthepreviousrow.WecallthisaToeplitzstructure.Anautocorrelationwith aToeplitzstructureisspecifiedonceweknowanyoftherows.AToeplitzstructurealso implies that the autocorrelation function does not depend on the pair (t ,t ) but only 1 2 onthedifferencet −t .Inotherwords,R (0,1)isthesameasR (11.6,12.6),andso 1 2 X X knowing R (0,1) is enough to know all R (t ,t +t). Not all random processes have X X 0 0 aToeplitzautocorrelationfunction.RandomprocesseswithaToeplitzautocorrelation function are “nice” processes that we will study in detail later. -1 1 -0.75 -0.5 0.5 -0.25 0 0 0.25 -0.5 0.5 0.75 -1 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 t 1 t 2 (cid:16) (cid:17) Figure 10.13: The autocorrelation function R (t ,t )= 1cos ω(t −t ) . X 1 2 2 1 2 The MATLAB code used to generate Figure 10.13 is shown below. % MATLAB code for Example 10.12 t = linspace(-1,1,1000); R = Toeplitz(0.5*cos(2*pi*t(:))); imagesc(t,t,R); grid on; xticks(-1:0.25:1); yticks(-1:0.25:1); 629
CHAPTER 10. RANDOM PROCESSES Practice Exercise 10.1. Let Θ∼Uniform[0,2π], X(t)=cos(ωt+Θ). Find the PDF of X(0). Solution. Let Z =X(0)=cosΘ. Then the CDF of Z is F (z)=P[Z ≤z] Z =P[cosΘ≤z] =P[cos−1z ≤Θ≤2π−cos−1z] cos−1z =1− . π Then by the fundamental theorem of calculus, 1 f (z)= √ . Z π 1−z2 A similar concept to the autocorrelation function is the autocovariance function. The idea is to remove the mean before computing the correlation. This is analogous to the covariance Cov(X,Y)=E[(X −µ )(Y −µ )] as opposed to the correlation E[XY] in the X Y random variable case. Definition 10.3. The autocovariance function of a random process X(t) is C (t ,t )=E[(X(t )−µ (t ))(X(t )−µ (t ))]. (10.7) X 1 2 1 X 1 2 X 2 As one might expect, the autocovariance function is closely related to the autocorrelation function. Theorem 10.1. C (t ,t )=R (t ,t )−µ (t )µ (t ). (10.8) X 1 2 X 1 2 X 1 X 2 Proof. Plugging in the definition, we have that C (t ,t )=E[X(t )X(t )−X(t )µ (t )−X(t )µ (t )+µ (t )µ (t )] X 1 2 1 2 1 X 2 2 X 1 X 1 X 2 =R (t ,t )−µ (t )µ (t )−µ (t )µ (t )+µ (t )µ (t ) X 1 2 X 1 X 2 X 1 X 2 X 1 X 2 =R (t ,t )−µ (t )µ (t ). (cid:3) X 1 2 X 1 X 2 Practice Exercise 10.2. If X(t)=Acos(2πt) for A∼Uniform[0,1], find C (t ,t ). X 1 2 630
10.2. MEAN AND CORRELATION FUNCTIONS Solution. 1 1 1 C (t ,t )= cos(2πt )cos(2πt )− cos(2πt )· cos(2πt ) X 1 2 3 1 2 2 1 2 2 1 = cos(2πt )cos(2πt ). 12 1 2 Practice Exercise 10.3. Suppose X(t)=cos(ωt+Θ) for Θ∼Uniform[−π,π]. Find C (t ,t ). X 1 2 Solution. C (t ,t )=R (t ,t )−µ (t )µ (t ) X 1 2 X 1 2 X 1 X 2 (cid:18) (cid:19) (cid:18) (cid:19) 1 1 = cos ω(t −t ) −0·0= cos ω(t −t ) . 2 1 2 2 1 2 In some problems we are interested in the correlation between two random processes X(t) and Y(t). This gives us the cross-correlation and the cross-covariance functions. Definition 10.4. The cross-correlation function of X(t) and Y(t) is R (t ,t )=E[X(t )Y(t )]. (10.9) X,Y 1 2 1 2 Definition 10.5. The cross-covariance function of X(t) and Y(t) is C (t ,t )=E[(X(t )−µ (t ))(Y(t )−µ (t ))]. (10.10) X,Y 1 2 1 X 1 2 Y 2 Remark. If µ (t )=µ (t )=0, then C (t ,t )=R (t ,t )=E[X(t )Y(t )]. X 1 Y 2 X,Y 1 2 X,Y 1 2 1 2 10.2.3 Independent processes Howdoweestablishindependencefortworandomprocesses?Weknowthatfortworandom variables to be independent, the joint PDF can be written as a product of two PDFs: f (x,y)=f (x)f (y). (10.11) X,Y X Y If we extrapolate this idea to random processes, a natural formulation would be f (x,y)=f (x)f (y). (10.12) X(t),Y(t) X(t) Y(t) But this definition has a problem because X(t) and Y(t) are functions. It is not enough to just look at one time index, say t=t . The way to think about this situation is to consider 0 a pair of random vectors X and Y. When you say X and Y are independent, you require f (x,y) = f (x)f (y). The PDF f (x) itself is a joint distribution, i.e., f (x) = X,Y X Y X X f (x ,...,x ). Therefore, for random processes, we need something similar. X1,...,XN 1 N 631
CHAPTER 10. RANDOM PROCESSES Definition 10.6. Two random processes X(t) and Y(t) are independent if for any t ,...,t , 1 N f (x ,...,x ,y ,...,y ) X(t1),...,X(tN),Y(t1),...,Y(tN) 1 N 1 N =f (x ,...,x )×f (y ,...,y ). X(t1),...,X(tN) 1 N Y(t1),...,Y(tN) 1 N This definition is reminiscent of f (x,y) = f (x)f (y). The requirement here is that X,Y X Y the factorization holds for any N, including very small N and very large N, because X(t) and Y(t) are infinitely long. Independence means that the behavior of one process will not influence the behavior of the other process. We define uncorrelated as follows. Definition 10.7. Two random processes are X(t) and Y(t) uncorrelated if E[X(t )Y(t )]=E[X(t )]E[Y(t )], (10.13) 1 2 1 2 Independence implies uncorrelation, as we can see from the following. If X(t) and Y(t) are independent, it follows that (cid:90) E[X(t )Y(t )]= X(t ,ξ)Y(t ,ζ)f (ξ,ζ)dξ dζ 1 2 1 2 X,Y (cid:90) = X(t ,ξ)Y(t ,ζ)f (ξ)f (ζ)dξ dζ, independence 1 2 X Y (cid:90) (cid:90) = X(t ,ξ)f (ξ)dξ Y(t ,ζ)f (ζ)dζ =E[X(t )]E[Y(t )]. 1 X 2 Y 1 2 If two random processes are uncorrelated, they are not necessarily independent. ⇒ Independent X and Y uncorrelated X and Y (cid:58) Example 10.13. Let Y(t) = X(t)+N(t), where X(t) and N(t) are independent. Then R (t ,t )=E[X(t )Y(t )]=E[X(t )(X(t )+N(t ))] X,Y 1 2 1 2 1 2 2 =R (t ,t )+µ (t )µ (t ). X 1 2 X 1 N 2 10.3 Wide-Sense Stationary Processes 632
10.3. WIDE-SENSE STATIONARY PROCESSES As we have seen in the previous sections, some random processes have a “nice” autocor- relation function, in the sense that the 2D function R (t ,t ) has a Toeplitz structure. X 1 2 Randomprocesseswiththispropertyareknownaswide-sense stationary(WSS)processes. WSS processes belong to a very small subset in the entire universe of random processes, but they are practically the most useful ones. Before we discuss how to use them, we first present a formal definition of a WSS process.1 10.3.1 Definition of a WSS process Definition 10.8. A random process X(t) is wide-sense stationary if: 1. µ (t)=constant, for all t, and X 2. R (t ,t )=R (t −t ) for all t ,t . X 1 2 X 1 2 1 2 There are two criteria that define a WSS process. The first criterion is that the mean is a constant.Thatis,themeanfunctiondoesnotchangewithtime.Thesecondcriterionisthat the autocorrelation function only depends on the difference t −t and not on the absolute 1 2 starting point. For example, R (0.1,1.1) needs to be the same as R (6.3,7.3), because the X X intervals are both 1. How can these two criteria be mapped to the Toeplitz structure we discussed in the previous examples? Figure 10.14 shows the autocorrelation function R (t ,t ), which is a X 1 2 2D function. We take three cross sections corresponding to t =−0.13, t =0 and t =0.3. 2 2 2 Asyoucanseefromthefigure,eachR (t ,t )isashiftedversionofanotherone.Toobtain X 1 2 any value R (t ,t ) on the function, there is no need to probe to the 2D map; you only X 1 2 need to probe to the red curve and locate the position marked as t −t , and you will be 1 2 able to obtain the value R (t ,t ). X 1 2 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 t 1 t 2 1 t = -0.13 2 t = 0 2 0.5 t = 0.3 2 0 -0.5 -1 -1 -0.75 -0.5 -0.25 0 0.25 0.5 0.75 1 t 1 (cid:16) (cid:17) Figure 10.14: Cross sections of the autocorrelation function R (t ,t )= 1cos ω(t −t ) . X 1 2 2 1 2 Not all random processes have a Toeplitz autocorrelation function. For example, the random process X(t)=Acos(2πt) is not a WSS process, because the autocorrelation func- 1Manytextbooksintroducestrictlystationaryprocessesbeforediscussingawide-sensestationaryprocess. Weskiptheformerbecause,throughoutourbook,weonlyuseWSSprocesses.Readersinterestedinstrictly stationaryprocessescanconsultthereferenceslistedattheendofthischapter. 633
CHAPTER 10. RANDOM PROCESSES tion is 1 R (t ,t )= cos(2πt )cos(2πt ), X 1 2 3 1 2 which cannot be written as the difference t −t . 1 2 Remark 1. WSS processes can also be defined using the autocovariance function instead of the autocorrelation function, because if a process is WSS, then the mean function is a constant. If the mean function is a constant, then C (t ,t ) = R (t ,t ) − µ2. So any X 1 2 X 1 2 geometric structure that R possesses will be translated to C , as the constant µ2 will not X X influence the geometry. Therefore, it is equally valid to say that a WSS process has C (t ,t )=C (t −t ). X 1 2 X 1 2 Remark 2. Because a WSS is completely characterized by the difference t −t , there is 1 2 no need to keep track of the absolute indices t and t . We can rewrite the autocorrelation 1 2 function as R (τ)=E[X(t+τ)X(t)]. (10.14) X There is nothing new in this equation: It only says that instead of writing R (t+τ,t), we X can write R (τ) because the time index t plays no role in terms of R . Thus from now on, X X for any WSS processes we will write the autocorrelation function as R (τ). X 10.3.2 Properties of R (τ) X When X(t) is WSS, R (τ) has several important properties. X Corollary 10.1. R (0)= average power of X(t). X Proof. Since R (0)=E[X(t+0)X(t)]=E[X(t)2], X and since E[X(t)2] is the average power, R (0) is the average power of X(t). (cid:3) X Corollary 10.2. R (τ) is symmetric. That is, R (τ)=R (−τ). X X X Proof.NotethatR (τ)=E[X(t+τ)X(t)].Byswitchingtheorderofmultiplicationinthe X expectation, we have E[X(t+τ)X(t)]=E[X(t)X(t+τ)]=R (−τ). X (cid:3) Corollary 10.3. 2(R (0)−R (τ)) P(|X(t+τ)−X(τ)|>(cid:15))≤ X X . (cid:15)2 634
10.3. WIDE-SENSE STATIONARY PROCESSES This result says that if R (τ) is slowly decaying from R (0), the probability of having a X X large deviation |X(t+τ)−X(τ)| is small. Proof. P(|X(t+τ)−X(τ)|>(cid:15))≤E[(X(t+τ)−X(τ))2]/(cid:15)2 (cid:16) (cid:17) = E[X(t+τ)2]−2E[X(t+τ)X(t)]+E[X(t)2] /(cid:15)2 (cid:16) (cid:17) = 2E[X(t)2]−2E[X(t+τ)X(t)] /(cid:15)2 (cid:16) (cid:17) =2 R (0)−R (τ) /(cid:15)2. X X (cid:3) Corollary 10.4. |R (τ)|≤R (0), for all τ. X X Proof. By Cauchy’s inequality E[XY]2 ≤E[X2]E[Y2], we can show that R (τ)2 =E[X(t)X(t+τ)]2 X ≤E[X(t)2]E[X(t+τ)2] =E[X(t)2]2 =R (0)2. X (cid:3) 10.3.3 Physical interpretation of R (τ) X How should we understand the autocorrelation function R (τ) for WSS processes? Cer- X tainly, by definition, R (τ)=E[X(t+τ)X(t)] means that we can analyze R (τ) from the X X statistical perspective. But in this section we want to take a slightly different approach by answering the question from a computational perspective. Consider the following function: 1 (cid:90) T def R(cid:98)X(τ) = 2T X(t+τ)X(t)dt. (10.15) −T ThisfunctionisthetemporalaverageofX(t+τ)X(t),asopposedtothestatisticalaverage. Why do we want to consider this temporal average? We first show the main result, that E[R(cid:98)X(τ)]=R X(τ). Lemma 10.1. Let R(cid:98)X(τ)d =ef 21 T (cid:82) −T T X(t+τ)X(t)dt. Then (cid:104) (cid:105) E R(cid:98)X(τ) =R X(τ). (10.16) Proof. (cid:104) (cid:105) 1 (cid:90) T E R(cid:98)X(τ) = 2T E[X(t+τ)X(t)] dt −T 1 (cid:90) T 1 (cid:90) T = R (τ)dt=R (τ) dt=R (τ). 2T X X 2T X −T −T 635
CHAPTER 10. RANDOM PROCESSES (cid:3) This lemma implies that if the signal X(t) is long enough, we can approximate R (τ) X by R(cid:98)X(τ). The approximation is asymptotically consistent, in the sense that E[R(cid:98)X(τ)] = R X(τ). Now, the more interesting question is the interpretation of R(cid:98)X(τ). What is it? How should we understand R(cid:98)X(τ)? R(cid:98)X(τ) is the “unflipped convolution”, or correlation, of X(τ) and X(t+τ). Correlation is analogous to convolution. For convolution, the definition is (cid:90) T Y(τ)= X(t−τ)X(t)dt, (10.17) −T whereas for correlation, the definition is (cid:90) T Y(τ)= X(t+τ)X(t)dt. (10.18) −T Clearly, R(cid:98)X(τ) is the latter. A graphical illustration of the difference between convolution andcorrelationisprovidedinFigure 10.15.Theonlydifferencebetweenthetwoisthatthe correlation does not flip the function, whereas the convolution does flip the function. 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 -0.1 -0.1 -10 -5 0 5 10 -10 -5 0 5 10 (a) Convolution (b) Correlation Figure 10.15: Thedifferencebetweenconvolutionandcorrelation.Inconvolution,thefunctionX(t)is flipped before we compute the result. For correlation, the function is not flipped. Thetemporalcorrelationiseasytovisualize.StartingwiththefunctionX(t+τ),ifyou makeτ largerorsmaller,theneffectivelyyouareshiftingX(t)leftorright.Theintegration (cid:82)T X(t+τ)X(t) dt calculates the energy accumulated. If the integral is large, there is a −T strong correlation between X(t) and X(t+τ). Otherwise the correlation is small. Here is an extreme example: 636
10.3. WIDE-SENSE STATIONARY PROCESSES Example 10.14. Consider a random process X(t) such that for every t, X(t) is an i.i.d. Gaussian random variable with zero mean and unit variance. Then (cid:40) E[X2(t)], τ =0, R (τ)=E[X(t+τ)X(t)]= X E[X(t+τ)]E[X(t)], τ (cid:54)=0. Using the fact that X(t) is i.i.d. Gaussian for all t, we can show that E[X2(t)]=1 for any t, and E[X(t+τ)]E[X(t)]=0. Therefore, we have (cid:40) 1, τ =0, R (τ)= X 0. τ (cid:54)=0. The equation says that since the random process is i.i.d. Gaussian, shifting and in- tegrating will give maximum correlation at the origin. As soon as the shift is not at the origin, the correlation is zero. This makes sense because the samples are just i.i.d. Gaussian. One pixel offset is enough to destroy any correlation. Now let’s calculate the temporal correlation. We know that 1 (cid:90) T R(cid:98)X(τ)= 2T X(t+τ)X(t)dτ. −T This equation says that we shift X(t) to the left and right and then integrate. If τ is not zero, the product X(t+τ)X(t) will sometimes be positive and sometimes be negative. After integrating the entire period, we cancel out most of the terms. Let’s plot the functions and see if all these steps make sense. In Figure 10.16(a), we show two random realizations of the random process X(t). They are just i.i.d. Gaussian samples. In Figure 10.16(b) we plot the temporal autocorrelation function R(cid:98)X(τ). Since R(cid:98)X(τ) itself is a random process, it has different realizations. We plot two random realizations, which are computed based on shifting and integrating X(t). In the same plot, we also show the statistical expectation R (τ). As we can see from the plot, X the temporal correlation and the statistical correlation match reasonably well except for the fluctuation in R(cid:98)X(τ), which is expected because it is computed from a finite number of samples. 637
CHAPTER 10. RANDOM PROCESSES 4 1.2 correlation of sample 1 3 correlation of sample 2 1 auto-correlation function 2 0.8 1 0.6 0 0.4 -1 -2 0.2 -3 0 -4 -0.2 0 200 400 600 800 1000 0 500 1000 1500 2000 (a) X(t) (b) R(cid:98)X(τ) Figure 10.16: (a)ArandomprocessX(t)withtwodifferentrealizations.(b)Aswecalculatethe temporal correlation of each of the two realizations, we obtain a noisy function that is nearly an impulse. If we take the average of many of these realizations, we obtain a pure delta function. On a computer, the commands to do the autocorrelation function are xcorr in MAT- LAB and np.correlate in Python. Below are the codes used to generate Figure 10.16. % MATLAB code to demonstrate autocorrelation N = 1000; % number of sample paths T = 1000; % number of time stamps X = 1*randn(N,T); xc = zeros(N,2*T-1); for i=1:N xc(i,:) = xcorr(X(i,:))/T; end plot(xc(1,:),’b:’, ’LineWidth’, 2); hold on; plot(xc(2,:),’k:’, ’LineWidth’, 2); # Python code to demonstrate autocorrelation N = 1000 T = 1000 X = np.random.randn(N,T) xc= np.zeros((N,2*T-1)) for i in range(N): xc[i,:] = np.correlate(X[i,:],X[i,:],mode=’full’)/T plt.plot(xc[0,:],’b:’) plt.plot(xc[1,:],’k:’) plt.show() Under what conditions will R(cid:98)X(τ)→R X(τ) as T →∞? The answer to this question is provided by an important theorem called Mean-Square Ergodic Theorem, which can be thought of as the random process version of the weak law of large numbers. We leave the discussion of the mean ergodic theorem to the Appendix. 638
10.4. POWER SPECTRAL DENSITY Everything you need to know about a WSS process • The mean of a WSS process is a constant (does not need to be zero) • Thecorrelationfunctiononlydependsonthedifference,soR (t ,t )isToeplitz. X 1 2 • You can write R (t ,t ) as R (τ), where τ =t −t . X 1 2 X 1 2 • R (τ) tells you how much correlation you have with someone located at a time X instant τ from you. 10.4 Power Spectral Density BeginningwiththissectionwearegoingtofocusonWSSprocesses.ByWSS,wemeanthat the autocorrelation function R (t ,t ) has a Toeplitz structure. Putting it in other words, X 1 2 we assume R (t ,t ) can be simplified to R (τ), where τ = t −t . We call this property X 1 2 X 1 2 time invariance. 10.4.1 Basic concepts AssumingthatR (τ)issquareintegrable,i.e.,(cid:82)∞ R (τ)2 dτ <∞,wecannowdefinethe X −∞ X Fourier transform of R (τ) which is called the power spectral density. X Theorem 10.2 (Einstein-Wiener-Khinchin Theorem). The power spectral density S (ω) of a WSS process is X (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ =F(R (τ)), X X X −∞ assuming that (cid:82)∞ R (τ)2 dτ <∞ so that the Fourier transform of R (τ) exists. −∞ X X Practice Exercise 10.4. Let R (τ)=e−2α|τ|. Find S (ω). X X Solution. Using the Fourier transform table, 4α S (ω)=F{R (τ)}= . X X 4α2+ω2 Figure 10.17 shows the autocorrelation function and the power spectral density. 639
CHAPTER 10. RANDOM PROCESSES 1 1 R ( ) S ( ) 0.8 X 0.8 X 0.6 0.6 0.4 0.4 0.2 0.2 0 0 -2 -1 0 1 2 -10 -5 0 5 10 Figure 10.17: Example for R (τ)=e−2α|τ|, with α=1. X Why is Theorem 10.2 a theorem rather than a definition? This is because power spectral densityhasitsdefinition.Thereisnowaythatyoucangetany“power”informationmerely bylookingattheFouriertransformofR (τ).Wewilldiscusstheoriginofthepowerspectral X density later, but for now, we only need to know that S (ω) is the Fourier transform of X R (τ). X Remark. The power spectral density is defined for WSS processes. If the process is not WSS, then R will be a 2D function instead of a 1D function of τ, so we cannot take the X Fourier transform in τ. We will discuss this in detail shortly. PracticeExercise10.5.LetX(t)=acos(ω t+Θ), Θ∼Uniform[0,2π].FindS (ω). 0 X Solution. We know that the autocorrelation function is a2 R (τ)= cos(ω τ) X 2 0 a2 (cid:18) ejω0τ +e−jω0τ(cid:19) = . 2 2 By taking the Fourier transform of both sides, we have a2 (cid:20) 2πδ(ω−ω )+2πδ(ω+ω )(cid:21) S (ω)= 0 0 X 2 2 πa2 = [δ(ω−ω )+δ(ω+ω )]. 2 0 0 The result is shown in Figure 10.18. 0.5 2 R ( ) S ( ) X X 1.5 0 1 0.5 -0.5 0 -2 -1 0 1 2 -10 -5 0 5 10 640
10.4. POWER SPECTRAL DENSITY Figure 10.18: Example for R (τ)= a2 cos(ω τ), with a=1 and ω =2π. X 2 0 0 Practice Exercise 10.6. Let S X(ω)= N 20rect( 2ω W). Find R X(τ). Solution. Since S (ω)=F(R (τ)), the inverse holds: X X N W R (τ)= 0 sinc(Wτ). X 2 π This example shows what we call the bandlimited white noise. The power spectral density S (ω) is uniform, meaning that it covers all frequencies (or wavelengths in X optics). It is called “white noise” because white light is essentially a mixture of all wavelengths. The bandwidth of the power spectral density W defines the zero crossings of R (τ). It is easy to show that when W → ∞, R (τ) converges to a delta function. X X ThishappenswhenX(t)isi.i.d.Gaussian.Therefore,thepureGaussiannoiserandom process is also known as the white noise process. Reshaping the i.i.d. Gaussian noise toanarbitrarypowerspectraldensitycanbedonebypassingitthroughalinearfilter, as we will explain later. 2 1.5 R ( ) S ( ) 1.5 X X 1 1 0.5 0.5 0 0 -0.5 -0.5 -2 -1 0 1 2 -10 -5 0 5 10 Figure 10.19: Example for S (ω)= N0rect( ω ), with N =2 and W =5. X 2 2W 0 Finding S (ω) from R (τ) is straightforward, at least in principle. The more inter- X X esting questions to ask are: (1) Why do we need to learn about power spectral density? (2) Why do we need WSS to define power spectral density? How is power spectral density useful? • Powerspectraldensitiesareusefulwhenwepassarandomprocessthroughsome linear operations, e.g., convolution, running average, or running difference. • Power spectral densities are the Fourier transforms of the autocorrelation func- tions. Fourier transforms are useful for speeding up computation and drawing random samples from a given power spectral density. Arandomprocessitselfisnotinterestinguntilweprocessit;therearemanywaystodo this.Themostbasicoperationistosendtherandomprocessthroughalineartime-invariant system, e.g., a convolution. Convolution is equivalent to filtering the random process. For example, if the input process contains noise, we can design a linear time-invariant filter to 641
CHAPTER 10. RANDOM PROCESSES denoise the random process. The power spectral density, which is the Fourier transform of the autocorrelation function, makes the filtering easier because everything can be done in the spectral (Fourier) domain. Moreover, we can analyze the performance and quantify the limit using standard results in Fourier analysis. For some specialized problems such as imaging through atmospheric turbulence, the distortions happen in the phase domain. This canbesimulatedbydrawingsamplesfromthepowerspectraldensity,e.g.,theKolmogorov spectrum or the von K´arm´an spectrum. Power spectral densities have many important engineering applications. Why does the power spectral density require wide-sense stationarity? • If a process is WSS, then R will have a Toeplitz structure. X • A Toeplitz matrix is important. If you do eigendecomposition to a Toeplitz ma- trix, the eigenvectors are the Fourier bases. • So if R is Toeplitz, then you can diagonalize it using the Fourier transform. X • Therefore, the power spectral density can be defined. Why does power spectral density require WSS? This has to do with the Toeplitz structure of the autocorrelation function. To make our discussion easier let us discretize the autocorrelation function R (t ,t ) by considering R [m,n]. (You can do a mental X 1 2 X calculation by converting t to integer indices m, and t to n. See any textbook on signals 1 2 and systems if you need help. This is called the “discrete time signal”.) Following the range of t and t , R [m,n] can be expressed as: 1 2 X   R [0] R [1] ··· R [N −1] X X X  R X[1] R X[0] ··· R X[N −2] R=   . . . . . . ... . . .   , R [N −1] R [N −1] ··· R [0] X X X whereweusedthefactthatR [m,n]=R [m−n]forWSSprocessesandR [k]=R [−k]. X X X X We call the resulting matrix R the autocorrelation matrix, which is a discretized version of the autocorrelation function R (t ,t ). Looking at R, we again observe the Toeplitz X 1 2 structure. For example, Figure 10.20 shows one Toeplitz structure and one non-Toeplitz structure. Any Toeplitz matrix R can be diagonalized using the Fourier transforms. That is, we can write R as R=FHΛF, whereF isthe(discrete)Fourier transform matrixandΛisadiagonalmatrix.Thiscanbe understood as the eigendecomposition of R. The important point here is that only Toeplitz matrices can be eigendecomposed using the Fourier transforms; an arbitrary symmetric matrixcannot.Figure 10.20illustratesthispoint.IfyourmatrixisToeplitz,youcandiago- nalizeit,andhenceyoucandefinethepowerspectraldensity,justasinthefirstexample.If yourmatrixisnotToeplitz,thenthepowerspectraldensityisundefined.TogettheToeplitz matrix, you must start with a WSS process. Before moving on, we define cross power spectral densities, which will be useful in some applications. 642
10.4. POWER SPECTRAL DENSITY Figure10.20:WeshowtwoautocorrelationfunctionsR [m,n]ontheleft-handside.Thefirstautocor- X relation function comes from a WSS process that has a Toeplitz structure. The second autocorrelation function does not have Toeplitz structure. For the Toeplitz matrix, we can diagonalize it using the Fourier transform. The eigenvalues are the power spectral density. Definition 10.9. The cross power spectral density between two random processes X(t) and Y(t) is S (ω)=F(R (τ)) where R (τ)=E[X(t+τ)Y(t)], X,Y X,Y X,Y (10.19) S (ω)=F(R (τ)) where R (τ)=E[Y(t+τ)X(t)]. Y,X Y,X Y,X Remark. In general, S (ω) (cid:54)= S (ω). Rather, since R (τ) = R (−τ), we have X,Y Y,X X,Y Y,X S (ω)=S (ω). X,Y Y,X 10.4.2 Origin of the power spectral density To understand the power spectral density, it is crucial to understand where it comes from and why it is the Fourier transform of the autocorrelation function. We begin by assuming that X(t) is a WSS random process with mean µ and auto- X correlation R (τ). We now consider the notion of power. Consider a random process X(t). X The power within a period [−T,T] is 1 (cid:90) T P(cid:98)X = 2T |X(t)|2 dt. −T P(cid:98)X defines the power because the integration alone is the energy, and the normalization by 1/2T gives us the power. However, there are two problems. First, since X(t) is random, the power P(cid:98)X is also random. Is there a way we can eliminate the randomness? Second, T is a finite period of time. It does not capture the entire process, and so we do not know the power of the entire process. A natural solution to these two problems is to consider (cid:34) (cid:35) 1 (cid:90) T P d =efE lim |X(t)|2 dt . (10.20) X T→∞2T −T 643
CHAPTER 10. RANDOM PROCESSES Here, we take the limit of T to infinity so that we can compute the power of the entire process. We also take the expectation to eliminate the randomness. Therefore, P can be X regarded as the average power of the complete random process X(t). Next, we need one definition and one lemma. The definition defines S (ω), and the X lemma will link S (ω) with the power P . X X Definition 10.10. The power spectral density (PSD) of a WSS process is defined as (cid:104) (cid:105) E |X(cid:101)T(ω)|2 S (ω)= lim , (10.21) X T→∞ 2T where (cid:90) T X(cid:101)T(ω)= X(t)e−jωt dt (10.22) −T is the Fourier transform of X(t) limited to [−T,T]. This definition is abstract, but in a nutshell, it simply considers everything in the Fourier domain. The ratio |X(cid:101)T(ω)|2/2T is the power, but in the frequency domain. The reason is thatifX(t)isFouriertransformable,thenParseval’s theoremwillhold.Parseval’stheorem states that energy in the original space is conserved in the Fourier space. Since the ratio |X(cid:101)T(ω)|2/2T istheenergydividedbytime,itisthepower.However,thisisstillnotenough to help us understand power spectral density: We need a lemma. Lemma 10.2. Define (cid:34) (cid:35) 1 (cid:90) T P d =efE lim |X(t)|2 dt . X T→∞2T −T Then 1 (cid:90) ∞ P = S (ω)dω. (10.23) X 2π X −∞ Thelemmahastobereadtogetherwiththepreviousdefinition.Ifwecanprovethelemma, we know that by integrating S (ω) we will obtain the power. Therefore, S (ω) can be X X viewed as a density function, specifically the density function of the power. S (ω) is called X thepowerspectraldensitybecauseeverythingisdefinedintheFourierdomain.Puttingthis all together gives us “power spectral density”. Proof. First, we recall that P is the expectation of the average power of X(t). Let X (cid:26) X(t) −T ≤t≤T, X (t)= T 0 otherwise. It follows that integrating over −∞ to ∞ is equivalent to (cid:90) ∞ (cid:90) T |X (t)|2 dt= |X(t)|2 dt. T −∞ −T 644
10.4. POWER SPECTRAL DENSITY By Parseval’s theorem, energy is conserved in both the time and the frequency domain: (cid:90) ∞ 1 (cid:90) ∞ |X T(t)|2 dt= 2π |X(cid:101)T(ω)|2 dω. −∞ −∞ Therefore, P satisfies X (cid:34) (cid:35) 1 (cid:90) T P =E lim |X(t)|2 dt X T→∞2T −T (cid:20) 1 1 (cid:90) ∞ (cid:21) =E Tl →im ∞2π2T −∞|X(cid:101)T(ω)|2 dω 1 (cid:90) ∞ 1 (cid:104) (cid:105) = 2π −∞Tl →im ∞2TE |X(cid:101)T(ω)|2 dω. (cid:124) (cid:123)(cid:122) (cid:125) d=efSX(ω) (cid:3) The power spectral densities are functions whose integrations give us the power. If we want to determine the power of a random process, the Einstein-Wiener-Khinchin theorem (Theorem 10.2) says that S (ω) is just the Fourier transform of R (τ): X X (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ =F(R (τ)). X X X −∞ The proof of the Einstein-Wiener-Khinchin theorem is quite intricate, so we defer the proof to the Appendix. The significance of the theorem is that it turns an abstract quantity, the power spectral density, into a very easily computable quantity, namely the Fouriertransformoftheautocorrelationfunction.Fornow,wewillhappilyusethistheorem because it saves us a great deal of trouble when we want to determine the power spectral density from the first principles. 645
CHAPTER 10. RANDOM PROCESSES 10.5 WSS Process through LTI Systems Random processes have limited usefulness until we can apply operations to them. In this sectionwediscusshowWSSprocessesrespondtoalineartime-invariant(LTI)system.This technique is most useful in signal processing, communication, speech analysis, and imaging. We will be brief here since you can find most of this information in any standard textbook on signals and systems. 10.5.1 Review of linear time-invariant systems When we say a “system”, we mean that there exists an input-output relationship as shown in Figure 10.21. Figure 10.21: A system can be viewed as a black box that takes an input X(t) and turns it into an output Y(t). Linear time-invariant (LTI) systems are the simplest systems we use in engineering problems. An LTI system has two properties. • Linearity.Linearitymeansthatwhentwoinputrandomprocessesareaddedand scaled,theoutputrandomprocesseswillalsobeaddedandscaledinexactlythe same way. Mathematically, linearity says that if X (t) → Y (t) and X (t) → 1 1 2 Y (t), then 2 aX (t)+bX (t)→aY (t)+bY (t). 1 2 1 2 • Time-invariant:Timeinvariancemeansthatifweshifttheinputrandomprocess by a certain time period, the output will be shifted in the same way. Mathemat- ically, time invariance means that if X(t)→Y(t), then X(t+τ)→Y(t+τ). Ifasystemislineartime-invariant,theinput-to-outputrelationisgivenbyconvolution: The convolution between two functions X(t) and h(t) is defined as (cid:90) ∞ Y(t)=h(t)∗X(t)= h(τ)X(t−τ)dτ, −∞ in which we call h(t) the system response or impulse response. 646
10.5. WSS PROCESS THROUGH LTI SYSTEMS The function h(t) is called the impulse response because if X(t) = δ(t), then according to the convolution equation we have (cid:90) ∞ Y(t)= h(τ)δ(t−τ)dτ =h(t). −∞ Therefore, if we send an impulse to the system, the output will be h(t). Convolution is commutative, meaning that h(t)∗X(t)=X(t)∗h(t). Written as inte- grations, we have (cid:90) ∞ (cid:90) ∞ h(τ)X(t−τ)dτ = h(t−τ)X(τ)dτ. (10.24) −∞ −∞ For LTI systems, Y(t) can be determined through the Fourier transforms. The Fourier transform of a (squared-integrable) function X(t) is (cid:90) ∞ X(ω)=F{X(t)}= X(τ)e−jωτ dτ. (10.25) −∞ A basic property of convolution is that convolution in the time domain is equivalent to multiplication in the Fourier domain. Therefore Y(ω)=H(ω)X(ω), (10.26) where H(ω)=F{h(t)} is the Fourier transform of h(t), and Y(ω)=F(Y(t)) is the Fourier transform of Y(t). Intherestofthissectionwestudythepairofinputandoutputrandomprocessesthat are defined as follows • X(t) = input. It is a WSS random process. • Y(t) = output. It is constructed by sending X(t) through an LTI system with impulse response h(t). Therefore, Y(t)=h(t)∗X(t). 10.5.2 Mean and autocorrelation through LTI Systems SinceX(t)isWSS,themeanfunctionofX(t)staysconstant,i.e.,µ (t)=µ .Thefollowing X X theorem gives the mean function of the output. Theorem10.3. IfX(t)passesthroughanLTIsystemtoyieldY(t),themeanfunction of Y(t) is (cid:90) ∞ E[Y(t)]=µ h(τ)dτ. (10.27) X −∞ 647
CHAPTER 10. RANDOM PROCESSES Proof. Suppose that Y(t)=h(t)∗X(t). Then, (cid:20)(cid:90) ∞ (cid:21) µ (t)=E[Y(t)]=E h(τ)X(t−τ)dτ Y −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(τ)E[X(t−τ)]dτ = h(τ)µ dτ =µ h(τ)dτ, X X −∞ −∞ −∞ where the second to last equality is valid because E[X(t−τ)]=µ . (cid:3) X The theorem suggests that if the input X(t) has a constant mean, the output Y(t) should also have a constant mean. This should not be a surprise because if the system is linear, a constant input will give a constant output. Example 10.15. Consider a WSS random process X(t) such that each sample is an i.i.d.Gaussianrandomvariablewithzeromeanandunitvariance.Wesendthisprocess through an LTI system with impulse response h(t), where (cid:40) 10(1−|t|), −1≤t≤1, h(t)= 0, otherwise. The mean function of X(t) is µ (t)=0, and that of Y(t) is µ (t)=0. Figure 10.22 X Y illustrates a numerical example, in which we see that the random processes X(t) and Y(t) have different shapes but the mean functions remain constant. 4 0.2 X(t) (t) R X(t) X R (t) 2 Y(t) 0.15 Y (t) Y 0.1 0 0.05 -2 0 -4 -0.05 -10 -5 0 5 10 -2 -1 0 1 2 (a) µ (t) and µ (t) (b) R (t) and R (t) X Y X Y Figure 10.22: When sending a WSS random process through an LTI system, the mean and the autocorrelation functions are changed. Next, we derive the autocorrelation function of a random process when sent through an LTI system. Theorem 10.4. If X(t) passes through an LTI system to yield Y(t), the autocorre- lation function of Y(t) is (cid:90) ∞ (cid:90) ∞ R (τ)= h(s)h(r)R (τ +s−r)dsdr. (10.28) Y X −∞ −∞ 648
10.5. WSS PROCESS THROUGH LTI SYSTEMS Proof. We start with the definition of Y(t): R (τ)=E[Y(t)Y(t+τ)] Y (cid:20)(cid:90) ∞ (cid:90) ∞ (cid:21) =E h(s)X(t−s)ds h(r)X(t+τ −r)dr −∞ −∞ (cid:90) ∞ (cid:90) ∞ ( =a) h(s)h(r)E[X(t−s)X(t+τ −r)dsdr] −∞ −∞ (cid:90) ∞ (cid:90) ∞ = h(s)h(r)R (τ +s−r)dsdr, X −∞ −∞ where in (a) we assume that integration and expectation are interchangeable. (cid:3) AshorthandnotationoftheaboveformulaisR (t)=[h(cid:126)(h∗R )](t),where∗denotes Y X the convolution and (cid:126) denotes the correlation. Figure 10.22(b) shows the autocorrelation functions R and R . In this example R is a delta function because for i.i.d. Gaussian X Y X noise the power spectral density is a constant. After convolving with the system response, the autocorrelation R has a different shape. Y 10.5.3 Power spectral density through LTI systems Denoting the Fourier transform of the impulse response by H(ω) = F(h(t)), we derive the power spectral density of the output. Theorem 10.5. If X(t) passes through an LTI system to yield Y(t), the power spec- tral density of Y(t) is S (ω)=|H(ω)|2S (ω). (10.29) Y X Proof. By definition, the power spectral density S (ω) is the Fourier transform of the Y autocorrelation function R (ω). Therefore, Y (cid:90) ∞ S (ω)= R (τ)e−jωτ dτ Y Y −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(s)h(r)R (τ +s−r)dsdre−jωτ dτ. X −∞ −∞ −∞ Letting u=τ +s−r, we have (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ S (ω)= h(s)h(r)R (u)e−jω(u−s+r) dsdr du Y X −∞ −∞ −∞ (cid:90) ∞ (cid:90) ∞ (cid:90) ∞ = h(s)ejωs ds h(r)e−jωr dr R (u)e−jωu du X −∞ −∞ −∞ =H(ω)H(ω)S (ω), X where H(ω) is the complex conjugate of H(ω). (cid:3) ItistemptingtothinkthatsinceY(t)=h(t)∗X(t),thepowerspectraldensityshould also be S (ω) = H(ω)X(ω), but this is not true. The above result shows that we need an Y 649
CHAPTER 10. RANDOM PROCESSES additional complex conjugate H(ω) because S (ω) is the power, which means the square Y of the signal. Note that R is “squared” because we have convolved it with itself, and R X Y is also squared. Therefore, to match R and R , the impulse response h also needs to be X Y squared in the Fourier domain. Example 10.16. A WSS process X(t) has a correlation function R (τ)=sinc(πτ). X Suppose that X(t) passes through an LTI system with input/output relationship d2 d d2 d 2 Y(t)+2 Y(t)+4Y(t)=3 X(t)−3 X(t)+6X(t). dt2 dt dt2 dt Find R (τ). Y Solution: The sinc function has a Fourier transform given by π (cid:16) ω (cid:17) sinc(Wt)←→ rect . F W 2W Therefore, the autocorrelation function is π (cid:16) ω (cid:17) R (τ)=sinc(πτ) ←→ rect . X F π 2π By taking the Fourier transform on both sides, we have (cid:40) 1, −π ≤ω ≤π, S (ω)= X 0, elsewhere. The system response is found from the differential equation: 3(jω)2−3(jω)+6 H(ω)= 2(jω)2+2(jω)+4 3(cid:2) (2−ω2)−jω(cid:3) = . 2[(2−ω2)+jω] Taking the magnitude square yields 3(cid:2) (2−ω2)−jω(cid:3) 3(cid:2) (2−ω2)+jω(cid:3) |H(ω)|2 = 2[(2−ω2)+jω] 2[(2−ω2)−jω] 9(2−ω2)2+ω2 9 = = . 4(2−ω2)2+ω2 4 Therefore, the output power spectral density is 9 S (ω)=|H(ω)|2S (ω)= S (ω). Y X 4 X 650
10.5. WSS PROCESS THROUGH LTI SYSTEMS Taking the inverse Fourier transform, we have 9 R (τ)= sinc(πτ). Y 4 Example 10.17. A random process X(t) has zero mean and R (t,s) = min(t,s). X Consider a new process Y(t)=etX(e−2t). 1. Is Y(t) WSS? 2. Suppose Y(t) passes through a LTI system to yield an output Z(t) according to d d Z(t)+2Z(t)= Y(t)+Y(t). dt dt Find R (τ). Z Solution: 1. InordertoverifywhetherY(t)isWSS,weneedtocheckthemeanfunctionand the autocorrelation function. The mean function is E[Y(t)]=E(cid:2) etX(e−2t)(cid:3) =etE(cid:2) X(e−2t)(cid:3) . Since X(t) has zero mean, E[X(t)] = 0 for all t. This implies that if u = e−2t, then E[X(u)]=0 because u is just another time instant. Thus E[X(e−2t)]=0, and hence E[Y(t)]=0. The autocorrelation is (cid:104) (cid:105) E[Y(t+τ)Y(t)]=E et+τX(e−2(t+τ))etX(e−2t) (cid:104) (cid:105) =e2t+τE X(e−2(t+τ))X(e−2t) =e2t+τR (e−2(t+τ),e−2t). X Substituting R (t,s)=min(t,s), we have that X e2t+τR (e−2(t+τ),e−2t)=e2t+τmin(e−2(t+τ),e−2t) X (cid:26) e−2(t+τ), τ ≥0 =e2t+τ e−2t, τ <0 (cid:26) e−τ, τ ≥0 = eτ, τ <0 =e−|τ|. So R (τ)=e−|τ|. Since R (τ) is a function of τ, Y(t) is WSS. Y Y 651
CHAPTER 10. RANDOM PROCESSES 2. The system response is given by 1+jω H(ω)= . 2+jω The magnitude is therefore 1+ω2 |H(ω)|2 = . 4+ω2 Hence, the output autocorrelation function is 2 R (τ)=e−|τ| ←→S (ω)= , Y Y 1+ω2 and S (ω)=|H(ω)|2S (ω) Z Y 1+ω2 2 2 = = . 4+ω21+ω2 4+ω2 Therefore 1 R (τ)= e−2|τ|. Z 2 10.5.4 Cross-correlation through LTI Systems Theaboveanalysesaredevelopedfortheautocorrelationfunction.Ifweconsiderthecross- correlationbetweentworandomprocesses,sayX(t)andY(t),thentheaboveresultsdonot hold. In this section, we discuss the cross-correlation through LTI systems. To begin with, we need to define WSS for a pair of random processes. Definition 10.11. Two random processes X(t) and Y(t) are jointly WSS if 1. X(t) is WSS and Y(t) is WSS, and 2. R (t ,t )=E[X(t )Y(t )] is a function of t −t . X,Y 1 2 1 2 1 2 If X(t) and Y(t) are jointly WSS, we write R (t ,t )=R (τ)d =efE[X(t+τ)Y(τ)]. X,Y 1 2 X,Y Thedefinitionof“jointlyWSS”isnecessaryherebecauseR isdefinedbyX andY.Just X,Y knowing that X(t) and Y(t) are WSS does not allow one to say that R (t ,t ) can be X,Y 1 2 written as the time difference. If we flip the order of X and Y to consider R (τ) and not R (τ), then we need Y,X X,Y to flip the argument. The following lemma explains why. 652
10.5. WSS PROCESS THROUGH LTI SYSTEMS Lemma10.3. ForanyrandomprocessesX(t)andY(t),the cross-correlationR (τ) X,Y is related to R (τ) as Y,X R (τ)=R (−τ). (10.30) X,Y Y,X Proof. Recall the definition of R (−τ) = E[Y(t−τ)X(t)]. This can be simplified as Y,X follows: R (−τ)=E[Y(t−τ)X(t)] Y,X =E[X(t)Y(t−τ)] =E[X(t(cid:48)+τ)Y(t(cid:48))] =R (τ), X,Y where we substituted t(cid:48) =t−τ. (cid:3) Example 10.18.LetX(t)andN(t)betwoindependentWSSrandomprocesseswith expectationsE[X(t)]=µ andE[N(t)]=0,respectively.LetY(t)=X(t)+N(t).We X want to show that X(t) and Y(t) are jointly WSS, and we want to find R (τ). X,Y Solution. Before we show the joint WSS property of X(t) and Y(t), we first show that Y(t) is WSS: E[Y(t)]=E[X(t)+N(t)]=µ . X R (t ,t )=E[(X(t )+N(t ))(X(t )+N(t ))] Y 1 2 1 1 2 2 =E[(X(t )X(t )]+E[(N(t )N(t )] 1 2 1 2 =R (t −t )+R (t −t ). X 1 2 N 1 2 Thus, Y(t) is WSS. To show that X(t) and Y(t) are jointly WSS, we need to check the cross- correlation function: R (t ,t )=E[X(t )Y(t )] X,Y 1 2 1 2 =E[X(t )(X(t )+N(t ))] 1 2 2 =E[X(t )(X(t )]+E[X(t )N(t )] 1 2 1 2 =R (t ,t )+E[X(t )]E[N(t )] X 1 2 1 2 =R (t ,t ). X 1 2 Since R (t ,t ) is a function of t −t , and since X(t) and Y(t) are WSS, X(t) and X,Y 1 2 1 2 Y(t) must be jointly WSS. Finally,tofindR (τ),wesubstituteτ =t −t andobtainR (τ)=R (τ). X,Y 1 2 X,Y X KnowingthedefinitionofjointlyWSS,weconsiderthecross-correlationbetweenX(t) and Y(t). Note that here we are asking about the cross-correlation between the input and the output of the same LTI system, as illustrated in Figure 10.23. The pair X(t) and Y(t)=h(t)∗X(t) are special because Y(t) is the convolved version of X(t). 653
CHAPTER 10. RANDOM PROCESSES Figure 10.23: The source of the signals when defining R (τ), R (τ), R (τ) and R (τ). X X,Y Y,X Y Theorem 10.6. Let X(t) and Y(t) be jointly WSS processes, and let Y(t) = h(t)∗ X(t). Then the cross-correlation R (τ) is Y,X R (τ)=h(τ)∗R (τ). (10.31) Y,X X Proof. Recalling the definition of cross-correlation, we have R (τ)=E[Y(t+τ)X(t)] Y,X (cid:20) (cid:90) ∞ (cid:21) =E X(t) X(t+τ −r)h(r)dr −∞ (cid:90) ∞ (cid:90) ∞ = E[X(t)X(t+τ −r)]h(r)dr = R (τ −r)h(r)dr, X −∞ −∞ which is the convolution R (τ)=h(τ)∗R (τ). Y,X X (cid:3) We next define the cross power spectral density of two jointly WSS processes as the Fourier transform of the cross-correlation function. Definition 10.12. The cross power spectral density of two jointly WSS processes X(t) and Y(t) is defined as S (ω)=F[R (τ)], X,Y X,Y S (ω)=F[R (τ)]. Y,X Y,X The relationship between S and S can be seen from the following theorem. X,Y Y,X Theorem 10.7. For two jointly WSS random processes X(t) and Y(t), the cross power spectral density satisfies the property that S (ω)=S (ω), (10.32) X,Y Y,X where (·) denotes the complex conjugate. 654
10.5. WSS PROCESS THROUGH LTI SYSTEMS Proof. Since S (ω)=F[R (τ)] by definition, it follows that X,Y X,Y (cid:90) ∞ F[R (τ)]= R (τ)e−jωτ dτ X,Y X,Y −∞ (cid:90) ∞ (cid:90) ∞ = R (−τ)e−jωτ dτ = R (τ(cid:48))ejωτ(cid:48) dτ(cid:48), Y,X X,Y −∞ −∞ which is exactly the conjugate S (ω). Y,X (cid:3) WhensendingtherandomprocessthroughanLTIsystem,thecross-correlationpower spectral density is given by the theorem below. Theorem 10.8. If X(t) passes through an LTI system to yield Y(t), then the cross power spectral density is S (ω)=H(ω)S (ω), Y,X X S (ω)=H(ω)S (ω). X,Y X Proof. By taking the Fourier transform on R (τ) we have that S (ω) = H(ω)S (ω). Y,X Y,X X Since R (τ)=R (−τ), it holds that S (ω)=H(ω)S (ω). X,Y Y,X X,Y X (cid:3) Example 10.19. Let X(t) be a WSS random process with R (τ)=e−τ2/2, H(ω)=e−ω2/2. X Find S (ω), R (τ), S (ω) and R (τ). X,Y X,Y Y Y Solution. First, by the Fourier transform table we know that √ S (ω)= 2πe−ω2/2. X Since H(ω)=e−ω2/2, we have S (ω)=H(ω)S (ω) X,Y X √ = 2πe−ω2 . The cross-correlation function is (cid:104)√ (cid:105) R (ω)=F−1 2πe−ω2 X,Y = √1 e−τ 42 . 2 655
CHAPTER 10. RANDOM PROCESSES The power spectral density of Y(t) is S (ω)=|H(ω)|2S (ω) Y X √ = 2πe−3ω 22 . Therefore, the autocorrelation function of Y(t) is R Y(τ)=F−1(cid:104)√ 2πe−3ω 22(cid:105) 1 = √ e−τ2/6. 3 10.6 Optimal Linear Filter In the previous sections, we have built many tools to analyze random processes. Our next goal is to apply these techniques. To that end, we will discuss optimal linear filter design, whichisasetofestimationtechniquesforpredictingandrecoveringinformationfromatime series. 10.6.1 Discrete-time random processes We begin by introducing some notations. In the previous sections, we have been using continuous-time random processes to study statistics. In this section, we mainly focus on discrete-timerandomprocesses.Theshiftfromcontinuous-timetodiscrete-timeisstraight- forward as far as the theories are concerned — we switch the continuous-time index t to a discrete-time index n. However, shifting to discrete-time random processes can simplify manydifficultproblemsbecausemanydiscrete-timeproblemscanbesolvedbymatricesand vectors. This will make the computations and implementations much easier. To make this transition easier, we provide a few definitions and results without proof. Notations for discrete-time random processes • Wedenotethediscrete-timeindicesbymandn,correspondingtothecontinuous- time indices t and t , respectively. 1 2 • A discrete-time random process is denoted by X[n]. • Its mean function and the autocorrelation function are µ [n]=E[X[n]], X R [m,n]=E[X[m]X[n]]. X • We say that X[n] is WSS if µ [n] = constant, and R [m,n] is a function of X X m−n. 656
10.6. OPTIMAL LINEAR FILTER • If X[n] is WSS, we write R [m,n] as X R [m,n]=R [m−n]=R [k], X X X where k =m−n is the interval. • If X[n] is WSS, we define the power spectral density as S (ejω)=F{R [k]}, X X where S (ejω) denotes the discrete-time Fourier transform. X When a random process X[n] is sent through an LTI system with an impulse response h[n], the output is ∞ (cid:88) Y[n]=h[n]∗X[n]= h[k]X[n−k]. (10.33) k=−∞ WhenaWSSprocessX[n]passesthroughanLTIsystemh[n]toyieldanoutputY[n], the auto- and cross-correlation function and power spectral densities are • R [k]=E[Y[n+k]Y[n]], S (ejω)=F{R [k]}=|H(ejω)|2S (ejω). Y Y Y X • R [k]=E[X[n+k]Y[n]], S (ejω)=F{R [k]}=H(ejω)S (ejω). XY XY XY X • R [k]=E[Y[n+k]X[n]], S (ejω)=F{R [k]}=H(ejω)S (ejω). YX YX YX X 10.6.2 Problem formulation Theproblemwestudyhereisknownastheoptimal linear filter design.Supposethatthere is a WSS process X[n] that we want to process. For example, if X[n] is a corrupted version of some clean time-series, we may want to remove the noise by filtering (also known as averaging) X[n]. Conceptualizing the denoising process as a linear time-invariant system with an impulse response h[n], our goal is to determine the optimal h[n] such that the estimated time series Y(cid:98)[n] is as close to the true time series Y[n] as possible. Referring to Figure 10.24, we refer to X[n] as the input function and to Y[n] as the target function. X[n] and Y[n] are related according to the equation K−1 (cid:88) Y[n]= h[k]X[n−k]+E[n], (10.34) k=0 (cid:124) (cid:123)(cid:122) (cid:125) Y(cid:98)[n] where E[n] is a noise random process to model the error. The linear part of the equation is known as the prediction and is constructed by sending X[n] through the system. For simplicity we assume that X[n] is WSS. Thus, it follows that Y[n] is also WSS. We may also assume that we can estimate R [k], R [k], R [k] and R [k]. X YX XY Y 657
CHAPTER 10. RANDOM PROCESSES Figure10.24:Aschematicdiagramillustratingtheoptimallinearfilterproblem:Givenaninputfunction X[n],wewanttodesignafilterh[n]suchthatthepredictionY(cid:98)[n]isclosetothetargetfunctionY[n]. Example 10.20. If we let K =3, Equation (10.34) gives us Y[n]=h[0]X[n]+h[1]X[n−1]+h[2]X[n−2]+E[n]. Thatis,thecurrentsampleY[n]isalinearcombinationoftheprevioussamplesX[n], X[n−1] and X[n−2]. Given X[n] and Y[n], what would be the best guess of the impulse response h[n] so that the prediction is as close to the true values as possible? From our discussions of linear regression, we know that this is equivalent to solving the optimization problem (cid:32) K−1 (cid:33)2 (cid:88) minimize Y[n]− h[k]X[n−k] . (10.35) {h[k]}K−1 k=0 k=0 The choice of the squared error is more or less arbitrary, depending on how we want to model E[n]. By using the square norm, we implicitly assume that the error is Gaussian. This may not be true, but it is commonly used because the squared norm is differentiable. We will follow this tradition. The challenge associated with the minimization is that in most of the practical set- tings the random processes X[n] and Y[n] are changing rapidly because they are random processes. Therefore, even if we solve the optimization problem, the estimates h[k] will be randomvariablessincewearesolvingarandomequation.Toeliminatethisrandomness,we take the expectation over all the possible choices of X[n] and Y[n], yielding (cid:32) K−1 (cid:33)2 (cid:88) minimize Y[n]− h[k]X[n−k] , {h[k]}K−1 k=0 k=0 ⇓ (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k] . {h[k]}K−1 k=0 k=0 The resulting impulse responses h[k], derived by solving the above minimization, is known as the optimal linear filter. It is the best linear model for describing the input- output relationships between X[n] and Y[n]. 658
10.6. OPTIMAL LINEAR FILTER What is the optimal linear filter? The optimal linear filter is the solution to the optimization problem (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k] . (10.36) {h[k]}K−1 k=0 k=0 10.6.3 Yule-Walker equation To solve the optimal linear filter problem, we first perform some (slightly tedious) algebra to obtain the following results: Lemma 10.4. Let Y(cid:98)[n]=(cid:80)K−1h[k]X[n−k] be the prediction of Y[n]. The squared- k=0 norm error can be written as (cid:20)(cid:16) (cid:17)2(cid:21) E X,Y Y[n]−Y(cid:98)[n] K−1 K−1K−1 (cid:88) (cid:88) (cid:88) =R [0]−2 h[k]R [k]+ h[k]h[j]R [j−k]. (10.37) Y YX X k=0 k=0 j=0 Thus we can express the error in terms of R [k], R [k] and R [k]. YX X Y Proof. We expand the error as follows: E X,Y (cid:20)(cid:16) Y[n]−Y(cid:98)[n](cid:17)2(cid:21) =E Y (cid:2) (Y[n])2(cid:3) −2E X,Y (cid:104) Y[n]Y(cid:98)[n](cid:105) +E X(cid:104) (Y(cid:98)[n])2(cid:105) . The first term is the autocorrelation of Y[n]: E (cid:2) (Y[n])2(cid:3) =E[Y[n+0]Y[n]]=R [0]. (10.38) Y Y The second term is (cid:34) K−1 (cid:35) (cid:104) (cid:105) (cid:88) E X,Y Y[n]Y(cid:98)[n] =E X,Y Y[n] h[k]X[n−k] k=0 K−1 (cid:88) = h[k]E [Y[n]X[n−k]] X,Y k=0 K−1 (cid:88) = h[k]R [k]. (10.39) YX k=0 659
CHAPTER 10. RANDOM PROCESSES The third term is (cid:32)K−1 (cid:33) K−1  (cid:104) (cid:105) (cid:88) (cid:88) E X (Y(cid:98)[n])2 =E X h[k]X[n−k]  h[j]X[n−j] k=0 j=0   K−1K−1 (cid:88) (cid:88) =E X h[k]h[j]X[n−k]X[n−j] k=0 j=0 K−1K−1 (cid:88) (cid:88) = h[k]h[j]E [X[n−k]X[n−j]] X k=0 j=0 K−1K−1 (cid:88) (cid:88) = h[k]h[j]R [j−k]. (10.40) X k=0 j=0 This completes the proof. (cid:3) ThesignificanceofthistheoremisthatitallowsustowritetheerrorintermsofR [k], YX R [k] and R [k]. As we have mentioned, while we can solve the randomized optimization X Y Equation(10.35),theresultingsolutionwillbearandomvectordependingontheparticular realizationsX[n]andY[n].SwitchingfromEquation(10.35)toEquation(10.36)eliminates therandomnessbecausewehavetakentheexpectation.Theresultingoptimizationaccording to the theorem is also convenient. Instead of seeking individual realizations, we only need to know the overall statistical description of the data through R [k], R [k] and R [k]. YX X Y These can be estimated through modeling or pseudorandom signals. The solution to the optimal linear filter problem is summarized by the Yule-Walker equation: Theorem 10.9. The solution {h[0],...,h[K−1]} to the optimal linear filter problem (cid:32) K−1 (cid:33)2 (cid:88) minimize E X,Y  Y[n]− h[k]X[n−k]  (10.41) {h[k]}K−1 k=0 k=0 is given by the following matrix equation:      R RY YX X . . . [ [0 1] ]     =       R RX X . . .[ [0 1] ] R RX X . . .[ [1 0] ] · · ..· · .· · RX[K . . . . . . −1]            h h[ [ . . .0 1] ]     , (10.42) RYX[K−1] RX[K−1] RX[k−2] ··· RX[0] h[K−1] which is known as the Yule-Walker equation. Therefore, by solving the simple linear problem given by the Yule-Walker equation, we will find the optimal linear filter solution. Proof. Since the error is a squared norm, the optimal solution is obtained by taking the 660
10.6. OPTIMAL LINEAR FILTER derivative: d (cid:20)(cid:16) (cid:17)2(cid:21) dh[i]E X,Y Y[n]−Y(cid:98)[n]   d  K (cid:88)−1 K (cid:88)−1K (cid:88)−1  = R [0]−2 h[k]R [k]+ h[k]h[j]R [j−k] dh[i] Y YX X   k=0 k=0 j=0 K−1 (cid:88) =0−2R [i]+2 h[k]R [i−k], YX X k=0 in which the derivative of the last term is computed by noting that K−1K−1 d (cid:88) (cid:88) h[k]h[j]R [j−k] dh[i] X k=0 j=0 K−1 K−1 d (cid:88) d (cid:88) (cid:88) = h[j]2R [0]+ h[k]h[j]R [j−k] dh[i] X dh[i] X j=0 k=0 j(cid:54)=k K−1 (cid:88) =2 h[k]R [i−k]. X k=0 Equating the derivative to zero yields K−1 (cid:88) R [i]= h[k]R [i−k], i=0,...,K−1, YX X k=0 and putting the above equations into the matrix-vector form we complete the proof. (cid:3) The matrix in the Yule-Walker equation is a Toeplitz matrix, in which each row is a shifted version of the preceding row. This matrix structure is a consequence of a WSS process so that the autocorrelation function is determined by the time difference k and not by the starting and end times. Remark. If we take the derivative of the loss w.r.t. h[i], we have that d (cid:20)(cid:16) (cid:17)2(cid:21) (cid:104)(cid:16) (cid:17) (cid:105) 0= dh[i]E X,Y Y[n]−Y(cid:98)[n] =−2E Y[n]−Y(cid:98)[n] X[n−i] . Thisconditionisknownastheorthogonality condition,asitsaysthattheerrorY[n]−Y(cid:98)[n] is orthogonal to the signal X[n−i]. 10.6.4 Linear prediction We now demonstrate how to use the Yule-Walker equation in modeling an autoregressive process. The procedure in this simple example can be used in speech processing and time- series forecasting. SupposethatwehaveaWSSrandomprocessY[n].Wewouldliketopredictthefuture samples by using the most recent K samples through an autoregressive model. Since the 661
CHAPTER 10. RANDOM PROCESSES model is linear, we can write K (cid:88) Y(cid:98)[n]= h[k]Y[n−k]+E[n]. (10.43) k=1 In this model, we say that the predicted value Y(cid:98)[n] is a linear combination of the past K samples, albeit to approximation error E[n]. The problem we need to solve is (cid:20)(cid:16) (cid:17)2(cid:21) minimize E Y[n]−Y(cid:98)[n] . h[k] SinceY(cid:98)[n]iswrittenintermsofthepastsamplesofY[n]inthisproblem,intheYule-Walker equation we can replace X with Y. Consequently, we can write the matrix equation from  RYX[0]   RX[0] RX[1] ··· RX[K−1]  h[0]   RYX[1]   RX[1] RX[0] ··· RX[K−2]  h[1]     . . .   =   . . . . . . ... . . .      . . .   , RYX[K−1] RX[K−1] RX[k−2] ··· RX[0] h[K−1] to  RY[1]   RY[0] RY[1] ··· RY[K−1]  h[0]   RY[2]   RY[1] RY[0] ··· RY[K−2]  h[1]     . . .   =   . . . . . . ... . . .      . . .   . (10.44) RY[K] RY[K−1] RY[k−2] ··· RY[0] h[K−1] (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) r R On a computer, solving the Yule-Walker equation requires a few steps. First, we need to estimate the correlation N 1 (cid:88) R [k]=E[Y[n+k]Y[n]]≈ Y[n+k]Y[n]. Y N n=1 Theaveragingontheright-handsideisoftendoneusingxcorrinMATLABornp.correlate in Python. A graphical illustration of the input and the autocorrelation function is shown in Figure 10.25. After we have found R [n], we need to construct the Yule-Walker equation. For this Y linear prediction problem, the left-hand side of the Yule-Walker equation is the vector r, definedaccordingtoEquation(10.44).TheYule-WalkerequationalsorequiresthematrixR. This R can be constructed via the Toeplitz matrix as (cid:26) (cid:27) R=Toeplitz R [0],R [1],...,R [K−1] . Y Y Y In MATLAB, we can call Toeplitz to construct the matrix. In Python, the command is lin.Toeplitz. To solve the Yule-Walker equation, we need to invert the matrix R. There are built-in commands for such an operation. In MATLAB, the command is \ (the backslash), whereas in Python the command is np.linalg.lstsq. 662
10.6. OPTIMAL LINEAR FILTER 0.2 0.4 Y[n] R [k] Y 0.3 0.1 0.2 0 0.1 0 -0.1 -0.1 -0.2 -0.2 0 50 100 150 200 250 300 -300 -200 -100 0 100 200 300 (a) Y[n] (b) R [k] Y Figure 10.25: An example time-series and its autocorrelation function. % MATLAB code to solve the Yule Walker Equation y = load(’data_ch10.txt’); K = 10; N = 320; y_corr = xcorr(y); R = Toeplitz(y_corr(N+[0:K-1])); lhs = y_corr(N+[1:K]); h = R\lhs; # Python code to solve the Yule Walker Equation y = np.loadtxt(’./data_ch10.txt’) K = 10 N = 320 y_corr = np.correlate(y,y,mode=’full’) R = lin.Toeplitz(y_corr[N-1:N+K-1]) #call scipy.linalg lhs = y_corr[N:N+K] h = np.linalg.lstsq(R,lhs,rcond = None)[0] Note that in both the MATLAB and Python codes the Toeplitz matrix R starts with the index N. This is because, as you can see from Figure 10.25, the origin of the autocor- relation function is the middle index of the computed autocorrelation function. For r, the starting index is N +1 because the vector starts with R [1]. Y To predict the future samples, we recall the autoregressive model for this problem: K−1 (cid:88) Y(cid:98)[n]= h[k]Y[n−k]. k=0 Therefore, given Y[n−1],Y[n−2],...,Y[n−K], we can predict Y(cid:98)[n]. Then we insert this predicted Y(cid:98)[n] into the sequence and increment the estimation problem to the next time index. By repeating the process, we will be able to predict the future samples of Y[n]. 663
CHAPTER 10. RANDOM PROCESSES Figure 10.26illustratesthepredictionresultsoftheYule-Walkerequation.Asyoucan see, the predictions are reasonably meaningful since the patterns follow the trend. 0.2 0.1 0 -0.1 -0.2 Prediction Input -0.3 0 50 100 150 200 250 300 350 Figure 10.26: An example of the predictions made by the autoregressive model. The MATLAB and Python codes are shown below. % MATLAB code to predict the samples z = y(311:320); yhat = zeros(340,1); yhat(1:320) = y; for t = 1:20 predict = z’*h; z = [z(2:10); predict]; yhat(320+t) = predict; end plot(yhat, ’r’, ’LineWidth’, 3); hold on; plot(y, ’k’, ’LineWidth’, 4); # Python code to predict the samples z = y[310:320] yhat = np.zeros((340,1)) yhat[0:320,0] = y for t in range(20): predict = np.inner(np.reshape(z,(1,10)),h) z = np.concatenate((z[1:10], predict)) yhat[320+t,0] = predict plt.plot(yhat,’r’) plt.plot(y,’k’) plt.show() 664
10.6. OPTIMAL LINEAR FILTER 10.6.5 Wiener filter In the previous formulation, we notice that the impulse response has a finite length. There are, however, problems in which the impulse response is infinite. For example, a recur- sive filter h[n] will be infinitely long. The extension from finite length to infinite length is straightforward. We can model the problem as ∞ (cid:88) Y[n]= h[k]X[n−k]+E[n]. k=−∞ However, when h[n] is infinitely long the Yule-Walker equation does not hold because the matrix R will be infinitely large. Nevertheless, the building block equation for Yule-Walker is still valid: ∞ (cid:88) R [i]= h[k]R [i−k]. (10.45) YX X k=−∞ To maintain the spirit of the Yule-Walker equation while enabling computation, we recognize that the infinite sum on the right-hand side is, in fact, a convolution. Thus we can take the (discrete-time) Fourier transform of both sides to obtain S (ejω)=H(ejω)S (ejω). (10.46) YX X Therefore, the corresponding optimal linear filter (in the Fourier domain) is S (ejω) H(ejω)= YX , (10.47) S (ejω) X and (cid:26) S (e−jω)(cid:27) h[n]=F−1 YX . S (e−jω) X The filter obtained in this way is known as the Wiener filter. Example 10.21. (Denoising) Suppose X[n]=Y[n]+W[n], where W[n] is the noise term that is independent of Y[n], as shown in Figure 10.27. Figure10.27:DesignofaWienerfilterthattakesaninputfunctionX[n]andoutputsanestimate Y(cid:98)[n] that is close to the true function Y[n]. Now, given the input function X[n], can we construct the Wiener filter h[n] such that the predicted function Y(cid:98)[n] is as close to Y[n] as possible? The Wiener filter for this problem is also the optimal denoising filter. 665
CHAPTER 10. RANDOM PROCESSES Solution. The following correlation functions can easily be seen: R [k]=E[X[n+k]X[n]] X =E[(Y[n+k]+W[n+k])(Y[n]+W[n])] =E[Y[n+k]Y[n]]+E[Y[n+k]W[n]] +E[W[n+k]Y[n]]+E[W[n+k]W[n]] =E[Y[n+k]Y[n]]+0+0+E[W[n+k]W[n]] =R [k]+R [k]. Y W Similarly, we have R [k]=E[Y[n+k]X[n]] YX =E[Y[n](Y[n+k]+W[n+k])]=R [k]. Y Consequently, the optimal linear filter is S (ejω) H(ejω)= YX S (ejω) X F{R [k]} = YX F{R [k]} X S (ejω) = Y . S (ejω)+S (ejω) Y W What is the Wiener filter for a denoising problem? • SupposethecorruptedfunctionX[n]isrelatedtothecleanfunctionY[n]through X[n]=Y[n]+W[n], for some noise function W[n]. • The Wiener filter is S (ejω) H(ejω)= Y . (10.48) S (ejω)+S (ejω) Y W • To perform the filtering, the denoised function Y(cid:98)[n] is Y(cid:98)[n]=F−1(cid:8) H(ejω)X(ejω)(cid:9) . Figure 10.28 shows an example of applying the Wiener filter to a noise removal prob- lem. In this example we let W[n] be an i.i.d. Gaussian process with standard deviation σ = 0.05 and mean µ = 0. The noisy samples of random process X[n] are defined as X[n]=Y[n]+W[n],whereY[n]isthecleanfunction.AsyoucanseefromFigure 10.28(a), the Wiener filter is able to denoise the function reasonably well. Theoptimallinearfilterusedforthisdenoisingtaskisinfinitelylong.Thiscanbeseen in Figure 10.28(b), where the filter length is the same as the length of the observed time series X[n]. If X[n] is longer, the filter h[n] will also become longer. Therefore, finite-length approaches such as the Yule-Walker equation do not apply here. 666
10.6. OPTIMAL LINEAR FILTER 0.2 0.25 h[n] 0.1 0.2 0.15 0 0.1 -0.1 0.05 Noisy Input X[n] Wiener Filtered Yhat[n] 0 -0.2 Ground Truth Y[n] -0.05 0 50 100 150 200 250 300 -300 -200 -100 0 100 200 300 (a) Noise removal by Wiener filtering (b) Wiener filter Figure 10.28: (a) Applying a Wiener filter to denoise a function. (b) The Wiener filter used for the denoising task. The MATLAB / Python codes used to generate Figure 10.28(a) are shown below. Themaincommandsherearescipy.fftandscipy.ifft,whichareavailableinthescipy library. The commands Yhat = H.*fft(x, 639) in MATLAB execute the Wiener filtering step. Here, we resample the function x to 639 samples so that it matches with the Wiener filter H. Similar commands in Python are H * fft(x, 639). % MATLAB code for Wiener filtering w = 0.05*randn(320,1); x = y + w; Ry = xcorr(y); Rw = xcorr(w); Sy = fft(Ry); Sw = fft(Rw); H = Sy./(Sy + Sw); Yhat = H.*fft(x, 639); yhat = real(ifft(Yhat)); plot(x, ’LineWidth’, 4, ’Color’, [0.7, 0.7, 0.7]); hold on; plot(yhat(1:320), ’r’, ’LineWidth’, 2); plot(y, ’k:’, ’LineWidth’, 2); # Python code for Wiener filtering from scipy.fft import fft, ifft w = 0.05*np.random.randn(320) x = y + w Ry = np.correlate(y,y,mode=’full’) Rw = np.correlate(w,w,mode=’full’) Sy = fft(Ry) Sw = fft(Rw) H = Sy / (Sy+Sw) Yhat = H * fft(x, 639) 667
CHAPTER 10. RANDOM PROCESSES yhat = np.real(ifft(Yhat)) plt.plot(x,color=’gray’) plt.plot(yhat[0:320],’r’) plt.plot(y,’k:’) Example 10.22. (Deconvolution) Suppose that the corrupted function is generated according to a linear process given by ∞ (cid:88) X[n]= g[(cid:96)]Y[n−(cid:96)]+W[n], (cid:96)=−∞ where g[n] is the impulse response of some kind of degradation process and W[n] is theGaussiannoiseterm,asshowninFigure 10.29.Findtheoptimallinearfilter(i.e., the Wiener filter) to estimate Y(cid:98)[n]. Figure10.29:DesignofaWienerfilterthattakesaninputfunctionX[n]andoutputsanestimate Y(cid:98)[n] that is close to the true function Y[n]. Solution.ToconstructtheWienerfilter,wefirstdeterminethecross-correlationfunc- tion: (cid:34) ∞ (cid:35) (cid:88) R [k]=E[Y[n+k]X[n]]=E Y[n+k] g[(cid:96)]Y[n−(cid:96)]+W[n] . YX (cid:96)=−∞ Using algebra, it follows that (cid:34) ∞ (cid:35) (cid:88) E Y[n+k] g[(cid:96)]Y[n−(cid:96)]+W[n] (cid:96)=−∞ ∞ (cid:88) = g[(cid:96)]E[Y[n+k]Y[n−(cid:96)]]+E[Y[n+k]W[n]] (cid:96)=−∞ ∞ (cid:88) = g[(cid:96)]R [k+(cid:96)]+0=(g(cid:126)R )[k], Y Y (cid:96)=−∞ whichisthecorrelationbetweeng andR .Therefore,thecrosspowerspectraldensity Y S (ejω) is YX S (ejω)=G(ejω)S (ejω). YX Y 668
10.6. OPTIMAL LINEAR FILTER The autocorrelation of this problem is R [k]=E[X[n+k]X[n]] X =E[((g∗Y)[n+k]+W[n+k])((g∗Y)[n]+W[n])] =E[(g∗Y)[n+k](g∗Y)[n]]+E[W[n+k]W[n]] =(g(cid:126)(g∗R ))[k]+R [k], Y W where,accordingtotheprevioussection,thefirstpartisthecorrelation(cid:126)followedby a convolution ∗. Therefore, the power spectral density of X is S (ejω)=|G(ejω)|2S (ejω)+S (ejω). X Y W Combining the results, the Wiener filter is S (ejω) G(ejω)S (ejω) H(ejω)= YX = Y . S (ejω) |G(ejω)|2S (ejω)+S (ejω) X Y W What is the Wiener filter for a deconvolution problem? • Suppose that the corrupted function X[n] is related to the clean function Y[n] through X[n]=(g∗Y)[n]+W[n], for some degradation g[n] and noise W[n]. • The Wiener filter is G(ejω)S (ejω) H(ejω)= Y . (10.49) |G(ejω)|2S (ejω)+S (ejω) Y W • To perform the filtering, the estimated function Y(cid:98)[n] is Y(cid:98)[n]=F−1(cid:8) H(ejω)X(ejω)(cid:9) . As an example of the deconvolution problem, we show a WSS function Y[n] in Fig- ure 10.30.ThiscleanfunctionY[n]isconstructedbypassingani.i.d.noiseprocessthrough an arbitrary LTI system so that the WSS property is guaranteed. Given this Y[n], we con- structadegradationprocessinwhichtheimpulseresponseisgivenbyg[n].Inthisexample, we assume that g[n] is a uniform function. We then add noise W[n] to the time series to obtain the corrupted observation X[n]. The reconstruction by the Wiener filter is shown in Figure 10.30. The MATLAB and Python codes used to generate Figure 10.30 are shown below. % MATLAB code to solve the Wiener deconvolution problem load(’ch10_wiener_deblur_data’); g = ones(32,1)/32; w = 0.02*randn(320,1); x = conv(y,g,’same’) + w; Ry = xcorr(y); 669
CHAPTER 10. RANDOM PROCESSES 0.6 0.4 0.2 0 -0.2 -0.4 Noisy Input X[n] -0.6 Wiener Filtered Yhat[n] Ground Truth Y[n] -0.8 50 100 150 200 250 300 Figure 10.30: Reconstructing time series from degraded observations using a Wiener filter. Rw = xcorr(w); Sy = fft(Ry); Sw = fft(Rw); G = fft(g,639); H = (conj(G).*Sy)./(abs(G).^2.*Sy + Sw); Yhat = H.*fft(x, 639); yhat = real(ifft(Yhat)); figure; plot(x, ’LineWidth’, 4, ’Color’, [0.5, 0.5, 0.5]); hold on; plot(16:320+15, yhat(1:320), ’r’, ’LineWidth’, 2); plot(1:320, y, ’k:’, ’LineWidth’, 2); # Python code to solve the Wiener deconvolution problem y = np.loadtxt(’./ch10_wiener_deblur_data.txt’) g = np.ones(64)/64 w = 0.02*np.random.randn(320) x = np.convolve(y,g,mode=’same’) + w Ry = np.correlate(y,y,mode=’full’) Rw = np.correlate(w,w,mode=’full’) Sy = fft(Ry) Sw = fft(Rw) G = fft(g,639) H = (np.conj(G)*Sy)/( np.power(np.abs(G),2)*Sy + Sw ) Yhat = H * fft(x, 639) yhat = np.real(ifft(Yhat)) plt.plot(x,color=’gray’) 670
10.6. OPTIMAL LINEAR FILTER plt.plot(np.arange(32,320+32),yhat[0:320],’r’) plt.plot(y,’k:’) Caveat to Wiener filtering. In practice, the above Wiener filter needs to be modified because S (ejω) and S (ejω) cannot be estimated from the data via the temporal corre- Y W lation (as we did in the MATLAB/Python programs). The reason is that we never have accesstoY[n]andW[n].Inthiscase,onehastoguess thepowerspectraldensitiesS (ejω) Y and S (ejω). The noise power S (ejω) is usually not difficult to estimate. For example, W W in the program we showed above, the noise power spectral density is Sw = 0.02^2*320 (MATLAB), which is the noise standard deviation times the number of samples. The signal S (ejω) is often the hard part. In the absence of any knowledge about the Y groundtruth’spowerspectraldensity,theWienerfilterdoesnotwork.However,forcertain problems in which S (ejω) can be predetermined by prior knowledge, the Wiener filter is Y guaranteed to be optimal — optimal in the mean-squared-error sense over the entire time axis. Wiener filter versus ridge regression. The Wiener filter equation can be interpreted as a ridge regression. Denoting the forward observation model by x=Gy+w, the corresponding ridge regression minimization is y =argmin (cid:107)x−Gy(cid:107)2+λ(cid:107)y(cid:107)2 (cid:98) y =(GTG+λI)−1GTx. If G is a convolutional matrix, the above solution can be written in the Fourier domain (by using the Fourier transform as the eigenvectors): (cid:34) (cid:35) G(ejω) Y(cid:98)(ejω)= X(ejω). |G(ejω)|2+λ (cid:124) (cid:123)(cid:122) (cid:125) H(ejω) Comparing this “optimal linear filter” with the Wiener filter, we observe that the Wiener filter has slightly more generality: (cid:34) (cid:35) G(ejω)S (ejω) Y(cid:98)(ejω)= Y X(ejω). |G(ejω)|2S (ejω)+S (ejω) Y W Therefore, in the absence of S (ejω) and assuming that S (ejω) is a constant (e.g., for Y W Gaussian noise), the Wiener filter is exactly a ridge regression. 671
CHAPTER 10. RANDOM PROCESSES 10.7 Summary Random processes are very useful tools for analyzing random variables over time. In this chapter, we have introduced some of the most basic mechanisms: • Statistical versus temporal analysis: The statistical analysis of a random process looksattherandomprocessvertically.IttreatsX(t)asarandomvariableandstudies the randomness across different realizations. The temporal analysis is the horizontal perspective.IttreatsX(t)asafunctionintimewithafixedrandomindex.Ingeneral, statistical average (cid:54)= temporal average. • Mean function µ (t): The mean function is the expectation of the random process. X At every time t, we take the expectation to obtain the expected value E[X(t)]. • Autocorrelation function R (t ,t ). This is the joint expectation of the random pro- X 1 2 cessattwodifferenttimeinstantst andt .ThecorrespondingvaluesX(t )andX(t ) 1 2 1 2 are two random variables, and so the joint expectation measures how correlated these two variables are. • Wide-sense stationary (WSS): This is a special class of random processes in which µ (t)isaconstantandR (t ,t )isafunctionoft −t .Whenthishappens,theauto- X X 1 2 1 2 correlation function (which is originally a 2D function) will have a Toeplitz structure. We write R (t ,t ) as R (τ), where τ =t −t . X 1 2 X 1 2 • Power spectral density (PSD): This is the Fourier transform of the autocorrelation function R (τ), according to the Einstein-Wiener-Khinchin theorem. It is called the X power spectral density because we can integrate it in the Fourier space to retrieve the power.Thisprovidesuswithsomeconvenientcomputationaltoolsforanalyzingdata. • Random process through a linear time-invariant (LTI) system: This tells us how a randomprocessbehavesaftergoingthroughanLTIsystem.Theanalysiscanbedone at the realization level, where we look at each random process, or at the statistical level, where we look at the autocorrelation function and the PSD. • Optimal linear filter: A set of techniques that can be used to retrieve signals by using the statistical information of the data and the system. We introduced two specific approaches: the Yule-Walker equation for a finite-length filter and the Wiener filter for an infinite-length filter. We demonstrated how these techniques could be applied to forecast a time series and recover a time series from corrupted measurements. While we have covered some of the most basic ideas in random processes, there are also several topics we have not discussed. These include, but are not limited to: strictly stationary process, a more restrictive class of random process than WSS; Poisson process, a useful model for arrival analysis; Markov chain, a discrete-time random process where the current state only depends on the previous state. Readers interested in these materials should consult the references listed at the end of this chapter. 672
10.8. APPENDIX 10.8 Appendix The Einstein-Wiener-Khinchin theorem TheEinstein-Wiener-Khinchintheoremisafundamentalresult.Itstatesthatforanywide- sense stationary process, the power spectral density S (ω) is the Fourier transform of the X autocorrelation function. Theorem 10.10 (TheEinstein-Wiener-Khinchintheorem). ForaWSSrandompro- cess X(t), S (ω)=F{R (τ)}, (10.50) X X whenever the Fourier transform of R (τ) exists. X Proof. First, let’s recall the definition of S (ω): X 1 (cid:104) (cid:105) S X(ω)d =ef Tl →im ∞2TE |X(cid:101)T(ω)|2 . (10.51) By expanding the expectation, we have (cid:34)(cid:32) (cid:33)(cid:32) (cid:33)∗(cid:35) (cid:90) T (cid:90) T E[|X(cid:101)T(ω)|2]=E X(t)e−jωt dt X(θ)e−jωθ dθ −T −T (cid:90) T (cid:90) T (cid:90) T (cid:90) T = E[X(t)X(θ)]e−jω(t−θ) dtdθ = R (t−θ)e−jω(t−θ) dtdθ. X −T −T −T −T (10.52) Our next step is to analyze R (t−θ). Define X Q (v)=F{R (τ)}. (10.53) X X Then, by inverse Fourier transform 1 (cid:90) ∞ R (τ)= Q (v)ejvτ dv, X 2π X −∞ and therefore 1 (cid:90) ∞ R (t−θ)= Q (v)ejv(t−θ) dv. X 2π X −∞ Substituting this into Equation (10.52) yields (cid:90) T (cid:90) T (cid:18) 1 (cid:90) ∞ (cid:19) E[|X(cid:101)T(ω)|2 = 2π Q X(v)ejv(t−θ) dv e−jω(t−θ) dtdθ −T −T −∞ (cid:32) (cid:33)(cid:32) (cid:33) 1 (cid:90) ∞ (cid:90) T (cid:90) T = Q (v) ejt(v−ω) dt ejθ(ω−v) dθ dv. 2π X −∞ −T −T 673
CHAPTER 10. RANDOM PROCESSES We now need to simplify the two inner integrals. Recall by Fourier pair that (cid:18) (cid:19) (cid:18) (cid:19) t ωT rect F Tsinc . T ←→ 2 This implies that (cid:90) T (cid:90) T ejt(v−ω) dt= e−j(ω−v)t dt −T −T (cid:90) ∞ t sin((ω−v)T) = rect( )e−j(ω−v)t dt=2T sinc((ω−v)T)=2T . 2T (ω−v)T −∞ Hence, we have (cid:104) (cid:105) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 E |X(cid:101)T(ω)|2 = 2π Q X(v) 2T (ω−v)T dv. (10.54) −∞ and so 1 2T (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 2TE[|X(cid:101)T(ω)|2 = 2π Q X(v) (ω−v)T dv. (10.55) −∞ As T →∞ (see Lemma 10.5 below), we have (cid:18) sin((ω−v)T)(cid:19)2 2T −→ 2πδ(ω−v). (ω−v)T Therefore, 1 (cid:104) (cid:105) 1 (cid:90) ∞ (cid:34) (cid:18) sin((ω−v)T)(cid:19)2(cid:35) Tl →im ∞2TE |X(cid:101)T(ω)|2 = 2π −∞Q X(v) Tl →im ∞2T (ω−v)T dv (cid:90) ∞ = Q (v)δ(ω−v)dv =Q (ω). X X −∞ Since Q (ω)=F[R (τ)], we conclude that X X 1 S X(ω)= Tl →im ∞2TE[|X(cid:101)T(ω)|2]=Q X(ω)=F[R X(τ)]. Lemma 10.5. 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 lim Q (v)2T dv =Q (ω). (10.56) T→∞2π −∞ X (ω−v)T X To prove this lemma, we first define δ (ω)=2T(sin(ωT))2. It is sufficient to show that T ωT (cid:12) (cid:12) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 (cid:12) (cid:12) (cid:12) (cid:12)Tl →im ∞2π −∞Q X(v)2T (ω−v)T dv−Q X(ω)(cid:12) (cid:12)→0 as T →∞. (10.57) We will proceed by demonstrating the following three facts about δ (ω): T 674
10.8. APPENDIX 1. 1 (cid:90) ∞ δ (ω)dω =1 2π T −∞ . 2. For any (cid:52)>0, (cid:90) δ (ω)dω →0 as T →∞ T {ω:|ω|>(cid:52)} . 3. For any |ω|≥(cid:52)>0, we have |δ (ω)|≤ 2 . T T(cid:52)2 Proof of Fact 1. 1 (cid:90) ∞ 1 (cid:90) ∞ (cid:18) sin(ωT)(cid:19)2 δ (ω)dω = 2T dω. 2π T 2π ωT −∞ −∞ (cid:124) (cid:123)(cid:122) (cid:125) sinc2(ωT) Note that (cid:18) (cid:19) t Λ ←→2Tsinc2(ωT). 4T Therefore, 1 (cid:90) ∞ 1 (cid:90) ∞ 2Tsinc2(ωT)dω = 2Tsinc2(ωT)ejω0 dω 2π 2π −∞ −∞ (cid:18) (cid:19) 0 =Λ =1. 4T Proof of Fact 2. δ (ω) is symmetric, so, it is sufficient to check only one side: T (cid:90) ∞ (cid:90) ∞ (cid:18) sin(ωt)(cid:19)2 δ (ω)dω = 2T dω T ωT (cid:52) (cid:52) 2T (cid:90) ∞ sin2(ωt) = dω T2 ω2 (cid:52) 2 (cid:90) ∞ 1 ≤ dω |sin(.)|2 ≤1 T ω2 (cid:52) 2 (cid:20) 1(cid:21)∞ 2 = − = →0 as T →∞. T ω T(cid:52) (cid:52) Proof of Fact 3. (cid:18) sin(ωT)(cid:19)2 (cid:18) 1 (cid:19) 2 2 |δ (ω)|=2T ≤2T = ≤ . T ωT (ωT)2 ω2T T(cid:52)2 Proof of Lemma. Consider Q (ω). By Property 1, X 1 (cid:90) ∞ 1 (cid:90) ∞ Q (ω)=Q (ω). δ (ω−v)dv = Q (ω)δ (ω−v)dv. X X 2π T 2π X T −∞ −∞ 675
CHAPTER 10. RANDOM PROCESSES Therefore, (cid:12) (cid:12) 1 (cid:90) ∞ (cid:12) (cid:12) (cid:12) (cid:12)2π Q X(v)δ T(ω−v)dv−Q X(ω)(cid:12) (cid:12) −∞ (cid:12) (cid:12) 1 (cid:90) ∞ 1 (cid:90) ∞ (cid:12) (cid:12) =(cid:12) (cid:12)2π Q X(v)δ T(ω−v)dv− 2π Q X(ω)δ T(ω−v)dv(cid:12) (cid:12) −∞ −∞ 1 (cid:12) (cid:12)(cid:90) ∞ (cid:12) (cid:12) 1 (cid:90) ∞ (cid:12) (cid:12) = 2π(cid:12) (cid:12) (Q X(v)−Q X(ω))δ T(ω−v)dv(cid:12) (cid:12)≤ 2π (cid:12)Q X(v)−Q X(ω)(cid:12)δ T(ω−v)dv. −∞ −∞ For any (cid:15)>0, let (cid:52) be a constant such that |ω−v|<(cid:52) whenever |Q (v)−Q (ω)|<(cid:15). X X Then we can partition the above integral into 1 (cid:90) ∞ (cid:12) (cid:12) 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv = 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv (1) −∞ ω−(cid:52) 1 (cid:90) ∞ (cid:12) (cid:12) + 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv (2) ω+(cid:52) 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) + 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv. (3) −∞ Partition (1) above can be evaluated as follows: 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω−(cid:52) 1 (cid:90) ω+(cid:52) ≤ (cid:15)δ (ω−v)dv 2π T ω−(cid:52) (cid:15) (cid:90) ω+(cid:52) = δ (ω−v)dv 2π T ω−(cid:52) (cid:15) (cid:90) ∞ ≤ δ (ω−v)dv =(cid:15), 2π T −∞ where the last inequality holds because δ (ω−v)≥0. Since (cid:15) can be arbitrarily small, the T only possibility for 1 (cid:90) ω+(cid:52) (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω−(cid:52) for all (cid:15) is that the integral is 0. Partition (2) above can be evaluated as follows: 1 (cid:90) ∞ (cid:12) (cid:12) 2π (cid:12)Q X(ω)−Q X(v)(cid:12)δ T(ω−v)dv ω+(cid:52) 1 (cid:90) ∞ (cid:0)(cid:12) (cid:12) (cid:12) (cid:12)(cid:1) ≤ 2π (cid:12)Q X(ω)(cid:12)+(cid:12)Q X(v)(cid:12) δ T(ω−v)dv ω+(cid:52) 1 (cid:90) ∞ 1 (cid:90) ∞ =Q (ω) δ (ω−v)dv+ Q (v)δ (ω−v)dv. X 2π T 2π X T ω+(cid:52) ω+(cid:52) 676
10.8. APPENDIX By Property 2, 1 (cid:82)∞ δ (ω−v)dv →0 as T →∞. By Property 3, 2π ω+(cid:52) T 1 (cid:90) ∞ 1 2 (cid:90) ∞ Q (v)δ (ω−v)dv ≤ Q (v)dv →0. 2π X T 2πT(cid:52)2 X ω+(cid:52) ω+(cid:52) (cid:124) (cid:123)(cid:122) (cid:125) <∞becauseQX(v)=F[RX(τ)] Therefore, we conclude that 1 (cid:90) ∞ Q (v)δ (ω−v)dv →0 as T →∞. 2π X T ω+(cid:52) and hence (1), (2) and (3) all →0 as T →∞. So we have (cid:12) (cid:12) 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 (cid:12) (cid:12) (cid:12) (cid:12)Tl →im ∞2π −∞Q X(v)2T (ω−v)T dv−Q X(ω)(cid:12) (cid:12)→0 as T →∞, which completes the proof. 10.8.1 The Mean-Square Ergodic Theorem The mean-square ergodic theorem states that for any WSS random process, the statistical average is the same as the temporal average. This provides an important tool in practice because finding the statistical average is typically very difficult. With the mean ergodic theorem, one can easily estimate the statistical average using the temporal average. Theorem 10.11 (Mean-Square Ergodic Theorem). Let Y(t) be a WSS process, with mean E[Y(t)]=m and autocorrelation function R (τ). Assume that the Fourier Y transform of R (τ) exists. Define Y 1 (cid:90) T def M = Y(t)dt. (10.58) T 2T −T Then E(cid:104)(cid:12) (cid:12)M T −m(cid:12) (cid:12)2(cid:105) →0 as T →∞. Proof of Mean Ergodic Theorem. Let X(t)=Y(t)−m. It follows that 1 (cid:90) T 1 (cid:90) T M −m= Y(t)dt−m= X(t)dt. T 2T 2T −T −T We define the finite-window approximation of X(t): (cid:26) X(t), −T ≤t≤T, X (t)= T 0, elsewhere. Then the difference M −m can be computed as T M T −m= 21 T (cid:90) T X(t)dt= 21 T (cid:90) ∞ X(t)e−j0t dt= 21 TX(cid:101)T(ω)(cid:12) (cid:12) ω=0 = X(cid:101) 2T T(0) . −T −∞ 677
CHAPTER 10. RANDOM PROCESSES Taking the expectation of the squares yields E(cid:2) |M −m|2(cid:3) = E(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) . T 4T2 Recall from the Einstein-Wiener-Khinchin theorem, 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(ω)(cid:12) (cid:12)2(cid:105) = 21 π (cid:90) ∞ S X(v)2T (cid:18) sin (( ω(ω −− v)v T)T)(cid:19)2 dv. −∞ Putting the limit T →∞, if we have that 1 (cid:90) ∞ (cid:18) sin((ω−v)T)(cid:19)2 lim S (v)2T dv =S (ω), T→∞2π −∞ X (ω−v)T X then we will have 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(ω)(cid:12) (cid:12)2(cid:105) →S X(ω) and 21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) →S X(0). Hence, Tl →im ∞E(cid:104)(cid:12) (cid:12)M T −m(cid:12) (cid:12)2(cid:105) = Tl →im ∞21 TE(cid:104)(cid:12) (cid:12)X(cid:101)T(0)(cid:12) (cid:12)2(cid:105) = Tl →im ∞21 TS X(0)=0. This completes the proof. 10.9 References Basic texts The following textbooks are basic texts about random processes. They offer many comple- mentary materials to our book. For example, we omitted the topics of straightly stationary processes and memoryless properties. We have also omitted a few classical examples, such as the random telegraph signal, the incremental independence of Poisson processes, and Markov chains. These materials can be found in the texts below. 10-1 John A. Gubner, Probability and Random Processes for Electrical and Computer En- gineers, Cambridge University Press, Illustrated edition, 2006. 10-2 AlbertoLeon-Garcia,Probability, Statistics, and Random Processes For Electrical En- gineering, Pearson, 3rd Edition, 2007. 10-3 AthanasiosPapoulis,S.UnnikrishnaPillai,Probability,RandomVariablesandStochas- tic Processes, McGraw-Hill, 4th Edition, 2012. 10-4 Henry Stark and John Woods, Probability and Random Processes With Applications to Signal Processing, Prentice Hall, 3rd Edition, 2001. 678
10.10. PROBLEMS 10-5 EugeneWongandBruceHajek,StochasticProcessesinEngineeringSystems,Springer- Verlag, 1985. 10-6 Bruce Hajek, Random Processes for Engineers, Cambridge University Press, 2015. 10-7 Dimitri P. Bertsekas and John N. Tsitsiklis, Introduction to Probability, Athena Sci- entific, 2nd Edition, 2008. 10-8 Robert G. Gallager, Stochastic Processes: Theory for Applications, Cambridge Uni- versity Press, 1st Edition, 2014. Signal and systems / Fourier transforms The following references are classic references on signal and systems. 10-9 Alan Oppenheim and Ronald Schafer, Discrete-Time Signal Processing, 2nd Edition, Prentice Hall 1999. 10-10 Alan Oppenheim and Alan Willsky, Signals and Systems, Pearson, 2nd Edition, 1996. 10-11 Martin Vetterli, Jelena Kovacevic, and Vivek K. Goyal, Foundations of Signal Pro- cessing, Cambridge University Press, 3rd Edition, 2014. 10-12 ToddK.MoonandWynnC.Stirling,MathematicalMethodsandAlgorithmsforSignal Processing, Prentice-Hall, 2000. Engineering applications 10-13 John G. Proakis and Masoud Salehi, Communication Systems Engineering, Pearson, 2nd Edition, 2001. 10-14 Rodger E. Ziemer, William H. Tranter, Principles of Communications, Wiley, 7th Edition, 2014. 10-15 Joseph W. Goodman, Statistical Optics, Wiley, 2015. 10.10 Problems Exercise 1. (Video Solution) Consider the random process X(t)=2Acos(t)+(B−1)sin(t), where A and B are two independent random variables with E[A]=E[B]=0, and E[A2]= E[B2]=1. (a) Find µ (t). X 679
CHAPTER 10. RANDOM PROCESSES (b) Find R (t ,t ). X 1 2 (c) Find C (t ,t ). X 1 2 Exercise 2. (Video Solution) Let X[n] be a discrete-time random process with mean function m [n] = E{X[n]} and X correlation function R [n,m]=E{X[n]X[m]}. Suppose that X ∞ (cid:88) Y[n]= h[n−i]X[i]. (10.59) i=−∞ (a) Find µ [n]. Y (b) Find R [n,m]. XY Exercise 3. (Video Solution) Let Y(t)=X(t)−X(t−d). (a) Find R (τ) and S (ω). X,Y X,Y (b) Find R (τ). Y (c) Find S (ω). Y Exercise 4. (Video Solution) Let X(t) be a zero-mean WSS process with autocorrelation function R (τ). Let Y(t) = X X(t)cos(ωt+Θ), where Θ∼uniform(−π,π) and Θ is independent of the process X(t). (a) Find the autocorrelation function R (τ). Y (b) Find the cross-correlation function of X(t) and Y(t). (c) Is Y(t) WSS? Why or why not? Exercise 5. (Video Solution) A WSS process X(t) with autocorrelation function R (τ)=1/(1+τ2) X is passed through an LTI system with impulse response h(t)=3sin(πt)/(πt). Let Y(t) be the system output. Find S (ω) and sketch S (ω). Y Y Exercise 6. (Video Solution) A white noise X(t) with power spectral density S (ω)=N /2 is applied to a lowpass filter X 0 h(t) with impulse response 1 h(t)= e−t/RC, t>0. (10.60) RC Find the followings. 680
10.10. PROBLEMS (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y Exercise 7. (Video Solution) Consider a WSS process X(t) with autocorrelation function R (τ)=sinc(πτ). X The process is sent to an LTI system with input-output relationship d2 d d2 d 2 Y(t)+2 Y(t)+4Y(t)=3 X(t)−3 X(t)+6X(t). dt2 dt dt2 dt Find the autocorrelation function R (τ). Y Exercise 8. (Video Solution) Given the functions a(t), b(t) and c(t), let g(t,1)=a(t), g(t,2)=b(t), g(t,3)=c(t). Let X(t) = g(t,Z), where Z is a discrete random variable with PMF P[Z = 1] = p , 1 P[Z =2]=p and P[Z =3]=p . Find, in terms of the p , p , p , a(t), b(t) and c(t), 2 3 1 2 3 (a) µ (t). X (b) R (t ,t ). X 1 2 Exercise 9. In the previous problem, let a(t)=e−λ|t|, b(t)=sin(πt) and c(t)=−1. (a) Choose p , p , p so that X(t) is WSS. 1 2 3 (b) Choose p , p , p so that X(t) is not WSS. 1 2 3 Exercise 10. (Video Solution) FindtheautocorrelationfunctionR (τ)correspondingtoeachofthefollowingpowerspec- X tral densities: (a) δ(ω−ω )+δ(ω+ω ). 0 0 (b) e−ω2/2. (c) e−|ω|. 681
CHAPTER 10. RANDOM PROCESSES Exercise 11. (Video Solution) A WSS process X(t) with autocorrelation function R X(τ) = e−τ2/(2σ T2) is passed through an LTI system with transfer function H(ω) = e−ω2/(2σ H2). Denote the system output by Y(t). Find the followings. (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y Exercise 12. (Video Solution) A white noise X(t) with power spectral density S (ω)=N /2 is applied to a lowpass filter X 0 h(t) with (cid:40) 1−ω2, if |ω|≤π, H(ω)= 0, otherwise. Find E[|Y(t)|2], where Y(t) is the output of the filter. Exercise 13. (Video Solution) Let X(t) be a WSS process with correlation function (cid:40) 1−|τ|, if −1≤τ ≤1, R (τ)= (10.61) X 0, otherwise. It is known that when X(t) is input to a system with transfer function H(ω), the system output Y(t) has a correlation function sinπτ R (τ)= . (10.62) Y πτ Find the transfer function H(ω). Exercise 14. Consider the system (cid:90) t Y(t)=e−t eτX(τ)dτ. −∞ Assume that X(t) is zero-mean white noise with power spectral density S (ω) = N /2. X 0 Find the followings: (a) S (ω). XY (b) R (τ). XY (c) S (ω). Y (d) R (τ). Y 682
Chapter A Appendix Useful Identities ∞ 1. (cid:80) rk =1+r+r2+···= 1 1−r k=0 n 2. (cid:80) k =1+2+3+···+n= n(n+1) 2 k=1 ∞ 3. ex = (cid:80) xk =1+ x + x2 +··· k! 1! 2! k=0 ∞ 4. (cid:80) krk−1 =1+2r+3r2+···= 1 (1−r)2 k=1 n 5. (cid:80) k2 =12+22+33+···+n2 = n3 + n2 + n 3 2 6 k=1 n 6. (a+b)n = (cid:80) (cid:0)n(cid:1) akbn−k k k=0 Common Distributions Distribution PMF/PDF E[X] Var[X] M (s) X Bernoulli p (1)=pandp (0)=1−p p p(1−p) 1−p+pes X X Binomial p (k)=(cid:0)n(cid:1) pk(1−p)n−k np np(1−p) (1−p+pes)n X k 1 1−p pes Geometric p (k)=p(1−p)k−1 X p p2 1−(1−p)es λke−λ Poisson p (k)= λ λ eλ(es−1) X k! 1 (cid:26) (x−µ)2(cid:27) (cid:26) σ2s2(cid:27) Gaussian f (x)= √ exp − µ σ2 exp µs+ X 2πσ2 2σ2 2 1 1 λ Exponential f (x)=λexp{−λx} X λ λ2 λ−s a+b (b−a)2 esb−esa Uniform f (x)= 1 X b−a 2 12 s(b−a) 683
CHAPTER A. APPENDIX Sum of Two Random Variables X X Sum X +X 1 2 1 2 Bernoulli(p) Bernoulli(p) Binomial(2,p) Binomial(n,p) Binomial(m,p) Binomial(m+n,p) Poisson(λ ) Poisson(λ ) Poisson(λ +λ ) 1 2 1 2 Exponential(λ) Exponential(λ) Erlang(2,λ) Gaussian(µ ,σ2) Gaussian(µ ,σ2) Gaussian(µ +µ , σ2+σ2) 1 1 2 2 1 2 1 2 Fourier Transform Table (cid:90) ∞ F(ω)= f(t)e−jωt dt. −∞ f(t)←→F(ω) f(t)←→F(ω) 1 (cid:18)Wt(cid:19) 2π (cid:16) ω (cid:17) 1. e−atu(t)←→ ,a>0 10. sinc2 ←→ ∆ a+jω 2 W 2W 1 ω 2. eatu(−t)←→ ,a>0 11. e−atsin(ω t)u(t)←→ 0 ,a>0 a−jω 0 (a+jω)2+ω2 0 2a a+jω 3. e−a|t|←→ ,a>0 12. e−atcos(ω t)u(t)←→ ,a>0 a2+ω2 0 (a+jω)2+ω2 0 a2 (cid:110) (cid:111) √ (cid:110) (cid:111) 4. ←→πae−a|ω|,a>0 13. exp − t2 ←→ 2πσexp −σ2ω2 a2+t2 2σ2 2 1 5. te−atu(t)←→ ,a>0 14. δ(t)←→1 (a+jω)2 n! 6. tne−atu(t)←→ ,a>0 15. 1←→2πδ(ω) (a+jω)n+1 (cid:18)t(cid:19) (cid:16)ωτ(cid:17) 7. rect τ ←→τsinc 2 16. δ(t−t 0)←→e−jwt0 π (cid:16) ω (cid:17) 8. sinc(Wt)←→ Wrect 2W 17. ejω0t←→2πδ(ω−ω 0) (cid:18)t(cid:19) τ (cid:16)ωτ(cid:17) 9. ∆ τ ←→ 2sinc2 4 18. f(t)ejω0t←→F(ω−ω 0) Some definitions: sin(t) sinc(t)= t (cid:40) 1, −0.5≤t≤0.5, rect(t)= 0, otherwise. (cid:40) 1−2|t|, −0.5≤t≤0.5, ∆(t)= 0, otherwise. 684
Basic Trigonometric Identities ejθ =cosθ+jsinθ sin2θ =2sinθcosθ cos2θ =2cos2θ−1 1 cosAcosB = (cos(A+B)+cos(A−B)) 2 1 sinAsinB =− (cos(A+B)−cos(A−B)) 2 1 sinAcosB = (sin(A+B)+sin(A−B)) 2 1 cosAsinB = (sin(A+B)−sin(A−B)) 2 cos(A+B)=cosAcosB−sinAsinB cos(A−B)=cosAcosB+sinAsinB sin(A+B)=sinAcosB+cosAsinB sin(A−B)=sinAcosB−cosAsinB 685
Index absolutely integrable, 183 definition, 143 almost sure convergence, 362 MATLAB and Python, 144 autocorrelation function properties, 146 2D visualization, 624 binomial series, 6 interpretation, 625, 635 binomial theorem, 6 LTI system, 647 proof, 9 properties, 634 birthday paradox, 31, 321 temporal average, 638 bootstrapping, 561 definition, 620 bootstrapped distribution, 564 MATLAB and Python, 628 confidence interval, 561 autocovariance function definition, 561 definition, 620 distribution of samples, 562 relationtoautocorrelationfunction,630 interpretation, 566 autoregressive model, 406, 661 MATLAB and Python, 567 linear prediction, 661 procedure, 563 MATLAB and Python, 407, 662 standard error, 567 prediction, 663 when to use, 562 Toeplitz, 662 Yule-Walker equation, 661 Cauchy distribution, 331, 360 Cauchy-Schwarz inequality, 261, 335 Basel problem, 5 Central Limit Theorem, 323, 367, 372, 381 basis functions, 405 Berry-Esseen Theorem, 375 Bayes’ theorem, 89 examples, 376 conditional probability, 81 interpretation, 375 law of total probability, 90 limitations, 379 Bayesian, 43 proof, 374 Bernoulli random variable characteristic function, 329 definition, 137 alternative definition, 329 MATLAB and Python, 137 Fourier transform, 330 maximum variance, 140 Chebyshev’s inequality, 341 properties, 138 proof, 342 bias-variance Chernoff’s bound, 343 average predictor, 433 compare with Chebyshev, 344 MATLAB and Python, 434 Chernoff, Herman, 343 noise-free case, 430 combination, 35 noisy case, 433 concave function, 336 trade off, 429 conditional distribution binomial random variable conditional expectation, 275 alternative definition, 148 conditional PDF, 272 686
INDEX conditional PMF, 267 examples, 653 conditional probability, 81 through LTI systems, 652 Bayes’ theorem, 89 cross-covariance function, 631 definition, 81 cross-correlation function, 631 independence, 85 cumulative distribution function properties, 84 continuous, 186 ratio, 81 discrete, 121 confidence interval, 543 left- and right-continuous, 190 bootstrapping, 561 MATLAB and Python, 186 critical value, 553 properties, 188 definition, 548 distribution of estimator, 546 delta function, 178 estimator, 545 discrete cosine transform (DCT), 23 examples, 549 eigenvalues and eigenvectors, 295 how to construct, 549 Gaussian, 296 interpretation, 547 MATLAB and Python, 296 margin of error, 554 Erd˝os-R´enyi graph, 140 MATLAB and Python, 552 MATLAB and Python, 480 number of samples, 555 even functions, 15 properties, 553 event, 61 standard error, 553 event space, 61 Student’s t-distribution, 556 expectation, 104 conjugate prior, 513 continuous, 180 convergence in distribution, 368 properties, 130, 182 convergence in probability, 356 transformation, 182 convex function, 336 center of mass, 127 convex optimization discrete, 125 CVXPY, 451 existence, 130, 183 convolution, 220, 641 exponential random variables correlation, 641 definition, 205 filtering, 641 MATLAB and Python, 205 correlation, 635 origin, 207, 209 autocorrelation function, 620 properties, 206 autocovariance function, 620 exponential series, 12 cross-correlation function, 652 convolution, 641 field, 64 correlation coefficient σ-field, 65 MATLAB and Python, 265 Borel σ-field, 65 properties, 263 Fourier transform, 647 definition, 263 table, 330 cosine angle, 26 characteristic function, 330 covariance, 262 frequentist, 43 covariance matrix, 289 Fundamental Theorem of Calculus, 17 independent, 289 chain rule, 19 cross power spectral density, 654 proof, 18 cross-correlation function cross-covariance function, 631 Gaussian random variables definition, 631 CDF, 214 687
INDEX definition, 211 joint PDF, 247 MATLAB and Python, 212 joint PMF, 245 origin, 220 joint expectation, 257 properties, 212 cosine angle, 258 standard Gaussian, 213 geometric random variable kurtosis, 216 definition, 149 MATLAB and Python, 217 MATLAB and Python, 150 Laplace transform, 324 properties, 151 law of large numbers, 323, 351, 381 geometric sequence strong law of large numbers, 361 finite, 4 weak law of large numbers, 354 infinite, 4 learning curve, 427 geometric series, 3 MATLAB and Python, 427 finite, 4 Legendre polynomial, 403 infinite, 4 MATLAB and Python, 404 harmonic series, 5 likelihood, 466, 468, 503 histogram, 2, 113 log-likelihood, 469 Hoeffding’s inequality, 348 linear algebra Hoeffding lemma, 348 basis vector, 23 proof, 348 representation, 23 hypothesis testing span, 22 p-value test, 569, 573 standard basis vector, 22 T-test, 576 linear combination, 21 Z-test, 576 linear model, 21 alternative hypothesis, 568 linear prediction, 661 critical level, 571 linear programming, 414 critical-value test, 569 linear regression definition, 568 MATLAB and Python, 30 MATLAB and Python, 570 linear time-invariant (LTI) null hypothesis, 568 convolution, 641 definition, 646 impulse response, 646 system, 646 independence, 85 conditional probability, 88 marginal distribution, 250 versus disjoint, 86 Markov’s inequality, 339 independent proof, 339 random variables, 252 tight, 341 independentandidenticallydistributed(i.i.d.),matrix calculus, 28 253 maximum-a-posteriori (MAP), 503 indicator function, 182 choosing prior, 505 inner product, 24 conjugate prior, 513 MATLAB and Python, 24 MAP versus LASSO, 519 MAP versus ML, 504 Jensen’s inequality, 336 MAP versus regression, 517 proof, 338 MAP versus ridge, 519 joint distribution posterior, 503, 512 definition, 241 prior, 503 joint CDF, 255 solution, 506 688
INDEX maximum-likelihood MATLAB and Python, 291 1D Gaussian, 484 covariance, 293 consistent estimator, 494 transformation, 293 estimation, 468 whitening, 299 estimator, 491 high-dimensional Gaussian, 486 Neyman-Pearson test, 579 image reconstruction, 481 decision rule, 584 independent observations, 469 likelihood ratio, 586 invariance principle, 500 rejection zone, 580 MATLAB and Python, 472 likelihood ratio test, 580 number of training samples, 474 norm, 24, 26 Poisson, 485 (cid:96) 1, 27 regression versus ML, 488 (cid:96) ∞, 27 social networks, 478 MATLAB and Python, 26 unbiased estimator, 492 weighted, 27 visualization, 471 normalization property, 112 mean, 199 odd functions, 15 mean function open and closed intervals, 45 LTI system, 647 optimal linear filter, 656 definition, 620 deconvolution, 668 MATLAB and Python, 623 denoising, 665 mean squared error (MSE), 521, 522 orthogonality condition, 661 measure, 68 Wiener filter, 664 almost surely, 73 Yule-Walker equation, 659 finite sets, 68 input function, 657 intervals, 68 prediction, 657 Lebesgue integration, 71 target function, 657 measure zero sets, 71 orthogonality condition, 661 definition, 72 overdetermined system, 409 examples, 72 overfitting, 418 regions, 68 factors, 420 size, 69 LASSO, 454 median, 196 linear analysis, 425 minimum mean-square estimation (MMSE), source, 429 521 conditional expectation, 524 parameter estimation, 165, 465 Gaussian, 530 Pascal triangle, 8 minimum-norm least squares, 411 Pascal’s identity, 7 mode, 198 performance guarantee model selection, 165 average case, 321 moment, 133 worst case, 321 continuous case, 184 permutation, 33 moment-generating function, 322, 324 Poisson random variable common distributions, 326 applications, 154 derivative, 325 definition, 152 existence, 331 origin, 157 sum of random variables, 327 photon arrivals, 161 multidimensional Gaussian, 290 Poisson approximation of binomial, 159 689
INDEX properties, 155 random process MATLAB and Python, 152 discrete time, 656 positive semi-definite, 297 definition, 614 posterior, 466, 503 example power spectral density, 639 random amplitude, 614 Einstein-Wiener-KhinchinTheorem,639 random phase, 615 through LTI systems, 649 function, 614 cross power spectral density, 642, 654 independent, 631 eigendecomposition, 641 index, 614 Fourier transform, 642 sample space, 616 origin, 643 statistical average, 616 wide-sense stationary, 642 temporal average, 616 PR (precision-recall) curve uncorrelated, 632 definition, 603 random variable, 104, 105 MATLAB and Python, 605 function of, 223 precision, 603 transformation of, 223 recall, 603 random vector, 286 principal-component analysis, 303 expectation, 288 limitations, 311 independent, 286 main idea, 303 regression, 391, 394 MATLAB and Python, 306 loss, 394 prior, 466, 503 MATLAB and Python, 400 probability, 43, 45 outliers, 412 measure of a set, 43 prediction model, 394 probability axioms, 74 solution, 397 additivity, 75 linear model, 395 corollaries, 77 outliers, 417 countable additivity, 75 squared error, 396 measure, 76 regularization, 440 non-negativity, 75 LASSO, 449 normalization, 75 MATLAB and Python, 442 probability density function, 172 parameter, 445 definition, 175 ridge, 440 discrete cases, 178 sparse solution, 449 properties, 174 robust linear regression, 412 intuition, 172 MATLAB and Python, 416 per unit length, 173 linear programming, 414 probability inequality, 323, 333 ROC probability law, 66 comparing performance, 599 definition, 66 computation, 594 examples, 66 definition, 591 measure, 67 MATLAB and Python, 595 probability mass function, 104, 110 properties, 593 probability space Receiver operating characteristic, 591 (Ω,F,P), 58 sample average, 320, 351 Rademacher random variable, 140 sample space, 59 random number generator, 228 continuous outcomes, 59 690
INDEX counterexamples, 61 Taylor approximation, 10 discrete outcomes, 59 first-order, 11 examples, 59 second-order, 11 exclusive, 61 exponential, 12 exhaustive, 61 logarithmic, 13 functions, 59 testing error, 420 set, 45 analysis, 424 associative, 56 testing set, 420 commutative, 56 Three Prisoners problem, 92 complement, 52 Toeplitz, 407, 633 countable, 45 training error, 420 De Morgan’s Law, 57 analysis, 421 difference, 53 training set, 420 disjoint, 54 type 1 error distributive, 56 definition, 581 empty set, 48 false alarm, 582 finite, 45 false positive, 581 improper subset, 47 power of test, 583 infinite, 45 type 2 error intersection, 50 definition, 581 finite, 50 false negative, 581 infinite, 51 miss, 582 of functions, 46 partition, 55 underdetermined system, 409 proper subset, 47 uniform random variables, 202 subset, 47 MATLAB and Python, 203 uncountable, 45 union bound, 333 union, 48 validation, 165 finite, 48 variance, 134 infinite, 49 properties, 135 universal set, 48 continuous case, 184 simplex method, 414 skewness, 216 white noise, 641 MATLAB and Python, 217 wide-sense stationary, 632 statistic, 320 jointly, 652 Student’s t-distribution Wiener filter, 664 definition, 556 deconvolution, 668 degrees of freedom, 557 definition, 664 MATLAB and Python, 558 denoising, 665 relation to Gaussian, 557 MATLAB and Python, 664 sum of random variables, 280 power spectral density, 665 Bernoulli, 327 recursive filter, 664 binomial, 328 Gaussian, 283, 329 Yule-Walker equation, 659 Poisson, 328 MATLAB and Python, 662 common distributions, 282 convolution, 281 symmetric matrices, 296 691
--- End of content from file: 2021 Chan, Stanley ~ Introduction to Probability for Data Science [Michigan Publishing] _.pdf ---
